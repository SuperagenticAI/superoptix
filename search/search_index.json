{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":"SUPEROPTIX AI Full Stack Agentic AI Optimization Framework <p>Evaluation-First \u26a1 Optimization-Core \ud83d\udd78\ufe0f Multi-Agent Orchestration</p> <p>Powered by DSPy. Refined by Superagentic AI.</p> <p>Build once with SuperSpec and compile to your preferred framework.</p> \ud83d\ude80 Quick Start \u2b50 Golden Workflow \ud83d\udcda Guides \ud83d\udcca Feature Matrix <p>Now supported: \ud83e\uddea RLM (Experimental) \u00b7 \ud83d\uddc2\ufe0f StackOne Connectors \u00b7 \ud83e\uddec GEPA Optimization</p> <p> RLM Guide \u00b7     StackOne Guide \u00b7     Framework Support </p>"},{"location":"#what-is-superoptix","title":"What is SuperOptiX?","text":"<p>SuperOptiX is a universal agent optimization framework that lets you build, evaluate, and optimize agents across major frameworks with one workflow.</p> <p>It keeps generated pipelines framework-native and readable in minimal mode, and enables full optimization lifecycle only when you opt in with <code>--optimize</code>.</p>"},{"location":"#core-workflow","title":"Core Workflow","text":"<pre><code># Pull agent\nsuper agent pull developer\n\n# Compile minimal pipeline\nsuper agent compile developer --framework dspy\n\n# Run\nsuper agent run developer --framework dspy --goal \"Design a migration plan\"\n\n# Optional optimization path\nsuper agent compile developer --framework dspy --optimize\nsuper agent optimize developer --framework dspy --auto light\n</code></pre>"},{"location":"#why-teams-use-it","title":"Why Teams Use It","text":"\ud83d\udd2c Multi-Framework Freedom <ul> <li>Compile one SuperSpec into DSPy, OpenAI SDK, Claude SDK, Pydantic AI, CrewAI, Google ADK, DeepAgents</li> <li>Microsoft support remains available in legacy mode</li> <li>Switch frameworks without rewriting agent intent</li> </ul> \ud83e\uddec Optimization by Default Path <ul> <li>Minimal runtime pipeline first</li> <li>GEPA optimization loop when needed</li> <li>BDD-style evaluation and repeatable quality checks</li> </ul> \ud83d\uddc2\ufe0f Connector-Driven Agents <ul> <li>StackOne integration for SaaS tool access</li> <li>Cross-framework connector compilation</li> <li>Clear connector demos including Calendly flows</li> </ul> \ud83e\uddea RLM Support <ul> <li>Experimental RLM support in active integrations</li> <li>Fallback-friendly pipeline strategy</li> <li>Unified sandbox support coming soon</li> </ul>"},{"location":"#local-and-cloud-routing","title":"Local and Cloud Routing","text":"<pre><code># Local Ollama\nsuper agent run developer --framework dspy --local --provider ollama --model llama3.1:8b --goal \"...\"\n\n# Cloud Google\nsuper agent run developer --framework dspy --cloud --provider google-genai --model gemini-2.5-flash --goal \"...\"\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Golden Workflow</li> <li>Troubleshooting by Symptom</li> <li>Framework Feature Matrix</li> <li>CLI Complete Guide</li> </ul>"},{"location":"agent-patterns/","title":"Agent Patterns","text":"<p>This guide explains AI Agents in SuperOptiX and the 5-tier evolutionary system that scales from simple automation to enterprise-grade AI operations.</p>"},{"location":"agent-patterns/#what-are-ai-agents","title":"\ud83e\udd16 What are AI Agents?","text":"<p>There is no standard definition of AI Agents yet in the industry, but we can define them in SuperOptiX as:</p> <p>AI Agents are intelligent software systems that can:</p> <ul> <li>Perceive their environment through data and inputs</li> <li>Reason about tasks and goals using language models</li> <li>Act by executing tools, making decisions, or generating outputs</li> <li>Learn from interactions to improve performance over time</li> </ul> <p>In SuperOptiX, agents are built using DSPy (Declarative Self-Improving Python) - a framework that optimizes prompts and reasoning chains automatically.</p>"},{"location":"agent-patterns/#the-5-tier-evolutionary-system","title":"\ud83c\udfad The 5-Tier Evolutionary System","text":"<p>SuperOptiX introduces a progressive architecture inspired by Nick Bostrom's Superintelligence book and Sam Altman's 5 tiers to AGI. Each tier builds upon the previous one, adding complexity and capabilities.</p> <p>Note: The industry is still figuring out agent communication protocols and patterns, so these will evolve with time as standards emerge.</p> <pre><code>graph TD\n    A[\ud83e\uddd9\u200d\u2642\ufe0f Oracles&lt;br/&gt;Simple Q&amp;A] --&gt; B[\ud83e\uddde\u200d\u2642\ufe0f Genies&lt;br/&gt;Tools &amp; Memory]\n    B --&gt; C[\ud83c\udfad Protocols&lt;br/&gt;Emerging Protocols like MCP/A2A]\n    C --&gt; D[\ud83e\udd16 Superagents&lt;br/&gt;Multi-Agent Systems with different topologies]\n    D --&gt; E[\ud83d\udc51 Sovereigns&lt;br/&gt;Autonomous Operations Agent Self Discovery]\n\n    style A fill:#4CAF50\n    style B fill:#2196F3\n    style C fill:#FF9800\n    style D fill:#9C27B0\n    style E fill:#F44336</code></pre> <p>Status: Open Source Complexity: Low Best For: Simple automation, Q&amp;A systems</p>"},{"location":"agent-patterns/#what-oracles-can-do","title":"What Oracles Can Do","text":"<p>Oracles are single-purpose agents that provide fast question-answering capabilities. They interact directly with language models and respond to queries without external connections.</p> <p>Note: Oracles are designed for demos and prototypes. Their output is not configured for chaining in multi-agentic systems or orchestras, and they are not optimized to work with parallel configurations.</p> <p>Key Capabilities:</p> <ul> <li>\ud83e\udde9 Single-step reasoning - Direct question-to-answer mapping</li> <li>\ud83d\udcdd Template-based responses - Consistent output formats</li> <li>\u26a1 Built-in optimization - DSPy-powered prompt tuning</li> <li>\ud83d\udee1\ufe0f Simple validations - Basic output verification</li> <li>\ud83e\udd16 Any LLM support - Works with any language model</li> </ul>"},{"location":"agent-patterns/#example-use-cases","title":"Example Use Cases","text":"<pre><code># FAQ Bot\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: faq-bot\n  tier: oracle\nspec:\n  tasks:\n    - name: answer_faq\n      template: \"Answer this FAQ: {question}\"\n</code></pre> <pre><code># Data Formatter\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: data-formatter\n  tier: oracle\nspec:\n  tasks:\n    - name: format_data\n      template: \"Format this data as JSON: {input}\"\n</code></pre> <p>Perfect for: Simple Q&amp;A, data formatting, basic automation, prototyping</p>"},{"location":"agent-patterns/#genies-tier-intermediate","title":"\ud83e\uddde\u200d\u2642\ufe0f Genies Tier - Intermediate","text":"<p>Status: Open Source Complexity: Medium Best For: Customer service, content creation, complex problem-solving</p>"},{"location":"agent-patterns/#what-genies-can-do","title":"What Genies Can Do","text":"<p>Genies are multi-step reasoning agents that can interact with external systems through tools, memory, and RAG (Retrieval-Augmented Generation). They use reasoning and action (ReAct) patterns.</p> <p>Note: Genies are designed for demos and prototypes. Their output is not configured for chaining in multi-agentic systems or orchestras, and they are not optimized to work with parallel configurations.</p> <p>Key Capabilities:</p> <ul> <li>\ud83e\udde0 Multi-step reasoning - Chain-of-thought problem solving</li> <li>\ud83d\udee0\ufe0f Dynamic tool selection - Intelligent tool usage</li> <li>\ud83d\uddc2\ufe0f Memory integration - Learning from interactions</li> <li>\ud83d\udcda RAG support - Knowledge retrieval from vector databases</li> <li>\ud83d\udcde Function calling - Advanced LLM capabilities</li> </ul>"},{"location":"agent-patterns/#example-use-cases_1","title":"Example Use Cases","text":"<pre><code># Customer Service Agent\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: customer-service\n  tier: genie\nspec:\n  context:\n    memory: true\n    tools: true\n    retrieval: true\n  tasks:\n    - name: handle_inquiry\n      description: \"Handle customer inquiries with context\"\n    - name: lookup_order\n      description: \"Look up order information\"\n    - name: process_return\n      description: \"Process return requests\"\n</code></pre> <pre><code># Content Creator\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: content-creator\n  tier: genie\nspec:\n  context:\n    memory: true\n    tools: [\"research\", \"writing\", \"editing\"]\n    retrieval: true\n  tasks:\n    - name: research_topic\n      description: \"Research content topics\"\n    - name: write_content\n      description: \"Create engaging content\"\n    - name: edit_content\n      description: \"Polish and refine content\"\n</code></pre> <p>Perfect for: Customer service, content creation, research, tool-based workflows</p>"},{"location":"agent-patterns/#protocols-tier-advanced","title":"\ud83c\udfad Protocols Tier - Advanced","text":"<p>Status: \ud83d\udd12 Closed Source (Contact Us) Complexity: High Best For: Business processes, decision making, complex workflows</p>"},{"location":"agent-patterns/#what-protocols-can-do","title":"What Protocols Can Do","text":"<p>Protocols support emerging protocols like MCP (Model Context Protocol) and A2A (Agent-to-Agent) communication. They combine all Oracle and Genie capabilities with advanced orchestration.</p> <p>Key Capabilities:</p> <ul> <li>\ud83d\udd17 Advanced agent protocols - MCP, A2A integration</li> <li>\ud83c\udfd7\ufe0f Complex workflow management - Multi-step business processes</li> <li>\ud83e\uddf5 Parallel orchestration - Concurrent agent execution</li> <li>\ud83d\ude80 Production deployment - Enterprise-grade infrastructure</li> <li>\ud83e\uddec Advanced optimization - Custom DSPy pipelines</li> </ul>"},{"location":"agent-patterns/#example-use-cases_2","title":"Example Use Cases","text":"<pre><code># Sales Qualification Agent\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: sales-qualifier\n  tier: protocol\nspec:\n  context:\n    memory: true\n    tools: [\"crm\", \"email\", \"calendar\"]\n    protocols: [\"mcp\", \"a2a\"]\n  workflow:\n    - name: lead_analysis\n      type: \"parallel\"\n    - name: qualification_scoring\n      type: \"sequential\"\n    - name: follow_up_scheduling\n      type: \"orchestrated\"\n</code></pre> <pre><code># Risk Assessment Agent\n# In the Agent Playbook we can use this as:\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: risk-assessor\n  tier: protocol\nspec:\n  context:\n    memory: true\n    tools: [\"risk_models\", \"regulatory_db\"]\n    protocols: [\"mcp\", \"a2a\"]\n  workflow:\n    - name: data_collection\n      type: \"parallel\"\n    - name: risk_calculation\n      type: \"sequential\"\n    - name: compliance_check\n      type: \"orchestrated\"\n</code></pre> <p>Perfect for: Complex business workflows, decision-making systems, enterprise applications</p>"},{"location":"agent-patterns/#superagents-tier-expert","title":"\ud83e\udd16 Superagents Tier - Expert","text":"<p>Status: \ud83d\udd12 Closed Source (Work in Progress) Complexity: Expert Best For: Complex multi-agent systems, research teams, e-commerce platforms</p>"},{"location":"agent-patterns/#what-superagents-can-do","title":"What Superagents Can Do","text":"<p>Superagents are multi-agent systems where a lead agent manages and coordinates other agents. They can spawn ephemeral subagents and work with other superagents using AgentLines orchestration. This tier involves agent architectures and agent topologies for higher levels of orchestration.</p> <p>Key Capabilities:</p> <ul> <li>\ud83e\udd1d Multi-agent coordination - Lead agent management</li> <li>\ud83e\ude84 Dynamic subagent spawning - On-demand agent creation</li> <li>\ud83d\udd78\ufe0f AgentLines integration - Advanced orchestration</li> <li>\ud83d\udef0\ufe0f High-level protocols - Beyond MCP and A2A</li> <li>\ud83d\uddc4\ufe0f Resource management - Compute and memory optimization</li> </ul>"},{"location":"agent-patterns/#example-use-cases_3","title":"Example Use Cases","text":"<pre><code># E-commerce Platform\n# In the Agent Playbook we can use this as:\napiVersion: superagent/v1\nkind: Superagent\nmetadata:\n  name: ecommerce-platform\n  tier: superagent\nspec:\n  subagents:\n    - name: inventory-manager\n      role: stock_management\n    - name: pricing-optimizer\n      role: dynamic_pricing\n    - name: customer-service\n      role: support_coordination\n    - name: recommendation-engine\n      role: product_suggestions\n  coordination: \"agentlines\"\n</code></pre> <pre><code># Research Team\n# In the Agent Playbook we can use this as:\napiVersion: superagent/v1\nkind: Superagent\nmetadata:\n  name: research-team\n  tier: superagent\nspec:\n  subagents:\n    - name: data-collector\n      role: information_gathering\n    - name: analyst\n      role: data_analysis\n    - name: synthesizer\n      role: insight_generation\n    - name: writer\n      role: report_creation\n  coordination: \"agentlines\"\n</code></pre> <p>Perfect for: Complex multi-agent systems, research and development, e-commerce platforms</p>"},{"location":"agent-patterns/#sovereigns-tier-enterprise","title":"\ud83d\udc51 Sovereigns Tier - Enterprise","text":"<p>Status: \ud83d\udd12 Closed Source (Coming Soon) Complexity: Enterprise Best For: Large-scale AI operations, AI-powered companies, research labs</p>"},{"location":"agent-patterns/#what-sovereigns-can-do","title":"What Sovereigns Can Do","text":"<p>Sovereigns are autonomous AI systems that can discover agents based on tasks, make decisions, and handle complex enterprise workflows. They represent the highest level of AI autonomy with advanced agent architectures and agent topologies for sovereign-level orchestration.</p> <p>Key Capabilities:</p> <ul> <li>\ud83e\uddbe Autonomous decision-making - Independent operation</li> <li>\ud83d\udd0d Agent discovery - Automatic agent selection</li> <li>\ud83c\udf10 Cross-domain synthesis - Multi-domain knowledge</li> <li>\ud83d\udd70\ufe0f Real-time governance - Dynamic management</li> <li>\ud83d\udee1\ufe0f Enterprise-grade security - Production security</li> </ul>"},{"location":"agent-patterns/#example-use-cases_4","title":"Example Use Cases","text":"<pre><code># AI-Powered Company\n# In the Agent Playbook we can use this as:\napiVersion: sovereign/v1\nkind: Sovereign\nmetadata:\n  name: ai-company\n  tier: sovereign\nspec:\n  capabilities:\n    - agent_discovery\n    - cross_domain_synthesis\n    - real_time_governance\n    - autonomous_decision_making\n  governance: \"enterprise_grade\"\n  security: \"production_ready\"\n</code></pre> <pre><code># Research Lab\n# In the Agent Playbook we can use this as:\napiVersion: sovereign/v1\nkind: Sovereign\nmetadata:\n  name: research-lab\n  tier: sovereign\nspec:\n  capabilities:\n    - research_coordination\n    - experiment_management\n    - publication_assistance\n    - collaboration_facilitation\n  governance: \"academic_grade\"\n</code></pre> <p>Perfect for: Large-scale AI operations, AI-powered companies, research laboratories</p>"},{"location":"agent-patterns/#complexity-progression","title":"\ud83d\udcca Complexity Progression","text":"Tier Reasoning Tools Memory Orchestration Deployment \ud83e\uddd9\u200d\u2642\ufe0f Oracles Single-step Basic Simple Sequential Demo \ud83e\uddde\u200d\u2642\ufe0f Genies Multi-step Advanced Multi-layer Sequential Demo \ud83c\udfad Protocols Complex Protocol-based Layered Parallel Production \ud83e\udd16 Superagents Orchestrated Multi-agent Advanced AgentLines Enterprise \ud83d\udc51 Sovereigns Autonomous Discovery Sovereign Autonomous Sovereign"},{"location":"agent-patterns/#choosing-your-tier","title":"\ud83d\ude80 Choosing Your Tier","text":""},{"location":"agent-patterns/#start-simple-oracles","title":"Start Simple - Oracles","text":"<ul> <li>When: You need basic automation or Q&amp;A</li> <li>Example: FAQ bot, data formatter, simple chatbot</li> <li>Complexity: Low - perfect for beginners</li> </ul>"},{"location":"agent-patterns/#add-power-genies","title":"Add Power - Genies","text":"<ul> <li>When: You need tools, memory, or complex reasoning</li> <li>Example: Customer service agent, content creator, research assistant</li> <li>Complexity: Medium - great for most use cases</li> </ul>"},{"location":"agent-patterns/#scale-up-protocols","title":"Scale Up - Protocols","text":"<ul> <li>When: You need complex workflows or production deployment</li> <li>Example: Sales qualification, risk assessment, business processes</li> <li>Complexity: High - for advanced applications</li> </ul>"},{"location":"agent-patterns/#go-multi-agent-superagents","title":"Go Multi-Agent - Superagents","text":"<ul> <li>When: You need multiple agents working together</li> <li>Example: E-commerce platform, research team, complex systems</li> <li>Complexity: Expert - for sophisticated multi-agent systems</li> </ul>"},{"location":"agent-patterns/#enterprise-scale-sovereigns","title":"Enterprise Scale - Sovereigns","text":"<ul> <li>When: You need autonomous AI operations</li> <li>Example: AI-powered company, research lab, government systems</li> <li>Complexity: Enterprise - for large-scale operations</li> </ul>"},{"location":"agent-patterns/#common-patterns-by-tier","title":"\ud83c\udfaf Common Patterns by Tier","text":""},{"location":"agent-patterns/#oracles-patterns","title":"Oracles Patterns","text":"<ul> <li>Q&amp;A Templates - Standard question-answer formats</li> <li>Data Transformation - Input/output formatting</li> <li>Simple Validation - Basic output verification</li> </ul>"},{"location":"agent-patterns/#genies-patterns","title":"Genies Patterns","text":"<ul> <li>ReAct Loops - Reasoning and action cycles</li> <li>Tool Integration - External system connections</li> <li>Memory Management - Context preservation</li> <li>RAG Workflows - Knowledge retrieval patterns</li> </ul>"},{"location":"agent-patterns/#protocols-patterns","title":"Protocols Patterns","text":"<ul> <li>Multi-Step Workflows - Complex business processes</li> <li>Parallel Execution - Concurrent task processing</li> <li>Protocol Integration - MCP and A2A patterns</li> <li>Production Deployment - Enterprise infrastructure</li> </ul>"},{"location":"agent-patterns/#superagents-patterns","title":"Superagents Patterns","text":"<ul> <li>Agent Coordination - Lead agent management</li> <li>Subagent Spawning - Dynamic agent creation</li> <li>Resource Management - Compute optimization</li> <li>AgentLines Orchestration - Advanced workflow patterns</li> </ul>"},{"location":"agent-patterns/#sovereigns-patterns","title":"Sovereigns Patterns","text":"<ul> <li>Agent Discovery - Automatic agent selection</li> <li>Autonomous Decision Making - Independent operation</li> <li>Cross-Domain Synthesis - Multi-domain knowledge</li> <li>Real-Time Governance - Dynamic oversight</li> </ul>"},{"location":"agent-patterns/#getting-started","title":"\ud83d\udca1 Getting Started","text":"<ol> <li>Choose your tier based on your use case and complexity needs</li> <li>Start with Oracles if you're new to AI agents</li> <li>Upgrade gradually as your needs grow</li> <li>Follow the patterns for your chosen tier</li> <li>Leverage SuperSpec for declarative agent specifications</li> </ol> <p>Note: The Playbook uses Agent Specification with SuperSpec format which is human-readable and optimized for Context Engineering and Agent Engineering.</p> <p>Ready to build your first agent? Check out the Quick Start Guide to get started with SuperOptiX! \ud83d\ude80 </p>"},{"location":"changelog/","title":"Release Notes &amp; Changelog","text":"<pre><code>{!../CHANGELOG.md!}\n</code></pre>"},{"location":"contributing/","title":"\ud83e\udd1d Contributing to SuperOptiX","text":"<p>Welcome to SuperOptiX! We're excited that you're interested in our professional AI agent development framework. \ud83d\ude80</p> <p>Note: SuperOptiX is an open source project. We welcome code contributions, feedback, bug reports, and feature requests. Please read our contribution guidelines below.</p>"},{"location":"contributing/#our-mission","title":"\ud83c\udf1f Our Mission","text":"<p>SuperOptiX provides enterprise-grade AI agent development through our powerful framework. We believe in:</p> <ul> <li>\ud83c\udfaf Evaluation-first design - Professional tools with built-in validation</li> <li>\ud83e\udde0 Enterprise-grade - Production-ready capabilities out of the box</li> <li>\ud83c\udfad Progressive complexity - Simple to start, enterprise-scale when needed</li> <li>\ud83e\udd1d Developer-focused - Built by developers, for developers</li> </ul>"},{"location":"contributing/#getting-started-with-superoptix","title":"\ud83d\ude80 Getting Started with SuperOptiX","text":"<pre><code># Install the SuperOptiX Framework\npip install superoptix\n\n# Initialize your first project\nsuper init my-project\ncd my-project\n\n# Try the CLI to see available commands\nsuper --help\n</code></pre>"},{"location":"contributing/#framework-structure","title":"\ud83d\udcc1 Framework Structure","text":"<p>Understanding our framework layout:</p> <pre><code>superoptix/\n\u251c\u2500\u2500 \ud83c\udfaf superoptix/           # Core framework\n\u2502   \u251c\u2500\u2500 cli/                 # Command-line interface\n\u2502   \u251c\u2500\u2500 agents/              # Pre-built agent playbooks\n\u2502   \u251c\u2500\u2500 compiler/            # Agent compilation system\n\u2502   \u251c\u2500\u2500 memory/              # Multi-layered memory systems\n\u2502   \u251c\u2500\u2500 observability/       # Tracing and monitoring\n\u2502   \u251c\u2500\u2500 runners/             # Agent execution engines\n\u2502   \u2514\u2500\u2500 tools/               # Built-in agent tools\n\u251c\u2500\u2500 \ud83d\udcda docs/                 # Documentation\n\u2514\u2500\u2500 \ud83e\uddea tests/                # Test suite\n</code></pre>"},{"location":"contributing/#how-to-provide-feedback","title":"\ud83d\udd0d How to Provide Feedback","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ol> <li>Check existing issues</li> <li>Search for similar reports before creating new ones</li> <li>Create a new issue with detailed information about the problem</li> </ol>"},{"location":"contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ol> <li>Check existing feature requests</li> <li>Create a new issue describing your use case and expected benefits</li> <li>Consider if it aligns with our platform vision</li> </ol>"},{"location":"contributing/#framework-improvements","title":"\ud83d\udca1 Framework Improvements","text":"<ol> <li>Suggest improvements to our framework features</li> <li>Request new tools or integrations</li> <li>Share feedback on documentation or examples</li> </ol>"},{"location":"contributing/#using-the-superoptix-framework","title":"\ud83e\uddea Using the SuperOptiX Framework","text":""},{"location":"contributing/#framework-best-practices","title":"Framework Best Practices","text":"<p>Follow these guidelines for optimal framework usage:</p> <pre><code># Use the framework for agent development\nfrom superoptix import SuperOptiX\n\n# Create and run agents\nagent = SuperOptiX.create_agent(\"developer\")\nresult = agent.run(\"Build a web application\")\n</code></pre>"},{"location":"contributing/#performance","title":"Performance","text":"<ul> <li>Optimize agent configurations for your use case</li> <li>Use appropriate model backends</li> <li>Monitor resource usage</li> </ul>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<ul> <li>Always handle framework exceptions</li> <li>Check agent execution status</li> <li>Log errors for debugging</li> </ul>"},{"location":"contributing/#types-of-feedback","title":"\ud83c\udfd7\ufe0f Types of Feedback","text":""},{"location":"contributing/#agent-templates","title":"\ud83c\udfad Agent Templates","text":"<p>Suggest new pre-built agents for specific industries:</p> <pre><code># Example: Healthcare agent request\nmetadata:\n  name: \"medical_assistant\"\n  category: \"healthcare\"\n  tier: \"genies\"\n\nspec:\n  persona:\n    role: \"Medical Assistant\"\n    expertise: \"Patient care, medical terminology, appointment scheduling\"\n</code></pre>"},{"location":"contributing/#platform-features","title":"\ud83e\udde0 Platform Features","text":"<p>Request enhancements to our platform:</p> <ul> <li>Memory Systems: Better learning algorithms</li> <li>Orchestration: Multi-agent workflow improvements</li> <li>Observability: Enhanced monitoring capabilities</li> </ul>"},{"location":"contributing/#framework-improvements_1","title":"\ud83d\udd27 Framework Improvements","text":"<p>Suggest framework enhancements:</p> <ul> <li>New Features: Additional functionality</li> <li>Tool Integrations: Better third-party integrations</li> <li>Performance: Optimization improvements</li> </ul>"},{"location":"contributing/#documentation","title":"\ud83d\udcca Documentation","text":"<p>Help improve our documentation:</p> <ul> <li>\ud83d\udcdd Examples: Real-world use cases</li> <li>\ud83d\udcda Guides: Better tutorials and guides</li> <li>\ud83d\udca1 Best Practices: Usage recommendations</li> </ul>"},{"location":"contributing/#feedback-guidelines","title":"\ud83d\udcbb Feedback Guidelines","text":""},{"location":"contributing/#bug-reports_1","title":"Bug Reports","text":"<ul> <li>Reproduction Steps: Clear steps to reproduce the issue</li> <li>Environment: API client version, Python version</li> <li>Error Messages: Full error messages and stack traces</li> <li>Expected Behavior: What you expected to happen</li> </ul>"},{"location":"contributing/#feature-requests_1","title":"Feature Requests","text":"<ul> <li>Use Case: Describe your specific use case</li> <li>Benefits: How this feature would help you</li> <li>Alternatives: What you've tried as workarounds</li> <li>Priority: How important this is for your workflow</li> </ul>"},{"location":"contributing/#framework-feedback","title":"Framework Feedback","text":"<ul> <li>Feature: Which framework feature you're using</li> <li>Use Case: Example usage and expected behavior</li> <li>Performance: Any performance issues you've noticed</li> <li>Documentation: What could be clearer in the docs</li> </ul>"},{"location":"contributing/#priority-feedback-areas","title":"\ud83c\udfaf Priority Feedback Areas","text":""},{"location":"contributing/#high-priority","title":"\ud83d\udd27 High Priority","text":"<ul> <li>Framework Performance: Execution speed and reliability</li> <li>Agent Templates: Industry-specific use cases</li> <li>Framework Features: Missing functionality</li> <li>Integration: Third-party service connections</li> </ul>"},{"location":"contributing/#general-feedback","title":"\ud83c\udf1f General Feedback","text":"<ul> <li>Documentation: Clarity and completeness</li> <li>CLI Experience: User interface improvements</li> <li>Examples: Real-world use cases</li> <li>Error Messages: Better debugging information</li> <li>Tool Integration: Add simple utility tools</li> <li>Test Coverage: Expand test scenarios</li> </ul>"},{"location":"contributing/#advanced-projects","title":"\ud83d\ude80 Advanced Projects","text":"<ul> <li>Multi-Agent Orchestration: Complex workflow patterns</li> <li>Custom Optimizers: New DSPy optimization strategies</li> <li>Enterprise Features: Security, scalability, monitoring</li> <li>Research Integration: Latest AI/ML advances</li> </ul>"},{"location":"contributing/#communication","title":"\ud83d\udcac Communication","text":""},{"location":"contributing/#getting-help","title":"\ud83d\udce2 Getting Help","text":"<ul> <li>Website: super-agentic.ai - Visit our website for support</li> <li>GitHub Discussions: Join discussions</li> <li>Email Support: support@super-agentic.ai</li> </ul>"},{"location":"contributing/#feedback-standards","title":"\ud83d\udccb Feedback Standards","text":"<ul> <li>User-focused: Focus on developer experience</li> <li>Examples: Include specific use cases</li> <li>Impact: Explain how feedback improves the framework</li> </ul>"},{"location":"contributing/#recognition","title":"\ud83c\udfc6 Recognition","text":"<p>Users who provide valuable feedback get:</p> <ul> <li>\ud83d\udcdb Beta Access: Early access to new features</li> <li>\ud83c\udf89 Feature Credits: Recognition in release notes</li> <li>\ud83e\udd1d Recognition: Recognition in our project</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"\ud83d\ude4f Code of Conduct","text":"<p>We're committed to providing a welcoming and inclusive environment for all users. Please read our Code of Conduct before participating.</p> <p>Your feedback makes SuperOptiX better for everyone. Whether you're reporting a bug or suggesting a feature, every piece of feedback matters.</p> <p>Let's build the future of AI agent development together! \ud83d\ude80 </p>"},{"location":"debugging-guide/","title":"\ud83d\udc1b Debugging Guide","text":"<p>Having trouble with SuperOptiX? This guide will help you diagnose and fix common issues, from installation to agent orchestration.</p>"},{"location":"debugging-guide/#common-errors-fixes","title":"\ud83d\udea6 Common Errors &amp; Fixes","text":"<ul> <li>Installation issues: </li> <li>Python version mismatch  </li> <li>Missing dependencies  </li> <li> <p>Solution: Check <code>pyproject.toml</code>, use a clean virtual environment</p> </li> <li> <p>CLI errors: </p> </li> <li>\u201ccommand not found\u201d, \u201cnot a super project\u201d  </li> <li> <p>Solution: Run from project root, ensure <code>.super</code> file exists</p> </li> <li> <p>Agent compilation errors: </p> </li> <li>YAML syntax errors, missing playbooks  </li> <li> <p>Solution: Validate YAML, check agent directory</p> </li> <li> <p>Model/server errors: </p> </li> <li>Model not found, server not running  </li> <li> <p>Solution: Check model install, server logs, port conflicts</p> </li> <li> <p>Memory/tool integration errors: </p> </li> <li>Backend not installed, misconfigured settings  </li> <li>Solution: Check backend install, review config</li> </ul>"},{"location":"debugging-guide/#debugging-tools-techniques","title":"\ud83d\udee0\ufe0f Debugging Tools &amp; Techniques","text":"<ul> <li>Use <code>super &lt;command&gt; --verbose</code> for more output</li> <li>Check logs in <code>logs/</code> or as printed in the terminal</li> <li>Use Python debuggers (<code>pdb</code>, IDE breakpoints)</li> <li>Enable tracing/observability (if available)</li> </ul>"},{"location":"debugging-guide/#troubleshooting-checklist","title":"\ud83d\udcdd Troubleshooting Checklist","text":"<ol> <li>Are you in the project root? (<code>.super</code> file present)</li> <li>Did you activate your virtual environment?</li> <li>Are all dependencies installed?</li> <li>Is your YAML valid? (use YAML Linter)</li> <li>Is the model installed and server running?</li> <li>Are you using the correct CLI command?</li> </ol>"},{"location":"debugging-guide/#faq","title":"\u2753 FAQ","text":"<ul> <li> <p>\u201cWhy do I get \u2018command not found\u2019?\u201d   Make sure you installed SuperOptiX and activated your environment.</p> </li> <li> <p>\u201cWhy does my agent not appear?\u201d   Check the agent directory and playbook naming.</p> </li> <li> <p>\u201cWhy do I get a backend error?\u201d   Ensure the required backend (memory, tool, model) is installed and configured.</p> </li> </ul>"},{"location":"debugging-guide/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Documentation</li> </ul> <p>If you find a bug, please report it with your OS, Python version, CLI command, and error output! </p>"},{"location":"environment-setup/","title":"\ud83d\udc0d Environment Setup Guide","text":"<p>This guide covers setting up your development environment for SuperOptiX, including Python environments, virtual environments, and best practices for different operating systems.</p>"},{"location":"environment-setup/#overview","title":"\ud83c\udfaf Overview","text":"<p>A properly configured environment is crucial for SuperOptiX development. This guide covers:</p> <ul> <li>\ud83d\udc0d Python Setup: Installing and configuring Python</li> <li>\ud83d\udd27 Virtual Environments: Isolating project dependencies (uv recommended)</li> <li>\ud83d\udce6 Package Management: Using uv, pip, conda, and poetry</li> <li>\ud83d\udee0\ufe0f Development Tools: IDE setup and debugging tools</li> </ul>"},{"location":"environment-setup/#python-setup","title":"\ud83d\udc0d Python Setup","text":""},{"location":"environment-setup/#python-version-requirements","title":"Python Version Requirements","text":"<p>SuperOptiX requires Python 3.11+ (3.12 recommended):</p> <pre><code># Check your Python version\npython --version\n# or\npython3 --version\n\n# Should show Python 3.11.x or 3.12.x\n</code></pre>"},{"location":"environment-setup/#installing-python","title":"Installing Python","text":"\ud83c\udf4e macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83e\ude9f Windows <pre><code># Using Homebrew (recommended)\nbrew install python@3.12\n\n# Using uv (to manage python versions)\nuv python install 3.12\n</code></pre> <pre><code># Update package list\nsudo apt update\n\n# Install Python 3.12\nsudo apt install python3.12 python3.12-venv python3.12-pip\n</code></pre> <pre><code># Using winget\nwinget install Python.Python.3.12\n\n# Using uv\nuv python install 3.12\n</code></pre> <p>Windows Users - Important!</p> <p>On Windows, set PYTHONUTF8=1 to ensure proper UTF-8 encoding support:</p> <pre><code>set PYTHONUTF8=1\n</code></pre> <p>Or add it to your system environment variables for permanent setting.</p>"},{"location":"environment-setup/#virtual-environments","title":"\ud83d\udd27 Virtual Environments","text":""},{"location":"environment-setup/#why-virtual-environments","title":"Why Virtual Environments?","text":"<p>Virtual environments isolate project dependencies, preventing conflicts between different projects:</p> <ul> <li>Isolation: Each project has its own Python packages</li> <li>Reproducibility: Exact dependency versions for consistent builds</li> <li>Clean Development: No system-wide package pollution</li> <li>Easy Cleanup: Remove entire environment when done</li> </ul> \u26a1 uv (Recommended)\ud83d\udc0d venv (Built-in)\ud83d\udce6 conda <p><code>uv</code> is an extremely fast Python package installer and resolver, written in Rust.</p> <pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a new virtual environment\nuv venv\n\n# Activate the environment\n# macOS/Linux:\nsource .venv/bin/activate\n# Windows:\n.venv\\Scripts\\activate\n\n# Install SuperOptiX\nuv pip install superoptix\n</code></pre> <pre><code># Create a new virtual environment\npython -m venv superoptix-env\n\n# Activate the environment\n# On macOS/Linux:\nsource superoptix-env/bin/activate\n\n# On Windows:\nsuperoptix-env\\Scripts\\activate\n\n# Install SuperOptiX\npip install superoptix\n</code></pre> <pre><code># Create a new conda environment\nconda create -n superoptix python=3.12\n\n# Activate the environment\nconda activate superoptix\n\n# Install SuperOptiX\npip install superoptix\n</code></pre>"},{"location":"environment-setup/#package-management","title":"\ud83d\udce6 Package Management","text":"\u26a1 uv (Recommended)\ud83d\udce6 pip (Standard) <pre><code># Install SuperOptiX\nuv pip install superoptix\n\n# Install with optional dependencies\nuv pip install \"superoptix[vectordb,ui,observability]\"\n\n# Install as a global CLI tool\nuv tool install superoptix\n\n# Create requirements.txt\nuv pip freeze &gt; requirements.txt\n\n# Install from requirements\nuv pip install -r requirements.txt\n</code></pre> <pre><code># Install SuperOptiX\npip install superoptix\n\n# Install with optional dependencies\npip install superoptix[vectordb,ui,observability]\n\n# Install latest version\npip install --upgrade superoptix\n</code></pre>"},{"location":"environment-setup/#development-tools","title":"\ud83d\udee0\ufe0f Development Tools","text":""},{"location":"environment-setup/#ide-setup","title":"IDE Setup","text":"\ud83d\udcbb VS Code\ud83d\ude80 Cursor <pre><code># Install VS Code extensions\ncode --install-extension ms-python.python\ncode --install-extension charliermarsh.ruff\n</code></pre> <pre><code># Install Cursor extensions\n# Cursor comes with excellent Python support built-in\n</code></pre>"},{"location":"environment-setup/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"environment-setup/#ruff-code-formatting-linting","title":"Ruff (Code Formatting &amp; Linting)","text":"<p>Ruff is an extremely fast Python linter and formatter written in Rust:</p> <pre><code># Install Ruff\nuv pip install ruff\n\n# Format code\nruff format .\n\n# Lint code\nruff check .\n</code></pre>"},{"location":"environment-setup/#environment-variables","title":"\ud83d\udd27 Environment Variables","text":""},{"location":"environment-setup/#setting-environment-variables","title":"Setting Environment Variables","text":"\ud83c\udf4e macOS/Linux\ud83e\ude9f Windows\ud83d\udcc4 .env Files <pre><code># Temporary (current session)\nexport OPENAI_API_KEY=\"your-api-key\"\nexport ANTHROPIC_API_KEY=\"your-api-key\"\n\n# Permanent (add to ~/.bashrc or ~/.zshrc)\necho 'export OPENAI_API_KEY=\"your-api-key\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <pre><code># Temporary (current session)\nset OPENAI_API_KEY=your-api-key\n\n# Permanent (System Properties &gt; Environment Variables)\n# Or use PowerShell:\n[Environment]::SetEnvironmentVariable(\"OPENAI_API_KEY\", \"your-api-key\", \"User\")\n</code></pre> <pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nOPENAI_API_KEY=your-api-key\nANTHROPIC_API_KEY=your-api-key\nEOF\n\n# Load with python-dotenv\nuv pip install python-dotenv\n</code></pre>"},{"location":"environment-setup/#testing-your-environment","title":"\ud83e\uddea Testing Your Environment","text":""},{"location":"environment-setup/#environment-validation","title":"Environment Validation","text":"<pre><code># Test Python installation\npython --version\n\n# Test SuperOptiX installation\nsuper --version\n\n# Test basic functionality\npython -c \"import superoptix; print('SuperOptiX imported successfully!')\"\n</code></pre>"},{"location":"environment-setup/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"environment-setup/#performance-optimization","title":"Performance Optimization","text":"\u26a1 Using uv for Faster Installs <pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Use uv instead of pip\nuv pip install superoptix\n</code></pre>"},{"location":"environment-setup/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Install SuperOptiX following the Installation Guide</li> <li>Configure LLMs with the LLM Setup Guide</li> <li>Create your first agent with the Quick Start Guide</li> </ol>"},{"location":"faq/","title":"\u2753 Frequently Asked Questions","text":""},{"location":"faq/#framework-overview","title":"\ud83d\ude80 Framework Overview","text":""},{"location":"faq/#how-is-superoptix-different-from-other-agent-frameworks","title":"How is SuperOptiX different from other agent frameworks?","text":"<p>SuperOptiX is a full-stack, all-in-one agent framework that provides:</p> <ul> <li>\ud83c\udfaf Evaluation-First Approach - Built-in BDD/TDD methodology with executable specifications</li> <li>\u26a1 Optimization-First Design - Advanced DSPy optimization with production-ready techniques</li> <li>\ud83c\udfad Multi-Agent Orchestration - Sophisticated coordination patterns (pipeline, broadcast, conditional, parallel)</li> <li>\ud83e\udde0 Advanced Memory Systems - Multi-layered memory with episodic, semantic, and working memory</li> <li>\ud83d\udd27 Complete Tool Ecosystem - 50+ built-in tools across 17 industry categories</li> <li>\ud83d\udcca Production Observability - Comprehensive tracing, debugging, and monitoring</li> <li>\ud83d\udd04 CI/CD Integration - Built-in quality gates and automated testing workflows</li> </ul> <p>Unlike other frameworks that focus on single components, SuperOptiX provides a complete ecosystem for building, testing, optimizing, and deploying production-ready AI agents.</p>"},{"location":"faq/#is-superoptix-just-a-dspy-wrapper","title":"Is SuperOptiX just a DSPy wrapper?","text":"<p>No, SuperOptiX is \"Agentic DSPy\" - an evolution beyond DSPy. While SuperOptiX harnesses the full power of DSPy's optimization principles, it transforms them into a production-ready agentic AI platform:</p> <ul> <li>\ud83e\uddec Advanced Agentic Modules - Custom modules for multi-agent coordination, protocol support (MCP, A2A), and memory-optimized interactions</li> <li>\ud83c\udfd7\ufe0f Application-Layer Abstractions - SuperSpec DSL for declarative agent building at the application layer</li> <li>\ud83c\udfaf BDD Testing Framework - Behavior-driven specifications and executable tests for agent validation</li> <li>\ud83c\udfad Multi-Tier Architecture - Progressive complexity from Oracles to Sovereigns</li> <li>\ud83d\udd04 Production-Ready Features - Memory management, observability, and deployment automation</li> <li>\ud83d\udd27 Modular Optimization - DSPy as primary adapter with framework-agnostic design for future optimization frameworks</li> </ul> <p>SuperOptiX treats DSPy as the optimization engine while providing the complete agentic ecosystem around it.</p>"},{"location":"faq/#whats-the-relationship-between-superoptix-and-dspy","title":"What's the relationship between SuperOptiX and DSPy?","text":"<p>SuperOptiX and DSPy have a symbiotic relationship where each framework amplifies the other's strengths:</p> <p>\ud83d\ude80 DSPy's Core Strengths: - Only framework with systematic optimization - Iterative optimization principles (perfect for TDD/BDD) - Assertions and evaluations for validation - Signature generation and module composition</p> <p>\ud83e\uddec SuperOptiX's Agentic Evolution: - Advanced Custom Modules for agentic scenarios beyond standard DSPy - Automatic Pipeline Generation from high-level SuperSpec specifications - Protocol Support for MCP, A2A, and emerging agentic standards - Application-Layer DSL that compiles to optimized DSPy implementations</p> <p>\ud83d\udca1 Key Insight: DSPy provides the optimization foundation; SuperOptiX provides the agentic architecture, production tooling, and developer experience.</p>"},{"location":"faq/#why-did-superoptix-choose-dspy-over-other-frameworks","title":"Why did SuperOptiX choose DSPy over other frameworks?","text":"<p>DSPy is the only framework that systematically optimizes language model programs, making it uniquely suited for building reliable agentic systems:</p> <p>\ud83d\udd2c Scientific Approach: - Optimization-First Philosophy - Treats prompt engineering as a systematic optimization problem - Evaluation-Driven Development - Built-in assertions and metrics for validation - Composable Architecture - Modules that can be optimized individually and collectively</p> <p>\ud83e\uddea Perfect for Agentic TDD/BDD: - Iterative Refinement - Aligns with test-driven development principles - Behavioral Validation - Assertions that ensure agent behavior meets specifications - Systematic Testing - Framework for comprehensive agent evaluation</p> <p>\ud83d\ude80 Production Readiness: - Transparent Optimization - Clear visibility into optimization process - Reproducible Results - Deterministic optimization with version control - Scalable Architecture - Designed for complex, multi-component systems</p> <p>SuperOptiX recognizes that DSPy's approach is fundamentally different from other frameworks - it's built for systematic optimization rather than ad-hoc prompt engineering.</p>"},{"location":"faq/#can-superoptix-work-with-other-optimization-frameworks","title":"Can SuperOptiX work with other optimization frameworks?","text":"<p>Yes, SuperOptiX is designed with a modular optimization architecture:</p> <p>\ud83d\udd27 Current Implementation: - DSPy as Primary Adapter - Leverages DSPy's proven optimization capabilities - Comprehensive Integration - Full DSPy feature support with agentic extensions</p> <p>\ud83d\ude80 Future-Ready Design: - Framework Agnostic - Ready to integrate other optimization frameworks as they emerge - Adapter Pattern - Clean interfaces for adding new optimization backends - User Choice - Multiple optimization strategies for different use cases - Custom Implementations - Support for specialized optimization layers</p> <p>\ud83d\udca1 Philosophy: DSPy is currently the gold standard for optimization, but SuperOptiX is prepared to evolve as the optimization landscape develops.</p>"},{"location":"faq/#licensing-tiers","title":"\ud83d\udcb0 Licensing &amp; Tiers","text":""},{"location":"faq/#is-superoptix-open-source","title":"Is SuperOptiX open source?","text":"<p>Yes! SuperOptiX is open source under the MIT License. We welcome contributions from the community.</p> <ul> <li>\ud83c\udfad Oracle Tier - Basic Q&amp;A and simple evaluation capabilities</li> <li>\ud83e\uddde Genie Tier - Tools, RAG, memory, and streaming features</li> <li>\ud83c\udfad Basic Orchestration - Simple multi-agent coordination</li> <li>\ud83d\udee0\ufe0f Core Tools - Essential tools for development and prototyping</li> <li>\ud83d\udcca Basic Observability - Tracing and debugging capabilities</li> </ul> <p>\u26a0\ufe0f Important: The OSS version is designed for demos and prototypes. It's not recommended for production use unless you have significant expertise in AI system deployment and optimization.</p>"},{"location":"faq/#installation-dependencies","title":"\ud83d\udd27 Installation &amp; Dependencies","text":""},{"location":"faq/#im-getting-dependency-conflicts-when-installing-crewai-with-superoptix-whats-happening","title":"I'm getting dependency conflicts when installing CrewAI with SuperOptiX. What's happening?","text":"<p>This is a known dependency conflict between CrewAI and DSPy due to incompatible <code>json-repair</code> version requirements:</p> <p>\ud83d\udd0d The Problem: - DSPy 3.0.0 requires <code>json-repair&gt;=0.30.0</code> - CrewAI 0.157.0 requires <code>json-repair==0.25.2</code> (exact version)</p> <p>The Solution: Install CrewAI manually after installing SuperOptiX with DSPy support:</p> <pre><code># Install SuperOptiX with DSPy support (this gets compatible json-repair)\npip install \"superoptix[optimas]\"\n\n# Install CrewAI without dependencies to avoid conflicts\npip install crewai==0.157.0 --no-deps\n\n# Ensure compatible json-repair version\npip install \"json-repair&gt;=0.30.0\"\n</code></pre> <p>\ud83d\udca1 Why This Works: - The <code>--no-deps</code> flag prevents pip from trying to resolve conflicting dependencies - We manually install the version of <code>json-repair</code> that satisfies both packages - Both packages work together at runtime despite metadata conflicts</p> <p>\ud83d\udcda Alternative: If you only need CrewAI functionality, you can install SuperOptiX without DSPy support and then add CrewAI normally.</p>"},{"location":"faq/#how-can-i-access-higher-tiers-protocols-and-beyond","title":"How can I access higher tiers (Protocols and beyond)?","text":"<p>Higher tiers require enterprise licensing from Superagentic AI:</p> <ul> <li>\ud83c\udfaf Protocols Tier - Advanced features, parallel execution, and production optimizations</li> <li>\ud83d\ude80 Enterprise Features - Protocol integrations (MCP, A2A), advanced orchestration, and custom deployments</li> <li>\ud83d\udd27 Custom Solutions - Tailored packages based on specific use cases and requirements</li> </ul> <p>\ud83d\udcde Contact: There's no set pricing as it varies by use case. Contact Superagentic AI for a tailored package based on your specific needs and requirements.</p>"},{"location":"faq/#learning-usage","title":"\ud83c\udf93 Learning &amp; Usage","text":""},{"location":"faq/#do-i-need-to-know-dspy-to-use-superoptix","title":"Do I need to know DSPy to use SuperOptiX?","text":"<p>Not necessarily, but it helps:</p> <ul> <li>\ud83d\udee1\ufe0f SuperOptiX handles DSPy complexity - The framework abstracts most DSPy internals</li> <li>\ud83d\ude80 You can start immediately - Basic agents work out-of-the-box with minimal DSPy knowledge</li> <li>\ud83c\udfaf DSPy knowledge = Full control - Understanding DSPy helps you create production-worthy pipelines</li> <li>\ud83d\udcda Learning path - Start with SuperOptiX basics, then learn DSPy for advanced customization</li> </ul> <p>\ud83d\udca1 Recommendation: Start with SuperOptiX's high-level APIs, then learn DSPy as you need more control over optimization and pipeline design.</p>"},{"location":"faq/#optimization-performance","title":"\u26a1 Optimization &amp; Performance","text":""},{"location":"faq/#what-optimization-strategies-does-superoptix-support","title":"What optimization strategies does SuperOptiX support?","text":"<p>SuperOptiX provides multiple optimization strategies:</p> <ul> <li>\ud83c\udfaf BootstrapFewShot - Automatic few-shot learning with bootstrapped demonstrations</li> <li>\ud83d\udd04 ReAct - Reasoning and acting optimization for tool-using agents</li> <li>\ud83d\udcca Multi-Metric Optimization - Optimize for multiple metrics simultaneously</li> <li>\ud83c\udfad Tier-Specific Optimization - Different strategies for Oracles, Genies, and Protocols tiers</li> <li>\ud83d\udee0\ufe0f Tool-Aware Optimization - Optimization that considers tool usage patterns</li> </ul>"},{"location":"faq/#can-i-optimize-agents-for-specific-use-cases","title":"Can I optimize agents for specific use cases?","text":"<p>Yes, absolutely:</p> <ul> <li>\ud83c\udfaf Custom Evaluation Metrics - Define domain-specific evaluation criteria</li> <li>\ud83d\udcca BDD Scenarios - Create executable specifications for your use case</li> <li>\ud83d\udee0\ufe0f Tool Integration - Optimize for specific tool usage patterns</li> <li>\ud83e\udde0 Memory Optimization - Tune memory systems for your data patterns</li> <li>\ud83d\udcc8 Performance Profiling - Identify and optimize bottlenecks</li> </ul>"},{"location":"faq/#evaluation-testing","title":"\ud83e\uddea Evaluation &amp; Testing","text":""},{"location":"faq/#how-does-superoptixs-evaluation-system-work","title":"How does SuperOptiX's evaluation system work?","text":"<p>The evaluation system provides:</p> <ul> <li>\ud83c\udfaf BDD/TDD Approach - Executable specifications as test cases</li> <li>\ud83d\udcca Multiple Metrics - Semantic F1, exact match, reasoning quality, tool efficiency</li> <li>\ud83d\udd04 Continuous Evaluation - Automated testing in CI/CD pipelines</li> <li>\ud83d\udcc8 Quality Gates - Pass/fail thresholds for automated deployment</li> <li>\ud83c\udfad Scenario Testing - Complex multi-step scenario validation</li> </ul>"},{"location":"faq/#what-evaluation-metrics-are-available","title":"What evaluation metrics are available?","text":"<p>SuperOptiX includes:</p> <ul> <li>\ud83c\udfaf Semantic F1 - Semantic similarity scoring</li> <li>Exact Match - Precise answer matching</li> <li>\ud83e\udde0 Reasoning Quality - Assessment of reasoning process</li> <li>\ud83d\udee0\ufe0f Tool Usage Efficiency - Evaluation of tool selection and usage</li> <li>\ud83d\udcca Response Time - Performance and latency metrics</li> <li>\ud83d\udcb0 Cost Metrics - Token usage and cost tracking</li> <li>\ud83c\udfad Custom Metrics - Domain-specific evaluation criteria</li> </ul>"},{"location":"faq/#how-do-i-set-up-automated-testing","title":"How do I set up automated testing?","text":"<p>The framework provides:</p> <ul> <li>\ud83d\udd04 CI/CD Integration - GitHub Actions, GitLab CI, Jenkins, Azure DevOps</li> <li>\ud83d\udcca Quality Gates - Automated pass/fail thresholds</li> <li>\ud83c\udfaf BDD Scenarios - Executable specifications as tests</li> <li>\ud83d\udcc8 Performance Monitoring - Continuous performance tracking</li> <li>\ud83d\ude80 Automated Deployment - Deploy only when tests pass</li> </ul>"},{"location":"faq/#memory-systems","title":"\ud83e\udde0 Memory Systems","text":""},{"location":"faq/#what-types-of-memory-does-superoptix-support","title":"What types of memory does SuperOptiX support?","text":"<p>The framework provides three memory layers:</p> <ul> <li>\ud83d\udcdd Episodic Memory - Conversation history and interaction episodes</li> <li>\ud83e\udde0 Semantic Memory - Persistent knowledge and relationships</li> <li>\u26a1 Working Memory - Temporary session information</li> </ul>"},{"location":"faq/#what-storage-backends-are-available","title":"What storage backends are available?","text":"<p>SuperOptiX supports multiple backends:</p> <ul> <li>\ud83d\uddc4\ufe0f SQLite - Lightweight, file-based storage (default)</li> <li>\ud83d\udd34 Redis - High-performance, in-memory storage</li> <li>\ud83d\udcc1 File - Simple file-based storage with JSON/YAML formats</li> </ul>"},{"location":"faq/#how-does-memory-integration-work","title":"How does memory integration work?","text":"<p>Memory integration provides:</p> <ul> <li>\ud83c\udfaf Context Retrieval - Automatic relevant context for responses</li> <li>\ud83d\udcca Memory Statistics - Usage tracking and analytics</li> <li>\ud83d\udd04 Automatic Cleanup - Retention policies and cleanup</li> <li>\ud83e\udde0 Semantic Search - Find relevant memories by content</li> <li>\u26a1 Working Memory - Temporary data with TTL support</li> </ul>"},{"location":"faq/#tools-integrations","title":"\ud83d\udee0\ufe0f Tools &amp; Integrations","text":""},{"location":"faq/#what-tools-are-included-in-superoptix","title":"What tools are included in SuperOptiX?","text":"<p>The framework includes 50+ tools across 17 categories:</p> <ul> <li>\ud83d\udd27 Core Tools - Web search, calculator, file operations, date/time</li> <li>\ud83d\udcbb Development - Git, API testing, database queries, code review</li> <li>\ud83d\udcb0 Finance - Currency conversion, tax calculation, loan analysis</li> <li>\ud83c\udfe5 Healthcare - BMI calculator, medical lookups, drug interactions</li> <li>\ud83d\udcc8 Marketing - SEO analysis, email validation, social metrics</li> <li>\u2696\ufe0f Legal - Legal lookups, contract analysis, case search</li> <li>\ud83c\udf93 Education - Grade calculation, study scheduling, quiz generation</li> </ul>"},{"location":"faq/#can-i-create-custom-tools","title":"Can I create custom tools?","text":"<p>Absolutely:</p> <ul> <li>\ud83d\udd27 BaseTool Class - Inherit from BaseTool for custom tools</li> <li>\ud83d\udee0\ufe0f Factory Pattern - Use tool factories for easy creation</li> <li>\ud83d\udcca Schema Validation - Automatic parameter validation</li> <li>\ud83c\udfaf Category Organization - Organize tools by industry/function</li> <li>\ud83d\udd04 Registry System - Automatic tool discovery and registration</li> </ul>"},{"location":"faq/#how-do-i-integrate-external-apis","title":"How do I integrate external APIs?","text":"<p>SuperOptiX provides:</p> <ul> <li>\ud83d\udd11 API Key Management - Secure API key handling</li> <li>\ud83d\udd04 Retry Logic - Automatic retry with exponential backoff</li> <li>\ud83d\udcca Rate Limiting - Built-in rate limit handling</li> <li>\ud83d\udee1\ufe0f Error Handling - Graceful error handling and fallbacks</li> <li>\ud83d\udcc8 Monitoring - API usage tracking and metrics</li> </ul> <p>\ud83d\udca1 Need more help? Check out our documentation, guides, or contact support. </p>"},{"location":"glossary/","title":"Glossary","text":"<p>A list of SuperOptiX terminology will be curated here. </p>"},{"location":"introduction/","title":"Introduction - SuperOptiX AI","text":"Introduction to SuperOptiX <p>The world's first universal agent optimization framework</p> \ud83d\ude80 Quick Start \u2699\ufe0f Setup Guide \ud83e\uddec GEPA Optimizer \ud83d\udd2c Framework Guide"},{"location":"introduction/#what-is-superoptix","title":"\ud83d\udd0d What is SuperOptiX?","text":"<p>SuperOptiX is the world's first universal agent optimization framework that brings end-to-end optimization to AI agents across 6 major frameworks. Build, evaluate, and optimize agents from any framework with a unified workflow powered by GEPA (Genetic-Pareto Algorithm).</p> <p>Unlike most frameworks that bolt on evals and monitoring as an afterthought, SuperOptiX makes evaluation, optimization, and guardrails core to the development lifecycle. Whether you're using DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, or DeepAgents, SuperOptiX helps you ship reliable agents.</p> <p>SuperOptiX brings together declarative agent specification, universal GEPA optimization, built-in evaluation, multi-framework support, and multi-agent orchestration, all grounded in the principles of test-driven development and context engineering.</p>"},{"location":"introduction/#whats-new","title":"What's New","text":"<ul> <li>\ud83e\uddea RLM Support (Experimental): Available for early adoption and testing. Unified sandbox support is coming soon. See RLM guide.</li> <li>\ud83d\uddc2\ufe0f Connector Integrations: StackOne connector support for cross-framework tool-driven agents. See StackOne integration.</li> <li>\u2699\ufe0f Minimal + Optimize Flow: Default compile generates minimal framework-native pipelines, with expanded optimization flow enabled via <code>--optimize</code>.</li> </ul>"},{"location":"introduction/#core-philosophy","title":"Core Philosophy","text":"<p>Unlike frameworks that lock you into a single approach, SuperOptiX gives you freedom of choice with consistency of experience. Build agents in DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, or DeepAgents, then optimize them all with the same powerful GEPA optimizer.</p> <p>Key Principle</p> <p>Declarative by Design. Universally Optimized. Framework-Agnostic.</p> <p>With its native DSL (SuperSpec), GEPA optimization engine, support for major frameworks, and full-stack abstractions, SuperOptiX empowers you to build reliable, adaptive, and intelligent agentic systems without vendor lock-in.</p>"},{"location":"introduction/#future-proof-by-design","title":"\ud83d\udd2e Future-Proof by Design","text":"<p>SuperOptiX provides true extensibility across three critical abstraction layers:</p> \ud83d\udd2c Framework Layer <p>Swap AI frameworks on demand</p> <p>Currently supported: DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, DeepAgents</p> <p>Future ready: New framework? Add an adapter. Your agents keep working.</p> <pre>target_framework: any_framework</pre> \ud83d\udd0c Protocol Layer <p>Swap communication protocols</p> <p>Currently supported: MCP (Model Context Protocol)</p> <p>Future ready: A2A, custom protocols. Protocol-first design means seamless adoption.</p> <pre>protocol: mcp | a2a | custom</pre> \ud83e\uddec Optimizer Layer <p>Swap optimization engines</p> <p>Currently supported: GEPA, DSPy optimizers (BootstrapFewShot, MIPRO, MIPROv2)</p> <p>Future ready: New optimizer research? Plugin system makes integration trivial.</p> <pre>optimizer: gepa | mipro | custom</pre> \ud83d\udc8e Abstraction Without Lock-In <p>Your agent context stays constant. Frameworks evolve. Protocols emerge. Optimizers improve.</p> <p>SuperOptiX adapts without breaking your agents.</p> <p>Built for today. Ready for tomorrow. Future-proof forever.</p>"},{"location":"introduction/#full-stack-agentic-optimization","title":"\ud83c\udfd7\ufe0f Full-Stack Agentic Optimization","text":"<p>SuperOptiX provides end-to-end optimization across every layer of the agentic stack. While other frameworks optimize prompts in isolation, SuperOptiX takes a holistic approach with universal GEPA optimization that works independently of any specific framework:</p> \ud83d\udcac Prompts &amp; Instructions <p>Evolutionary optimization of:</p> <ul> <li>System prompts</li> <li>Agent instructions</li> <li>Reasoning patterns</li> <li>Task descriptions</li> </ul> \ud83d\udcda RAG &amp; Retrieval <p>Context optimization across:</p> <ul> <li>Retrieval queries</li> <li>Chunking strategies</li> <li>Embedding selection</li> <li>Relevance scoring</li> </ul> \ud83d\udee0\ufe0f Tools &amp; Functions <p>MCP protocol optimization for:</p> <ul> <li>Tool selection</li> <li>Parameter tuning</li> <li>Execution strategies</li> <li>Error handling</li> </ul> \ud83e\udde0 Memory &amp; Context <p>Intelligent optimization of:</p> <ul> <li>Memory retrieval</li> <li>Context windows</li> <li>Storage strategies</li> <li>Attention patterns</li> </ul> \u26a1 Universal Optimization Engine <p>Framework-agnostic optimization means GEPA works the same whether you're using DSPy, CrewAI, OpenAI SDK, or any other framework.</p> <p>No framework-specific tricks. No vendor lock-in. Just pure, portable optimization across your entire agentic stack.</p> <p>Optimize everything. Switch anything. Break nothing.</p>"},{"location":"introduction/#core-features","title":"\u26a1 Core Features","text":"\ud83c\udfaf Evaluation-First Architecture <p>RSpec-style BDD specifications with built-in testing and validation from day one. Define behavior scenarios, run automated evaluations, track metrics across optimization iterations.</p> \ud83e\uddec GEPA Universal Optimizer <p>Automatic optimization across all 6 major agent frameworks with proven results. Achieve 37.5% \u2192 80% improvements with just 3-10 training scenarios.</p> \ud83d\udd2c Multi-Framework Support <p>Works with DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, and DeepAgents. One workflow, six frameworks, unlimited possibilities.</p> \ud83d\udcdd SuperSpec DSL <p>Declarative language for agent specifications with Kubernetes-style versioning. Write once, deploy to any framework.</p> \ud83e\udde0 Context Engineering <p>Systematic approach to delivering optimal information and tools to agents. Includes RAG, MCP protocol, and memory optimization.</p> \ud83c\udfd7\ufe0f Framework Architecture <p>Progressive capabilities with built-in safety, evaluation, and optimization workflows.</p>"},{"location":"introduction/#how-superoptix-differs-from-other-frameworks","title":"\ud83c\udf1f How SuperOptiX Differs from Other Frameworks","text":"Feature Other Frameworks SuperOptiX \ud83c\udfaf Evaluation-First Add evaluation as an afterthought Evaluation built into core development cycle RSpec-Style BDD Development Manual prompt engineering Behavior-driven specifications with automated testing \ud83e\uddec Universal GEPA Optimizer Manual optimization or framework-specific tools One optimizer for all frameworks \ud83d\udd2c Multi-Framework Freedom Locked into one approach Choose from multiple frameworks, optimize with one tool \ud83d\ude80 Deployment-Ready Basic deployment capabilities Built-in observability, guardrails, and enterprise features"},{"location":"introduction/#multi-framework-support-choose-your-tool-keep-your-workflow","title":"Multi-Framework Support: Choose Your Tool, Keep Your Workflow","text":"<p>SuperOptiX is the world's first framework-agnostic agent optimizer. Build agents in any of the 6 major frameworks, then optimize them all with the same powerful GEPA engine.</p>"},{"location":"introduction/#supported-frameworks","title":"\ud83d\udd2c Supported Frameworks","text":"Framework Variables Local Models Best For Status \ud83d\udd2c DSPy 10+ Ollama Complex reasoning, research Proven: 37.5% \u2192 80% \ud83e\udd16 OpenAI SDK 1 Ollama Simple &amp; fast Proven: 100% pass rate \ud83d\udc65 CrewAI 5 Ollama Multi-agent teams Proven: 100% pass rate \ud83d\udd2e Google ADK 1 \u2601\ufe0f Cloud Gemini native, free access Ready \ud83c\udfe2 Microsoft 1 Ollama Enterprise Azure Ready \ud83c\udf0a DeepAgents 1 Ollama Complex planning Ready"},{"location":"introduction/#one-workflow-all-frameworks","title":"One Workflow, All Frameworks","text":"<pre><code># Same workflow regardless of framework!\nsuper agent compile my_agent --framework [dspy|openai|crewai|google-adk|microsoft|deepagents]\nsuper agent evaluate my_agent\nsuper agent optimize my_agent --auto medium  # GEPA works on ALL frameworks\nsuper agent run my_agent\n</code></pre> <p>Learn More</p> <p>See our Multi-Framework Support Guide for detailed comparisons and examples.</p>"},{"location":"introduction/#universal-workflow","title":"Universal Workflow","text":"<p>SuperOptiX provides a consistent workflow across all frameworks:</p> <ol> <li>Specify: Define your agent using SuperSpec DSL</li> <li>Compile: Generate framework-specific code</li> <li>Evaluate: Test with RSpec-style BDD scenarios</li> <li>Optimize: Improve with GEPA</li> <li>Deploy: Run</li> </ol> <pre><code># Initialize project\nsuper init my_project\ncd my_project\n\n# Pull a pre-built agent\nsuper agent pull sentiment_analyzer\n\n# Compile for your chosen framework\nsuper agent compile sentiment_analyzer\n\n# Evaluate performance\nsuper agent evaluate sentiment_analyzer\n\n# Optimize with GEPA\nsuper agent optimize sentiment_analyzer --auto medium\n\n# Run\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"introduction/#architecture-overview","title":"Architecture Overview","text":"<p>SuperOptiX consists of several key components:</p> <ul> <li>SuperSpec DSL: Declarative specification language</li> <li>GEPA Optimizer: Universal optimization engine</li> <li>Framework Adapters: Support for 6 major frameworks</li> <li>Evaluation Engine: RSpec-style BDD testing framework</li> <li>Memory System: Multi-layered memory management</li> <li>RAG Integration: Knowledge retrieval across frameworks</li> <li>Orchestra: Multi-agent coordination</li> <li>Observability: Built-in monitoring and tracing</li> </ul>"},{"location":"introduction/#proven-results","title":"\ud83d\udcc8 Proven Results","text":"<p>GEPA has delivered dramatic improvements across all tested frameworks:</p> Framework Baseline After GEPA Improvement \ud83d\udd2c DSPy 37.5% 80.0% +42.5 pts \ud83c\udfc6 \ud83e\udd16 OpenAI SDK 100% 100% Maintained \ud83d\udc65 CrewAI 75% 100% +25 pts \u2b50 \ud83d\udd2e Google ADK Ready for optimization \ud83c\udfe2 Microsoft Ready for optimization \ud83c\udf0a DeepAgents Ready for optimization <p>Sample Efficient</p> <p>GEPA achieves these improvements with just 3-10 training scenarios, while traditional methods require hundreds of examples.</p>"},{"location":"introduction/#getting-started","title":"\ud83d\ude80 Getting Started1\ufe0f\u20e32\ufe0f\u20e33\ufe0f\u20e34\ufe0f\u20e35\ufe0f\u20e3","text":"<p>Ready to build your first optimized agent?</p> Quick Start Guide <p>Build your first agent in 10 minutes</p> Installation <p>Set up your development environment</p> LLM Setup <p>Configure your language models</p> SuperSpec Guide <p>Learn the specification language</p> GEPA Optimization <p>Master universal optimization</p>"},{"location":"introduction/#capability-coverage","title":"\ud83c\udfd7\ufe0f Capability Coverage","text":"<p>SuperOptiX focuses on practical capability coverage:</p> <ul> <li>Framework support (DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, DeepAgents, Pydantic AI)</li> <li>SuperSpec-based agent definitions</li> <li>Built-in evaluation and optimization loops</li> <li>Observability and orchestration support</li> </ul> \ud83c\udf10 View Framework Details"},{"location":"introduction/#community-and-support","title":"Community and Support","text":"<ul> <li>Documentation: https://superoptix.ai/docs</li> <li>GitHub: https://github.com/SuperagenticAI/SuperOptiX</li> <li>Website: https://superoptix.ai</li> </ul>"},{"location":"introduction/#next-steps","title":"\ud83c\udfaf Next Steps\ud83d\ude80\ud83d\udcda\u2699\ufe0f\ud83d\udca1\ud83d\ude80 Ready to Build Optimized AI Agents?","text":"Quick Start <p>Build your first agent in 10 minutes</p> Guides <p>In-depth tutorials and guides</p> API Reference <p>Complete API documentation</p> Examples <p>Real-world agent examples</p> <p>Start with quick start and build your first agent today.</p> \ud83d\ude80 Start Free \ud83e\udd16 Create First Agent \ud83c\udf10 Framework Guide"},{"location":"llm-setup/","title":"\ud83e\udd16 LLM Setup Guide","text":"<p>Welcome to SuperOptiX's LLM Setup Guide! This guide will help you configure and use local language models for your AI agents. We focus on local models for privacy, speed, and cost-effectiveness.</p> <p>\ud83d\ude80 Quick Start</p> <p>New to local models? Start with Ollama - it's the easiest option for beginners!</p>"},{"location":"llm-setup/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX supports multiple local model backends, each optimized for different use cases:</p> Backend Best For Platform Ease of Use Performance \ud83e\udd99 Ollama Beginners, All platforms Cross-platform \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \ud83e\udd16 GPT-OSS Advanced reasoning, Agentic tasks Cross-platform \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \ud83c\udf4e MLX Apple Silicon users macOS only \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \ud83c\udfae LM Studio Windows users Windows/macOS \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \ud83e\udd17 HuggingFace Advanced users All platforms \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 <p>Production Inference Engines</p> <p>vLLM, SGLang, and TGI are not included in the current version of SuperOptiX. These production-worthy inference engines are part of our enterprise offering.</p>"},{"location":"llm-setup/#ollama-recommended","title":"\ud83e\udd99 Ollama (Recommended)","text":"<p>Ollama is the easiest way to run local models on any platform. Perfect for beginners!</p>"},{"location":"llm-setup/#quick-setup","title":"\ud83d\ude80 Quick Setup","text":"\ud83c\udf4e macOS\ud83d\udc27 Linux\ud83e\ude9f Windows <pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Start Ollama (runs in background)\nollama serve\n</code></pre> <pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Start Ollama\nollama serve\n</code></pre> <pre><code># Download from https://ollama.ai/download\n# Or use winget\nwinget install Ollama.Ollama\n\n# Start Ollama\nollama serve\n</code></pre>"},{"location":"llm-setup/#install-models-with-superoptix","title":"\ud83d\udce6 Install Models with SuperOptiX","text":"<pre><code># Install recommended models by tier\nsuper model install llama3.2:1b      # Oracles tier - Small tasks, fast responses\nsuper model install llama3.2:8b      # Genies tier - Complex reasoning, tools, memory\nsuper model install llama3.2:3b      # Alternative small model\nsuper model install qwen2.5:7b       # Great all-rounder\n</code></pre> Show Output <pre><code>\ud83d\ude80 SuperOptiX Model Intelligence - Installing llama3.2:3b\n\ud83e\udd99 Pulling model llama3.2:3b from Ollama...\n\u23f3 This may take a few minutes depending on your internet connection and model size.\n\npulling manifest \npulling dde5aa3fc5ff: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.0 GB                         \npulling 966de95ca8a6: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling fcc5a6bec9da: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.7 KB                         \npulling a70ff7e570d9: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 6.0 KB                         \npulling 56bb8bd477a5: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   96 B                         \npulling 34bb5ab01051: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  561 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \nModel pulled successfully!\n\n\ud83d\udca1 You can now use it with SuperOptiX:\n  super model dspy ollama/llama3.2:3b\n\n\ud83d\udcca Model details:\n  \u2022 Size: small\n  \u2022 Task: chat\n  \u2022 Parameters: 3B\n\n\ud83c\udf89 Installation completed successfully!\n\ud83e\udd99 Ollama running on http://localhost:11434 ready to use with SuperOptiX!\n</code></pre>"},{"location":"llm-setup/#server-management","title":"\ud83d\udda5\ufe0f Server Management","text":"<p>\ud83d\udca1 Important: Ollama automatically starts its server when you run <code>ollama serve</code> or when you first use a model. You don't need to manually start the server unless you want custom configuration.</p> <pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n\n# Or simply use a model - server starts automatically\nollama run llama3.2:1b\n</code></pre> <p>\ud83d\udd27 Custom Configuration: Only start the server manually if you need: - Different port: <code>OLLAMA_HOST=0.0.0.0:8080 ollama serve</code> - Custom model path: <code>OLLAMA_MODELS=/custom/path ollama serve</code> - GPU configuration: <code>OLLAMA_GPU_LAYERS=35 ollama serve</code></p> <p>Automatic Detection: SuperOptiX automatically detects and connects to Ollama running on the default port (11434). No additional configuration needed!</p>"},{"location":"llm-setup/#manage-ollama-models","title":"\ud83d\udccb Manage Ollama Models","text":"<pre><code># List installed models\nsuper model list --backend ollama\n</code></pre> <p>Example Output: <pre><code>                \ud83d\ude80 SuperOptiX Model Intelligence - 3 models                 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                   \u2503  Backend  \u2503    Status    \u2503  Size   \u2503   Task    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama3.1:8b             \u2502 \ud83e\udd99 ollama \u2502 installed \u2502 medium  \u2502   chat    \u2502\n\u2502 llama3.2:1b             \u2502 \ud83e\udd99 ollama \u2502 installed \u2502  tiny   \u2502   chat    \u2502\n\u2502 nomic-embed-text:latest \u2502 \ud83e\udd99 ollama \u2502 installed \u2502 Unknown \u2502 embedding \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Get model information\nsuper model info llama3.2:3b\n\n# List all available models\nsuper model list --all\n</code></pre>"},{"location":"llm-setup/#gpt-oss-models-openais-open-source","title":"\ud83e\udd16 GPT-OSS Models (OpenAI's Open Source)","text":"<p>GPT-OSS models are OpenAI's latest open-weight language models designed for powerful reasoning, agentic tasks, and versatile developer use cases. SuperOptiX now supports both GPT-OSS-20B and GPT-OSS-120B models with native Apple Silicon support!</p>"},{"location":"llm-setup/#apple-silicon-support","title":"\ud83c\udf4e Apple Silicon Support","text":"<p>MLX-LM v0.26.3 now provides native Apple Silicon support for GPT-OSS models, resolving the mixed precision issues that previously prevented these models from running on Apple Silicon.</p> Backend Model Status Performance Apple Silicon Recommendation \ud83e\udd99 Ollama gpt-oss:20b Works 19.7 t/s Optimized format \u2b50 RECOMMENDED \ud83c\udf4e MLX-LM openai_gpt-oss-20b Works 5.2 t/s Native support Apple Silicon only \ud83e\udd17 HuggingFace openai/gpt-oss-20b Broken N/A Mixed precision errors Avoid on Apple Silicon"},{"location":"llm-setup/#gpt-oss-model-overview","title":"\ud83c\udfaf GPT-OSS Model Overview","text":"Model Parameters Active Parameters Best For Hardware Requirements GPT-OSS-20B 21B 3.6B Lower latency, local/specialized use cases 16GB+ RAM GPT-OSS-120B 117B 5.1B Production, general purpose, high reasoning Single H100 GPU <p>\ud83d\ude80 Recommended: Use Ollama for GPT-OSS Models</p> <p>For the best performance and reliability with GPT-OSS models, we recommend using Ollama:</p> <ul> <li>Best Performance: 19.7 t/s vs 5.2 t/s (MLX) vs N/A (HuggingFace)</li> <li>Cross-Platform: Works on all platforms (Windows, macOS, Linux)</li> <li>Easy Setup: Simple installation and model management</li> <li>Optimized Format: GGUF format optimized for local inference</li> <li>No Server Required: Direct model execution</li> </ul> <p>Install and use GPT-OSS with Ollama: <pre><code># Install GPT-OSS models\nsuper model install gpt-oss:20b\nsuper model install gpt-oss:120b\n\n# Use in playbooks\nlanguage_model:\n  provider: ollama\n  model: gpt-oss:20b\n  api_base: http://localhost:11434\n</code></pre></p>"},{"location":"llm-setup/#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>\ud83d\udd13 Apache 2.0 License: Build freely without copyleft restrictions</li> <li>\u26a1 Native MXFP4 Quantization: Optimized for efficient inference</li> <li>\ud83c\udf4e Apple Silicon Native: No more mixed precision issues</li> </ul>"},{"location":"llm-setup/#install-gpt-oss-models","title":"\ud83d\udce6 Install GPT-OSS Models","text":""},{"location":"llm-setup/#via-ollama-cross-platform-recommended","title":"Via Ollama (Cross-Platform - RECOMMENDED)","text":"<pre><code># Install GPT-OSS models via Ollama (Best Performance)\nsuper model install gpt-oss:20b\nsuper model install gpt-oss:120b\n\n# Or use direct Ollama commands\nollama pull gpt-oss:20b\nollama pull gpt-oss:120b\n\n# Run with Ollama backend\nsuper model run gpt-oss:20b \"Your prompt\" --backend ollama\n</code></pre>"},{"location":"llm-setup/#via-mlx-lm-apple-silicon-native-support","title":"Via MLX-LM (Apple Silicon - Native Support)","text":"<pre><code># Install GPT-OSS models via Ollama\nsuper model install gpt-oss:20b\nsuper model install gpt-oss:120b\n\n# Or use direct Ollama commands\nollama pull gpt-oss:20b\nollama pull gpt-oss:120b\n\n# Run with Ollama backend\nsuper model run gpt-oss:20b \"Your prompt\" --backend ollama\n</code></pre> Show Ollama Installation Output <pre><code>\ud83d\ude80 SuperOptiX Model Intelligence - Installing gpt-oss:20b\n\ud83e\udd99 Pulling model gpt-oss:20b from Ollama...\n\u23f3 This may take a few minutes depending on your internet connection and model size.\n\npulling manifest \npulling 8f7b3c2a1d4e: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 12.5 GB                         \npulling 9a2b4c6d8e0f: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.2 KB                         \nverifying sha256 digest \nwriting manifest \nsuccess \nModel pulled successfully!\n\n\ud83d\udca1 You can now use it with SuperOptiX:\n  super model dspy ollama/gpt-oss:20b\n\n\ud83d\udcca Model details:\n  \u2022 Size: large\n  \u2022 Task: chat\n  \u2022 Parameters: 21B (3.6B active)\n\n\ud83c\udf89 Installation completed successfully!\n\ud83e\udd99 Ollama running on http://localhost:11434 ready to use with SuperOptiX!\n</code></pre>"},{"location":"llm-setup/#via-huggingface","title":"Via HuggingFace","text":"<pre><code># Install GPT-OSS models via HuggingFace\nsuper model install openai/gpt-oss-20b --backend huggingface\nsuper model install openai/gpt-oss-120b --backend huggingface\n\n# Start HuggingFace server\nsuper model server huggingface openai/gpt-oss-20b --port 8001\nsuper model server huggingface openai/gpt-oss-120b --port 8002\n</code></pre> Show HuggingFace Installation Output <pre><code>\ud83d\ude80 SuperOptiX Model Intelligence - Installing openai/gpt-oss-20b\n\ud83e\udd17 Downloading model from HuggingFace...\n\u23f3 This may take several minutes depending on your internet connection and model size.\n\nDownloading model files...\n  \u2022 config.json: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 2.1 KB\n  \u2022 model.safetensors: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 12.5 GB\n  \u2022 tokenizer.json: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.8 MB\n  \u2022 tokenizer_config.json: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.2 KB\n\nModel downloaded successfully!\n\n\ud83d\udca1 You can now use it with SuperOptiX:\n  super model server huggingface openai/gpt-oss-20b --port 8001\n\n\ud83d\udcca Model details:\n  \u2022 Size: large\n  \u2022 Task: chat\n  \u2022 Parameters: 21B (3.6B active)\n  \u2022 License: Apache 2.0\n\n\ud83c\udf89 Installation completed successfully!\n</code></pre>"},{"location":"llm-setup/#using-gpt-oss-with-superoptix","title":"\ud83c\udfaf Using GPT-OSS with SuperOptiX","text":""},{"location":"llm-setup/#configure-playbook-for-gpt-oss","title":"Configure Playbook for GPT-OSS","text":"<pre><code># Example playbook configuration for GPT-OSS\nlanguage_model:\n  provider: mlx  # or ollama or huggingface\n  model: lmstudio-community/gpt-oss-20b-MLX-8bit  # for MLX-LM\n  # model: gpt-oss:20b  # for Ollama\n  # model: openai/gpt-oss-20b  # for HuggingFace\n  api_base: http://localhost:11434  # for Ollama\n  # api_base: http://localhost:8001  # for HuggingFace\n  temperature: 0.7\n  max_tokens: 2048\n\n# GPT-OSS Language Model Configuration Examples\n\n**\ud83e\udd99 Ollama Backend (Cross-platform - RECOMMENDED):**\n```yaml\nlanguage_model:\n  provider: ollama\n  model: gpt-oss:20b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 4096\n</code></pre> <p>\ud83c\udf4e MLX Backend (Apple Silicon - Native Support): <pre><code>language_model:\n  provider: mlx\n  model: lmstudio-community/gpt-oss-20b-MLX-8bit\n  api_base: http://localhost:8000\n  temperature: 0.7\n  max_tokens: 4096\n</code></pre></p> <p>\ud83e\udd17 HuggingFace Backend (Limited on Apple Silicon): <pre><code>language_model:\n  provider: huggingface\n  model: openai/gpt-oss-20b\n  api_base: http://localhost:8001\n  temperature: 0.7\n  max_tokens: 4096\n</code></pre></p>"},{"location":"llm-setup/#starting-mlx-server-for-gpt-oss","title":"\ud83d\ude80 Starting MLX Server for GPT-OSS","text":"<p>Before using GPT-OSS with MLX in your playbook, start the MLX server:</p> <pre><code># Start MLX server for GPT-OSS model\nsuper model server mlx lmstudio-community/gpt-oss-20b-MLX-8bit --port 8000\n\n# Or start on a different port\nsuper model server mlx lmstudio-community/gpt-oss-20b-MLX-8bit --port 9000\n</code></pre> <p>Server Output: <pre><code>\ud83c\udf4e MLX Local Server\nStarting MLX server for lmstudio-community/gpt-oss-20b-MLX-8bit on port 8000...\n\ud83d\ude80 Starting MLX server...\npython -m mlx_lm.server --model lmstudio-community/gpt-oss-20b-MLX-8bit --port 8000\nMLX server is running on http://localhost:8000\n</code></pre></p> <p>Note: Keep the server running while using GPT-OSS models in your playbooks.</p> <pre><code>#### **Test GPT-OSS Models**\n\n```bash\n# Test with MLX-LM backend (Apple Silicon - Native)\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"Explain quantum computing with detailed reasoning\" --backend mlx\n\n# Test with Ollama backend (Cross-platform - Best Performance)\nsuper model run gpt-oss:20b \"Explain quantum computing with detailed reasoning\" --backend ollama\n\n# Test with HuggingFace backend (Limited on Apple Silicon)\nsuper model run openai/gpt-oss-20b \"Write a Python function to solve the traveling salesman problem\" --backend huggingface\n</code></pre>"},{"location":"llm-setup/#basic-usage-examples","title":"Basic Usage Examples","text":"<pre><code># MLX-LM (Apple Silicon - Native support)\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"What is 2+2?\" --backend mlx\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"Explain machine learning\" --backend mlx\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"Design a distributed system architecture\" --backend mlx\n\n# Ollama (Cross-platform - Best performance)\nsuper model run gpt-oss:20b \"What is 2+2?\" --backend ollama\nsuper model run gpt-oss:20b \"Explain machine learning\" --backend ollama\nsuper model run gpt-oss:20b \"Design a distributed system architecture\" --backend ollama\n</code></pre>"},{"location":"llm-setup/#manage-gpt-oss-models","title":"\ud83d\udccb Manage GPT-OSS Models","text":"<pre><code># List installed GPT-OSS models\nsuper model list | grep gpt-oss\n\n# Get detailed information\nsuper model info gpt-oss:20b\nsuper model info openai/gpt-oss-120b\n\n# Test model performance\nsuper model test gpt-oss:20b \"Hello, how are you?\"\n</code></pre>"},{"location":"llm-setup/#performance-recommendations","title":"\ud83c\udfaf Performance Recommendations","text":"Use Case Recommended Model Hardware Quick responses GPT-OSS-20B 16GB+ RAM Complex tasks GPT-OSS-120B H100 GPU Local development GPT-OSS-20B 16GB+ RAM"},{"location":"llm-setup/#troubleshooting-gpt-oss","title":"\ud83d\udd27 Troubleshooting GPT-OSS","text":"Apple Silicon Mixed Precision Issues <p>Error: <code>error: 'mps.matmul' op detected operation with both F16 and BF16 operands which is not supported</code></p> <p>Solution: ```bash</p> <p>super model run lmstudio-community/gpt-oss-20b-MLX-8bit \"prompt\" --backend mlx</p> <pre><code># Or use Ollama backend (optimized format)\nsuper model run gpt-oss:20b \"prompt\" --backend ollama\n```\n</code></pre> Model Not Found <p>Error: <code>Model not found</code> or <code>Model does not exist</code></p> <p>Solution: ```bash</p> <p>super model install lmstudio-community/gpt-oss-20b-MLX-8bit --backend mlx</p> <pre><code># For Ollama\nollama pull gpt-oss:20b\nollama pull gpt-oss:120b\n\n# For HuggingFace\nsuper model install openai/gpt-oss-20b --backend huggingface\nsuper model install openai/gpt-oss-120b --backend huggingface\n```\n</code></pre> Out of MemoryServer Connection Failed <p>Error: <code>CUDA out of memory</code> or <code>Not enough memory</code></p> <p>Solution: <pre><code># Use smaller model\nsuper model install gpt-oss:20b  # Instead of 120b\n\n# Use CPU inference\nsuper model server huggingface openai/gpt-oss-20b --device cpu\n</code></pre></p> <p>Error: <code>Connection refused</code> or <code>Cannot connect to server</code></p> <p>Solution: <pre><code># Check Ollama server\nollama serve\n\n# Check HuggingFace server\nsuper model server huggingface openai/gpt-oss-20b --port 8001\n</code></pre></p>"},{"location":"llm-setup/#use-mlx-lm-backend-native-apple-silicon-support","title":"Use MLX-LM backend (native Apple Silicon support)","text":""},{"location":"llm-setup/#for-mlx-lm-apple-silicon","title":"For MLX-LM (Apple Silicon)","text":""},{"location":"llm-setup/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>GPT-OSS-120B Model - HuggingFace repository</li> <li>GPT-OSS-20B Model - HuggingFace repository</li> <li>Ollama Library - Ollama model library</li> <li>SuperOptiX Documentation - Complete framework documentation</li> <li>DSPy Framework - Foundation framework</li> </ul>"},{"location":"llm-setup/#mlx-apple-silicon","title":"\ud83c\udf4e MLX (Apple Silicon)","text":"<p>MLX is Apple's native machine learning framework, offering blazing-fast inference on Apple Silicon Macs. MLX-LM v0.26.3 now provides native support for GPT-OSS models!</p> <p>Apple Silicon Only</p> <p>MLX only works on Apple Silicon Macs (M1, M2, M3). If you're on Intel Mac, use Ollama instead.</p>"},{"location":"llm-setup/#setup-mlx","title":"\ud83d\ude80 Setup MLX","text":"<pre><code># Install MLX dependencies\npip install mlx-lm==0.26.3\n\n# Or install with SuperOptiX\npip install \"superoptix[mlx]\"\n</code></pre>"},{"location":"llm-setup/#install-mlx-models","title":"\ud83d\udce6 Install MLX Models","text":"<pre><code># Install GPT-OSS models (native Apple Silicon support)\nsuper model install openai/gpt-oss-20b --backend mlx\nsuper model install openai/gpt-oss-120b --backend mlx\n\n# Install popular MLX models\nsuper model install -b mlx mlx-community/phi-2\nsuper model install -b mlx mlx-community/Llama-3.2-3B-Instruct-4bit\nsuper model install -b mlx mlx-community/Mistral-7B-Instruct-v0.2-4bit\nsuper model install -b mlx lmstudio-community/gpt-oss-20b-MLX-8bit\n</code></pre>"},{"location":"llm-setup/#start-mlx-servers","title":"\ud83d\udda5\ufe0f Start MLX Servers","text":"<pre><code># Start MLX server on specific port\nsuper model server mlx phi-2 --port 8000\nsuper model server mlx mlx-community/Llama-3.2-3B-Instruct-4bit --port 8000\n</code></pre> <p>Example Output: <pre><code>\ud83c\udf4e MLX Local Server\nStarting MLX server for mlx-community_Llama-3.2-3B-Instruct-4bit on port 8000...\n\ud83d\ude80 Starting MLX server...\n\ud83d\udce1 Server will be available at: http://localhost:8000\n\ud83d\udca1 Use this URL in your playbook's api_base configuration\n\ud83d\udd27 Manual server startup command:\n   python -m mlx_lm.server --model mlx-community_Llama-3.2-3B-Instruct-4bit --port 8000\n\ud83d\udccb Example playbook configuration:\n   language_model:\n     provider: mlx\n     model: mlx-community_Llama-3.2-3B-Instruct-4bit\n     api_base: http://localhost:8000\n\ud83d\udd04 Executing: /path/to/python -m mlx_lm.server --model mlx-community_Llama-3.2-3B-Instruct-4bit --port 8000\n\u23f3 Server is starting... (Press Ctrl+C to stop)\n</code></pre></p>"},{"location":"llm-setup/#manage-mlx-models","title":"\ud83d\udccb Manage MLX Models","text":"<pre><code># List MLX models\nsuper model list --backend mlx\n</code></pre> <p>Example Output: <pre><code>                    \ud83d\ude80 SuperOptiX Model Intelligence - 1 models                     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                    \u2503 Backend \u2503    Status    \u2503 Size  \u2503 Task \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 mlx-community_Llama-3.2-3B-Instruct-4bit \u2502 \ud83c\udf4e mlx  \u2502 installed \u2502 small \u2502 chat \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Get model information\nsuper model info mlx-community/phi-2\nsuper model info mlx-community_Llama-3.2-3B-Instruct-4bit\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#lm-studio","title":"\ud83c\udfae LM Studio","text":"<p>LM Studio provides a user-friendly interface for running local models, especially popular on Windows.</p>"},{"location":"llm-setup/#setup-lm-studio","title":"\ud83d\ude80 Setup LM Studio","text":"<ol> <li>Download LM Studio from https://lmstudio.ai</li> <li>Install and launch LM Studio</li> <li>Download a model through the interface</li> <li>Start the server (default port: 1234)</li> </ol>"},{"location":"llm-setup/#install-models-with-superoptix_1","title":"\ud83d\udce6 Install Models with SuperOptiX","text":"<pre><code># Install models (use the name from LM Studio)\nsuper model install -b lmstudio llama-3.2-1b-instruct\nsuper model install -b lmstudio llama-3.2-3b\nsuper model install -b lmstudio your-model-name\n</code></pre>"},{"location":"llm-setup/#start-lm-studio-servers","title":"\ud83d\udda5\ufe0f Start LM Studio Servers","text":"<pre><code># Start server with specific model\nsuper model server lmstudio llama-3.2-1b-instruct --port 1234\nsuper model server lmstudio llama-3.2-3b --port 1234\n</code></pre> <p>Example Output: <pre><code>\ud83c\udfae LM Studio Local Server\nStarting LM Studio server for llama-3.2-1b-instruct on port 1234...\n\ud83d\ude80 Starting LM Studio server...\n\ud83d\udce1 Server will be available at: http://localhost:1234\n\ud83d\udca1 Use this URL in your playbook's api_base configuration\n\ud83d\udd27 Manual server startup command:\n   # Start server in LM Studio app first, then connect\n\ud83d\udccb Example playbook configuration:\n   language_model:\n     provider: lmstudio\n     model: llama-3.2-1b-instruct\n     api_base: http://localhost:1234\n\u23f3 Server is starting... (Press Ctrl+C to stop)\n</code></pre></p>"},{"location":"llm-setup/#manage-lm-studio-models","title":"\ud83d\udccb Manage LM Studio Models","text":"<pre><code># List LM Studio models\nsuper model list --backend lmstudio\n</code></pre> <p>Example Output: <pre><code>                  \ud83d\ude80 SuperOptiX Model Intelligence - 3 models                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                          \u2503   Backend   \u2503    Status    \u2503  Size  \u2503 Task \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama-3.2-1b-instruct          \u2502 \ud83c\udfae lmstudio \u2502 installed \u2502 small  \u2502 chat \u2502\n\u2502 llama-3.3-70b-instruct         \u2502 \ud83c\udfae lmstudio \u2502 installed \u2502 large  \u2502 chat \u2502\n\u2502 llama-4-scout-17b-16e-instruct \u2502 \ud83c\udfae lmstudio \u2502 installed \u2502 medium \u2502 chat \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Get model information\nsuper model info llama-3.2-1b-instruct\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#huggingface","title":"\ud83e\udd17 HuggingFace","text":"<p>HuggingFace offers access to thousands of models, perfect for advanced users who want maximum flexibility.</p>"},{"location":"llm-setup/#setup-huggingface","title":"\ud83d\ude80 Setup HuggingFace","text":"<pre><code># Install HuggingFace dependencies\npip install transformers torch fastapi uvicorn\n\n# Or install with SuperOptiX\npip install \"superoptix[huggingface]\"\n</code></pre>"},{"location":"llm-setup/#install-huggingface-models","title":"\ud83d\udce6 Install HuggingFace Models","text":"<pre><code># Install popular models\nsuper model install -b huggingface microsoft/Phi-4\nsuper model install -b huggingface microsoft/DialoGPT-small\nsuper model install -b huggingface microsoft/DialoGPT-medium\nsuper model install -b huggingface meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"llm-setup/#start-huggingface-servers","title":"\ud83d\udda5\ufe0f Start HuggingFace Servers","text":"<pre><code># Start server with specific model\nsuper model server huggingface microsoft/Phi-4 --port 8001\nsuper model server huggingface microsoft/DialoGPT-small --port 8001\nsuper model server huggingface microsoft/DialoGPT-medium --port 8001\n</code></pre> <p>Example Output: <pre><code>\ud83e\udd17 HuggingFace Local Server\nStarting HuggingFace server for microsoft/DialoGPT-small on port 8002...\n\ud83d\ude80 Starting HuggingFace server...\n\ud83d\udce1 Server will be available at: http://localhost:8002\n\ud83d\udca1 Use this URL in your playbook's api_base configuration\n\ud83d\udd27 Manual server startup command:\n   python -m superoptix.models.backends.huggingface_server microsoft/DialoGPT-small --port 8002\n\ud83d\udccb Example playbook configuration:\n   language_model:\n     provider: huggingface\n     model: microsoft/DialoGPT-small\n     api_base: http://localhost:8002\nDevice set to use cpu\nINFO:     Started server process [4652]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)\n</code></pre></p>"},{"location":"llm-setup/#manage-huggingface-models","title":"\ud83d\udccb Manage HuggingFace Models","text":"<pre><code># List HuggingFace models\nsuper model list --backend huggingface\n</code></pre> <p>Example Output: <pre><code>                \ud83d\ude80 SuperOptiX Model Intelligence - 2 models                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                    \u2503    Backend     \u2503    Status    \u2503 Size  \u2503 Task \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 microsoft/DialoGPT-small \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502 small \u2502 chat \u2502\n\u2502 microsoft/Phi-4          \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502 small \u2502 chat \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Get model information\nsuper model info microsoft/Phi-4\nsuper model info microsoft/DialoGPT-small\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#model-management-commands","title":"\ud83c\udfaf Model Management Commands","text":""},{"location":"llm-setup/#server-commands","title":"\ud83d\udda5\ufe0f Server Commands","text":"<pre><code># Get help for server commands\nsuper model server --help\n</code></pre> <p>Example Output: <pre><code>usage: super model server [-h] [--port PORT] {mlx,huggingface,lmstudio} model_name\n\n\ud83d\ude80 Start local model servers for MLX, HuggingFace, or LM Studio. Examples: \nsuper model server mlx mlx-community/Llama-3.2-3B-Instruct-4bit \nsuper model server huggingface microsoft/DialoGPT-small --port 8001\nsuper model server lmstudio llama-3.2-1b-instruct \n\nBackends: \nmlx Apple Silicon optimized (default: port 8000) \nhuggingface Transformers models (default: port 8001) \nlmstudio Desktop app models (default: port 1234) \n\nNote: Ollama servers use 'ollama serve' command separately.\n\npositional arguments:\n  {mlx,huggingface,lmstudio}  Backend type\n  model_name                   Model name to start server for\n\noptions:\n  -h, --help                   show this help message and exit\n  --port PORT, -p PORT         Port to run server on\n</code></pre></p>"},{"location":"llm-setup/#list-and-explore-models","title":"\ud83d\udccb List and Explore Models","text":"<pre><code># List all installed models\nsuper model list\n</code></pre> <p>Example Output: <pre><code>                           \ud83d\ude80 SuperOptiX Model Intelligence - 9 models                   \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                    \u2503    Backend     \u2503    Status    \u2503  Size   \u2503   Task    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama-3.2-1b-instruct                    \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 llama-3.3-70b-instruct                   \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502  large  \u2502   chat    \u2502\n\u2502 llama-4-scout-17b-16e-instruct           \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502 medium  \u2502   chat    \u2502\n\u2502 llama3.1:8b                              \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502 medium  \u2502   chat    \u2502\n\u2502 llama3.2:1b                              \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502  tiny   \u2502   chat    \u2502\n\u2502 microsoft/DialoGPT-small                 \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 microsoft/Phi-4                          \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 mlx-community_Llama-3.2-3B-Instruct-4bit \u2502     \ud83c\udf4e mlx     \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 nomic-embed-text:latest                  \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502 Unknown \u2502 embedding \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udd0d Discover more models: super model discover\n\ud83d\udce5 Install a model: super model install &lt;model_name&gt;\n</code></pre></p> <pre><code># List all available models (including uninstalled)\nsuper model list --all\n\n# Filter by backend\nsuper model list --backend ollama\nsuper model list --backend mlx\nsuper model list --backend lmstudio\nsuper model list --backend huggingface\n\n# Verbose information\nsuper model list --verbose\n</code></pre>"},{"location":"llm-setup/#get-model-information","title":"\ud83d\udcca Get Model Information","text":"<pre><code># Get detailed model info\nsuper model info llama3.2:3b\nsuper model info mlx-community/phi-2\nsuper model info microsoft/Phi-4\nsuper model info llama-3.2-1b-instruct\n</code></pre>"},{"location":"llm-setup/#choose-your-setup","title":"\ud83c\udfaf Choose Your Setup","text":""},{"location":"llm-setup/#beginner-recommended","title":"\ud83d\ude80 Beginner (Recommended)","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Install SuperOptiX\npip install superoptix\n\n# Install a model\nsuper model install llama3.2:3b\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#apple-silicon-user","title":"\ud83c\udf4e Apple Silicon User","text":"<pre><code># Install MLX dependencies\npip install mlx-lm\n\n# Install SuperOptiX\npip install superoptix\n\n# Install MLX model\nsuper model install -b mlx mlx-community/phi-2\n\n# Start server\nsuper model server mlx phi-2 --port 8000\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#windows-user","title":"\ud83c\udfae Windows User","text":"<pre><code># Install LM Studio from https://lmstudio.ai\n# Download a model in LM Studio\n# Start server in LM Studio\n\n# Install SuperOptiX\npip install superoptix\n\n# Connect to LM Studio\nsuper model server lmstudio your-model-name --port 1234\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#advanced-user","title":"\ud83e\udd17 Advanced User","text":"<pre><code># Install HuggingFace dependencies\npip install transformers torch fastapi uvicorn\n\n# Install SuperOptiX\npip install superoptix\n\n# Install HuggingFace model\nsuper model install -b huggingface microsoft/Phi-4\n\n# Start server\nsuper model server huggingface microsoft/Phi-4 --port 8001\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"llm-setup/#multiple-servers","title":"\ud83c\udf10 Multiple Servers","text":"<p>Run multiple models simultaneously:</p> <pre><code># Terminal 1: Ollama model\n# Models are ready to use with SuperOptiX agents\n\n# Terminal 2: MLX model (Apple Silicon)\nsuper model server mlx phi-2 --port 8000\n# Models are ready to use with SuperOptiX agents\n\n# Terminal 3: HuggingFace model\nsuper model server huggingface microsoft/Phi-4 --port 8001\n# Models are ready to use with SuperOptiX agents\n\n# Terminal 4: LM Studio model\nsuper model server lmstudio llama-3.2-1b-instruct --port 1234\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"llm-setup/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"llm-setup/#common-issues","title":"Common Issues","text":"Model Not FoundServer Connection FailedPort Already in UseApple Silicon RequiredMissing Python PackagesMissing CLI ToolsAuthentication Errors <p>Error: <code>Model not found</code> or <code>Model does not exist</code></p> <p>Solution: <pre><code># Check available models\nsuper model list --all\n\n# Use correct model name\nsuper model install llama3.2:3b  # Correct\nsuper model install llama3.2     # Wrong\n</code></pre></p> <p>Error: <code>Connection refused</code> or <code>Cannot connect to server</code></p> <p>Solution: <pre><code># Check if server is running\n# For Ollama: ollama serve\n# For MLX: super model server mlx phi-2 --port 8000\n# For LM Studio: Start in LM Studio app\n# For HuggingFace: super model server huggingface model --port 8001\n</code></pre></p> <p>Error: <code>Address already in use</code></p> <p>Solution: <pre><code># Use different port\nsuper model server mlx phi-2 --port 8001\nsuper model server huggingface microsoft/Phi-4 --port 8002\n</code></pre></p> <p>Error: <code>MLX requires Apple Silicon</code></p> <p>Solution: <pre><code># Use Ollama instead\nsuper model install llama3.2:3b\nsuper model dspy ollama/llama3.2:3b\n</code></pre></p> <p>Error: <code>ModuleNotFoundError: No module named 'mlx_lm'</code> or <code>ModuleNotFoundError: No module named 'transformers'</code></p> <p>Solution: <pre><code># Install MLX dependencies\npip install mlx-lm\n\n# Install HuggingFace dependencies\npip install transformers torch fastapi uvicorn\n\n# Or install with SuperOptiX extras\npip install \"superoptix[mlx]\"\npip install \"superoptix[huggingface]\"\n</code></pre></p> <p>Error: <code>Command 'ollama' not found</code> or <code>Command 'lms' not found</code></p> <p>Solution: <pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Install LM Studio\n# Download from https://lmstudio.ai\n# Or use winget on Windows:\nwinget install LMStudio.LMStudio\n\n# Verify installation\nollama --version\nlms --version\n</code></pre></p> <p>Error: <code>401 Unauthorized</code> or <code>Repository Not Found</code></p> <p>Solution: <pre><code># For HuggingFace models, login:\nhuggingface-cli login\n\n# For MLX models, ensure you have access:\n# Some models require accepting terms on HuggingFace website\n\n# Use public models instead:\nsuper model install -b mlx mlx-community/phi-2\nsuper model install -b huggingface microsoft/Phi-4\n</code></pre></p>"},{"location":"llm-setup/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Now that you have your local models set up:</p> <ol> <li>\ud83d\ude80 Quick Start Guide - Build your first agent with local models</li> <li>\ud83e\udd16 Create Your First Genies Agent - Step-by-step tutorial</li> <li>\ud83c\udfea Marketplace - Discover pre-built agents</li> <li>\ud83d\udd0d Model Intelligence Guide - Advanced model management</li> </ol>"},{"location":"llm-setup/#need-help","title":"\ud83d\udcac Need Help?\ud83e\udd16 Ready to Run Local Models?","text":"<ul> <li>\ud83d\udcd6 Documentation - Comprehensive guides</li> <li>\ud83d\udc1b Support Portal - Report bugs</li> </ul>"},{"location":"llm-setup/#huggingface_1","title":"\ud83e\udd17 HuggingFace","text":"<p>HuggingFace offers access to thousands of models, perfect for advanced users who want maximum flexibility.</p>"},{"location":"llm-setup/#setup-huggingface_1","title":"\ud83d\ude80 Setup HuggingFace","text":"<pre><code># Install HuggingFace dependencies\npip install transformers torch fastapi uvicorn\n\n# Or install with SuperOptiX\npip install \"superoptix[huggingface]\"\n</code></pre>"},{"location":"llm-setup/#install-huggingface-models_1","title":"\ud83d\udce6 Install HuggingFace Models","text":"<pre><code># Install popular models\nsuper model install -b huggingface microsoft/Phi-4\nsuper model install -b huggingface microsoft/DialoGPT-small\nsuper model install -b huggingface microsoft/DialoGPT-medium\nsuper model install -b huggingface meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"llm-setup/#start-huggingface-servers_1","title":"\ud83d\udda5\ufe0f Start HuggingFace Servers","text":"<pre><code># Start server with specific model\nsuper model server huggingface microsoft/Phi-4 --port 8001\nsuper model server huggingface microsoft/DialoGPT-small --port 8001\nsuper model server huggingface microsoft/DialoGPT-medium --port 8001\n</code></pre> <p>Example Output: <pre><code>\ud83e\udd17 HuggingFace Local Server\nStarting HuggingFace server for microsoft/DialoGPT-small on port 8002...\n\ud83d\ude80 Starting HuggingFace server...\n\ud83d\udce1 Server will be available at: http://localhost:8002\n\ud83d\udca1 Use this URL in your playbook's api_base configuration\n\ud83d\udd27 Manual server startup command:\n   python -m superoptix.models.backends.huggingface_server microsoft/DialoGPT-small --port 8002\n\ud83d\udccb Example playbook configuration:\n   language_model:\n     provider: huggingface\n     model: microsoft/DialoGPT-small\n     api_base: http://localhost:8002\nDevice set to use cpu\nINFO:     Started server process [4652]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)\n</code></pre></p>"},{"location":"llm-setup/#manage-huggingface-models_1","title":"\ud83d\udccb Manage HuggingFace Models","text":"<pre><code># List HuggingFace models\nsuper model list --backend huggingface\n</code></pre> <p>Example Output: <pre><code>                \ud83d\ude80 SuperOptiX Model Intelligence - 2 models                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                    \u2503    Backend     \u2503    Status    \u2503 Size  \u2503 Task \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 microsoft/DialoGPT-small \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502 small \u2502 chat \u2502\n\u2502 microsoft/Phi-4          \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502 small \u2502 chat \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Get model information\nsuper model info microsoft/Phi-4\nsuper model info microsoft/DialoGPT-small\n\n# Models are ready to use with SuperOptiX agents\n</code></pre>"},{"location":"mission-superoptix/","title":"\ud83c\udfaf Mission SuperOptiX","text":"<p>As a full-stack agentic AI company, Superagentic AI is building SuperOptiX - a comprehensive framework designed to create production-worthy AI agents that embody our vision for the future of autonomous AI systems.</p>"},{"location":"mission-superoptix/#our-mission","title":"\ud83c\udfd7\ufe0f Our Mission","text":"<p>SuperOptiX is more than just another AI framework - it's the embodiment of our commitment to building a safer, more intelligent, and more collaborative AI future. Our mission is to provide developers and organizations with the tools they need to create AI agents that are:</p> <ul> <li>\ud83d\udd12 Production-Ready - Built for real-world deployment at scale</li> <li>\u26a1 Performance-Optimized - Designed with optimization as a first-class concern</li> <li>\ud83d\udee1\ufe0f Secure &amp; Safe - Enterprise-grade security and safety measures</li> <li>\ud83e\udd1d Human-Collaborative - Designed for seamless human-AI interaction</li> <li>\ud83c\udf10 Scalable - Ready to grow with your organization's needs</li> </ul>"},{"location":"mission-superoptix/#superoptix-features","title":"\ud83d\ude80 SuperOptiX Features","text":"<p>SuperOptiX is built on a framework architecture that progressively implements our foundational pillars, with the first two capabilities now available as core components.</p>"},{"location":"mission-superoptix/#superspec-dsl","title":"\ud83d\udc8e SuperSpec DSL","text":"<ul> <li>Declarative Agent Specification - Define agents using our intuitive domain-specific language</li> <li>Rapid Development - Build complex agents in minutes, not days</li> <li>Type Safety - Built-in validation and error checking</li> <li>Extensible - Custom extensions and plugins support</li> </ul>"},{"location":"mission-superoptix/#agent-pipelines","title":"\ud83c\udfd7\ufe0f Agent Pipelines","text":"<ul> <li>Automated Workflows - Streamlined development and deployment processes</li> <li>Custom Agent Building - Full developer control over agent behavior</li> <li>Secure Coding Practices - Built-in security and validation frameworks</li> <li>CI/CD Integration - Seamless integration with existing development workflows</li> </ul>"},{"location":"mission-superoptix/#memory-systems","title":"\ud83e\udde0 Memory Systems","text":"<ul> <li>Multi-Modal Memory - Support for various memory backends (ChromaDB, LanceDB, Weaviate, Qdrant, Milvus)</li> <li>Context Management - Intelligent context switching and management</li> <li>Long-term Learning - Persistent memory across sessions</li> <li>Memory Optimization - Efficient storage and retrieval mechanisms</li> </ul>"},{"location":"mission-superoptix/#rag-integration","title":"\ud83d\udd0d RAG Integration","text":"<ul> <li>Vector Database Support - Native integration with leading vector databases</li> <li>Document Processing - Automated document ingestion and processing</li> <li>Semantic Search - Advanced search capabilities with context awareness</li> <li>Knowledge Management - Centralized knowledge base management</li> </ul>"},{"location":"mission-superoptix/#tool-ecosystem","title":"\ud83d\udee0\ufe0f Tool Ecosystem","text":"<ul> <li>Built-in Tools - Comprehensive set of pre-built tools for common tasks</li> <li>Custom Tool Development - Easy creation of custom tools and integrations</li> <li>Tool Orchestration - Intelligent tool selection and execution</li> <li>API Integration - Seamless integration with external APIs and services</li> </ul>"},{"location":"mission-superoptix/#observability","title":"\ud83d\udcca Observability","text":"<ul> <li>Real-time Monitoring - Live monitoring of agent performance and behavior</li> <li>Debugging Tools - Advanced debugging and troubleshooting capabilities</li> <li>Performance Analytics - Detailed performance metrics and insights</li> <li>Traceability - Complete audit trail of agent decisions and actions</li> </ul>"},{"location":"mission-superoptix/#multi-agent-orchestration","title":"\ud83c\udfbc Multi-Agent Orchestration","text":"<ul> <li>Agent Teams - Coordinate multiple agents working together</li> <li>Workflow Management - Complex workflow orchestration and management</li> <li>Resource Optimization - Intelligent resource allocation and management</li> <li>Scalable Architecture - Designed for large-scale multi-agent systems</li> </ul>"},{"location":"mission-superoptix/#how-superoptix-embodies-our-pillars","title":"\ud83c\udfdb\ufe0f How SuperOptiX Embodies Our Pillars","text":"<p>SuperOptiX implements our foundational pillars through a framework architecture, with Oracles and Genies current capabilities, embodying Context Engineering and Agent Engineering principles respectively.</p>"},{"location":"mission-superoptix/#agent-engineering","title":"\u2699\ufe0f Agent Engineering","text":"<p>SuperOptiX provides comprehensive agent engineering capabilities that align with our Agent Engineering pillar:</p> <p>Context Engineering via SuperSpec: - No-Code/Low-Code Development - Business users can define agent behavior without coding - Declarative Specifications - Clear, readable agent definitions - Rapid Prototyping - Quick iteration and testing of agent concepts</p> <p>Agent Engineering via SuperSpec + Agent Pipelines: - Developer Control - Full programmatic control over agent behavior - Custom Logic - Implementation of complex business logic and workflows - Integration Capabilities - Seamless integration with existing systems and APIs - Security &amp; Validation - Built-in security measures and validation frameworks</p>"},{"location":"mission-superoptix/#agent-experience-agentex","title":"\ud83c\udfaf Agent Experience (AgentEx)","text":"<p>SuperOptiX delivers pure-class agent experience through:</p> <p>Protocol Layers: - MCP Integration - Model Context Protocol for standardized agent communication - A2A Protocols - Agent-to-Agent communication protocols - Upcoming Protocols - Support for emerging agent communication standards</p> <p>Multi-Agentic Systems: - Seamless Communication - Agents can communicate and collaborate effectively - Resource Sharing - Intelligent sharing of resources and information - Coordinated Actions - Synchronized actions across multiple agents</p> <p>Optimized Interfaces: - Purpose-Built Environments - Interfaces designed specifically for AI agents - Performance Optimization - Optimized for speed and efficiency - Context Awareness - Agents receive relevant information when needed</p>"},{"location":"mission-superoptix/#agentic-co-intelligence","title":"\ud83e\udd1d Agentic Co-Intelligence","text":"<p>SuperOptiX implements our Agentic Co-Intelligence vision through:</p> <p>Human-in-the-Loop: - Upper-Tier Controls - Human oversight and control mechanisms - Approval Workflows - Human approval for critical decisions - Intervention Capabilities - Ability to intervene when needed</p> <p>Behavioral Control: - Human-Defined Criteria - Behavior rules defined by humans - Ethical Guidelines - Built-in ethical considerations and constraints - Transparency - Clear visibility into agent decision-making processes</p> <p>Collaborative Workflows: - Seamless Interaction - Natural interaction between humans and agents - Context Sharing - Shared understanding of goals and constraints - Learning from Humans - Agents learn and adapt based on human feedback</p>"},{"location":"mission-superoptix/#key-differentiators","title":"\ud83c\udf1f Key Differentiators","text":""},{"location":"mission-superoptix/#optimization-first-approach","title":"\ud83c\udfaf Optimization-First Approach","text":"<p>Unlike other frameworks that focus primarily on functionality, SuperOptiX is built with optimization as a core principle from the ground up.</p>"},{"location":"mission-superoptix/#full-stack-solution","title":"\ud83c\udfd7\ufe0f Full-Stack Solution","text":"<p>SuperOptiX provides everything needed to build, deploy, and manage AI agents - from development tools to production infrastructure.</p>"},{"location":"mission-superoptix/#enterprise-grade-security","title":"\ud83d\udd12 Enterprise-Grade Security","text":"<p>Built with security and safety as fundamental principles, not afterthoughts.</p>"},{"location":"mission-superoptix/#human-centric-design","title":"\ud83e\udd1d Human-Centric Design","text":"<p>Designed for human-AI collaboration, not just autonomous operation.</p>"},{"location":"mission-superoptix/#scalable-architecture","title":"\ud83c\udf10 Scalable Architecture","text":"<p>Built to scale from development to enterprise deployment.</p>"},{"location":"mission-superoptix/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Ready to build the future of AI agents? Get started with SuperOptiX:</p> <ol> <li>\ud83d\udce6 Installation - Quick setup and installation</li> <li>\ud83d\ude80 Quick Start - Build your first agent in minutes</li> <li>\ud83d\udcda Documentation - Comprehensive guides and tutorials</li> <li>\ud83d\udc8e SuperSpec - Learn our declarative agent specification language</li> <li>\ud83d\udee0\ufe0f Tools &amp; Integrations - Explore our tool ecosystem</li> </ol>"},{"location":"mission-superoptix/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Superagentic AI Blog - Latest insights and updates</li> <li>Agentic Co-Intelligence - Our approach to human-AI collaboration</li> <li>Agent Experience - AgentEx principles and implementation</li> <li>Agent Engineering - Our engineering methodologies</li> </ul> <p>SuperOptiX represents our commitment to building a future where AI agents are not just tools, but intelligent collaborators that work seamlessly with humans to solve complex problems and create value. Join us in building the next generation of autonomous AI systems. </p>"},{"location":"project-structure/","title":"\ud83c\udfd7\ufe0f Project Structure Guide","text":"<p>This guide shows you how to create and explore a real SuperOptiX Agentic System project using the CLI, and explains the meaning of each directory and file.</p>"},{"location":"project-structure/#step-1-initialize-your-agentic-system","title":"\ud83d\ude80 Step 1: Initialize Your Agentic System","text":"<p>To start, create a new Agentic System project using the <code>super init</code> command. For example, to create a software engineering system called <code>swe</code>:</p> <pre><code>super init swe\n</code></pre> <p>You\u2019ll see a message confirming your project is ready.</p>"},{"location":"project-structure/#step-2-explore-the-project-structure","title":"\ud83d\udcc2 Step 2: Explore the Project Structure","text":"<p>Change into your new project directory:</p> <pre><code>cd swe\nls -la\n</code></pre> <p>You\u2019ll see output like:</p> <pre><code>total 48\ndrwxr-xr-x@ 10 user  staff   320 ... ./\ndrwxr-xr-x  ... ../\n-rw-r--r--@  1 user  staff   ... .env\n-rw-r--r--@  1 user  staff   ... .gitignore\n-rw-r--r--@  1 user  staff   ... .pre-commit-config.yaml\n-rw-r--r--@  1 user  staff   ... .super\n-rw-r--r--@  1 user  staff   ... README.md\n-rw-r--r--@  1 user  staff   ... pyproject.toml\ndrwxr-xr-x@ 12 user  staff   ... swe/\ndrwxr-xr-x@  5 user  staff   ... tests/\n</code></pre> <p>Key files and folders:</p> <ul> <li> <p>\ud83d\udcc4 <code>.super</code> - This file marks the root of your Agentic System. Always run <code>super</code> commands from this directory.</p> </li> <li> <p>\u2699\ufe0f <code>pyproject.toml</code> - Python package configuration for your agentic system.</p> </li> <li> <p>\ud83d\udcd6 <code>README.md</code> - Project overview and documentation.</p> </li> <li> <p>\ud83d\udce6 <code>swe/</code> - Main Python package for your agentic modules and logic.</p> </li> <li> <p>\ud83e\uddea <code>tests/</code> - Place your tests here.</p> </li> </ul>"},{"location":"project-structure/#step-3-explore-the-agentic-modules","title":"\ud83e\udde9 Step 3: Explore the Agentic Modules","text":"<p>List the contents of the main package directory:</p> <pre><code>ls -la swe/\n</code></pre> <p>You\u2019ll see subdirectories for each agentic module:</p> <pre><code>agents/     guardrails/ memory/     protocols/  teams/\nevals/      knowledge/  optimizers/ servers/    tools/\n</code></pre> <p>Directory meanings:</p> <ul> <li> <p>\ud83e\udd16 <code>agents/</code> - Each agent lives in its own subdirectory here. Agent playbooks, pipelines, and optimized pipelines are stored here.</p> </li> <li> <p>\ud83d\udee1\ufe0f <code>guardrails/</code> - Guardrails for safety, validation, and compliance.</p> </li> <li> <p>\ud83e\udde0 <code>memory/</code> - Memory modules for your agents.</p> </li> <li> <p>\ud83d\udce1 <code>protocols/</code> - Communication and orchestration protocols.</p> </li> <li> <p>\ud83d\udc65 <code>teams/</code> - Multi-agent team configurations.</p> </li> <li> <p><code>evals/</code> - Evaluation scenarios and test cases.</p> </li> <li> <p>\ud83d\udcda <code>knowledge/</code> - Knowledge bases and data sources.</p> </li> <li> <p>\u26a1 <code>optimizers/</code> - Optimization strategies and modules.</p> </li> <li> <p>\ud83c\udf10 <code>servers/</code> - Server and API integration code.</p> </li> <li> <p>\ud83d\udd27 <code>tools/</code> - Custom tools and utilities for your agents.</p> </li> </ul>"},{"location":"project-structure/#dependencies-and-extras","title":"\ud83d\udce6 Dependencies and Extras","text":"<p>SuperOptiX supports various optional dependencies through extras. You can install specific functionality as needed:</p> <pre><code># Core AI framework (DSPy, OpenAI, AutoGen)\npip install \"superoptix[optimas]\"\n\n# UI and visualization\npip install \"superoptix[ui]\"\n\n# Vector databases for RAG\npip install \"superoptix[vectordb]\"\n\n# Observability and monitoring\npip install \"superoptix[observability]\"\n</code></pre>"},{"location":"project-structure/#important-crewai-dependency-conflict","title":"\u26a0\ufe0f Important: CrewAI Dependency Conflict","text":"<p>CrewAI has a known dependency conflict with SuperOptiX due to incompatible <code>json-repair</code> version requirements:</p> <ul> <li>DSPy 3.0.0 requires <code>json-repair&gt;=0.30.0</code></li> <li>CrewAI 0.157.0 requires <code>json-repair==0.25.2</code></li> </ul> <p>To use CrewAI with SuperOptiX, install it manually: <pre><code># Install SuperOptiX with DSPy support\npip install \"superoptix[optimas]\"\n\n# Install CrewAI without dependencies\npip install crewai==0.157.0 --no-deps\n\n# Ensure compatible json-repair version\npip install \"json-repair&gt;=0.30.0\"\n</code></pre></p> <p>See our Troubleshooting Guide for more details.</p>"},{"location":"project-structure/#step-4-pull-and-compile-an-agent","title":"\ud83c\udff7\ufe0f Step 4: Pull and Compile an Agent","text":"<p>Let\u2019s add a pre-built agent and see what files are created.</p> <pre><code>super agent pull developer\n</code></pre> <p>This creates a new agent directory structure:</p> <p>\ud83d\udcc1 <code>swe/agents/developer/</code> - \ud83d\udccb <code>playbook/</code> - Contains the agent's configuration files   - \ud83d\udcc4 <code>developer_playbook.yaml</code> - Agent definition and configuration</p> <p>Now compile the agent:</p> <pre><code>super agent compile developer\n</code></pre> <p>This generates a pipeline structure:</p> <p>\ud83d\udcc1 <code>swe/agents/developer/</code> - \ud83d\udccb <code>playbook/</code> - Agent configuration files   - \ud83d\udcc4 <code>developer_playbook.yaml</code> - Agent definition - \u2699\ufe0f <code>pipelines/</code> - Generated pipeline files   - \ud83d\udc0d <code>developer_pipeline.py</code> - Executable agent pipeline</p>"},{"location":"project-structure/#step-5-explore-agent-files","title":"\ud83d\udcdc Step 5: Explore Agent Files","text":"<p>Agent Playbook: <code>swe/agents/developer/playbook/developer_playbook.yaml</code> This YAML file defines the agent\u2019s persona, tasks, evaluation scenarios, and optimization strategy.</p> <p>Agent Pipeline: <code>swe/agents/developer/pipelines/developer_pipeline.py</code> This Python file is an auto-generated, executable pipeline for the agent, ready for further customization.</p>"},{"location":"project-structure/#example-playbook-and-pipeline","title":"\ud83d\udcdd Example: Playbook and Pipeline","text":"<p>Playbook (YAML): <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Developer Assistant\n  id: developer\n  ...\nspec:\n  language_model:\n    provider: ollama\n    model: llama3.2:1b\n    api_base: http://localhost:11434\n  persona:\n    name: DevBot\n    role: Software Developer\n    goal: Write clean, efficient, and maintainable code\n  ...\n</code></pre></p> <p>Pipeline (Python): <pre><code>class DeveloperPipeline(\n    TracingMixin,\n    ModelSetupMixin, \n    ToolsMixin,\n    BDDTestMixin,\n    UsageTrackingMixin,\n    EvaluationMixin\n):\n    ...\n    def __init__(self):\n        ...\n        self.module = DeveloperModule()\n        ...\n</code></pre></p>"},{"location":"project-structure/#tips","title":"\ud83d\udca1 Tips","text":"<ul> <li>All <code>super</code> CLI commands (e.g., <code>super agent</code>, <code>super orchestra</code>, <code>super spec</code>) must be run from the root directory containing the <code>.super</code> file.</li> <li>Each agent\u2019s logic, playbooks, and pipelines are isolated in their own subdirectories under <code>agents/</code>.</li> <li>The project is a standard Python package - you can ship and reuse it in other Agentic Systems. </li> </ul>"},{"location":"quick-start/","title":"\ud83d\ude80 Quick Start Guide: Two Paths to SuperOptiX Mastery","text":"**Two hands-on experiences that highlight the SuperOptiX workflow**   <p>Choose Your Path</p> <p>Part 1 \u2013 Sentiment Analyzer Demo: A lightweight project that walks through evaluation and GEPA optimization in minutes.</p> <p>Part 2 \u2013 SWE Orchestration: A full multi-agent software engineering workflow that showcases the orchestration features.</p> <p>Getting Started</p> <p>You can complete Part 1 on its own, then move on to Part 2 when you're ready to build larger teams.</p>"},{"location":"quick-start/#requirements","title":"\ud83d\udccb Requirements","text":""},{"location":"quick-start/#hardware","title":"\ud83d\udda5\ufe0f Hardware","text":"Component Requirement GPU RAM 16 GB recommended if you plan to run GEPA optimization System RAM 8 GB+ for smooth execution"},{"location":"quick-start/#software","title":"\ud83d\udc0d Software","text":"Software Version/Details Python 3.11 or higher SuperOptiX Install via uv (recommended) or pip Ollama For local LLMs (alternatives like MLX or Hugging Face also work) <p>Install Ollama (if needed):</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"quick-start/#install-superoptix","title":"\ud83d\udd27 Install SuperOptiX","text":"<p>We recommend using <code>uv</code> for fast, reliable installation.</p> uv (Recommended)pip <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nuv tool install superoptix\nsuper --version\n</code></pre> <pre><code>pip install superoptix\nsuper --version\n</code></pre>"},{"location":"quick-start/#part-1-sentiment-analyzer-demo-evaluation-optimization","title":"\ud83c\udfa8 Part 1 \u2014 Sentiment Analyzer Demo (Evaluation &amp; Optimization)","text":"<p>Overview</p> <p>This mini-project validates that your environment is ready. You'll initialize a project, pull a sample dataset, run the agent, evaluate it, and apply GEPA optimization.</p>"},{"location":"quick-start/#step-1-initialize-the-project","title":"Step 1. Initialize the Project","text":"<pre><code>super init sentiment_analyzer\ncd sentiment_analyzer\n</code></pre>"},{"location":"quick-start/#step-2-pull-the-dataset","title":"Step 2. Pull the Dataset","text":"<pre><code>super dataset pull sentiment_reviews\n</code></pre> <p>Dataset Location</p> <p>This stores <code>sentiment_reviews.csv</code> in your project's <code>data/</code> directory.</p>"},{"location":"quick-start/#step-3-pull-compile-the-agent","title":"Step 3. Pull &amp; Compile the Agent","text":"<pre><code>super agent pull sentiment_analyzer\nsuper agent compile sentiment_analyzer\n</code></pre>"},{"location":"quick-start/#step-4-run-the-agent","title":"Step 4. Run the Agent","text":"<pre><code>super agent run sentiment_analyzer \\\n    --goal \"Classify the sentiment of the review: 'I love this product but the shipping was slow.'\"\n</code></pre> <p>Output</p> <p>The agent responds with a sentiment label and a confidence score.</p> What Happened <ul> <li>The pipeline (<code>agents/sentiment_analyzer/pipelines/sentiment_analyzer_pipeline.py</code>) executed end-to-end with your goal.</li> <li>DSPy configured the local Ollama model <code>llama3.1:8b</code> (temperature 0.3, max 512 tokens).</li> <li>The ReAct chain generated both the structured fields (<code>sentiment</code>, <code>confidence</code>) and the reasoning trace.</li> <li>Output is shown in the terminal and the pipeline remains inspectable under <code>agents/sentiment_analyzer/pipelines/</code>.</li> </ul>"},{"location":"quick-start/#step-5-evaluate-the-agent","title":"Step 5. Evaluate the Agent","text":"<pre><code>super agent evaluate sentiment_analyzer\n</code></pre> <p>What This Does</p> <p>Runs the playbook scenarios plus the dataset samples.</p> What Happened <ul> <li>Evaluation pulled every BDD scenario defined in <code>agents/sentiment_analyzer/playbook/sentiment_analyzer_playbook.yaml</code>.</li> <li>Each scenario is scored with the <code>answer_exact_match</code> metric (threshold 0.7).</li> <li>Examples from <code>data/sentiment_reviews.csv</code> were converted into DSPy <code>Example</code>s and included in the run.</li> <li>A rich pass/fail summary (capability score, recommendations) was printed to the terminal.</li> </ul>"},{"location":"quick-start/#step-6-optimize-with-gepa-re-evaluate","title":"Step 6. Optimize with GEPA &amp; Re-evaluate","text":"<pre><code>super agent optimize sentiment_analyzer --auto light\nsuper agent evaluate sentiment_analyzer\n</code></pre> <p>GEPA Optimization</p> <p>GEPA tunes prompts based on failed scenarios; the follow-up evaluation measures any change.</p> What Happened <ul> <li>GEPA iteratively mutated the sentiment pipeline and scored each candidate against the same evaluation set.</li> <li>Optimized weights were saved to <code>agents/sentiment_analyzer/pipelines/sentiment_analyzer_optimized.json</code>.</li> <li>The second <code>evaluate</code> command automatically loaded those weights before re-running the scenarios.</li> </ul> <p>Part 1 Complete!</p> <p>You've now completed the full evaluation-first loop! Continue exploring or move on to the multi-agent SWE workflow below.</p>"},{"location":"quick-start/#part-2-swe-multi-agent-orchestration","title":"\ud83c\udfd7\ufe0f Part 2 \u2014 SWE Multi-Agent Orchestration","text":"<p>Overview</p> <p>In this section you'll build an end-to-end software development workflow with multiple cooperating agents.</p>"},{"location":"quick-start/#step-1-initialize-the-swe-project","title":"Step 1. Initialize the SWE Project","text":"<pre><code>cd ..          # if you're still inside sentiment_analyzer\nsuper init swe\ncd swe\n</code></pre>"},{"location":"quick-start/#step-2-pull-compile-the-developer-agent","title":"Step 2. Pull &amp; Compile the Developer Agent","text":"<pre><code>super agent pull developer\nsuper agent compile developer\n</code></pre> <p>Compilation Output</p> <p>Compilation generates an explicit DSPy pipeline at <code>agents/developer/pipelines/developer_pipeline.py</code>. This is your starting point for customization.</p>"},{"location":"quick-start/#step-3-run-the-developer-agent","title":"Step 3. Run the Developer Agent","text":"<pre><code>super agent run developer \\\n    --goal \"Create a Python function that validates email addresses using regex\"\n</code></pre> <p>What to Expect</p> <p>Watch the agent reason about the task and emit code along with explanations. The output file is stored in <code>pipelines/</code> and the CLI displays the result inline.</p>"},{"location":"quick-start/#step-4-add-qa-devops-agents","title":"Step 4. Add QA &amp; DevOps Agents","text":"<pre><code>super agent pull qa_engineer\nsuper agent pull devops_engineer\nsuper agent compile qa_engineer\nsuper agent compile devops_engineer\n</code></pre>"},{"location":"quick-start/#step-5-create-run-the-orchestra","title":"Step 5. Create &amp; Run the Orchestra","text":"<pre><code>super orchestra create sdlc\nsuper orchestra list\nsuper orchestra run sdlc --goal \"Build a task management web app with auth, CRUD, tests, and deployment config\"\n</code></pre> <p>Orchestra Workflow</p> <p>This generates <code>orchestras/sdlc_orchestra.yaml</code> and a compiled entry-point under <code>pipelines/orchestras/</code>. The sample goal walks through a three-phase SDLC:</p> <ol> <li>Developer: analyzes the goal, outlines the plan, and produces implementation artifacts.</li> <li>DevOps Engineer: translates the plan into CI/CD configuration and deployment notes.</li> <li>QA Engineer: derives comprehensive manual + automated test coverage from the preceding outputs.</li> </ol> <p>Output Files</p> <p>Orchestra results are saved to the project root (e.g., <code>implement_feature_implementation.txt</code>, <code>configure_ci_pipeline_result.json</code>, <code>create_test_plan_test_plan.txt</code>).</p>"},{"location":"quick-start/#step-6-observe-and-monitor","title":"Step 6. Observe and Monitor","text":"<pre><code>super observe traces developer\nsuper observe dashboard\n</code></pre> <p>Observability Tools</p> <ul> <li>Traces: Step through each agent's reasoning, model calls, and artifacts</li> <li>Dashboard: Higher-level view for debugging orchestration runs or comparing pre/post optimization behavior</li> </ul>"},{"location":"quick-start/#summary","title":"Summary","text":"<p>What You've Accomplished</p> <p>Part 1: Demonstrated evaluation-first development using a sentiment analyzer, including GEPA optimization.</p> <p>Part 2: Showed the full SWE orchestration flow with multiple agents collaborating on an SDLC task.</p> <p>Next Steps</p> <p>From here you can explore the marketplace (<code>super market</code>), design custom agents (<code>super agent design</code>), or build orchestras tailored to your workflows. Happy building! \ud83c\udf89</p>"},{"location":"setup/","title":"\ud83d\udd27 Installation Guide","text":"<p>Welcome to SuperOptiX! This guide will help you install the Full Stack Agentic AI Optimization Framework on your system.</p> <p>\ud83d\ude80 Quick Start</p> <p>New to SuperOptiX? Start with our Quick Start Guide after installation!</p> <p>Stable Release Available!</p> <p>SuperOptiX is now available as a stable release. We recommend using <code>uv</code> for the best experience. <pre><code>uv tool install superoptix\n</code></pre></p>"},{"location":"setup/#prerequisites","title":"\ud83d\udccb Prerequisites","text":""},{"location":"setup/#required","title":"Required","text":"<ul> <li>Python 3.11+ (required)</li> <li>Git (required for DSPy installation)</li> <li>Package Manager (uv recommended, pip also supported)</li> </ul>"},{"location":"setup/#verify-requirements","title":"Verify Requirements","text":"<pre><code># Check Python version\npython --version  # Should be 3.11 or higher\n\n# Check Git\ngit --version  # Should show git version\n</code></pre>"},{"location":"setup/#install-git-if-needed","title":"Install Git (if needed)","text":"macOSLinuxWindows <pre><code>xcode-select --install\n</code></pre> <pre><code># Ubuntu/Debian\nsudo apt-get install git\n\n# CentOS/RHEL\nsudo yum install git\n</code></pre> <p>Download from git-scm.com</p> <p>Python Version Requirement</p> <p>SuperOptiX requires Python 3.11 or higher. Check your version with: <pre><code>python --version\n</code></pre></p>"},{"location":"setup/#installation-methods","title":"\ud83c\udfaf Installation Methods","text":"<p>Framework-Free Core</p> <p>SuperOptiX core is now framework-independent! \ud83c\udf89</p> <p>Install only what you need. Choose from 6 AI frameworks, or use core without any.</p>"},{"location":"setup/#recommended-using-uv","title":"Recommended: Using uv","text":"<p>We highly recommend using <code>uv</code> for faster, more reliable installations.</p> <pre><code># Install UV (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install SuperOptiX CLI globally (isolated tool env)\nuv tool install superoptix\n\n# Verify\nsuper --version\n</code></pre>"},{"location":"setup/#add-framework-dependencies-with-uv-tool","title":"Add Framework Dependencies with uv Tool","text":"<p>Use <code>--with</code> to install framework dependencies in the same tool environment:</p> <pre><code># OpenAI SDK support\nuv tool install superoptix --with \"superoptix[frameworks-openai]\"\n\n# Claude SDK support\nuv tool install superoptix --with \"superoptix[frameworks-claude-sdk]\"\n\n# Google ADK support\nuv tool install superoptix --with \"superoptix[frameworks-google]\"\n\n# Pydantic AI support\nuv tool install superoptix --with \"superoptix[frameworks-pydantic-ai]\"\n</code></pre> <p>Upgrade later:</p> <pre><code>uv tool upgrade superoptix\n</code></pre>"},{"location":"setup/#alternative-using-pip","title":"Alternative: Using pip","text":"<pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install SuperOptiX\npip install superoptix\n</code></pre>"},{"location":"setup/#optional-frameworks-extras","title":"\ud83d\udce6 Optional Frameworks &amp; Extras","text":"<p>Customize your installation by adding only what you need:</p>"},{"location":"setup/#ai-frameworks","title":"\ud83c\udf10 AI Frameworks","text":"Framework Install Command Includes DSPy \u2b50 <code>uv tool install superoptix --with \"superoptix[frameworks-dspy]\"</code> DSPy + GEPA OpenAI SDK <code>uv tool install superoptix --with \"superoptix[frameworks-openai]\"</code> openai-agents, openai SDK Claude SDK <code>uv tool install superoptix --with \"superoptix[frameworks-claude-sdk]\"</code> claude-agent-sdk Google ADK <code>uv tool install superoptix --with \"superoptix[frameworks-google]\"</code> google-adk, google-generativeai Microsoft (Legacy) <code>uv tool install superoptix --with \"superoptix[frameworks-microsoft]\"</code> agent-framework, azure-identity DeepAgents <code>uv tool install superoptix --with \"superoptix[frameworks-deepagents]\"</code> deepagents Pydantic AI <code>uv tool install superoptix --with \"superoptix[frameworks-pydantic-ai]\"</code> Pydantic AI CrewAI \u26a0\ufe0f <code>uv tool install superoptix --with \"superoptix[frameworks-crewai]\"</code> crewai (conflicts with DSPy) <p>\u2b50 Recommended: DSPy for GEPA optimization \u26a0\ufe0f Note: CrewAI and DSPy cannot be installed together in the same environment.</p>"},{"location":"setup/#tool-optimization-mcp","title":"\ud83d\udd0c Tool Optimization &amp; MCP","text":"<pre><code>uv tool install superoptix --with \"superoptix[mcp]\"\n</code></pre>"},{"location":"setup/#vector-databases-rag","title":"\ud83e\udde0 Vector Databases (RAG)","text":"<pre><code># All vector databases\nuv tool install superoptix --with \"superoptix[vectordb]\"\n\n# Or specific ones\nuv tool install superoptix --with \"superoptix[chromadb]\"    # ChromaDB (recommended)\nuv tool install superoptix --with \"superoptix[qdrant]\"      # Qdrant\n</code></pre>"},{"location":"setup/#observability","title":"\ud83d\udd0d Observability","text":"<p><pre><code>uv tool install superoptix --with \"superoptix[observability]\"\n</code></pre> Includes MLflow, Pandas, Plotly.</p>"},{"location":"setup/#local-model-management","title":"\ud83d\udcbb Local Model Management","text":"<pre><code># Apple Silicon (MLX)\nuv tool install superoptix --with \"superoptix[mlx]\"\n\n# HuggingFace\nuv tool install superoptix --with \"superoptix[huggingface]\"\n</code></pre>"},{"location":"setup/#verification","title":"\ud83d\udd0d Verification","text":"<p>After installation, verify SuperOptiX is working correctly:</p> <pre><code># Check CLI\nsuper --version\n\n# Check available commands\nsuper --help\n</code></pre>"},{"location":"setup/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ol> <li>Set up your LLM: Follow our LLM Setup Guide</li> <li>Create your first agent: Try our Quick Start Guide</li> <li>Explore the framework: Check out our Agent Patterns</li> </ol>"},{"location":"setup/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"setup/#common-issues","title":"Common Issues","text":"<p>Import Error: Make sure you're using Python 3.11+ <pre><code>python --version\n</code></pre></p> <p>Package Not Found: Update pip/uv <pre><code>uv tool upgrade superoptix\n</code></pre></p> <p>CrewAI Installation Conflicts: If you encounter dependency conflicts when installing CrewAI with SuperOptiX: <pre><code># The issue: CrewAI requires json-repair==0.25.2, but DSPy needs json-repair&gt;=0.30.0\n# Solution: Install manually with --no-deps flag\nuv tool install superoptix --with \"superoptix[frameworks-dspy]\"  # Install DSPy support first\nuv tool install superoptix --with crewai==0.157.0  # Add CrewAI manually\nuv tool install superoptix --with \"json-repair&gt;=0.30.0\"  # Ensure compatible version\n</code></pre></p>"},{"location":"setup/#still-having-issues","title":"Still Having Issues?","text":"<ul> <li>\ud83d\udcd6 Check our Troubleshooting Guide</li> <li>\ud83d\udc1b Report issues on GitHub</li> </ul>"},{"location":"superagentic-ai/","title":"\ud83c\udfe2 Superagentic AI","text":"Superagentic AI <p>Building the future of autonomous AI systems</p> \ud83c\udf10 Visit Website \ud83d\ude80 Get Started \ud83d\udcda Documentation"},{"location":"superagentic-ai/#who-we-are","title":"\ud83c\udfaf Who We Are","text":"<p>Superagentic AI is the full-stack agentic AI company behind SuperOptiX, pioneering a safer, future-ready AI ecosystem through innovative foundational pillars.</p> <p>As a full-stack agentic AI company, we are building SuperOptiX - a comprehensive framework designed to create production-worthy AI agents that embody our vision for the future of autonomous AI systems.</p>"},{"location":"superagentic-ai/#our-mission","title":"\ud83c\udfd7\ufe0f Our Mission\ud83d\udd12\u26a1\ud83d\udee1\ufe0f\ud83e\udd1d\ud83c\udf10","text":"<p>SuperOptiX is more than just another AI framework, it's the embodiment of our commitment to building a safer, more intelligent, and more collaborative AI future. Our mission is to provide developers and organizations with the tools they need to create AI agents that are:</p> Production-Ready <p>Built for real-world deployment at scale</p> Performance-Optimized <p>Designed with optimization as a first-class concern</p> Secure &amp; Safe <p>Enterprise-grade security and safety measures</p> Human-Collaborative <p>Designed for seamless human-AI interaction</p> Scalable <p>Ready to grow with your organization</p>"},{"location":"superagentic-ai/#superoptix-our-flagship-product","title":"\ud83d\ude80 SuperOptiX: Our Flagship Product","text":"<p>SuperOptiX is the revolutionary AI agent framework built by Superagentic AI. The framework's first two tiers, Oracles and Genies, are now available, making advanced AI agent capabilities accessible to the broader developer community.</p>"},{"location":"superagentic-ai/#key-features","title":"\ud83d\udc8e Key Features\ud83d\udc8e SuperSpec DSL\ud83c\udfd7\ufe0f Agent Pipelines\ud83e\udde0 Memory Systems\ud83d\udd0d RAG Integration\ud83d\udee0\ufe0f Tool Ecosystem\ud83d\udcca Observability\ud83c\udfbc Multi-Agent Orchestration\ud83e\uddec GEPA Optimizer","text":"<p>Declarative agent specification with rapid development, type safety, and extensibility. Build complex agents in minutes, not days.</p> <p>Automated workflows with custom agent building, secure coding practices, and seamless CI/CD integration.</p> <p>Multi-modal memory with ChromaDB, LanceDB, Weaviate, Qdrant, Milvus support. Intelligent context management and long-term learning.</p> <p>Vector database support with automated document processing, semantic search, and centralized knowledge management.</p> <p>Built-in and custom tools with intelligent tool orchestration and seamless API integration.</p> <p>Real-time monitoring with advanced debugging, performance analytics, and complete audit trails.</p> <p>Agent teams with complex workflow management, resource optimization, and scalable architecture.</p> <p>Universal optimization across all frameworks with proven 37.5% \u2192 80% improvements.</p>"},{"location":"superagentic-ai/#our-foundational-pillars","title":"\ud83c\udfd7\ufe0f Our Foundational Pillars","text":"<p>We're building the future of AI through five core pillars that form the backbone of our vision:</p>"},{"location":"superagentic-ai/#agentic-devops","title":"\ud83e\udd16 Agentic DevOps","text":"<p>Streamlined development and deployment of AI agents with enterprise-grade infrastructure, automated testing, and continuous integration/continuous deployment (CI/CD) pipelines designed specifically for autonomous AI systems.</p>"},{"location":"superagentic-ai/#agent-engineering","title":"\u2699\ufe0f Agent Engineering","text":"<p>Advanced tools and methodologies for building production-worthy agents. This includes:</p> <ul> <li> <p>Development Frameworks - Comprehensive tooling for agent creation</p> </li> <li> <p>Testing &amp; Validation - Robust testing methodologies for AI agents</p> </li> <li> <p>Performance Optimization - Tools to ensure optimal agent performance</p> </li> <li> <p>Security &amp; Safety - Built-in safeguards and security measures</p> </li> </ul>"},{"location":"superagentic-ai/#agent-experience-agentex","title":"\ud83c\udfaf Agent Experience (AgentEx)","text":"<p>Just as UX is designed for humans and DevEx for developers, AgentEx is designed for autonomous AI agents. It's about creating environments, interfaces, and systems that agents can effectively navigate, understand, and utilize.</p> <p>AgentEx isn't just a nice-to-have. As agents become more prevalent in building and using software, the quality of their experience directly impacts their effectiveness, reliability, and safety.</p> <p>Key AgentEx Principles: - Intuitive Interfaces - Environments that agents can easily understand and navigate - Efficient Communication - Optimized protocols for agent-to-agent and agent-to-human interaction - Contextual Awareness - Systems that provide agents with the right information at the right time - Performance Optimization - Interfaces designed for speed and efficiency</p>"},{"location":"superagentic-ai/#agentic-co-intelligence","title":"\ud83e\udd1d Agentic Co-Intelligence","text":"<p>The idea that humans and agents must evolve together - as orchestrators, trainers, validators, and high-context collaborators. This pillar focuses on:</p> <ul> <li>Human-AI Collaboration - Seamless interaction between humans and AI agents</li> <li>Training &amp; Validation - Human oversight and guidance for AI systems</li> <li>Orchestration - Coordinated workflows between multiple agents and humans</li> <li>High-Context Understanding - Deep comprehension of human intent and business context</li> </ul>"},{"location":"superagentic-ai/#quantum-ai","title":"\ud83d\udd2e Quantum AI","text":"<p>Next-generation quantum computing integration for AI systems, exploring the intersection of quantum computing and artificial intelligence to unlock new capabilities and performance improvements.</p>"},{"location":"superagentic-ai/#superoptix-tiers-pillar-alignment","title":"\ud83c\udfd7\ufe0f SuperOptiX Tiers &amp; Pillar Alignment","text":"<p>SuperOptiX is structured in tiers that progressively embody our foundational pillars, with the first two tiers - Oracles and Genies - now available:</p> Tier Description Superagentic AI Pillars Status \ud83d\udd2e Oracles High-level reasoning and decision-making agents with advanced context understanding Context Engineering + Agent Engineering Available \ud83e\uddde Genies Task-oriented agents with specialized capabilities and tool integration Agent Engineering Open Source \ud83d\udd17 Protocols Multi-agent communication and orchestration layer Agent Experience \ud83d\udd04 Coming Soon \ud83e\udd1d SuperAgents Advanced human-AI collaboration with human-in-the-loop capabilities Agentic Co-Intelligence \ud83d\udd04 Coming Soon \ud83d\udd2e Quantum Agents Next-generation agents leveraging quantum computing capabilities Quantum AI \ud83d\udd2e Future"},{"location":"superagentic-ai/#pillar-integration","title":"\ud83c\udfaf Pillar Integration","text":"<ul> <li>\u2699\ufe0f Context Engineering - Implemented through SuperSpec DSL in Oracles tier</li> <li>\u2699\ufe0f Agent Engineering - Core framework across Oracles and Genies tiers</li> <li>\ud83c\udfaf Agent Experience - Protocol layer for seamless agent communication</li> <li>\ud83e\udd1d Agentic Co-Intelligence - Human-in-the-loop capabilities in upper tiers</li> <li>\ud83d\udd2e Quantum AI - Future integration with quantum computing systems</li> </ul>"},{"location":"superagentic-ai/#how-superoptix-embodies-our-pillars","title":"\ud83c\udfd7\ufe0f How SuperOptiX Embodies Our Pillars","text":""},{"location":"superagentic-ai/#agent-engineering_1","title":"\u2699\ufe0f Agent Engineering","text":"<p>SuperOptiX provides comprehensive agent engineering capabilities through:</p> <ul> <li>SuperSpec DSL - Declarative agent specification language for rapid development</li> <li>Automated Agent Pipelines - Streamlined development workflows</li> <li>Custom Agent Building - Full developer control over agent behavior and capabilities</li> <li>Secure Coding Practices - Built-in security and validation frameworks</li> </ul> <p>Context Engineering via SuperSpec (no-code/low-code) Agent Engineering via SuperSpec + Agent Pipelines (Developer + Business collaboration)</p>"},{"location":"superagentic-ai/#agent-experience-agentex_1","title":"\ud83c\udfaf Agent Experience (AgentEx)","text":"<p>Just as UX is designed for humans and DevEx for developers, AgentEx is designed for autonomous AI agents. It's about creating environments, interfaces, and systems that agents can effectively navigate, understand, and utilize.</p> <p>AgentEx isn't just a nice-to-have. As agents become more prevalent in building and using software, the quality of their experience directly impacts their effectiveness, reliability, and safety.</p> <p>SuperOptiX provides pure-class agent experience through: - Protocol Layers - Integration with MCP, A2A, and upcoming protocols - Multi-Agentic Systems - Seamless agent-to-agent communication - Optimized Interfaces - Purpose-built environments for AI agents</p>"},{"location":"superagentic-ai/#agentic-co-intelligence_1","title":"\ud83e\udd1d Agentic Co-Intelligence","text":"<p>Agentic Co-Intelligence is the idea that humans and agents must evolve together - as orchestrators, trainers, validators, and high-context collaborators.</p> <p>SuperOptiX implements this through: - Human-in-the-Loop - Upper-tier controls for human oversight - Behavioral Control - Human-defined criteria for agent behavior - Collaborative Workflows - Seamless human-AI interaction patterns</p>"},{"location":"superagentic-ai/#our-vision","title":"\ud83c\udfaf Our Vision","text":"<p>We envision a world where AI agents are: - Safe and Reliable - Built with security and safety as core principles - Intelligent and Adaptive - Capable of learning and evolving with their environment - Collaborative - Working seamlessly with humans and other agents - Production-Ready - Deployed at scale in real-world applications - Ethical - Operating with transparency and accountability</p>"},{"location":"superagentic-ai/#our-vision_1","title":"\ud83c\udfaf Our Vision","text":"<p>We envision a world where AI agents are:</p> Vision Element Description \ud83d\udd12 Safe and Reliable Built with security and safety as core principles \ud83e\udde0 Intelligent and Adaptive Capable of learning and evolving with their environment \ud83e\udd1d Collaborative Working seamlessly with humans and other agents \ud83d\ude80 Production-Ready Deployed at scale in real-world applications Ethical Operating with transparency and accountability"},{"location":"superagentic-ai/#getting-started-with-superoptix","title":"\ud83d\ude80 Getting Started with SuperOptiX\ud83d\udce6\ud83d\ude80\ud83d\udcda\ud83d\udc8e","text":"<p>Ready to build the future of AI agents?</p> Installation <p>Quick setup and installation</p> Quick Start <p>Build your first agent in minutes</p> Documentation <p>Comprehensive guides and tutorials</p> SuperSpec <p>Learn our declarative DSL</p>"},{"location":"superagentic-ai/#learn-more-about-superagentic-ai","title":"\ud83d\udcda Learn More About Superagentic AI","text":"\ud83e\udd1d Agentic Co-Intelligence \ud83c\udfaf Agent Experience \u2699\ufe0f Agent Engineering \ud83d\udcd6 Blog"},{"location":"superagentic-ai/#the-superagentic-ai-show","title":"\ud83c\udfa7 The Superagentic AI Show","text":"<p>Listen to our podcast on Apple Podcasts for in-depth discussions about the future of AI and agentic systems.</p> <p>Superagentic AI is more than a company, we're a movement dedicated to building a safer, more intelligent, and more collaborative AI future. Join us in shaping the next generation of autonomous AI systems.</p> \ud83c\udf10 Visit Our Website \ud83d\ude80 Try SuperOptiX Free"},{"location":"troubleshooting/","title":"\ud83d\udd27 Troubleshooting Guide","text":"<p>This guide covers common issues and their solutions when working with SuperOptiX.</p> <p>For fast error-to-fix mapping, use Troubleshooting by Symptom.</p>"},{"location":"troubleshooting/#dependency-conflicts","title":"\ud83d\udea8 Dependency Conflicts","text":""},{"location":"troubleshooting/#crewai-installation-conflicts","title":"CrewAI Installation Conflicts","text":"<p>Problem: You encounter dependency conflicts when trying to install CrewAI with SuperOptiX.</p> <p>Error Message: <pre><code>ERROR: Cannot install crewai==0.157.0 and dspy==3.0.0 because these package versions have conflicting dependencies.\n\nThe conflict is caused by:\n    dspy 3.0.0 depends on json-repair&gt;=0.30.0\n    crewai 0.157.0 depends on json-repair==0.25.2\n</code></pre></p> <p>Root Cause: This is a known dependency conflict between CrewAI and DSPy due to incompatible <code>json-repair</code> version requirements: - DSPy 3.0.0 requires <code>json-repair&gt;=0.30.0</code> - CrewAI 0.157.0 requires <code>json-repair==0.25.2</code> (exact version)</p> <p>Solution: Install CrewAI manually after installing SuperOptiX with DSPy support:</p> <pre><code># Install SuperOptiX with DSPy support (this gets compatible json-repair)\npip install \"superoptix[optimas]\"\n\n# Install CrewAI without dependencies to avoid conflicts\npip install crewai==0.157.0 --no-deps\n\n# Ensure compatible json-repair version\npip install \"json-repair&gt;=0.30.0\"\n</code></pre> <p>Why This Works: - The <code>--no-deps</code> flag prevents pip from trying to resolve conflicting dependencies - We manually install the version of <code>json-repair</code> that satisfies both packages - Both packages work together at runtime despite metadata conflicts</p> <p>Alternative Solutions: 1. Use only CrewAI: Install SuperOptiX without DSPy support, then add CrewAI normally 2. Wait for updates: Monitor for newer CrewAI versions that might fix the dependency conflict 3. Use different versions: Consider using older versions of DSPy if compatible with your needs</p>"},{"location":"troubleshooting/#build-issues","title":"\ud83d\udd0d Build Issues","text":""},{"location":"troubleshooting/#build-fails-or-uv-build-missing","title":"Build Fails or <code>uv build</code> Missing","text":"<p>Problem: Build fails due to missing build tooling or an older uv version.</p> <p>Solutions: - Install build tooling: <code>uv pip install build</code> (or <code>pip install build</code>). - Use <code>uv build</code> if available, otherwise run <code>uv run python -m build --wheel</code>.</p>"},{"location":"troubleshooting/#python-version-issues","title":"\ud83d\udc0d Python Version Issues","text":""},{"location":"troubleshooting/#python-version-too-old","title":"Python Version Too Old","text":"<p>Problem: SuperOptiX requires Python 3.11+ but you have an older version.</p> <p>Solution: Upgrade to Python 3.11 or higher:</p> <pre><code># Check current version\npython --version\n\n# Install Python 3.11+ (example for macOS with Homebrew)\nbrew install python@3.11\n\n# Or use conda\nconda create -n superoptix python=3.11 -y\nconda activate superoptix\n</code></pre>"},{"location":"troubleshooting/#package-installation-issues","title":"\ud83d\udce6 Package Installation Issues","text":""},{"location":"troubleshooting/#permission-errors","title":"Permission Errors","text":"<p>Problem: Installation fails due to permission issues.</p> <p>Solution: Use virtual environments:</p> <pre><code># Create virtual environment\npython -m venv superoptix-env\n\n# Activate (Linux/macOS)\nsource superoptix-env/bin/activate\n\n# Activate (Windows)\nsuperoptix-env\\Scripts\\activate\n\n# Install SuperOptiX\npip install superoptix\n</code></pre>"},{"location":"troubleshooting/#package-not-found","title":"Package Not Found","text":"<p>Problem: SuperOptiX package cannot be found.</p> <p>Solution: Update pip and check PyPI:</p> <pre><code># Update pip\npip install --upgrade pip\n\n# Check if package exists\npip search superoptix\n\n# Install from PyPI\npip install superoptix\n</code></pre>"},{"location":"troubleshooting/#still-having-issues","title":"\ud83d\ude80 Still Having Issues?","text":"<p>If you're still experiencing problems:</p> <ol> <li>\ud83d\udcd6 Check this guide for your specific error</li> <li>\ud83d\udd0d Search existing issues on GitHub</li> <li>\ud83d\udc1b Report new issues with detailed error messages and system information</li> <li>\ud83d\udcac Join our community for support and discussions</li> </ol> <p>When reporting issues, please include: - Python version (<code>python --version</code>) - Operating system and version - Complete error message - Steps to reproduce the issue - Package versions (<code>pip list | grep superoptix</code>) </p>"},{"location":"examples/","title":"\ud83d\ude80 SuperOptiX Examples","text":"<p>Welcome to the SuperOptiX Examples section! These demos showcase specific technologies and capabilities within the SuperOptiX framework. Each demo focuses on a particular technology or feature, providing hands-on experience with different aspects of the framework.</p>"},{"location":"examples/#demo-overview","title":"\ud83c\udfaf Demo Overview","text":"<p>The examples are organized into three main categories:</p>"},{"location":"examples/#model-backend-demos","title":"\ud83e\udd16 Model Backend Demos","text":"<p>Learn how to configure and use different local model backends:</p> <ul> <li>\ud83c\udf4e MLX Demo - Apple Silicon optimization with MLX models</li> <li>\ud83e\udd99 Ollama Demo - Easy local model management with Ollama</li> <li>\ud83e\udd17 HuggingFace Demo - Advanced NLP with HuggingFace models</li> <li>\ud83c\udfae LM Studio Demo - GUI-based model management with LM Studio</li> </ul>"},{"location":"examples/#rag-technology-demos","title":"\ud83d\udd0d RAG Technology Demos","text":"<p>Explore Retrieval-Augmented Generation capabilities:</p> <ul> <li>\ud83d\udd0d RAG ChromaDB Demo - RAG with ChromaDB vector database</li> <li>\ud83d\ude80 RAG LanceDB Demo - High-performance RAG with LanceDB</li> <li>\ud83d\uddc4\ufe0f Weaviate Demo - Advanced semantic search with Weaviate</li> <li>\ud83c\udfaf Qdrant Demo - Lightning-fast vector search with Qdrant</li> <li>\ud83c\udfd7\ufe0f Milvus Demo - Enterprise-scale vector database with Milvus</li> </ul>"},{"location":"examples/#framework-feature-demos","title":"\ud83d\udee0\ufe0f Framework Feature Demos","text":"<p>Discover core framework capabilities:</p> <ul> <li>\ud83d\udee0\ufe0f Tools Demo - Comprehensive tool integration across 20+ categories</li> <li>\ud83e\udde0 Memory Demo - Multi-layered memory system (short-term, long-term, episodic)</li> <li>\ud83d\udcca Observability Demo - Monitoring, tracing, and debugging capabilities</li> </ul>"},{"location":"examples/#observability-integrations","title":"\ud83d\udd0c Observability Integrations","text":"<p>Integrate with external observability platforms:</p> <ul> <li>\ud83e\uddea MLFlow Integration - Experiment tracking and model monitoring</li> <li>\ud83d\udd0d LangFuse Integration - LLM observability and performance tracking</li> </ul>"},{"location":"examples/#technology-focus-matrix","title":"\ud83c\udfaf Technology Focus Matrix","text":"Demo Primary Technology Key Features Use Case MLX Apple Silicon Models Native performance, 4-bit quantization Apple Silicon development Ollama Easy Model Management Simple setup, cross-platform Quick local model usage HuggingFace Advanced NLP Transformer models, custom models NLP research and development LM Studio GUI Model Management Visual interface, Windows-friendly Desktop model management RAG ChromaDB Knowledge Retrieval Semantic search, document retrieval Knowledge base queries RAG LanceDB High-Performance RAG Scalable, production-ready Large-scale deployments Weaviate Advanced Semantic Search Sophisticated similarity algorithms Academic research, enterprise search Qdrant Lightning-Fast Search Optimized performance, high throughput Industrial applications, real-time systems Milvus Enterprise-Scale RAG Cloud-native, distributed architecture Massive-scale deployments, enterprise search Tools Tool Integration 20+ categories, specialized tools Enhanced agent capabilities Memory Context Retention Multi-layered, persistent storage Conversational AI Observability Monitoring &amp; Debugging Tracing, metrics, dashboard Production monitoring MLFlow Experiment Tracking Model monitoring, metrics, artifacts ML lifecycle management LangFuse LLM Observability Token tracking, cost monitoring, feedback LLM application monitoring"},{"location":"examples/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install SuperOptiX <pre><code>pip install superoptix\n</code></pre></p> </li> <li> <p>Choose Your Demo</p> </li> <li>Start with Ollama for easiest setup</li> <li>Use MLX if you have Apple Silicon</li> <li>Try RAG for knowledge retrieval</li> <li>Explore Tools for enhanced capabilities</li> </ol>"},{"location":"examples/#quick-start-pattern","title":"Quick Start Pattern","text":"<p>Each demo follows this pattern:</p> <pre><code># Install model\nsuper model install &lt;model_name&gt;\n\n# Start server (if needed)\nsuper model server &lt;backend&gt; &lt;model_name&gt;\n\n# Pull and run demo\nsuper agent pull &lt;demo_name&gt;\nsuper agent compile &lt;demo_name&gt;\nsuper agent run &lt;demo_name&gt; --goal \"Your question here\"\n</code></pre>"},{"location":"examples/#learning-path","title":"\ud83c\udfaf Learning Path","text":""},{"location":"examples/#beginner-path","title":"Beginner Path","text":"<ol> <li>Ollama Demo - Learn basic model setup</li> <li>Tools Demo - Explore tool integration</li> <li>Memory Demo - Understand context retention</li> </ol>"},{"location":"examples/#intermediate-path","title":"Intermediate Path","text":"<ol> <li>MLX Demo - Apple Silicon optimization</li> <li>RAG ChromaDB Demo - Knowledge retrieval</li> <li>Observability Demo - Monitoring and debugging</li> </ol>"},{"location":"examples/#advanced-path","title":"Advanced Path","text":"<ol> <li>HuggingFace Demo - Advanced NLP capabilities</li> <li>LM Studio Demo - GUI model management</li> <li>RAG LanceDB Demo - Production-scale RAG</li> <li>MLFlow Integration - Experiment tracking and monitoring</li> <li>LangFuse Integration - LLM observability and cost tracking</li> </ol>"},{"location":"examples/#customization-guide","title":"\ud83d\udd27 Customization Guide","text":"<p>Each demo serves as a template for building custom agents:</p>"},{"location":"examples/#model-backend-customization","title":"Model Backend Customization","text":"<ul> <li>Change models in <code>language_model.model</code></li> <li>Adjust performance settings (temperature, max_tokens)</li> <li>Configure different backends (mlx, ollama, huggingface, lmstudio)</li> </ul>"},{"location":"examples/#rag-customization","title":"RAG Customization","text":"<ul> <li>Modify retrieval settings (top_k, chunk_size)</li> <li>Change vector databases (chroma, lancedb, weaviate, qdrant, milvus)</li> <li>Adjust embedding models</li> </ul>"},{"location":"examples/#framework-feature-customization","title":"Framework Feature Customization","text":"<ul> <li>Enable/disable specific tools</li> <li>Configure memory settings</li> <li>Adjust observability levels</li> <li>Integrate external observability platforms (MLFlow, LangFuse)</li> </ul>"},{"location":"examples/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"examples/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Model Not Found <pre><code># Check available models\nsuper model list --backend &lt;backend&gt;\n\n# Install required model\nsuper model install &lt;model_name&gt;\n</code></pre></p> </li> <li> <p>Server Not Running <pre><code># Check server status\ncurl http://localhost:&lt;port&gt;/health\n\n# Start server\nsuper model server &lt;backend&gt; &lt;model_name&gt;\n</code></pre></p> </li> <li> <p>Demo Not Working <pre><code># Check agent status\nsuper agent inspect &lt;demo_name&gt;\n\n# View logs\nsuper agent logs &lt;demo_name&gt;\n</code></pre></p> </li> </ol>"},{"location":"examples/#getting-help","title":"Getting Help","text":"<pre><code># General help\nsuper --help\n\n# Model help\nsuper model --help\n\n# Agent help\nsuper agent --help\n</code></pre>"},{"location":"examples/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Quick Start Guide - Get up and running quickly</li> <li>LLM Setup Guide - Complete model setup instructions</li> <li>Agent Development - Build custom agents</li> <li>Tool Development - Create custom tools</li> <li>RAG Guide - RAG implementation guide</li> <li>Memory Guide - Memory system guide</li> <li>Observability Guide - Monitoring and debugging</li> <li>MLFlow Integration - MLFlow integration guide</li> <li>LangFuse Integration - LangFuse integration guide</li> </ul>"},{"location":"examples/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>After exploring the demos:</p> <ol> <li>Build Custom Agents - Use demos as templates for your specific use cases</li> <li>Combine Technologies - Mix and match different technologies</li> <li>Scale to Production - Deploy optimized agents for production use</li> <li>Contribute - Share your custom agents and tools with the community</li> </ol> <p>Ready to explore SuperOptiX capabilities? Choose a demo and start building! \ud83d\ude80 </p>"},{"location":"examples/agents/gepa-integration/","title":"GEPA Integration Examples","text":"<p>This guide provides practical examples of integrating GEPA optimization into your SuperOptiX agents across different domains.</p>"},{"location":"examples/agents/gepa-integration/#mathematics-agent-with-gepa","title":"Mathematics Agent with GEPA","text":"<p>Example configuration for advanced mathematical problem solving:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Advanced Math Solver\n  id: math-solver-gepa\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.1\n  persona:\n    role: \"Mathematics Specialist\"\n    goal: \"Solve complex mathematical problems with step-by-step reasoning\"\n    traits: [\"analytical\", \"precise\", \"methodical\"]\n  tasks:\n    - name: solve_math_problem\n      description: Solve mathematical problems with verification\n      inputs:\n        - name: problem\n          type: str\n          required: true\n      outputs:\n        - name: solution\n          type: str\n        - name: answer\n          type: str\n        - name: reasoning\n          type: str\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: advanced_math_feedback\n        auto: light\n        reflection_lm: qwen3:8b\n        reflection_minibatch_size: 3\n        skip_perfect_score: true\n  feature_specifications:\n    scenarios:\n      - name: quadratic_equation\n        input:\n          problem: \"Solve x\u00b2 - 5x + 6 = 0\"\n        expected_output:\n          answer: \"x = 2 or x = 3\"\n          solution: \"Factor as (x-2)(x-3) = 0\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#enterprise-document-analyzer","title":"Enterprise Document Analyzer","text":"<p>GEPA configuration for business document processing:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: multi_component_enterprise_feedback\n        auto: light\n        reflection_lm: qwen3:8b\n        predictor_level_feedback: true\n  feature_specifications:\n    scenarios:\n      - name: urgent_request\n        input:\n          document: \"URGENT: Server room leak affecting critical infrastructure\"\n        expected_output:\n          urgency: \"Critical\"\n          sentiment: \"Negative\" \n          categories: \"facility_maintenance, infrastructure, emergency\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#privacy-conscious-agent","title":"Privacy-Conscious Agent","text":"<p>GEPA setup for secure information handling:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: privacy_preservation_feedback\n        auto: light\n        reflection_lm: qwen3:8b\n        llm_as_judge_feedback: true\n  feature_specifications:\n    scenarios:\n      - name: pii_redaction\n        input:\n          user_query: \"I'm John Smith, SSN 123-45-6789, need medical advice\"\n        expected_output:\n          redacted_request: \"A person needs medical information\"\n          privacy_assessment: \"Removed PII while preserving context\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#security-analysis-agent","title":"Security Analysis Agent","text":"<p>GEPA configuration for vulnerability detection:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: vulnerability_detection_feedback\n        auto: light\n        reflection_lm: qwen3:8b\n        format_failure_feedback: true\n  feature_specifications:\n    scenarios:\n      - name: sql_injection\n        input:\n          code_or_config: |\n            query = \"SELECT * FROM users WHERE name = '\" + name + \"'\"\n        expected_output:\n          vulnerabilities: \"Critical: SQL Injection vulnerability\"\n          risk_assessment: \"Critical severity - allows database compromise\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#running-gepa-optimization","title":"Running GEPA Optimization","text":""},{"location":"examples/agents/gepa-integration/#basic-workflow","title":"Basic Workflow","text":"<pre><code># Pull and set up agent\nsuper agent pull advanced-math-gepa\nsuper agent compile advanced-math-gepa\n\n# Evaluate baseline\nsuper agent evaluate advanced-math-gepa\n\n# Run GEPA optimization\nsuper agent optimize advanced-math-gepa\n\n# Measure improvement\nsuper agent evaluate advanced-math-gepa\n\n# Test optimized agent\nsuper agent run advanced-math-gepa --goal \"Solve x\u00b2 + 3x - 4 = 0\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#memory-efficient-configuration","title":"Memory-Efficient Configuration","text":"<p>For systems with memory constraints:</p> <pre><code>spec:\n  language_model:\n    model: llama3.1:8b        # ~8GB\n  optimization:\n    optimizer:\n      reflection_lm: qwen3:8b  # ~8GB, different model\n      auto: light              # Conservative budget\n      reflection_minibatch_size: 3\n</code></pre>"},{"location":"examples/agents/gepa-integration/#progressive-optimization","title":"Progressive Optimization","text":"<p>Start conservative and increase budget if needed:</p> <pre><code># Start with light optimization\nsuper agent optimize your_agent  # Uses auto: light\n\n# If results are promising, increase budget\n# Edit playbook: auto: medium\nsuper agent compile your_agent\nsuper agent optimize your_agent --force\n\n# For production, consider heavy optimization\n# Edit playbook: auto: heavy\nsuper agent optimize your_agent --force\n</code></pre>"},{"location":"examples/agents/gepa-integration/#gepa-output-analysis","title":"GEPA Output Analysis","text":""},{"location":"examples/agents/gepa-integration/#successful-gepa-logs","title":"Successful GEPA Logs","text":"<pre><code>INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 400 metric calls\nINFO dspy.evaluate.evaluate: Average Metric: 2.0 / 5 (40.0%)\nINFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program score: 0.4\nINFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\nINFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predictor\n</code></pre> <p>This indicates: - GEPA started with appropriate budget - Baseline performance measured (40%) - Iterative improvements occurring - Score improvements achieved (100%) - New prompts being generated</p>"},{"location":"examples/agents/gepa-integration/#understanding-timeout-behavior","title":"Understanding Timeout Behavior","text":"<p>GEPA optimization often exceeds 2-minute timeouts:</p> <pre><code>Error: Command timed out after 2m 0.0s\nINFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 400 metric calls\n</code></pre> <p>This is normal behavior because: - GEPA prioritizes quality over speed - Multiple reflection and generation cycles - Typical completion time: 3-5 minutes for light budget</p>"},{"location":"examples/agents/gepa-integration/#extending-timeout","title":"Extending Timeout","text":"<pre><code># Allow more time for GEPA completion\nsuper agent optimize your_agent --timeout 300  # 5 minutes\n</code></pre>"},{"location":"examples/agents/gepa-integration/#comparison-before-and-after-gepa","title":"Comparison: Before and After GEPA","text":""},{"location":"examples/agents/gepa-integration/#before-gepa-optimization","title":"Before GEPA Optimization","text":"<pre><code>Agent Response:\n\"To solve x\u00b2 - 5x + 6 = 0, we can factor: (x-2)(x-3) = 0, so x = 2 or x = 3.\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#after-gepa-optimization","title":"After GEPA Optimization","text":"<pre><code>Agent Response:\n\"**Method 1: Factoring**\nStep 1: Factor x\u00b2 - 5x + 6 into (x-2)(x-3) = 0\nFinding factors: need two numbers that multiply to 6 and add to -5\nThese are -2 and -3, so: (x-2)(x-3) = 0\n\n**Method 2: Quadratic Formula**\nUsing x = (-b \u00b1 \u221a(b\u00b2-4ac)) / 2a where a=1, b=-5, c=6\nx = (5 \u00b1 \u221a(25-24)) / 2 = (5 \u00b1 1) / 2\nx = 3 or x = 2\n\n**Verification:**\nFor x=2: (2)\u00b2 - 5(2) + 6 = 4 - 10 + 6 = 0 \u2713\nFor x=3: (3)\u00b2 - 5(3) + 6 = 9 - 15 + 6 = 0 \u2713\"\n</code></pre> <p>GEPA optimization resulted in: - Multiple solution methods - Step-by-step explanations - Solution verification - Better pedagogical structure</p>"},{"location":"examples/agents/gepa-integration/#best-practices","title":"Best Practices","text":""},{"location":"examples/agents/gepa-integration/#choose-appropriate-metrics","title":"Choose Appropriate Metrics","text":"<p>Match metrics to your domain:</p> <pre><code># Mathematics\nmetric: advanced_math_feedback\n\n# Business documents  \nmetric: multi_component_enterprise_feedback\n\n# Security analysis\nmetric: vulnerability_detection_feedback\n</code></pre>"},{"location":"examples/agents/gepa-integration/#start-conservative","title":"Start Conservative","text":"<p>Begin with light budgets:</p> <pre><code>optimization:\n  optimizer:\n    auto: light  # Start here\n    # Increase to medium/heavy if justified\n</code></pre>"},{"location":"examples/agents/gepa-integration/#quality-training-data","title":"Quality Training Data","text":"<p>Provide comprehensive scenarios:</p> <pre><code>feature_specifications:\n  scenarios:\n    - name: comprehensive_test\n      description: Cover main use cases and edge cases\n      input:\n        problem: \"Realistic, well-defined problem\"\n      expected_output:\n        answer: \"Complete expected response\"\n</code></pre>"},{"location":"examples/agents/gepa-integration/#monitor-progress","title":"Monitor Progress","text":"<p>Watch for improvement indicators:</p> <ul> <li>Score improvements in logs</li> <li>Quality of generated prompts</li> <li>Performance on evaluation scenarios</li> </ul>"},{"location":"examples/agents/gepa-integration/#validate-results","title":"Validate Results","text":"<p>Always measure GEPA effectiveness:</p> <pre><code># Before optimization\nsuper agent evaluate your_agent  # Note baseline\n\n# After optimization  \nsuper agent evaluate your_agent  # Compare improvement\n</code></pre>"},{"location":"examples/agents/gepa-integration/#related-documentation","title":"Related Documentation","text":"<ul> <li>GEPA Optimization Guide - Complete GEPA reference</li> <li>Optimization Guide - General optimization techniques</li> <li>Agent Development Guide - Development workflow</li> </ul>"},{"location":"examples/agents/huggingface-demo/","title":"\ud83e\udd17 HuggingFace Demo Agent","text":"<p>The HuggingFace Demo Agent showcases advanced NLP capabilities with HuggingFace models in SuperOptiX. This demo focuses specifically on how to configure and use HuggingFace models for sophisticated language understanding and generation.</p>"},{"location":"examples/agents/huggingface-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83e\udd17 HuggingFace Model Integration: How to configure HuggingFace models in SuperOptiX</li> <li>\ud83e\udde0 Advanced NLP Capabilities: Access to cutting-edge transformer models</li> <li>\ud83c\udfe0 Local Model Usage: Running models completely offline</li> <li>\u2699\ufe0f Playbook Configuration: How to set up HuggingFace in agent playbooks</li> </ul>"},{"location":"examples/agents/huggingface-demo/#setup-huggingface-model","title":"\ud83d\ude80 Setup HuggingFace Model","text":""},{"location":"examples/agents/huggingface-demo/#1-install-huggingface-dependencies","title":"1. Install HuggingFace Dependencies","text":"<pre><code># Install HuggingFace dependencies\npip install \"superoptix[huggingface]\"\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#2-install-huggingface-model","title":"2. Install HuggingFace Model","text":"<pre><code># Install the HuggingFace model used in this demo\nsuper model install -b huggingface microsoft/Phi-4\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#3-start-huggingface-server","title":"3. Start HuggingFace Server","text":"<pre><code># Start HuggingFace server on port 8001\nsuper model server huggingface microsoft/Phi-4 --port 8001\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#4-pull-and-run-the-demo","title":"4. Pull and Run the Demo","text":"<pre><code># Pull the HuggingFace demo agent\nsuper agent pull huggingface_demo\n\n# Compile the agent\nsuper agent compile huggingface_demo\n\n# Run the agent\nsuper agent run huggingface_demo --goal \"What are the key features of HuggingFace?\"\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#huggingface-configuration-in-playbook","title":"\ud83d\udd27 HuggingFace Configuration in Playbook","text":"<p>The HuggingFace demo showcases how to configure HuggingFace models in the agent playbook:</p>"},{"location":"examples/agents/huggingface-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: huggingface\n  model: microsoft/Phi-4\n  api_base: http://localhost:8001\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre> <p>Key Configuration Points:</p> <ul> <li>\ud83c\udfaf <code>provider: huggingface</code>: Specifies HuggingFace as the model backend</li> <li>\ud83e\udd16 <code>model</code>: The HuggingFace model identifier</li> <li>\ud83c\udf10 <code>api_base</code>: HuggingFace server endpoint (default: http://localhost:8001)</li> <li>\ud83c\udf21\ufe0f <code>temperature</code>: Controls response creativity (0.7 = balanced)</li> <li>\ud83d\udccf <code>max_tokens</code>: Maximum response length</li> </ul>"},{"location":"examples/agents/huggingface-demo/#huggingface-the-nlp-powerhouse","title":"\ud83e\udd17 HuggingFace: The NLP Powerhouse","text":"<p>HuggingFace is the go-to platform for state-of-the-art natural language processing. It offers unparalleled access to the latest AI research:</p> <ul> <li>\ud83c\udfc6 State-of-the-Art: Access to cutting-edge transformer models and architectures</li> <li>\ud83d\udcda Model Library: Thousands of pre-trained models for every NLP task</li> <li>\ud83d\udd27 Custom Models: Support for your own fine-tuned models and research</li> <li>\ud83e\uddea Research Ready: Perfect for academic research and experimentation</li> <li>\ud83d\udd13 Open Source Models: Most models are open source and freely available</li> <li>\ud83c\udf10 Open Source: Backed by the largest NLP community in the world</li> </ul>"},{"location":"examples/agents/huggingface-demo/#customizing-huggingface-configuration","title":"\ud83d\udd27 Customizing HuggingFace Configuration","text":""},{"location":"examples/agents/huggingface-demo/#change-model","title":"Change Model","text":"<p>Edit <code>agents/huggingface_demo/playbook/huggingface_demo_playbook.yaml</code>:</p> <pre><code>language_model:\n  model: microsoft/DialoGPT-small  # Different HuggingFace model\n  api_base: http://localhost:8001\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#adjust-performance-settings","title":"Adjust Performance Settings","text":"<pre><code>language_model:\n  temperature: 0.5  # More precise responses\n  max_tokens: 4096  # Longer responses\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#use-different-port","title":"Use Different Port","text":"<pre><code>language_model:\n  api_base: http://localhost:9001  # Custom port\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#troubleshooting-huggingface","title":"\ud83d\udea8 Troubleshooting HuggingFace","text":""},{"location":"examples/agents/huggingface-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>HuggingFace Server Not Running <pre><code># Check if HuggingFace server is running\ncurl http://localhost:8001/health\n\n# Start HuggingFace server\nsuper model server huggingface microsoft/Phi-4 --port 8001\n</code></pre></p> </li> <li> <p>Model Not Installed <pre><code># Check installed HuggingFace models\nsuper model list --backend huggingface\n\n# Install the required model\nsuper model install -b huggingface microsoft/Phi-4\n</code></pre></p> </li> <li> <p>Performance Issues</p> </li> <li>Ensure sufficient GPU memory for large models</li> <li>Close other resource-intensive applications</li> <li>Consider using smaller models for faster responses</li> </ol>"},{"location":"examples/agents/huggingface-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect huggingface_demo\n\n# View agent logs\nsuper agent logs huggingface_demo\n\n# Get HuggingFace help\nsuper model server --help\n</code></pre>"},{"location":"examples/agents/huggingface-demo/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>HuggingFace Setup Guide - Complete HuggingFace setup instructions</li> <li>Model Management - Managing HuggingFace models</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/huggingface-demo/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>After exploring the HuggingFace demo:</p> <ol> <li>Try Other Model Backends: Explore MLX, Ollama, or LM Studio demos</li> <li>Customize: Modify the playbook for your specific HuggingFace needs</li> <li>Build Your Own: Use this as a template for your custom HuggingFace agent</li> </ol> <p>Ready to explore advanced NLP? Start with the HuggingFace demo! \ud83d\ude80 </p>"},{"location":"examples/agents/langfuse-integration/","title":"\ud83d\udd0d LangFuse Integration","text":"<p>Learn how to integrate SuperOptiX with LangFuse for comprehensive LLM observability, tracing, and performance monitoring.</p>"},{"location":"examples/agents/langfuse-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>LangFuse is a modern observability platform specifically designed for LLM applications. This integration provides:</p> <ul> <li>Real-time LLM tracing with detailed token usage</li> <li>Performance monitoring and cost tracking</li> <li>User feedback collection and evaluation</li> <li>A/B testing for different agent configurations</li> <li>Production debugging with full trace visibility</li> </ul>"},{"location":"examples/agents/langfuse-integration/#quick-setup","title":"\ud83d\ude80 Quick Setup","text":""},{"location":"examples/agents/langfuse-integration/#prerequisites","title":"Prerequisites","text":"<pre><code># Install LangFuse\npip install langfuse\n\n# Install SuperOptiX with LangFuse support\npip install superoptix[langfuse]\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#basic-configuration","title":"Basic Configuration","text":"<p>Create a configuration file <code>langfuse_config.yaml</code>:</p> <pre><code># langfuse_config.yaml\nlangfuse:\n  public_key: \"your-public-key\"\n  secret_key: \"your-secret-key\"\n  host: \"https://cloud.langfuse.com\"  # or self-hosted URL\n\n# SuperOptiX observability configuration\nobservability:\n  enabled: true\n  backends:\n    - langfuse\n  trace_export:\n    format: \"langfuse\"\n    batch_size: 50\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#integration-setup","title":"\ud83d\udd27 Integration Setup","text":""},{"location":"examples/agents/langfuse-integration/#step-1-set-up-langfuse","title":"Step 1: Set Up LangFuse","text":"<p>Option A: LangFuse Cloud (Recommended) 1. Sign up at cloud.langfuse.com 2. Create a new project 3. Get your API keys from the project settings</p> <p>Option B: Self-Hosted LangFuse <pre><code># Using Docker Compose\nwget https://raw.githubusercontent.com/langfuse/langfuse/main/docker-compose.yml\ndocker-compose up -d\n</code></pre></p>"},{"location":"examples/agents/langfuse-integration/#step-2-configure-superoptix","title":"Step 2: Configure SuperOptiX","text":"<p>Update your agent playbook to include LangFuse integration:</p> <pre><code># developer_playbook.yaml\nname: \"Developer Assistant\"\ndescription: \"AI developer with LangFuse integration\"\ntier: \"genies\"\n\n# ... existing configuration ...\n\nobservability:\n  enabled: true\n  backends:\n    - langfuse\n  langfuse:\n    public_key: \"your-public-key\"\n    secret_key: \"your-secret-key\"\n    host: \"https://cloud.langfuse.com\"\n    project: \"superoptix-agents\"\n    tags:\n      agent_type: \"developer\"\n      tier: \"genies\"\n      environment: \"development\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#step-3-run-agent-with-langfuse-tracking","title":"Step 3: Run Agent with LangFuse Tracking","text":"<pre><code># Set environment variables\nexport LANGFUSE_PUBLIC_KEY=\"your-public-key\"\nexport LANGFUSE_SECRET_KEY=\"your-secret-key\"\n\n# Initialize project\nsuper init langfuse_demo\ncd langfuse_demo\n\n# Pull and compile agent\nsuper agent pull developer\nsuper agent compile developer\n\n# Run with LangFuse tracking\nsuper agent run developer --goal \"Create a REST API endpoint in Python\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#langfuse-integration-features","title":"\ud83d\udcca LangFuse Integration Features","text":""},{"location":"examples/agents/langfuse-integration/#automatic-llm-tracing","title":"Automatic LLM Tracing","text":"<p>SuperOptiX automatically captures detailed LLM interactions:</p> <p>Trace Structure: <pre><code>{\n  \"id\": \"trace-123\",\n  \"name\": \"developer_agent_execution\",\n  \"input\": {\n    \"goal\": \"Create a REST API endpoint in Python\",\n    \"agent_config\": {...}\n  },\n  \"output\": {\n    \"response\": \"Here's a Python REST API endpoint...\",\n    \"generated_code\": \"...\"\n  },\n  \"metadata\": {\n    \"agent_type\": \"developer\",\n    \"tier\": \"genies\",\n    \"model\": \"llama3.1:8b\"\n  }\n}\n</code></pre></p> <p>Span Details: - Model calls with token usage and costs - Tool executions with input/output - Memory operations with context retrieval - Pipeline steps with timing information</p>"},{"location":"examples/agents/langfuse-integration/#performance-metrics","title":"Performance Metrics","text":"<p>LangFuse tracks comprehensive performance data:</p> <p>LLM Metrics: - Token usage (input/output) - Response time - Cost per request - Model performance</p> <p>Agent Metrics: - Tool usage frequency - Memory hit rates - Success/failure rates - User satisfaction scores</p>"},{"location":"examples/agents/langfuse-integration/#advanced-configuration","title":"\ud83d\udd0d Advanced Configuration","text":""},{"location":"examples/agents/langfuse-integration/#custom-trace-attributes","title":"Custom Trace Attributes","text":"<p>Add custom attributes to traces:</p> <pre><code>observability:\n  langfuse:\n    custom_attributes:\n      - name: \"code_complexity\"\n        type: \"number\"\n        description: \"Complexity score of generated code\"\n      - name: \"user_expertise\"\n        type: \"string\"\n        description: \"User expertise level\"\n      - name: \"task_difficulty\"\n        type: \"number\"\n        description: \"Task difficulty rating\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#user-feedback-integration","title":"User Feedback Integration","text":"<p>Collect and track user feedback:</p> <pre><code>observability:\n  langfuse:\n    feedback:\n      enabled: true\n      score_range: [1, 5]\n      comment_enabled: true\n      categories:\n        - \"accuracy\"\n        - \"helpfulness\"\n        - \"code_quality\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#ab-testing-support","title":"A/B Testing Support","text":"<p>Configure A/B testing for different agent configurations:</p> <pre><code>observability:\n  langfuse:\n    ab_testing:\n      enabled: true\n      variants:\n        - name: \"baseline\"\n          config:\n            temperature: 0.7\n            model: \"llama3.1:8b\"\n        - name: \"optimized\"\n          config:\n            temperature: 0.5\n            model: \"llama3.1:70b\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#monitoring-and-analysis","title":"\ud83d\udcc8 Monitoring and Analysis","text":""},{"location":"examples/agents/langfuse-integration/#langfuse-dashboard","title":"LangFuse Dashboard","text":"<p>Access the LangFuse dashboard for real-time monitoring:</p> <pre><code># Open LangFuse dashboard\nopen https://cloud.langfuse.com\n</code></pre> <p>Available Views: - Traces: Individual execution traces - Sessions: User interaction sessions - Metrics: Performance dashboards - Feedback: User feedback analysis - Costs: Token usage and cost tracking</p>"},{"location":"examples/agents/langfuse-integration/#custom-analytics","title":"Custom Analytics","text":"<p>Use LangFuse API for custom analytics:</p> <pre><code>from langfuse import Langfuse\n\n# Initialize LangFuse client\nlangfuse = Langfuse(\n    public_key=\"your-public-key\",\n    secret_key=\"your-secret-key\"\n)\n\n# Query traces\ntraces = langfuse.get_traces(\n    project_id=\"your-project-id\",\n    limit=100\n)\n\n# Analyze performance\nfor trace in traces:\n    print(f\"Trace: {trace.name}\")\n    print(f\"Duration: {trace.duration}ms\")\n    print(f\"Tokens: {trace.input_tokens + trace.output_tokens}\")\n    print(f\"Cost: ${trace.cost}\")\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#alerting-and-notifications","title":"Alerting and Notifications","text":"<p>Set up alerts for performance issues:</p> <pre><code># Example alerting script\ndef check_langfuse_alerts():\n    traces = langfuse.get_traces(limit=10)\n\n    for trace in traces:\n        if trace.duration &gt; 30000:  # 30 seconds\n            send_alert(f\"Slow execution: {trace.name}\")\n\n        if trace.cost &gt; 0.10:  # $0.10\n            send_alert(f\"High cost execution: {trace.name}\")\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"examples/agents/langfuse-integration/#common-issues","title":"Common Issues","text":"<p>1. Authentication Errors <pre><code># Verify API keys\ncurl -H \"Authorization: Bearer your-secret-key\" \\\n     https://cloud.langfuse.com/api/public/traces\n</code></pre></p> <p>2. Connection Issues <pre><code># Test LangFuse connection\npython -c \"\nfrom langfuse import Langfuse\nlangfuse = Langfuse(public_key='your-key', secret_key='your-secret')\nprint('Connection successful')\n\"\n</code></pre></p> <p>3. Missing Traces <pre><code># Enable debug logging\nobservability:\n  langfuse:\n    debug: true\n    log_level: \"DEBUG\"\n    flush_interval: 5  # seconds\n</code></pre></p>"},{"location":"examples/agents/langfuse-integration/#debug-mode","title":"Debug Mode","text":"<p>Enable comprehensive debugging:</p> <pre><code>observability:\n  langfuse:\n    debug: true\n    log_level: \"DEBUG\"\n    verbose: true\n    test_mode: true  # For development\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#production-deployment","title":"\ud83d\ude80 Production Deployment","text":""},{"location":"examples/agents/langfuse-integration/#environment-configuration","title":"Environment Configuration","text":"<p>Set up production environment variables:</p> <pre><code># Production environment\nexport LANGFUSE_PUBLIC_KEY=\"prod-public-key\"\nexport LANGFUSE_SECRET_KEY=\"prod-secret-key\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\"\nexport LANGFUSE_PROJECT=\"production-agents\"\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#kubernetes-integration","title":"Kubernetes Integration","text":"<p>Deploy with Kubernetes secrets:</p> <pre><code># langfuse-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: langfuse-credentials\ntype: Opaque\ndata:\n  public-key: &lt;base64-encoded-public-key&gt;\n  secret-key: &lt;base64-encoded-secret-key&gt;\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#load-balancing","title":"Load Balancing","text":"<p>For high-volume deployments:</p> <pre><code>observability:\n  langfuse:\n    load_balancing:\n      enabled: true\n      batch_size: 100\n      flush_interval: 10\n      max_retries: 3\n      retry_delay: 1\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#cost-tracking","title":"\ud83d\udcca Cost Tracking","text":""},{"location":"examples/agents/langfuse-integration/#token-usage-monitoring","title":"Token Usage Monitoring","text":"<p>Track and optimize token usage:</p> <pre><code># Cost analysis script\ndef analyze_costs():\n    traces = langfuse.get_traces(limit=1000)\n\n    total_cost = sum(trace.cost for trace in traces)\n    total_tokens = sum(trace.input_tokens + trace.output_tokens for trace in traces)\n\n    print(f\"Total cost: ${total_cost:.4f}\")\n    print(f\"Total tokens: {total_tokens:,}\")\n    print(f\"Cost per token: ${total_cost/total_tokens:.6f}\")\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#cost-optimization","title":"Cost Optimization","text":"<p>Implement cost optimization strategies:</p> <pre><code>observability:\n  langfuse:\n    cost_optimization:\n      enabled: true\n      alerts:\n        cost_threshold: 0.05  # $0.05 per request\n        token_threshold: 2000  # 2000 tokens per request\n      recommendations:\n        enabled: true\n        model_switching: true\n        prompt_optimization: true\n</code></pre>"},{"location":"examples/agents/langfuse-integration/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Observability Guide - Complete observability overview</li> <li>Agent Development - Build custom agents</li> <li>LangFuse Documentation - Official LangFuse docs</li> <li>SuperOptiX CLI Reference - CLI commands reference</li> </ul>"},{"location":"examples/agents/langfuse-integration/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Set up LangFuse account and get API keys</li> <li>Configure agent playbooks with LangFuse integration</li> <li>Monitor agent performance through LangFuse dashboard</li> <li>Set up cost tracking and optimization alerts</li> <li>Implement user feedback collection</li> <li>Scale to production with robust monitoring</li> </ol> <p>Ready to monitor your SuperOptiX agents with LangFuse? Start with the basic setup and unlock powerful LLM observability! \ud83d\ude80 </p>"},{"location":"examples/agents/lmstudio-demo/","title":"\ud83c\udfae LM Studio Demo Agent","text":"<p>The LM Studio Demo Agent showcases GUI-based model management with LM Studio in SuperOptiX. This demo focuses specifically on how to configure and use LM Studio models with a visual interface for model management.</p>"},{"location":"examples/agents/lmstudio-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83c\udfae LM Studio Model Integration: How to configure LM Studio models in SuperOptiX</li> <li>\ud83d\udda5\ufe0f GUI Model Management: Visual interface for model management</li> <li>\ud83c\udfe0 Local Model Usage: Running models completely offline</li> <li>\u2699\ufe0f Playbook Configuration: How to set up LM Studio in agent playbooks</li> </ul>"},{"location":"examples/agents/lmstudio-demo/#setup-lm-studio-model","title":"\ud83d\ude80 Setup LM Studio Model","text":""},{"location":"examples/agents/lmstudio-demo/#1-install-lm-studio","title":"1. Install LM Studio","text":"<pre><code># Download and install LM Studio from https://lmstudio.ai\n# Launch LM Studio and download a model through the interface\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#2-install-lm-studio-model","title":"2. Install LM Studio Model","text":"<pre><code># Install the LM Studio model used in this demo\nsuper model install -b lmstudio llama-3.2-8b-instruct\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#3-start-lm-studio-server","title":"3. Start LM Studio Server","text":"<pre><code># Start LM Studio server on port 1234\nsuper model server lmstudio llama-3.2-8b-instruct --port 1234\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#4-pull-and-run-the-demo","title":"4. Pull and Run the Demo","text":"<pre><code># Pull the LM Studio demo agent\nsuper agent pull lmstudio_demo\n\n# Compile the agent\nsuper agent compile lmstudio_demo\n\n# Run the agent\nsuper agent run lmstudio_demo --goal \"What are the key features of LM Studio?\"\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#lm-studio-configuration-in-playbook","title":"\ud83d\udd27 LM Studio Configuration in Playbook","text":"<p>The LM Studio demo showcases how to configure LM Studio models in the agent playbook:</p>"},{"location":"examples/agents/lmstudio-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: lmstudio\n  model: llama-3.2-8b-instruct\n  api_base: http://localhost:1234\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre> <p>Key Configuration Points:</p> <ul> <li>\ud83c\udfaf <code>provider: lmstudio</code>: Specifies LM Studio as the model backend</li> <li>\ud83e\udd16 <code>model</code>: The LM Studio model identifier</li> <li>\ud83c\udf10 <code>api_base</code>: LM Studio server endpoint (default: http://localhost:1234)</li> <li>\ud83c\udf21\ufe0f <code>temperature</code>: Controls response creativity (0.7 = balanced)</li> <li>\ud83d\udccf <code>max_tokens</code>: Maximum response length</li> </ul>"},{"location":"examples/agents/lmstudio-demo/#lm-studio-visual-ai-management","title":"\ud83c\udfae LM Studio: Visual AI Management","text":"<p>LM Studio brings the power of local AI with the simplicity of a graphical interface. Perfect for users who prefer visual tools:</p> <ul> <li>\ud83d\udda5\ufe0f Visual Interface: Beautiful GUI for managing models and conversations</li> <li>\ud83d\udcca Real-time Monitoring: Watch your model's performance in real-time</li> <li>\ud83c\udfaf Easy Model Selection: Browse and select models with a visual interface</li> <li>\ud83d\uddb1\ufe0f Point-and-Click: No command line required for basic operations</li> <li>\ud83e\ude9f Windows Native: Optimized for Windows users with familiar interface</li> <li>\ud83c\udf4e macOS Support: Also works great on macOS systems</li> </ul>"},{"location":"examples/agents/lmstudio-demo/#customizing-lm-studio-configuration","title":"\ud83d\udd27 Customizing LM Studio Configuration","text":""},{"location":"examples/agents/lmstudio-demo/#change-model","title":"Change Model","text":"<p>Edit <code>agents/lmstudio_demo/playbook/lmstudio_demo_playbook.yaml</code>:</p> <pre><code>language_model:\n  model: llama-3.2-1b-instruct  # Different LM Studio model\n  api_base: http://localhost:1234\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#adjust-performance-settings","title":"Adjust Performance Settings","text":"<pre><code>language_model:\n  temperature: 0.5  # More precise responses\n  max_tokens: 4096  # Longer responses\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#use-different-port","title":"Use Different Port","text":"<pre><code>language_model:\n  api_base: http://localhost:8080  # Custom port\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#troubleshooting-lm-studio","title":"\ud83d\udea8 Troubleshooting LM Studio","text":""},{"location":"examples/agents/lmstudio-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>LM Studio Server Not Running <pre><code># Check if LM Studio server is running\ncurl http://localhost:1234/v1/models\n\n# Start LM Studio server\nsuper model server lmstudio llama3.2:8b --port 1234\n</code></pre></p> </li> <li> <p>Model Not Installed <pre><code># Check installed LM Studio models\nsuper model list --backend lmstudio\n\n# Install the required model\nsuper model install -b lmstudio llama3.2:8b\n</code></pre></p> </li> <li> <p>Performance Issues</p> </li> <li>Ensure sufficient RAM (8GB+ recommended)</li> <li>Close other resource-intensive applications</li> <li>Consider using smaller models for faster responses</li> </ol>"},{"location":"examples/agents/lmstudio-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect lmstudio_demo\n\n# View agent logs\nsuper agent logs lmstudio_demo\n\n# Get LM Studio help\nsuper model server --help\n</code></pre>"},{"location":"examples/agents/lmstudio-demo/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>LM Studio Setup Guide - Complete LM Studio setup instructions</li> <li>Model Management - Managing LM Studio models</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/lmstudio-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other Model Backends: Explore MLX, Ollama, or HuggingFace demos</li> </ol> <p>Ready to explore GUI model management? Start with the LM Studio demo! \ud83d\ude80 </p>"},{"location":"examples/agents/memory-demo/","title":"\ud83e\udde0 Memory Demo Agent","text":"<p>The Memory Demo Agent showcases multi-layered memory system capabilities in SuperOptiX. This demo focuses specifically on how to configure and use memory systems for context retention and knowledge persistence.</p>"},{"location":"examples/agents/memory-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83e\udde0 Memory Integration: How to configure memory systems in SuperOptiX agents</li> <li>\ud83c\udfd7\ufe0f Multi-Layered Memory: Short-term, long-term, and episodic memory</li> <li>\ud83d\udcad Context Retention: How agents remember and use past information</li> <li>\u2699\ufe0f Playbook Configuration: How to set up memory in agent playbooks</li> </ul>"},{"location":"examples/agents/memory-demo/#setup-memory-demo","title":"\ud83d\ude80 Setup Memory Demo","text":""},{"location":"examples/agents/memory-demo/#1-install-ollama-model","title":"1. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/memory-demo/#2-start-ollama-server","title":"2. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/memory-demo/#3-pull-and-run-the-demo","title":"3. Pull and Run the Demo","text":"<pre><code># Pull the Memory demo agent\nsuper agent pull memory_demo\n\n# Compile the agent\nsuper agent compile memory_demo\n\n# Run the agent\nsuper agent run memory_demo --goal \"What memory systems are available and how do they work?\"\n</code></pre>"},{"location":"examples/agents/memory-demo/#memory-configuration-in-playbook","title":"\ud83d\udd27 Memory Configuration in Playbook","text":"<p>The Memory demo showcases how to configure memory systems in the agent playbook:</p>"},{"location":"examples/agents/memory-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"examples/agents/memory-demo/#memory-configuration","title":"Memory Configuration","text":"<pre><code>memory:\n  enabled: true\n  short_term:\n    enabled: true\n    max_tokens: 1000\n    storage_type: in_memory\n  long_term:\n    enabled: true\n    storage_type: local\n    max_entries: 100\n    persistence_path: ./data/memory/long_term\n  episodic:\n    enabled: true\n    max_episodes: 50\n    storage_type: local\n    persistence_path: ./data/memory/episodic\n</code></pre> <p>Key Memory Configuration Points:</p> <ul> <li><code>enabled: true</code>: Enables memory functionality</li> <li>\u26a1 <code>short_term</code>: Immediate context retention (1000 tokens)</li> <li>\ud83d\udcbe <code>long_term</code>: Persistent knowledge storage (100 entries)</li> <li>\ud83d\udcda <code>episodic</code>: Conversation episode memory (50 episodes)</li> <li>\ud83d\uddc4\ufe0f <code>storage_type</code>: Local file storage for persistence</li> <li>\ud83d\udcc1 <code>persistence_path</code>: Local storage directories</li> </ul>"},{"location":"examples/agents/memory-demo/#memory-your-ais-brain","title":"\ud83e\udde0 Memory: Your AI's Brain","text":"<p>Memory systems give your AI agent the ability to learn, remember, and build relationships over time. It's like giving your AI a brain that grows smarter with each interaction:</p>"},{"location":"examples/agents/memory-demo/#three-layer-memory-architecture","title":"\ud83c\udfd7\ufe0f Three-Layer Memory Architecture","text":"<ul> <li>\u26a1 Short-term Memory: Holds the current conversation context (like your working memory)</li> <li>\ud83d\udcbe Long-term Memory: Stores important facts and knowledge permanently (like your long-term memory)</li> <li>\ud83d\udcda Episodic Memory: Remembers past conversations and experiences (like your episodic memory)</li> </ul>"},{"location":"examples/agents/memory-demo/#key-benefits","title":"\ud83c\udfaf Key Benefits","text":"<ul> <li>\ud83d\udd04 Context Continuity: Maintains conversation flow across multiple interactions</li> <li>\ud83d\udcc8 Learning Over Time: Builds knowledge and improves responses with experience</li> <li>\ud83d\udc64 Personalization: Remembers user preferences and adapts accordingly</li> <li>\ud83d\udd17 Relationship Building: Creates meaningful, ongoing relationships with users</li> </ul>"},{"location":"examples/agents/memory-demo/#customizing-memory-configuration","title":"\ud83d\udd27 Customizing Memory Configuration","text":""},{"location":"examples/agents/memory-demo/#adjust-memory-settings","title":"Adjust Memory Settings","text":"<p>Edit <code>agents/memory_demo/playbook/memory_demo_playbook.yaml</code>:</p> <pre><code>memory:\n  short_term:\n    max_tokens: 2000  # More context\n  long_term:\n    max_entries: 200  # More persistent storage\n  episodic:\n    max_episodes: 100  # More episodes\n</code></pre>"},{"location":"examples/agents/memory-demo/#change-storage-type","title":"Change Storage Type","text":"<pre><code>memory:\n  long_term:\n    storage_type: database  # Use database instead of local files\n    connection_string: \"sqlite:///memory.db\"\n</code></pre>"},{"location":"examples/agents/memory-demo/#disable-memory-types","title":"Disable Memory Types","text":"<pre><code>memory:\n  short_term:\n    enabled: false  # Disable short-term memory\n  long_term:\n    enabled: true\n  episodic:\n    enabled: true\n</code></pre>"},{"location":"examples/agents/memory-demo/#troubleshooting-memory","title":"\ud83d\udea8 Troubleshooting Memory","text":""},{"location":"examples/agents/memory-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>Memory Not Working <pre><code># Check memory configuration\nsuper agent inspect memory_demo\n\n# Verify memory is enabled\n</code></pre></p> </li> <li> <p>Memory Storage Issues <pre><code># Check memory storage directories\nls -la ./data/memory/\n\n# Clear memory data if needed\nrm -rf ./data/memory/\n</code></pre></p> </li> </ol>"},{"location":"examples/agents/memory-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect memory_demo\n\n# View agent logs\nsuper agent logs memory_demo\n\n# Get memory help\nsuper agent --help\n</code></pre>"},{"location":"examples/agents/memory-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/milvus-demo/","title":"Milvus RAG Demo \ud83c\udfd7\ufe0f","text":"<p>A comprehensive demonstration of SuperOptiX's RAG capabilities using Milvus vector database for enterprise-grade similarity search and scalable knowledge retrieval.</p>"},{"location":"examples/agents/milvus-demo/#overview","title":"Overview","text":"<p>This demo showcases how to integrate Milvus - a cloud-native vector database - with SuperOptiX's RAG system to create intelligent agents capable of retrieving and synthesizing information from massive knowledge bases with enterprise-level scalability.</p>"},{"location":"examples/agents/milvus-demo/#this-demo-demonstrates","title":"This demo demonstrates:","text":"<p>\ud83c\udfd7\ufe0f Milvus Vector Database Integration - Enterprise-grade connection to Milvus for scalable vector storage</p> <p>\ud83c\udf10 Cloud-Native Architecture - Distributed vector search with horizontal scaling capabilities</p> <p>\ud83d\udcc8 Massive Scale Operations - Handle millions of vectors with high performance</p> <p>\ud83d\udd0d Advanced Indexing - Multiple indexing strategies for different use cases</p> <p>\ud83d\udd04 RAG Pipeline Integration - Complete retrieval-augmented generation workflow</p>"},{"location":"examples/agents/milvus-demo/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/agents/milvus-demo/#install-superoptix","title":"Install SuperOptiX","text":"<pre><code>pip install superoptix\n</code></pre>"},{"location":"examples/agents/milvus-demo/#install-milvus-dependencies","title":"Install Milvus Dependencies","text":"<pre><code>pip install pymilvus\n</code></pre>"},{"location":"examples/agents/milvus-demo/#set-up-milvus-server","title":"Set Up Milvus Server","text":"<pre><code># Using Docker Compose (recommended)\nwget https://github.com/milvus-io/milvus/releases/download/v2.5.12/milvus-standalone-docker-compose.yml\ndocker-compose up -d\n</code></pre>"},{"location":"examples/agents/milvus-demo/#install-and-serve-model","title":"Install and Serve Model","text":"<pre><code># Install a model (if not already installed)\nsuper model install llama3.1:8b\n\n# Start Ollama server (if using Ollama backend)\nollama serve\n</code></pre>"},{"location":"examples/agents/milvus-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/agents/milvus-demo/#pull-the-demo-agent","title":"Pull the Demo Agent","text":"<pre><code>super agent pull rag_milvus_demo\n</code></pre>"},{"location":"examples/agents/milvus-demo/#compile-the-agent","title":"Compile the Agent","text":"<pre><code>super agent compile rag_milvus_demo\n</code></pre>"},{"location":"examples/agents/milvus-demo/#run-the-demo","title":"Run the Demo","text":"<pre><code>super agent run rag_milvus_demo --goal \"What are the key features of Milvus and how does it work with SuperOptiX?\"\n</code></pre>"},{"location":"examples/agents/milvus-demo/#key-configuration-points","title":"Key Configuration Points:","text":"<p>\ud83d\udd27 Vector Database Setup - Configured to connect to Milvus at <code>localhost:19530</code></p> <p>\ud83d\udcca Collection Management - Uses <code>superoptix_knowledge</code> collection for document storage</p> <p>\ud83c\udfaf Embedding Model - Leverages <code>sentence-transformers/all-MiniLM-L6-v2</code> for vector generation</p> <p>\u2699\ufe0f Search Parameters - Optimized with <code>top_k: 5</code> and <code>similarity_threshold: 0.7</code></p>"},{"location":"examples/agents/milvus-demo/#playbook-configuration","title":"Playbook Configuration","text":"<p>The demo uses a specialized playbook with Milvus-specific configurations:</p> <pre><code>rag:\n  enabled: true\n  retriever_type: milvus\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n    similarity_threshold: 0.7\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    host: localhost\n    port: 19530\n    collection_name: superoptix_knowledge\n</code></pre>"},{"location":"examples/agents/milvus-demo/#customization","title":"Customization","text":""},{"location":"examples/agents/milvus-demo/#modify-milvus-connection","title":"Modify Milvus Connection","text":"<pre><code>vector_store:\n  host: your-milvus-host\n  port: 19530\n  collection_name: YourCustomCollection\n  user: your-username  # If using authentication\n  password: your-password\n</code></pre>"},{"location":"examples/agents/milvus-demo/#adjust-search-parameters","title":"Adjust Search Parameters","text":"<pre><code>config:\n  top_k: 10  # Retrieve more documents\n  similarity_threshold: 0.8  # Higher similarity threshold\n  chunk_size: 1024  # Larger chunks\n</code></pre>"},{"location":"examples/agents/milvus-demo/#configure-collection-schema","title":"Configure Collection Schema","text":"<pre><code>vector_store:\n  schema:\n    fields:\n      - name: id\n        dtype: INT64\n        is_primary: true\n      - name: content\n        dtype: VARCHAR\n        max_length: 65535\n      - name: embedding\n        dtype: FLOAT_VECTOR\n        dim: 384\n</code></pre>"},{"location":"examples/agents/milvus-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/agents/milvus-demo/#connection-issues","title":"Connection Issues","text":"<ul> <li>Error: \"Connection refused\"</li> <li>Solution: Ensure Milvus server is running on port 19530</li> <li>Check: <code>curl http://localhost:9091/healthz</code></li> </ul>"},{"location":"examples/agents/milvus-demo/#collection-issues","title":"Collection Issues","text":"<ul> <li>Error: \"Collection not found\"</li> <li>Solution: Create collection manually or check collection name</li> <li>Check: Use Milvus CLI or Python client to list collections</li> </ul>"},{"location":"examples/agents/milvus-demo/#performance-issues","title":"Performance Issues","text":"<ul> <li>Slow queries: Optimize index parameters or reduce <code>top_k</code></li> <li>Memory issues: Adjust <code>chunk_size</code> and <code>chunk_overlap</code></li> </ul>"},{"location":"examples/agents/milvus-demo/#use-cases","title":"Use Cases","text":"<p>\ud83c\udfe2 Enterprise Search - Large-scale corporate knowledge management</p> <p>\ud83d\udd2c Scientific Research - Massive dataset analysis and discovery</p> <p>\ud83d\udcf1 Recommendation Systems - High-throughput content recommendation</p> <p>\ud83c\udfaf E-commerce - Product search and recommendation engines</p>"},{"location":"examples/agents/milvus-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>SuperOptiX RAG Guide</li> <li>RAG Integration Guide - Vector database setup and configuration</li> <li>Model Management</li> </ul> <p>Ready to scale your vector search with Milvus? \ud83d\ude80</p> <p>Start with this demo to understand how Milvus's enterprise-grade capabilities can power your AI applications with massive-scale knowledge retrieval and similarity search. </p>"},{"location":"examples/agents/mlflow-integration/","title":"\ud83e\uddea MLFlow Integration","text":"<p>Learn how to integrate SuperOptiX with MLFlow for comprehensive experiment tracking, model monitoring, and observability.</p>"},{"location":"examples/agents/mlflow-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>MLFlow is a popular open-source platform (third-party) for managing the complete machine learning lifecycle. This integration allows you to:</p> <ul> <li>Track experiments with detailed metrics and parameters</li> <li>Monitor model performance across different runs</li> <li>Compare agent versions and configurations</li> <li>Export trace data to MLFlow for analysis</li> <li>Visualize agent behavior in MLFlow UI</li> </ul>"},{"location":"examples/agents/mlflow-integration/#quick-setup","title":"\ud83d\ude80 Quick Setup","text":""},{"location":"examples/agents/mlflow-integration/#prerequisites","title":"Prerequisites","text":"<pre><code># Install MLFlow\npip install mlflow\n\n# Install SuperOptiX with MLFlow support\npip install superoptix[mlflow]\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#basic-configuration","title":"Basic Configuration","text":"<p>Create a configuration file <code>mlflow_config.yaml</code>:</p> <pre><code># mlflow_config.yaml\nmlflow:\n  tracking_uri: \"http://localhost:5000\"  # MLFlow server URL\n  experiment_name: \"superoptix_agents\"\n  log_artifacts: true\n  log_metrics: true\n  log_params: true\n\n# SuperOptiX observability configuration\nobservability:\n  enabled: true\n  backends:\n    - mlflow\n  trace_export:\n    format: \"mlflow\"\n    batch_size: 100\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#integration-setup","title":"\ud83d\udd27 Integration Setup","text":""},{"location":"examples/agents/mlflow-integration/#step-1-start-mlflow-server","title":"Step 1: Start MLFlow Server","text":"<pre><code># Start MLFlow tracking server\nmlflow server --host 0.0.0.0 --port 5000\n\n# Or use MLFlow UI\nmlflow ui --host 0.0.0.0 --port 5000\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#step-2-configure-superoptix","title":"Step 2: Configure SuperOptiX","text":"<p>Update your agent playbook to include MLFlow integration:</p> <pre><code># developer_playbook.yaml\nname: \"Developer Assistant\"\ndescription: \"AI developer with MLFlow integration\"\ntier: \"genies\"\n\n# ... existing configuration ...\n\nobservability:\n  enabled: true\n  backends:\n    - mlflow\n  mlflow:\n    experiment_name: \"developer_agent\"\n    tracking_uri: \"http://localhost:5000\"\n    log_artifacts: true\n    log_metrics: true\n    log_params: true\n    tags:\n      agent_type: \"developer\"\n      tier: \"genies\"\n      version: \"1.0.0\"\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#step-3-run-agent-with-mlflow-tracking","title":"Step 3: Run Agent with MLFlow Tracking","text":"<pre><code># Initialize project\nsuper init mlflow_demo\ncd mlflow_demo\n\n# Pull and compile agent\nsuper agent pull developer\nsuper agent compile developer\n\n# Run with MLFlow tracking\nsuper agent run developer --goal \"Write a Python function to calculate fibonacci numbers\"\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#mlflow-integration-features","title":"\ud83d\udcca MLFlow Integration Features","text":""},{"location":"examples/agents/mlflow-integration/#automatic-experiment-tracking","title":"Automatic Experiment Tracking","text":"<p>SuperOptiX automatically logs the following to MLFlow:</p> <p>Parameters: - Agent configuration - Model settings - Tool configurations - Execution parameters</p> <p>Metrics: - Execution time - Token usage - Tool usage frequency - Success/failure rates - Memory usage</p> <p>Artifacts: - Trace files (JSONL format) - Generated code - Tool outputs - Error logs</p>"},{"location":"examples/agents/mlflow-integration/#example-mlflow-run","title":"Example MLFlow Run","text":"<pre><code># MLFlow run structure\n{\n  \"run_id\": \"abc123\",\n  \"experiment_name\": \"developer_agent\",\n  \"parameters\": {\n    \"agent_name\": \"developer\",\n    \"tier\": \"genies\",\n    \"model\": \"llama3.1:8b\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 2048\n  },\n  \"metrics\": {\n    \"execution_time_ms\": 10270.0,\n    \"total_tokens\": 1250,\n    \"tools_used\": 3,\n    \"success_rate\": 1.0\n  },\n  \"artifacts\": {\n    \"traces.jsonl\": \"path/to/traces.jsonl\",\n    \"generated_code.py\": \"path/to/output.py\",\n    \"tool_outputs.json\": \"path/to/tools.json\"\n  }\n}\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#advanced-configuration","title":"\ud83d\udd0d Advanced Configuration","text":""},{"location":"examples/agents/mlflow-integration/#custom-metrics-logging","title":"Custom Metrics Logging","text":"<p>Extend the MLFlow integration with custom metrics:</p> <pre><code># Custom metrics configuration\nobservability:\n  mlflow:\n    custom_metrics:\n      - name: \"code_quality_score\"\n        type: \"float\"\n        description: \"Code quality assessment score\"\n      - name: \"tool_efficiency\"\n        type: \"float\"\n        description: \"Tool usage efficiency ratio\"\n      - name: \"response_relevance\"\n        type: \"float\"\n        description: \"Response relevance score\"\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#batch-processing","title":"Batch Processing","text":"<p>Configure batch processing for high-volume scenarios:</p> <pre><code>observability:\n  mlflow:\n    batch_processing:\n      enabled: true\n      batch_size: 100\n      flush_interval: 30  # seconds\n      max_queue_size: 1000\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#custom-tags-and-metadata","title":"Custom Tags and Metadata","text":"<p>Add custom tags and metadata to MLFlow runs:</p> <pre><code>observability:\n  mlflow:\n    tags:\n      environment: \"production\"\n      team: \"ai_engineering\"\n      project: \"code_assistant\"\n    metadata:\n      version: \"1.2.0\"\n      deployment: \"kubernetes\"\n      region: \"us-west-2\"\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#monitoring-and-analysis","title":"\ud83d\udcc8 Monitoring and Analysis","text":""},{"location":"examples/agents/mlflow-integration/#mlflow-ui-dashboard","title":"MLFlow UI Dashboard","text":"<p>Access the MLFlow UI to view experiments:</p> <pre><code># Open MLFlow UI\nopen http://localhost:5000\n</code></pre> <p>Available Views: - Experiments: Compare different agent runs - Runs: Detailed run information - Metrics: Performance trends over time - Artifacts: Generated outputs and traces - Parameters: Configuration comparisons</p>"},{"location":"examples/agents/mlflow-integration/#custom-dashboards","title":"Custom Dashboards","text":"<p>Create custom dashboards using MLFlow's API:</p> <pre><code>import mlflow\nimport pandas as pd\n\n# Query experiments\nexperiments = mlflow.search_experiments()\nruns = mlflow.search_runs(experiment_names=[\"developer_agent\"])\n\n# Analyze performance trends\nperformance_df = runs[[\"execution_time_ms\", \"total_tokens\", \"success_rate\"]]\nprint(performance_df.describe())\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#alerting-and-notifications","title":"Alerting and Notifications","text":"<p>Set up alerts for performance degradation:</p> <pre><code># Example alerting script\ndef check_performance_alerts():\n    runs = mlflow.search_runs(experiment_names=[\"developer_agent\"])\n    latest_run = runs.iloc[0]\n\n    if latest_run[\"execution_time_ms\"] &gt; 15000:  # 15 seconds\n        send_alert(\"Agent execution time exceeded threshold\")\n\n    if latest_run[\"success_rate\"] &lt; 0.9:\n        send_alert(\"Agent success rate below threshold\")\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"examples/agents/mlflow-integration/#common-issues","title":"Common Issues","text":"<p>1. MLFlow Server Connection <pre><code># Check MLFlow server status\ncurl http://localhost:5000/health\n\n# Restart MLFlow server\nmlflow server --host 0.0.0.0 --port 5000\n</code></pre></p> <p>2. Authentication Issues <pre><code># Set MLFlow tracking URI\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n\n# Or configure in code\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n</code></pre></p> <p>3. Artifact Storage <pre><code># Configure artifact storage\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlflow_artifacts\n</code></pre></p>"},{"location":"examples/agents/mlflow-integration/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for MLFlow integration:</p> <pre><code>observability:\n  mlflow:\n    debug: true\n    log_level: \"DEBUG\"\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#production-deployment","title":"\ud83d\ude80 Production Deployment","text":""},{"location":"examples/agents/mlflow-integration/#mlflow-server-setup","title":"MLFlow Server Setup","text":"<p>For production deployments, use a robust MLFlow setup:</p> <pre><code># Using PostgreSQL backend\nmlflow server \\\n  --backend-store-uri postgresql://user:pass@host:port/db \\\n  --default-artifact-root s3://bucket/mlflow \\\n  --host 0.0.0.0 \\\n  --port 5000\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Deploy MLFlow with Kubernetes:</p> <pre><code># mlflow-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mlflow-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mlflow\n  template:\n    metadata:\n      labels:\n        app: mlflow\n    spec:\n      containers:\n      - name: mlflow\n        image: python:3.9\n        command: [\"mlflow\", \"server\"]\n        args: [\"--host\", \"0.0.0.0\", \"--port\", \"5000\"]\n        ports:\n        - containerPort: 5000\n</code></pre>"},{"location":"examples/agents/mlflow-integration/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Observability Guide - Complete observability overview</li> <li>Agent Development - Build custom agents</li> <li>MLFlow Documentation - Official MLFlow docs</li> <li>SuperOptiX CLI Reference - CLI commands reference</li> </ul>"},{"location":"examples/agents/mlflow-integration/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Set up MLFlow server for your environment</li> <li>Configure agent playbooks with MLFlow integration</li> <li>Monitor agent performance through MLFlow UI</li> <li>Set up alerts for performance degradation</li> <li>Scale to production with robust MLFlow deployment</li> </ol> <p>Ready to track your SuperOptiX agents with MLFlow? Start with the basic setup and scale up as needed! \ud83d\ude80 </p>"},{"location":"examples/agents/mlx-demo/","title":"\ud83c\udf4e MLX Demo Agent","text":"<p>The MLX Demo Agent showcases Apple Silicon optimization with MLX models in SuperOptiX. This demo focuses specifically on how to configure and use MLX models for native Apple Silicon performance.</p>"},{"location":"examples/agents/mlx-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83c\udf4e MLX Model Integration: How to configure MLX models in SuperOptiX</li> <li>\u26a1 Apple Silicon Optimization: Native performance on Apple Silicon Macs</li> <li>\ud83c\udfe0 Local Model Usage: Running models completely offline</li> <li>\u2699\ufe0f Playbook Configuration: How to set up MLX in agent playbooks</li> </ul>"},{"location":"examples/agents/mlx-demo/#setup-mlx-model","title":"\ud83d\ude80 Setup MLX Model","text":""},{"location":"examples/agents/mlx-demo/#1-install-mlx-dependencies","title":"1. Install MLX Dependencies","text":"<pre><code># Install MLX dependencies\npip install \"superoptix[mlx]\"\n</code></pre>"},{"location":"examples/agents/mlx-demo/#2-install-mlx-model","title":"2. Install MLX Model","text":"<pre><code># Install the MLX model used in this demo\nsuper model install -b mlx mlx-community/Llama-3.2-3B-Instruct-4bit\n</code></pre>"},{"location":"examples/agents/mlx-demo/#3-start-mlx-server","title":"3. Start MLX Server","text":"<pre><code># Start MLX server on port 8000\nsuper model server mlx mlx-community/Llama-3.2-3B-Instruct-4bit --port 8000\n</code></pre>"},{"location":"examples/agents/mlx-demo/#4-pull-and-run-the-demo","title":"4. Pull and Run the Demo","text":"<pre><code># Pull the MLX demo agent\nsuper agent pull mlx_demo\n\n# Compile the agent\nsuper agent compile mlx_demo\n\n# Run the agent\nsuper agent run mlx_demo --goal \"What are the key features of MLX?\"\n</code></pre>"},{"location":"examples/agents/mlx-demo/#mlx-configuration-in-playbook","title":"\ud83d\udd27 MLX Configuration in Playbook","text":"<p>The MLX demo showcases how to configure MLX models in the agent playbook:</p>"},{"location":"examples/agents/mlx-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: mlx\n  model: mlx-community/Llama-3.2-3B-Instruct-4bit\n  api_base: http://localhost:8000\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre> <p>Key Configuration Points:</p> <ul> <li>\ud83c\udfaf <code>provider: mlx</code>: Specifies MLX as the model backend</li> <li>\ud83e\udd16 <code>model</code>: The MLX model identifier</li> <li>\ud83c\udf10 <code>api_base</code>: MLX server endpoint (default: http://localhost:8000)</li> <li>\ud83c\udf21\ufe0f <code>temperature</code>: Controls response creativity (0.7 = balanced)</li> <li>\ud83d\udccf <code>max_tokens</code>: Maximum response length</li> </ul>"},{"location":"examples/agents/mlx-demo/#why-choose-mlx","title":"\ud83c\udf4e Why Choose MLX?","text":"<p>MLX is Apple's native machine learning framework, designed specifically for Apple Silicon Macs. It offers:</p> <ul> <li>\u26a1 Native Performance: Leverages Apple's Metal Performance Shaders for blazing-fast inference</li> <li>\ud83d\udd0b Battery Efficient: Optimized power consumption perfect for MacBook users</li> <li>\ud83d\udcbe Memory Smart: Efficient memory usage with 4-bit quantized models</li> <li>\ud83c\udfe0 Completely Local: No internet required after model download</li> <li>\ud83d\ude80 Instant Start: Quick model loading and inference times</li> </ul>"},{"location":"examples/agents/mlx-demo/#customizing-mlx-configuration","title":"\ud83d\udd27 Customizing MLX Configuration","text":""},{"location":"examples/agents/mlx-demo/#change-model","title":"Change Model","text":"<p>Edit <code>agents/mlx_demo/playbook/mlx_demo_playbook.yaml</code>:</p> <pre><code>language_model:\n  model: mlx-community/phi-2  # Different MLX model\n  api_base: http://localhost:8000\n</code></pre>"},{"location":"examples/agents/mlx-demo/#adjust-performance-settings","title":"Adjust Performance Settings","text":"<pre><code>language_model:\n  temperature: 0.5  # More precise responses\n  max_tokens: 4096  # Longer responses\n</code></pre>"},{"location":"examples/agents/mlx-demo/#use-different-port","title":"Use Different Port","text":"<pre><code>language_model:\n  api_base: http://localhost:9000  # Custom port\n</code></pre>"},{"location":"examples/agents/mlx-demo/#troubleshooting-mlx","title":"\ud83d\udea8 Troubleshooting MLX","text":""},{"location":"examples/agents/mlx-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>MLX Server Not Running <pre><code># Check if MLX server is running\ncurl http://localhost:8000/health\n\n# Start MLX server\nsuper model server mlx mlx-community/Llama-3.2-3B-Instruct-4bit --port 8000\n</code></pre></p> </li> <li> <p>Model Not Installed <pre><code># Check installed MLX models\nsuper model list --backend mlx\n\n# Install the required model\nsuper model install -b mlx mlx-community/Llama-3.2-3B-Instruct-4bit\n</code></pre></p> </li> <li> <p>Apple Silicon Required</p> </li> <li>MLX only works on Apple Silicon Macs (M1, M2, M3)</li> <li>Use Ollama for Intel Macs</li> </ol>"},{"location":"examples/agents/mlx-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect mlx_demo\n\n# View agent logs\nsuper agent logs mlx_demo\n\n# Get MLX help\nsuper model server --help\n</code></pre>"},{"location":"examples/agents/mlx-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Model Management - Managing MLX models</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/mlx-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other Model Backends: Explore Ollama, HuggingFace, or LM Studio demos </li> </ol>"},{"location":"examples/agents/observability-demo/","title":"\ud83d\udcca Observability Demo Agent","text":"<p>The Observability Demo Agent showcases monitoring and debugging capabilities in SuperOptiX. This demo focuses specifically on how to configure and use observability features for monitoring, tracing, and debugging agent performance.</p>"},{"location":"examples/agents/observability-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83d\udcca Observability Integration: How to configure observability in SuperOptiX agents</li> <li>\ud83d\udcc8 Performance Monitoring: Real-time monitoring and metrics</li> <li>\ud83d\udc1b Debugging Capabilities: Tracing and debugging tools</li> <li>\u2699\ufe0f Playbook Configuration: How to set up observability in agent playbooks</li> </ul>"},{"location":"examples/agents/observability-demo/#setup-observability-demo","title":"\ud83d\ude80 Setup Observability Demo","text":""},{"location":"examples/agents/observability-demo/#1-install-ollama-model","title":"1. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/observability-demo/#2-start-ollama-server","title":"2. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/observability-demo/#3-pull-and-run-the-demo","title":"3. Pull and Run the Demo","text":"<pre><code># Pull the Observability demo agent\nsuper agent pull observability_demo\n\n# Compile the agent\nsuper agent compile observability_demo\n\n# Run the agent\nsuper agent run observability_demo --goal \"What observability features are available and how do they work?\"\n</code></pre>"},{"location":"examples/agents/observability-demo/#observability-configuration-in-playbook","title":"\ud83d\udd27 Observability Configuration in Playbook","text":"<p>The Observability demo showcases how to configure observability in the agent playbook:</p>"},{"location":"examples/agents/observability-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"examples/agents/observability-demo/#observability-configuration","title":"Observability Configuration","text":"<pre><code>react_config:\n  max_iters: 5\n  max_tool_calls: 3\n  tool_selection_strategy: automatic\n  reasoning_style: step_by_step\n  error_handling: retry\n  enable_tracing: true\n  enable_metrics: true\n  enable_debugging: true\n</code></pre> <p>Key Observability Configuration Points:</p> <ul> <li>\ud83d\udd0d <code>enable_tracing: true</code>: Enables request/response tracing</li> <li>\ud83d\udcca <code>enable_metrics: true</code>: Enables performance metrics collection</li> <li>\ud83d\udc1b <code>enable_debugging: true</code>: Enables debugging capabilities</li> <li>\ud83d\udd04 <code>error_handling: retry</code>: Automatic error retry with logging</li> <li>\ud83e\udde0 <code>reasoning_style: step_by_step</code>: Detailed reasoning logs</li> </ul>"},{"location":"examples/agents/observability-demo/#observability-your-ais-health-monitor","title":"\ud83d\udcca Observability: Your AI's Health Monitor","text":"<p>Observability gives you complete visibility into your AI agent's performance, health, and behavior. It's like having a comprehensive monitoring system for your AI:</p>"},{"location":"examples/agents/observability-demo/#what-you-can-monitor","title":"\ud83d\udd0d What You Can Monitor","text":"<ul> <li>\ud83d\udcc8 Performance Metrics: Real-time tracking of response times, throughput, and efficiency</li> <li>\ud83d\udd0d Request Tracing: Follow every request from start to finish through the system</li> <li>\ud83d\udc1b Debugging Tools: Step-by-step debugging to identify and fix issues</li> <li>\ud83d\udcca Analytics Dashboard: Visual insights into your AI's behavior and performance</li> </ul>"},{"location":"examples/agents/observability-demo/#key-capabilities","title":"\ud83c\udfaf Key Capabilities","text":"<ul> <li>\ud83d\udea8 Proactive Alerting: Get notified before problems become critical</li> <li>\ud83d\udcdd Comprehensive Logging: Detailed logs for troubleshooting and analysis</li> <li>\ud83c\udf9b\ufe0f Performance Tuning: Identify bottlenecks and optimize performance</li> <li>\ud83d\udd27 Production Ready: Built for monitoring AI systems in production environments</li> </ul>"},{"location":"examples/agents/observability-demo/#customizing-observability-configuration","title":"\ud83d\udd27 Customizing Observability Configuration","text":""},{"location":"examples/agents/observability-demo/#adjust-tracing-settings","title":"Adjust Tracing Settings","text":"<p>Edit <code>agents/observability_demo/playbook/observability_demo_playbook.yaml</code>:</p> <pre><code>react_config:\n  enable_tracing: true\n  enable_metrics: true\n  enable_debugging: true\n  trace_level: detailed  # More detailed tracing\n</code></pre>"},{"location":"examples/agents/observability-demo/#change-error-handling","title":"Change Error Handling","text":"<pre><code>react_config:\n  error_handling: log_and_continue  # Different error handling strategy\n  max_retries: 3  # Maximum retry attempts\n</code></pre>"},{"location":"examples/agents/observability-demo/#disable-observability-features","title":"Disable Observability Features","text":"<pre><code>react_config:\n  enable_tracing: false  # Disable tracing\n  enable_metrics: true\n  enable_debugging: true\n</code></pre>"},{"location":"examples/agents/observability-demo/#troubleshooting-observability","title":"\ud83d\udea8 Troubleshooting Observability","text":""},{"location":"examples/agents/observability-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>Observability Not Working <pre><code># Check observability configuration\nsuper agent inspect observability_demo\n\n# Verify observability is enabled\n</code></pre></p> </li> <li> <p>Performance Issues <pre><code># Check agent logs\nsuper agent logs observability_demo\n\n# Monitor performance metrics\n</code></pre></p> </li> </ol>"},{"location":"examples/agents/observability-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect observability_demo\n\n# View agent logs\nsuper agent logs observability_demo\n\n# Get observability help\nsuper agent --help\n</code></pre>"},{"location":"examples/agents/observability-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Observability Guide - Complete observability setup and usage</li> <li>Debugging Guide - Debugging techniques and tools</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/observability-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other Framework Features: Explore Tools or Memory demos </li> </ol>"},{"location":"examples/agents/ollama-demo/","title":"\ud83e\udd99 Ollama Demo Agent","text":"<p>The Ollama Demo Agent showcases easy local model management with Ollama in SuperOptiX. This demo focuses specifically on how to configure and use Ollama models for simple, reliable local inference.</p>"},{"location":"examples/agents/ollama-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83e\udd99 Ollama Model Integration: How to configure Ollama models in SuperOptiX</li> <li>\ud83d\ude80 Easy Model Management: Simple model installation and management</li> <li>\ud83c\udfe0 Local Model Usage: Running models completely offline</li> <li>\u2699\ufe0f Playbook Configuration: How to set up Ollama in agent playbooks</li> </ul>"},{"location":"examples/agents/ollama-demo/#setup-ollama-model","title":"\ud83d\ude80 Setup Ollama Model","text":""},{"location":"examples/agents/ollama-demo/#1-install-ollama","title":"1. Install Ollama","text":"<pre><code># Install Ollama (macOS/Linux)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Or download from https://ollama.ai/download for Windows\n</code></pre>"},{"location":"examples/agents/ollama-demo/#2-install-ollama-model","title":"2. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/ollama-demo/#3-start-ollama-server","title":"3. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/ollama-demo/#4-pull-and-run-the-demo","title":"4. Pull and Run the Demo","text":"<pre><code># Pull the Ollama demo agent\nsuper agent pull ollama_demo\n\n# Compile the agent\nsuper agent compile ollama_demo\n\n# Run the agent\nsuper agent run ollama_demo --goal \"What are the key features of Ollama?\"\n</code></pre>"},{"location":"examples/agents/ollama-demo/#ollama-configuration-in-playbook","title":"\ud83d\udd27 Ollama Configuration in Playbook","text":"<p>The Ollama demo showcases how to configure Ollama models in the agent playbook:</p>"},{"location":"examples/agents/ollama-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre> <p>Key Configuration Points:</p> <ul> <li>\ud83c\udfaf <code>provider: ollama</code>: Specifies Ollama as the model backend</li> <li>\ud83e\udd16 <code>model</code>: The Ollama model identifier</li> <li>\ud83c\udf10 <code>api_base</code>: Ollama server endpoint (default: http://localhost:11434)</li> <li>\ud83c\udf21\ufe0f <code>temperature</code>: Controls response creativity (0.7 = balanced)</li> <li>\ud83d\udccf <code>max_tokens</code>: Maximum response length</li> </ul>"},{"location":"examples/agents/ollama-demo/#the-ollama-advantage","title":"\ud83e\udd99 The Ollama Advantage","text":"<p>Ollama makes local AI accessible to everyone. It's the simplest way to run powerful language models on your own machine:</p> <ul> <li>\ud83c\udfaf One Command Setup: Install any model with a single command</li> <li>\ud83d\udd04 Seamless Updates: Models update automatically in the background</li> <li>\ud83c\udf0d Cross-Platform: Works perfectly on macOS, Linux, and Windows</li> <li>\ud83c\udfe0 True Local: Runs completely offline once installed</li> <li>\u26a1 Lightning Fast: Optimized for your local hardware</li> <li>\ud83d\udca1 Beginner Friendly: Perfect for newcomers to local AI</li> </ul>"},{"location":"examples/agents/ollama-demo/#customizing-ollama-configuration","title":"\ud83d\udd27 Customizing Ollama Configuration","text":""},{"location":"examples/agents/ollama-demo/#change-model","title":"Change Model","text":"<p>Edit <code>agents/ollama_demo/playbook/ollama_demo_playbook.yaml</code>:</p> <pre><code>language_model:\n  model: llama3.2:3b  # Different Ollama model\n  api_base: http://localhost:11434\n</code></pre>"},{"location":"examples/agents/ollama-demo/#adjust-performance-settings","title":"Adjust Performance Settings","text":"<pre><code>language_model:\n  temperature: 0.9  # More creative responses\n  max_tokens: 4096  # Longer responses\n</code></pre>"},{"location":"examples/agents/ollama-demo/#use-different-port","title":"Use Different Port","text":"<pre><code>language_model:\n  api_base: http://localhost:8080  # Custom port\n</code></pre>"},{"location":"examples/agents/ollama-demo/#troubleshooting-ollama","title":"\ud83d\udea8 Troubleshooting Ollama","text":""},{"location":"examples/agents/ollama-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>Model Not Installed <pre><code># Check installed Ollama models\nsuper model list --backend ollama\n\n# Install the required model\nsuper model install llama3.2:8b\n</code></pre></p> </li> <li> <p>Performance Issues</p> </li> <li>Ensure sufficient RAM (8GB+ recommended)</li> <li>Close other resource-intensive applications</li> <li>Consider using smaller models for faster responses</li> </ol>"},{"location":"examples/agents/ollama-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect ollama_demo\n\n# View agent logs\nsuper agent logs ollama_demo\n\n# Get Ollama help\nollama --help\n</code></pre>"},{"location":"examples/agents/ollama-demo/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Ollama Setup Guide - Complete Ollama setup instructions</li> <li>Model Management - Managing Ollama models</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/ollama-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other Model Backends: Explore MLX, HuggingFace, or LM Studio demos </li> </ol>"},{"location":"examples/agents/optimas-examples/","title":"\u26a1 Optimas Examples","text":"<p>Working examples and demos for all Optimas integration targets. These examples have been verified to work correctly with the latest SuperOptiX version.</p> <p>Background reading and references:</p> <ul> <li>Optimas website: optimas.stanford.edu</li> <li>Optimas paper (Wu et al., 2025): arXiv: 2507.03041</li> <li>DSPy: dspy.ai</li> <li>CrewAI: docs.crewai.com</li> <li>AutoGen: microsoft.github.io/autogen</li> <li>OpenAI Agent SDK: platform.openai.com/docs/agents</li> <li>LiteLLM: github.com/BerriAI/litellm</li> </ul>"},{"location":"examples/agents/optimas-examples/#quick-demo","title":"\ud83d\ude80 Quick Demo","text":""},{"location":"examples/agents/optimas-examples/#pull-demo-playbooks","title":"Pull Demo Playbooks","text":"<pre><code># Initialize a new project\nsuper init test_optimas\ncd test_optimas\n\n# Pull working demo playbooks for each target\nsuper agent pull optimas_openai      # OpenAI SDK (recommended)\nsuper agent pull optimas_crewai      # CrewAI\nsuper agent pull optimas_autogen     # AutoGen\nsuper agent pull optimas_dspy        # DSPy\n</code></pre>"},{"location":"examples/agents/optimas-examples/#full-flow-example","title":"Full Flow Example","text":"<pre><code># Compile\nsuper agent compile optimas_openai --target optimas-openai\n\n# Evaluate\nsuper agent evaluate optimas_openai --engine optimas --target optimas-openai\n\n# Optimize\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nsuper agent optimize optimas_openai --engine optimas --target optimas-openai --optimizer opro\n\n# Run\nsuper agent run optimas_openai --engine optimas --target optimas-openai --goal \"Write a Python function to add two numbers\"\n</code></pre>"},{"location":"examples/agents/optimas-examples/#target-specific-examples","title":"\ud83c\udfaf Target-Specific Examples","text":""},{"location":"examples/agents/optimas-examples/#openai-sdk-target-recommended","title":"OpenAI SDK Target (Recommended)","text":"<p>Status: Fully Working - Most reliable target</p>"},{"location":"examples/agents/optimas-examples/#quick-demo_1","title":"Quick Demo","text":"<pre><code># Pull and test\nsuper agent pull optimas_openai\nsuper agent compile optimas_openai --target optimas-openai\nsuper agent evaluate optimas_openai --engine optimas --target optimas-openai\n</code></pre>"},{"location":"examples/agents/optimas-examples/#full-workflow-with-optimization","title":"Full Workflow with Optimization","text":"<pre><code># Optimize with environment variables\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.8 \\\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=120 \\\nsuper agent optimize optimas_openai --engine optimas --target optimas-openai\n\n# Run optimized agent\nsuper agent run optimas_openai --engine optimas --target optimas-openai --goal \"Write a Python function to calculate prime numbers\"\n</code></pre>"},{"location":"examples/agents/optimas-examples/#why-its-great","title":"Why It's Great","text":"<ul> <li>No threading issues</li> <li>Fast optimization and execution</li> <li>Works perfectly with all optimizers</li> <li>Most stable target for production use</li> </ul>"},{"location":"examples/agents/optimas-examples/#crewai-target","title":"CrewAI Target","text":"<p>Status: Fully Working - Excellent for multi-agent workflows</p>"},{"location":"examples/agents/optimas-examples/#quick-demo_2","title":"Quick Demo","text":"<pre><code># Pull and test\nsuper agent pull optimas_crewai\nsuper agent compile optimas_crewai --target optimas-crewai\nsuper agent evaluate optimas_crewai --engine optimas --target optimas-crewai\n</code></pre>"},{"location":"examples/agents/optimas-examples/#full-workflow-with-optimization_1","title":"Full Workflow with Optimization","text":"<pre><code># Optimize with environment variables\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.8 \\\nLITELLM_TIMEOUT=60 \\\nLITELLM_MAX_RETRIES=3 \\\nsuper agent optimize optimas_crewai --engine optimas --target optimas-crewai\n\n# Run optimized agent\nsuper agent run optimas_crewai --engine optimas --target optimas-crewai --goal \"Write a Python function to calculate factorial\"\n</code></pre>"},{"location":"examples/agents/optimas-examples/#dependencies-required","title":"Dependencies Required","text":"<pre><code># Install manually to avoid conflicts\npip install crewai\npip install json-repair&gt;=0.30.0\n</code></pre>"},{"location":"examples/agents/optimas-examples/#why-its-great_1","title":"Why It's Great","text":"<ul> <li>Excellent for multi-agent scenarios</li> <li>Fast optimization and execution</li> <li>No threading issues</li> <li>Great for team-based tasks</li> </ul>"},{"location":"examples/agents/optimas-examples/#autogen-target","title":"AutoGen Target","text":"<p>Status: \u26a0\ufe0f Mostly Working - Optimization can be slow</p>"},{"location":"examples/agents/optimas-examples/#quick-demo_3","title":"Quick Demo","text":"<pre><code># Pull and test\nsuper agent pull optimas_autogen\nsuper agent compile optimas_autogen --target optimas-autogen\nsuper agent evaluate optimas_autogen --engine optimas --target optimas-autogen\n</code></pre>"},{"location":"examples/agents/optimas-examples/#full-workflow-with-optimization_2","title":"Full Workflow with Optimization","text":"<pre><code># Optimize with extended timeout\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=180 \\\nLITELLM_TIMEOUT=60 \\\nLITELLM_MAX_RETRIES=3 \\\nsuper agent optimize optimas_autogen --engine optimas --target optimas-autogen\n\n# Run optimized agent\nsuper agent run optimas_autogen --engine optimas --target optimas-autogen --goal \"Write a Python function to reverse a string\"\n</code></pre>"},{"location":"examples/agents/optimas-examples/#configuration-requirements","title":"Configuration Requirements","text":"<pre><code># Requires detailed model_info for non-OpenAI models\nlanguage_model:\n  provider: ollama\n  model: llama3.2:1b\n  base_url: http://localhost:11434\n  api_key: \"\"\n  model_info:\n    model_name: \"llama3.2:1b\"\n    max_tokens: 4096\n    temperature: 0.7\n    top_p: 0.9\n</code></pre>"},{"location":"examples/agents/optimas-examples/#why-its-great_2","title":"Why It's Great","text":"<ul> <li>Excellent for complex multi-agent workflows</li> <li>Reliable execution despite slow optimization</li> <li>Great for conversational agents</li> <li>Handles complex interactions well</li> </ul>"},{"location":"examples/agents/optimas-examples/#dspy-target","title":"DSPy Target","text":"<p>Status: Fully Working - All optimizers now working properly</p>"},{"location":"examples/agents/optimas-examples/#quick-demo_4","title":"Quick Demo","text":"<pre><code># Pull and test\nsuper agent pull optimas_dspy\nsuper agent compile optimas_dspy --target optimas-dspy\nsuper agent evaluate optimas_dspy --engine optimas --target optimas-dspy\n</code></pre>"},{"location":"examples/agents/optimas-examples/#full-workflow-with-optimization_3","title":"Full Workflow with Optimization","text":"<pre><code># Optimize with environment variables\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.8 \\\nsuper agent optimize optimas_dspy --engine optimas --target optimas-dspy\n\n# Run optimized agent\nsuper agent run optimas_dspy --engine optimas --target optimas-dspy --goal \"Write a Python function to calculate fibonacci numbers\"\n</code></pre>"},{"location":"examples/agents/optimas-examples/#root-cause","title":"Root Cause","text":"<p>The issue is with LiteLLM library version compatibility: - DSPy 3.0.0 uses LiteLLM for model communication - LiteLLM has threading issues with concurrent operations - When Optimas tries to run multiple optimization iterations, the thread pool gets corrupted</p>"},{"location":"examples/agents/optimas-examples/#workarounds","title":"Workarounds","text":"<pre><code># Option 1: Use other targets for optimization\n# Option 2: Reduce concurrency (may still fail)\nSUPEROPTIX_OPRO_MAX_WORKERS=1\n# Option 3: Use for research only (avoid optimization)\n</code></pre>"},{"location":"examples/agents/optimas-examples/#optimizer-options","title":"\ud83c\udfaf Optimizer Options","text":"<p>The <code>--optimizer</code> flag allows you to specify which optimization method to use:</p>"},{"location":"examples/agents/optimas-examples/#available-optimizers","title":"Available Optimizers","text":"<ul> <li><code>--optimizer opro</code>: OPRO (Optimization by PROmpting) - Single-iteration optimization</li> <li><code>--optimizer mipro</code>: MIPRO (Multi-Iteration PROmpting) - Multi-iteration optimization  </li> <li><code>--optimizer copro</code>: COPRO (Cooperative PROmpting) - Cooperative optimization</li> </ul>"},{"location":"examples/agents/optimas-examples/#example-usage","title":"Example Usage","text":"<pre><code># OPRO optimization (default)\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt; --optimizer opro\n\n# MIPRO optimization (great for DSPy)\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt; --optimizer mipro\n\n# COPRO optimization (cooperative approach)\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt; --optimizer copro\n</code></pre>"},{"location":"examples/agents/optimas-examples/#environment-variable-examples","title":"\ud83d\udd27 Environment Variable Examples","text":""},{"location":"examples/agents/optimas-examples/#fast-optimization-development","title":"Fast Optimization (Development)","text":"<pre><code>SUPEROPTIX_OPRO_MAX_TOKENS=128 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=2 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=2 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.7 \\\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=60 \\\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt;\n</code></pre>"},{"location":"examples/agents/optimas-examples/#high-quality-optimization-production","title":"High-Quality Optimization (Production)","text":"<pre><code>SUPEROPTIX_OPRO_MAX_TOKENS=512 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=5 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=4 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.9 \\\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=300 \\\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt;\n</code></pre>"},{"location":"examples/agents/optimas-examples/#litellm-configuration","title":"LiteLLM Configuration","text":"<pre><code>LITELLM_TIMEOUT=60 \\\nLITELLM_MAX_RETRIES=3 \\\nLITELLM_MAX_RESPONSE=4000 \\\nLITELLM_CACHE_ENABLED=false \\\nLITELLM_LOG_LEVEL=ERROR \\\nsuper agent optimize &lt;agent&gt; --engine optimas --target &lt;target&gt;\n</code></pre>"},{"location":"examples/agents/optimas-examples/#known-limitations","title":"\ud83d\udea8 Known Limitations","text":""},{"location":"examples/agents/optimas-examples/#dspy-optimization-issues","title":"DSPy Optimization Issues","text":"<ul> <li>Problem: LiteLLM threading conflicts during optimization</li> <li>Impact: Cannot use DSPy target for production optimization</li> <li>Workaround: Use OpenAI SDK or CrewAI targets instead</li> </ul>"},{"location":"examples/agents/optimas-examples/#autogen-optimization-speed","title":"AutoGen Optimization Speed","text":"<ul> <li>Problem: Optimization can be slow (120s+ timeout)</li> <li>Impact: Slower development iteration</li> <li>Workaround: Increase timeout or use faster models</li> </ul>"},{"location":"examples/agents/optimas-examples/#crewai-dependencies","title":"CrewAI Dependencies","text":"<ul> <li>Problem: Manual installation required due to conflicts</li> <li>Impact: Additional setup steps</li> <li>Workaround: Follow manual installation instructions</li> </ul>"},{"location":"examples/agents/optimas-examples/#litellm-version-compatibility","title":"LiteLLM Version Compatibility","text":"<ul> <li>Problem: DSPy 3.0.0 + LiteLLM threading issues</li> <li>Impact: DSPy target optimization fails</li> <li>Workaround: Use other targets or wait for LiteLLM fixes</li> </ul>"},{"location":"examples/agents/optimas-examples/#recommendations","title":"\ud83c\udfaf Recommendations","text":""},{"location":"examples/agents/optimas-examples/#for-production-use","title":"For Production Use","text":"<ol> <li>Primary: OpenAI SDK target (most reliable)</li> <li>Secondary: CrewAI target (excellent for multi-agent)</li> <li>Avoid: DSPy target (optimization issues)</li> </ol>"},{"location":"examples/agents/optimas-examples/#for-development","title":"For Development","text":"<ol> <li>Quick Testing: OpenAI SDK target</li> <li>Multi-agent: CrewAI target</li> <li>Research: DSPy target (compile/evaluate only)</li> </ol>"},{"location":"examples/agents/optimas-examples/#for-optimization","title":"For Optimization","text":"<ol> <li>Fast: OpenAI SDK or CrewAI targets</li> <li>Quality: Increase timeout and parameters</li> <li>Avoid: DSPy target optimization</li> </ol>"},{"location":"examples/agents/optimas-examples/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>Start Here: Optimas Integration Guide</li> <li>CLI Reference: Command Reference</li> <li>Agent Development: Building Custom Agents</li> <li>Troubleshooting: Common Issues &amp; Solutions</li> </ul>"},{"location":"examples/agents/qdrant-demo/","title":"Qdrant RAG Demo \ud83c\udfaf","text":"<p>A comprehensive demonstration of SuperOptiX's RAG capabilities using Qdrant vector database for high-performance similarity search and intelligent knowledge retrieval.</p>"},{"location":"examples/agents/qdrant-demo/#overview","title":"Overview","text":"<p>This demo showcases how to integrate Qdrant - a blazingly fast vector database - with SuperOptiX's RAG system to create intelligent agents capable of retrieving and synthesizing information from large knowledge bases with exceptional performance.</p>"},{"location":"examples/agents/qdrant-demo/#this-demo-demonstrates","title":"This demo demonstrates:","text":"<p>\u26a1 Qdrant Vector Database Integration - Lightning-fast connection to Qdrant for high-performance vector storage</p> <p>\ud83c\udfaf Precision Similarity Search - Intelligent retrieval using Qdrant's optimized similarity algorithms</p> <p>\ud83d\udcca Collection Management - Efficient document storage, indexing, and retrieval workflows</p> <p>\ud83d\ude80 Ultra-fast Query Processing - Exceptional response times with Qdrant's performance optimizations</p> <p>\ud83d\udd04 RAG Pipeline Integration - Complete retrieval-augmented generation workflow</p>"},{"location":"examples/agents/qdrant-demo/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/agents/qdrant-demo/#install-superoptix","title":"Install SuperOptiX","text":"<pre><code>pip install superoptix\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#install-qdrant-dependencies","title":"Install Qdrant Dependencies","text":"<pre><code>pip install qdrant-client\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#set-up-qdrant-server","title":"Set Up Qdrant Server","text":"<pre><code># Using Docker (recommended)\ndocker run -d \\\n  --name qdrant \\\n  -p 6333:6333 \\\n  -p 6334:6334 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant:latest\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#install-and-serve-model","title":"Install and Serve Model","text":"<pre><code># Install a model (if not already installed)\nsuper model install llama3.1:8b\n\n# Start Ollama server (if using Ollama backend)\nollama serve\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/agents/qdrant-demo/#pull-the-demo-agent","title":"Pull the Demo Agent","text":"<pre><code>super agent pull rag_qdrant_demo\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#compile-the-agent","title":"Compile the Agent","text":"<pre><code>super agent compile rag_qdrant_demo\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#run-the-demo","title":"Run the Demo","text":"<pre><code>super agent run rag_qdrant_demo --goal \"What are the key features of Qdrant and how does it work with SuperOptiX?\"\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#key-configuration-points","title":"Key Configuration Points:","text":"<p>\ud83d\udd27 Vector Database Setup - Configured to connect to Qdrant at <code>http://localhost:6333</code></p> <p>\ud83d\udcca Collection Management - Uses <code>superoptix_knowledge</code> collection for document storage</p> <p>\ud83c\udfaf Embedding Model - Leverages <code>sentence-transformers/all-MiniLM-L6-v2</code> for vector generation</p> <p>\u2699\ufe0f Search Parameters - Optimized with <code>top_k: 5</code> and <code>similarity_threshold: 0.7</code></p>"},{"location":"examples/agents/qdrant-demo/#playbook-configuration","title":"Playbook Configuration","text":"<p>The demo uses a specialized playbook with Qdrant-specific configurations:</p> <pre><code>rag:\n  enabled: true\n  retriever_type: qdrant\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n    similarity_threshold: 0.7\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    url: http://localhost:6333\n    collection_name: superoptix_knowledge\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#customization","title":"Customization","text":""},{"location":"examples/agents/qdrant-demo/#modify-qdrant-connection","title":"Modify Qdrant Connection","text":"<pre><code>vector_store:\n  url: http://your-qdrant-server:6333\n  collection_name: YourCustomCollection\n  api_key: your-api-key  # If using authentication\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#adjust-search-parameters","title":"Adjust Search Parameters","text":"<pre><code>config:\n  top_k: 10  # Retrieve more documents\n  similarity_threshold: 0.8  # Higher similarity threshold\n  chunk_size: 1024  # Larger chunks\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#configure-collection-settings","title":"Configure Collection Settings","text":"<pre><code>vector_store:\n  collection_config:\n    vectors:\n      size: 384  # Vector dimension\n      distance: Cosine  # Distance metric\n    optimizers_config:\n      memmap_threshold: 20000\n</code></pre>"},{"location":"examples/agents/qdrant-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/agents/qdrant-demo/#connection-issues","title":"Connection Issues","text":"<ul> <li>Error: \"Connection refused\"</li> <li>Solution: Ensure Qdrant server is running on port 6333</li> <li>Check: <code>curl http://localhost:6333/collections</code></li> </ul>"},{"location":"examples/agents/qdrant-demo/#collection-issues","title":"Collection Issues","text":"<ul> <li>Error: \"Collection not found\"</li> <li>Solution: Create collection manually or check collection name</li> <li>Check: <code>curl http://localhost:6333/collections/superoptix_knowledge</code></li> </ul>"},{"location":"examples/agents/qdrant-demo/#performance-issues","title":"Performance Issues","text":"<ul> <li>Slow queries: Optimize collection settings or reduce <code>top_k</code></li> <li>Memory issues: Adjust <code>chunk_size</code> and <code>chunk_overlap</code></li> </ul>"},{"location":"examples/agents/qdrant-demo/#use-cases","title":"Use Cases","text":"<p>\ud83c\udfed Industrial Applications - High-throughput document processing and retrieval</p> <p>\ud83c\udfae Gaming &amp; Entertainment - Real-time content recommendation systems</p> <p>\ud83c\udfe5 Healthcare - Medical document analysis and patient information retrieval</p> <p>\ud83d\udcb0 Financial Services - Risk assessment and regulatory compliance document search</p>"},{"location":"examples/agents/qdrant-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>SuperOptiX RAG Guide</li> <li>RAG Integration Guide - Vector database setup and configuration</li> <li>Model Management</li> </ul> <p>Ready to experience lightning-fast vector search with Qdrant? \u26a1</p> <p>Start with this demo to understand how Qdrant's performance optimizations can supercharge your AI applications with blazingly fast knowledge retrieval and similarity search capabilities. </p>"},{"location":"examples/agents/rag-chroma-demo/","title":"\ud83d\udd0d RAG ChromaDB Demo Agent","text":"<p>The RAG ChromaDB Demo Agent showcases Retrieval-Augmented Generation capabilities with ChromaDB vector database in SuperOptiX. This demo focuses specifically on how to configure and use RAG with ChromaDB for knowledge retrieval and context-aware responses.</p>"},{"location":"examples/agents/rag-chroma-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83d\udd0d RAG Integration: How to configure RAG with ChromaDB in SuperOptiX</li> <li>\ud83d\udcda Knowledge Retrieval: Semantic search and document retrieval</li> <li>\ud83e\udde0 Context-Aware Responses: Responses based on retrieved knowledge</li> <li>\u2699\ufe0f Playbook Configuration: How to set up RAG in agent playbooks</li> </ul>"},{"location":"examples/agents/rag-chroma-demo/#setup-rag-with-chromadb","title":"\ud83d\ude80 Setup RAG with ChromaDB","text":""},{"location":"examples/agents/rag-chroma-demo/#1-install-ollama-model","title":"1. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#2-start-ollama-server","title":"2. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#3-pull-and-run-the-demo","title":"3. Pull and Run the Demo","text":"<pre><code># Pull the RAG ChromaDB demo agent\nsuper agent pull rag_chroma_demo\n\n# Compile the agent\nsuper agent compile rag_chroma_demo\n\n# Run the agent\nsuper agent run rag_chroma_demo --goal \"What is the SuperOptiX framework?\"\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#rag-configuration-in-playbook","title":"\ud83d\udd27 RAG Configuration in Playbook","text":"<p>The RAG ChromaDB demo showcases how to configure RAG in the agent playbook:</p>"},{"location":"examples/agents/rag-chroma-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#rag-configuration","title":"RAG Configuration","text":"<pre><code>rag:\n  enabled: true\n  retriever_type: chroma\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n    similarity_threshold: 0.7\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    collection_name: rag_demo_knowledge\n    persist_directory: ./data/chromadb\n</code></pre> <p>Key RAG Configuration Points:</p> <ul> <li><code>enabled: true</code>: Enables RAG functionality</li> <li>\ud83d\uddc4\ufe0f <code>retriever_type: chroma</code>: Uses ChromaDB as vector database</li> <li>\ud83d\udd1d <code>top_k: 5</code>: Retrieves top 5 most similar documents</li> <li>\ud83d\udcc4 <code>chunk_size: 512</code>: Document chunk size for processing</li> <li>\ud83d\udd17 <code>chunk_overlap: 50</code>: Overlap between chunks for context</li> <li>\ud83c\udfaf <code>similarity_threshold: 0.7</code>: Minimum similarity score for retrieval</li> <li>\ud83e\udde0 <code>embedding_model</code>: Sentence transformers for embeddings</li> <li>\ud83d\udcc1 <code>collection_name</code>: ChromaDB collection name</li> <li>\ud83d\udcbe <code>persist_directory</code>: Local storage directory</li> </ul>"},{"location":"examples/agents/rag-chroma-demo/#rag-your-ais-memory-bank","title":"\ud83d\udd0d RAG: Your AI's Memory Bank","text":"<p>Retrieval-Augmented Generation (RAG) gives your AI agent the ability to access and use specific knowledge. Think of it as giving your AI a personal library:</p> <ul> <li>\ud83e\udde0 Semantic Understanding: Finds relevant information based on meaning, not just keywords</li> <li>\ud83d\udcda Knowledge Base: Access to your own documents, databases, and information sources</li> <li>\ud83c\udfaf Precise Answers: Generates responses based on actual retrieved content</li> <li>\ud83d\udcd6 Source Citations: Always provides references to where information came from</li> <li>\ud83d\udd04 Real-time Updates: Can access the latest information as it becomes available</li> </ul>"},{"location":"examples/agents/rag-chroma-demo/#customizing-rag-configuration","title":"\ud83d\udd27 Customizing RAG Configuration","text":""},{"location":"examples/agents/rag-chroma-demo/#adjust-retrieval-settings","title":"Adjust Retrieval Settings","text":"<p>Edit <code>agents/rag_chroma_demo/playbook/rag_chroma_demo_playbook.yaml</code>:</p> <pre><code>rag:\n  config:\n    top_k: 10  # Retrieve more documents\n    chunk_size: 1024  # Larger chunks\n    similarity_threshold: 0.8  # Higher similarity threshold\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#change-vector-database","title":"Change Vector Database","text":"<pre><code>rag:\n  retriever_type: lancedb  # Use LanceDB instead\n  vector_store:\n    table_name: lancedb_demo_table\n    database_path: ./data/lancedb\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#use-different-embedding-model","title":"Use Different Embedding Model","text":"<pre><code>rag:\n  vector_store:\n    embedding_model: sentence-transformers/all-mpnet-base-v2  # Different embeddings\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#troubleshooting-rag","title":"\ud83d\udea8 Troubleshooting RAG","text":""},{"location":"examples/agents/rag-chroma-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>ChromaDB Issues <pre><code># Check RAG configuration\nsuper agent inspect rag_chroma_demo\n\n# Clear ChromaDB data if needed\nrm -rf ./data/chromadb\n</code></pre></p> </li> <li> <p>RAG Not Working <pre><code># Check RAG configuration\nsuper agent inspect rag_chroma_demo\n\n# Verify knowledge base is populated\n</code></pre></p> </li> </ol>"},{"location":"examples/agents/rag-chroma-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect rag_chroma_demo\n\n# View agent logs\nsuper agent logs rag_chroma_demo\n\n# Get RAG help\nsuper agent --help\n</code></pre>"},{"location":"examples/agents/rag-chroma-demo/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>RAG Guide - Complete RAG setup and usage</li> <li>RAG Integration Guide - ChromaDB configuration and setup</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/rag-chroma-demo/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>After exploring the RAG ChromaDB demo:</p> <ol> <li>Try Other RAG Backends: Explore RAG LanceDB for production use</li> <li>Customize: Modify the playbook for your specific knowledge base</li> <li>Build Your Own: Use this as a template for your custom RAG agent</li> </ol> <p>Ready to explore knowledge retrieval? Start with the RAG ChromaDB demo! \ud83d\ude80 Ready to explore knowledge retrieval? Start with the RAG ChromaDB demo! \ud83d\ude80 </p>"},{"location":"examples/agents/rag-lancedb-demo/","title":"\ud83d\ude80 RAG LanceDB Demo Agent","text":"<p>The RAG LanceDB Demo Agent showcases high-performance RAG capabilities with LanceDB vector database in SuperOptiX. This demo focuses specifically on how to configure and use RAG with LanceDB for scalable, production-ready knowledge retrieval.</p>"},{"location":"examples/agents/rag-lancedb-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83d\ude80 High-Performance RAG: How to configure RAG with LanceDB in SuperOptiX</li> <li>\ud83d\udcc8 Scalable Knowledge Retrieval: Production-ready vector database</li> <li>\u26a1 Fast Semantic Search: Optimized retrieval performance</li> <li>\u2699\ufe0f Playbook Configuration: How to set up RAG with LanceDB in agent playbooks</li> </ul>"},{"location":"examples/agents/rag-lancedb-demo/#setup-rag-with-lancedb","title":"\ud83d\ude80 Setup RAG with LanceDB","text":""},{"location":"examples/agents/rag-lancedb-demo/#1-install-ollama-model","title":"1. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#2-start-ollama-server","title":"2. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#3-pull-and-run-the-demo","title":"3. Pull and Run the Demo","text":"<pre><code># Pull the RAG LanceDB demo agent\nsuper agent pull rag_lancedb_demo\n\n# Compile the agent\nsuper agent compile rag_lancedb_demo\n\n# Run the agent\nsuper agent run rag_lancedb_demo --goal \"What is the SuperOptiX framework?\"\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#rag-configuration-in-playbook","title":"\ud83d\udd27 RAG Configuration in Playbook","text":"<p>The RAG LanceDB demo showcases how to configure RAG with LanceDB in the agent playbook:</p>"},{"location":"examples/agents/rag-lancedb-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#rag-configuration","title":"RAG Configuration","text":"<pre><code>rag:\n  enabled: true\n  retriever_type: lancedb\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n    similarity_threshold: 0.7\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    table_name: lancedb_demo_table\n    database_path: ./data/lancedb\n</code></pre> <p>Key RAG Configuration Points:</p> <ul> <li><code>enabled: true</code>: Enables RAG functionality</li> <li>\ud83d\uddc4\ufe0f <code>retriever_type: lancedb</code>: Uses LanceDB as vector database</li> <li>\ud83d\udd1d <code>top_k: 5</code>: Retrieves top 5 most similar documents</li> <li>\ud83d\udcc4 <code>chunk_size: 512</code>: Document chunk size for processing</li> <li>\ud83d\udd17 <code>chunk_overlap: 50</code>: Overlap between chunks for context</li> <li>\ud83c\udfaf <code>similarity_threshold: 0.7</code>: Minimum similarity score for retrieval</li> <li>\ud83e\udde0 <code>embedding_model</code>: Sentence transformers for embeddings</li> <li>\ud83d\udccb <code>table_name</code>: LanceDB table name</li> <li>\ud83d\udcbe <code>database_path</code>: Local storage directory</li> </ul>"},{"location":"examples/agents/rag-lancedb-demo/#lancedb-enterprise-grade-rag","title":"\ud83d\ude80 LanceDB: Enterprise-Grade RAG","text":"<p>LanceDB is built for production environments where performance and scalability matter. It's the vector database of choice for serious applications:</p> <ul> <li>\u26a1 Lightning Fast: Optimized for high-speed semantic search across millions of documents</li> <li>\ud83d\udcc8 Enterprise Scale: Handles massive document collections with ease</li> <li>\ud83c\udfe2 Production Ready: Built with enterprise-grade reliability and consistency</li> <li>\u2601\ufe0f Cloud Native: Seamlessly works with cloud storage and distributed systems</li> <li>\ud83d\udd04 ACID Compliant: Guarantees data consistency and transaction safety</li> <li>\ud83d\udccb Version Control: Track changes and maintain document history</li> </ul>"},{"location":"examples/agents/rag-lancedb-demo/#customizing-rag-configuration","title":"\ud83d\udd27 Customizing RAG Configuration","text":""},{"location":"examples/agents/rag-lancedb-demo/#adjust-retrieval-settings","title":"Adjust Retrieval Settings","text":"<p>Edit <code>agents/rag_lancedb_demo/playbook/rag_lancedb_demo_playbook.yaml</code>:</p> <pre><code>rag:\n  config:\n    top_k: 10  # Retrieve more documents\n    chunk_size: 1024  # Larger chunks\n    similarity_threshold: 0.8  # Higher similarity threshold\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#change-vector-database","title":"Change Vector Database","text":"<pre><code>rag:\n  retriever_type: chroma  # Use ChromaDB instead\n  vector_store:\n    collection_name: chromadb_demo_collection\n    persist_directory: ./data/chromadb\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#use-different-embedding-model","title":"Use Different Embedding Model","text":"<pre><code>rag:\n  vector_store:\n    embedding_model: sentence-transformers/all-mpnet-base-v2  # Different embeddings\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#troubleshooting-rag","title":"\ud83d\udea8 Troubleshooting RAG","text":""},{"location":"examples/agents/rag-lancedb-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>LanceDB Issues <pre><code># Check RAG configuration\nsuper agent inspect rag_lancedb_demo\n\n# Clear LanceDB data if needed\nrm -rf ./data/lancedb\n</code></pre></p> </li> <li> <p>RAG Not Working <pre><code># Check RAG configuration\nsuper agent inspect rag_lancedb_demo\n\n# Verify knowledge base is populated\n</code></pre></p> </li> </ol>"},{"location":"examples/agents/rag-lancedb-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect rag_lancedb_demo\n\n# View agent logs\nsuper agent logs rag_lancedb_demo\n\n# Get RAG help\nsuper agent --help\n</code></pre>"},{"location":"examples/agents/rag-lancedb-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>RAG Guide - Complete RAG setup and usage</li> <li>RAG Integration Guide - LanceDB configuration and setup</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/rag-lancedb-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other RAG Backends: Explore RAG ChromaDB for development </li> </ol>"},{"location":"examples/agents/tools-demo/","title":"\ud83d\udee0\ufe0f Tools Demo Agent","text":"<p>The Tools Demo Agent showcases comprehensive tool integration capabilities in SuperOptiX. This demo focuses specifically on how to configure and use tools across multiple categories for enhanced agent functionality.</p>"},{"location":"examples/agents/tools-demo/#what-this-demo-shows","title":"\ud83c\udfaf What This Demo Shows","text":"<p>This demo demonstrates:</p> <ul> <li>\ud83d\udee0\ufe0f Tool Integration: How to configure tools in SuperOptiX agents</li> <li>\ud83d\udce6 Multi-Category Tools: Access to tools across 20+ categories</li> <li>\u26a1 Tool Execution: How agents use tools to perform tasks</li> <li>\u2699\ufe0f Playbook Configuration: How to set up tools in agent playbooks</li> </ul>"},{"location":"examples/agents/tools-demo/#setup-tools-demo","title":"\ud83d\ude80 Setup Tools Demo","text":""},{"location":"examples/agents/tools-demo/#1-install-ollama-model","title":"1. Install Ollama Model","text":"<pre><code># Install the Ollama model used in this demo\nsuper model install llama3.2:8b\n</code></pre>"},{"location":"examples/agents/tools-demo/#2-start-ollama-server","title":"2. Start Ollama Server","text":"<pre><code># Start Ollama server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"examples/agents/tools-demo/#3-pull-and-run-the-demo","title":"3. Pull and Run the Demo","text":"<pre><code># Pull the Tools demo agent\nsuper agent pull tools_demo\n\n# Compile the agent\nsuper agent compile tools_demo\n\n# Run the agent\nsuper agent run tools_demo --goal \"What tools are available and how do they work?\"\n</code></pre>"},{"location":"examples/agents/tools-demo/#tools-configuration-in-playbook","title":"\ud83d\udd27 Tools Configuration in Playbook","text":"<p>The Tools demo showcases how to configure tools in the agent playbook:</p>"},{"location":"examples/agents/tools-demo/#language-model-configuration","title":"Language Model Configuration","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: llama3.2:8b\n  api_base: http://localhost:11434\n  temperature: 0.7\n  max_tokens: 2048\n</code></pre>"},{"location":"examples/agents/tools-demo/#tools-configuration","title":"Tools Configuration","text":"<pre><code>tools:\n  enabled: true\n  categories:\n  - development\n  - core\n  - utilities\n  - education\n  - finance\n  - health\n  - marketing\n  - research\n  specific_tools:\n  - calculator\n  - file_reader\n  - text_analyzer\n  - web_search\n  - date_time\n  - json_processor\n  - code_formatter\n  - image_analyzer\n  - weather_checker\n  - currency_converter\n</code></pre> <p>Key Tools Configuration Points:</p> <ul> <li><code>enabled: true</code>: Enables tool functionality</li> <li>\ud83d\udcc2 <code>categories</code>: Tool categories to include</li> <li>\ud83d\udd27 <code>specific_tools</code>: Individual tools to enable</li> <li>\ud83c\udff7\ufe0f Tool Categories: development, core, utilities, education, finance, health, marketing, research</li> </ul>"},{"location":"examples/agents/tools-demo/#your-ais-swiss-army-knife","title":"\ud83d\udee0\ufe0f Your AI's Swiss Army Knife","text":"<p>SuperOptiX provides access to a comprehensive toolkit that transforms your AI agent into a multi-talented assistant. From basic utilities to specialized industry tools:</p>"},{"location":"examples/agents/tools-demo/#core-capabilities","title":"\ud83d\udd27 Core Capabilities","text":"<ul> <li>\ud83e\uddee Calculator: Complex mathematical operations and financial calculations</li> <li>\ud83d\udcc4 File Reader: Read, analyze, and process any text-based files</li> <li>\ud83d\udd0d Text Analyzer: Sentiment analysis, entity extraction, and text processing</li> <li>\ud83c\udf10 Web Search: Real-time information retrieval from the internet</li> <li>\ud83d\udcc5 Date &amp; Time: Calendar operations, scheduling, and time calculations</li> </ul>"},{"location":"examples/agents/tools-demo/#specialized-domains","title":"\ud83c\udfaf Specialized Domains","text":"<ul> <li>\ud83d\udcbc Finance: Investment analysis, market data, and financial planning</li> <li>\ud83c\udfe5 Healthcare: Medical information, health metrics, and wellness tools</li> <li>\ud83d\udcda Education: Learning assistance, content creation, and educational resources</li> <li>\ud83d\udcc8 Marketing: Campaign analysis, SEO tools, and social media management</li> <li>\ud83d\udd2c Research: Data analysis, research assistance, and academic tools</li> </ul>"},{"location":"examples/agents/tools-demo/#customizing-tools-configuration","title":"\ud83d\udd27 Customizing Tools Configuration","text":""},{"location":"examples/agents/tools-demo/#enable-specific-categories","title":"Enable Specific Categories","text":"<p>Edit <code>agents/tools_demo/playbook/tools_demo_playbook.yaml</code>:</p> <pre><code>tools:\n  categories:\n  - development\n  - core\n  - utilities\n  - your_custom_category\n</code></pre>"},{"location":"examples/agents/tools-demo/#enable-specific-tools","title":"Enable Specific Tools","text":"<pre><code>tools:\n  specific_tools:\n  - calculator\n  - file_reader\n  - your_custom_tool\n</code></pre>"},{"location":"examples/agents/tools-demo/#disable-tools","title":"Disable Tools","text":"<pre><code>tools:\n  enabled: false  # Disable all tools\n</code></pre>"},{"location":"examples/agents/tools-demo/#troubleshooting-tools","title":"\ud83d\udea8 Troubleshooting Tools","text":""},{"location":"examples/agents/tools-demo/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Server Not Running <pre><code># Check if Ollama server is running\ncurl http://localhost:11434/api/tags\n\n# Start Ollama server\nollama serve\n</code></pre></p> </li> <li> <p>Tools Not Working <pre><code># Check tools configuration\nsuper agent inspect tools_demo\n\n# Verify tools are enabled\n</code></pre></p> </li> <li> <p>Tool Execution Errors <pre><code># Check agent logs\nsuper agent logs tools_demo\n\n# Verify tool dependencies\n</code></pre></p> </li> </ol>"},{"location":"examples/agents/tools-demo/#getting-help","title":"Getting Help","text":"<pre><code># Check agent status\nsuper agent inspect tools_demo\n\n# View agent logs\nsuper agent logs tools_demo\n\n# Get tools help\nsuper agent --help\n</code></pre>"},{"location":"examples/agents/tools-demo/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Tool Development - Building custom tools</li> <li>Tool Categories - Available tool categories</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"examples/agents/tools-demo/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ol> <li>Try Other Framework Features: Explore Memory or Observability demos </li> </ol>"},{"location":"examples/agents/weaviate-demo/","title":"Weaviate RAG Demo \ud83d\uddc4\ufe0f","text":"<p>A comprehensive demonstration of SuperOptiX's RAG capabilities using Weaviate vector database for advanced semantic search and knowledge retrieval.</p>"},{"location":"examples/agents/weaviate-demo/#overview","title":"Overview","text":"<p>This demo showcases how to integrate Weaviate - a powerful vector database - with SuperOptiX's RAG system to create intelligent agents capable of retrieving and synthesizing information from large knowledge bases.</p>"},{"location":"examples/agents/weaviate-demo/#this-demo-demonstrates","title":"This demo demonstrates:","text":"<p>\ud83d\udd0d Weaviate Vector Database Integration - Seamless connection to Weaviate for high-performance vector storage</p> <p>\ud83e\udde0 Advanced Semantic Search - Intelligent retrieval using Weaviate's sophisticated similarity algorithms</p> <p>\ud83d\udcda Knowledge Base Management - Efficient document storage, indexing, and retrieval workflows</p> <p>\u26a1 Real-time Query Processing - Fast response times with Weaviate's optimized search capabilities</p> <p>\ud83d\udd04 RAG Pipeline Integration - Complete retrieval-augmented generation workflow</p>"},{"location":"examples/agents/weaviate-demo/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/agents/weaviate-demo/#install-superoptix","title":"Install SuperOptiX","text":"<pre><code>pip install superoptix\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#install-weaviate-dependencies","title":"Install Weaviate Dependencies","text":"<pre><code>pip install weaviate-client\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#set-up-weaviate-server","title":"Set Up Weaviate Server","text":"<pre><code># Using Docker (recommended)\ndocker run -d \\\n  --name weaviate \\\n  -p 8080:8080 \\\n  -e QUERY_DEFAULTS_LIMIT=25 \\\n  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \\\n  -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \\\n  -e DEFAULT_VECTORIZER_MODULE='none' \\\n  -e ENABLE_MODULES='text2vec-transformers' \\\n  -e TRANSFORMERS_INFERENCE_API='http://t2v-transformers:8080' \\\n  semitechnologies/weaviate:1.22.4\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#install-and-serve-model","title":"Install and Serve Model","text":"<pre><code># Install a model (if not already installed)\nsuper model install llama3.1:8b\n\n# Start Ollama server (if using Ollama backend)\nollama serve\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/agents/weaviate-demo/#pull-the-demo-agent","title":"Pull the Demo Agent","text":"<pre><code>super agent pull rag_weaviate_demo\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#compile-the-agent","title":"Compile the Agent","text":"<pre><code>super agent compile rag_weaviate_demo\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#run-the-demo","title":"Run the Demo","text":"<pre><code>super agent run rag_weaviate_demo --goal \"What is the SuperOptiX framework and its key features?\"\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#key-configuration-points","title":"Key Configuration Points:","text":"<p>\ud83d\udd27 Vector Database Setup - Configured to connect to Weaviate at <code>http://localhost:8080</code></p> <p>\ud83d\udcca Collection Management - Uses <code>SuperOptiXKnowledge</code> class for document storage</p> <p>\ud83c\udfaf Embedding Model - Leverages <code>sentence-transformers/all-MiniLM-L6-v2</code> for vector generation</p> <p>\u2699\ufe0f Search Parameters - Optimized with <code>top_k: 5</code> and <code>similarity_threshold: 0.7</code></p>"},{"location":"examples/agents/weaviate-demo/#playbook-configuration","title":"Playbook Configuration","text":"<p>The demo uses a specialized playbook with Weaviate-specific configurations:</p> <pre><code>rag:\n  enabled: true\n  retriever_type: weaviate\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n    similarity_threshold: 0.7\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    url: http://localhost:8080\n    class_name: SuperOptiXKnowledge\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#customization","title":"Customization","text":""},{"location":"examples/agents/weaviate-demo/#modify-weaviate-connection","title":"Modify Weaviate Connection","text":"<pre><code>vector_store:\n  url: http://your-weaviate-server:8080\n  class_name: YourCustomClass\n  auth_token: your-auth-token  # If using authentication\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#adjust-search-parameters","title":"Adjust Search Parameters","text":"<pre><code>config:\n  top_k: 10  # Retrieve more documents\n  similarity_threshold: 0.8  # Higher similarity threshold\n  chunk_size: 1024  # Larger chunks\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#add-custom-schema","title":"Add Custom Schema","text":"<pre><code>vector_store:\n  schema:\n    properties:\n      - name: content\n        dataType: text\n      - name: metadata\n        dataType: object\n</code></pre>"},{"location":"examples/agents/weaviate-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/agents/weaviate-demo/#connection-issues","title":"Connection Issues","text":"<ul> <li>Error: \"Connection refused\"</li> <li>Solution: Ensure Weaviate server is running on port 8080</li> <li>Check: <code>curl http://localhost:8080/v1/.well-known/ready</code></li> </ul>"},{"location":"examples/agents/weaviate-demo/#client-version-issues","title":"Client Version Issues","text":"<ul> <li>Error: \"Client.init() takes 1 positional argument but 2 were given\"</li> <li>Solution: Update to Weaviate client v4: <code>pip install weaviate-client&gt;=4.0.0</code></li> </ul>"},{"location":"examples/agents/weaviate-demo/#performance-issues","title":"Performance Issues","text":"<ul> <li>Slow queries: Reduce <code>top_k</code> or increase <code>similarity_threshold</code></li> <li>Memory issues: Adjust <code>chunk_size</code> and <code>chunk_overlap</code></li> </ul>"},{"location":"examples/agents/weaviate-demo/#use-cases","title":"Use Cases","text":"<p>\ud83c\udf93 Academic Research - Large-scale document analysis and citation retrieval</p> <p>\ud83c\udfe2 Enterprise Search - Corporate knowledge base with semantic understanding</p> <p>\ud83d\udcf0 Content Management - Intelligent article and document organization</p> <p>\ud83d\udd2c Scientific Literature - Research paper discovery and analysis</p>"},{"location":"examples/agents/weaviate-demo/#related-resources","title":"Related Resources","text":"<ul> <li>Weaviate Documentation</li> <li>SuperOptiX RAG Guide</li> <li>RAG Integration Guide - Vector database setup and configuration</li> <li>Model Management</li> </ul> <p>Ready to explore the power of Weaviate with SuperOptiX? \ud83d\ude80</p> <p>Start with this demo to understand how vector databases can enhance your AI applications with intelligent knowledge retrieval and semantic search capabilities. </p>"},{"location":"examples/agents/weights-biases-demo/","title":"Weights &amp; Biases Demo","text":""},{"location":"examples/agents/weights-biases-demo/#overview","title":"\ud83c\udfaf Overview","text":"<p>This demo shows how to use Weights &amp; Biases (W&amp;B) with SuperOptiX to track agent experiments, GEPA optimization runs, and multi-framework comparisons.</p> <p>What you'll learn: - Set up W&amp;B integration - Track agent execution metrics - Monitor GEPA optimization - Compare different frameworks - Create custom dashboards</p>"},{"location":"examples/agents/weights-biases-demo/#quick-demo","title":"\ud83d\ude80 Quick Demo","text":""},{"location":"examples/agents/weights-biases-demo/#setup-wb","title":"Setup W&amp;B","text":"<pre><code># Install W&amp;B\npip install wandb\n\n# Login to W&amp;B\nwandb login\n# Enter your API key from: https://wandb.ai/authorize\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#run-agent-with-wb-tracking","title":"Run Agent with W&amp;B Tracking","text":"<pre><code># Initialize project\nsuper init wb_demo\ncd wb_demo\n\n# Pull a demo agent\nsuper agent pull assistant_openai\n\n# Compile and run with W&amp;B tracking\nsuper agent compile assistant_openai\nsuper agent run assistant_openai \\\n  --query \"What is artificial intelligence?\" \\\n  --observe wandb \\\n  --tags [\"demo\", \"openai-sdk\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#view-results-in-wb","title":"View Results in W&amp;B","text":"<p>Visit: https://wandb.ai/your-username/superoptix</p> <p>You'll see: - Execution metrics (latency, token usage, cost) - Agent performance (success rate, response quality) - Custom tags for organization</p>"},{"location":"examples/agents/weights-biases-demo/#advanced-demo-gepa-optimization","title":"\ud83d\udcca Advanced Demo: GEPA Optimization","text":""},{"location":"examples/agents/weights-biases-demo/#track-optimization-runs","title":"Track Optimization Runs","text":"<pre><code># Run GEPA optimization with W&amp;B tracking\nsuper agent optimize assistant_openai \\\n  --auto medium \\\n  --observe wandb \\\n  --tags [\"gepa\", \"optimization\", \"medium\"]\n\n# View optimization progress in W&amp;B\n# Look for metrics like:\n# - gepa/generation\n# - gepa/fitness_score\n# - gepa/improvement\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#optimization-dashboard","title":"Optimization Dashboard","text":"<p>In W&amp;B, create a dashboard with:</p> <p>GEPA Progress Panel: <pre><code># Query: run.tags contains \"gepa\"\n# Metrics: gepa/fitness_score\n# Chart: Line plot over generations\n</code></pre></p> <p>Improvement Panel: <pre><code># Query: run.tags contains \"optimization\"\n# Metrics: gepa/improvement\n# Chart: Bar chart showing score increases\n</code></pre></p>"},{"location":"examples/agents/weights-biases-demo/#multi-framework-comparison-demo","title":"\ud83d\udd2c Multi-Framework Comparison Demo","text":""},{"location":"examples/agents/weights-biases-demo/#compare-different-frameworks","title":"Compare Different Frameworks","text":"<pre><code># Test DSPy agent\nsuper agent pull sentiment_analyzer\nsuper agent compile sentiment_analyzer\nsuper agent run sentiment_analyzer \\\n  --query \"This is amazing!\" \\\n  --observe wandb \\\n  --tags [\"framework:dspy\", \"comparison\"]\n\n# Test OpenAI SDK agent\nsuper agent run assistant_openai \\\n  --query \"This is amazing!\" \\\n  --observe wandb \\\n  --tags [\"framework:openai\", \"comparison\"]\n\n# Test CrewAI agent\nsuper agent pull researcher_crew\nsuper agent compile researcher_crew\nsuper agent run researcher_crew \\\n  --query \"Research artificial intelligence trends\" \\\n  --observe wandb \\\n  --tags [\"framework:crewai\", \"comparison\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#framework-comparison-dashboard","title":"Framework Comparison Dashboard","text":"<p>Create a comparison dashboard:</p> <p>Accuracy Comparison: <pre><code># Query: run.tags contains \"comparison\"\n# Metrics: execution/accuracy\n# Chart: Bar chart by framework\n</code></pre></p> <p>Latency Comparison: <pre><code># Query: run.tags contains \"comparison\"\n# Metrics: execution/latency\n# Chart: Box plot by framework\n</code></pre></p>"},{"location":"examples/agents/weights-biases-demo/#custom-metrics-demo","title":"\ud83c\udfa8 Custom Metrics Demo","text":""},{"location":"examples/agents/weights-biases-demo/#add-custom-business-metrics","title":"Add Custom Business Metrics","text":"<pre><code># Create custom_metrics.py\nimport wandb\nimport time\n\ndef track_custom_metrics(agent_response, user_query):\n    \"\"\"Track custom business metrics.\"\"\"\n\n    # Calculate business value\n    business_value = len(agent_response) * 0.1  # Simple example\n\n    # Calculate user satisfaction (mock)\n    user_satisfaction = 0.8 if \"helpful\" in agent_response.lower() else 0.6\n\n    # Calculate cost per interaction\n    cost_per_interaction = 0.002  # Mock cost\n\n    # Log to W&amp;B\n    wandb.log({\n        \"business/value\": business_value,\n        \"business/user_satisfaction\": user_satisfaction,\n        \"business/cost_per_interaction\": cost_per_interaction,\n        \"business/response_length\": len(agent_response),\n        \"business/query_complexity\": len(user_query.split())\n    })\n\n# Use in your agent\ndef run_agent_with_custom_metrics():\n    response = \"This is a helpful response about AI.\"\n    track_custom_metrics(response, \"What is AI?\")\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#run-with-custom-metrics","title":"Run with Custom Metrics","text":"<pre><code># Run agent with custom metrics\npython custom_metrics.py\nsuper agent run assistant_openai \\\n  --query \"Explain machine learning\" \\\n  --observe wandb \\\n  --tags [\"custom-metrics\", \"business-tracking\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#hyperparameter-optimization-demo","title":"\ud83d\udcc8 Hyperparameter Optimization Demo","text":""},{"location":"examples/agents/weights-biases-demo/#create-sweep-configuration","title":"Create Sweep Configuration","text":"<p>sweep_config.yaml: <pre><code>program: \"super agent optimize\"\nmethod: bayes\nmetric:\n  name: \"gepa/fitness_score\"\n  goal: maximize\nparameters:\n  reflection_lm:\n    values: [\"qwen3:8b\", \"llama3:8b\", \"gemma2:9b\"]\n  reflection_minibatch_size:\n    distribution: int_uniform\n    min: 2\n    max: 8\n  auto:\n    values: [\"light\", \"medium\", \"intensive\"]\n</code></pre></p>"},{"location":"examples/agents/weights-biases-demo/#run-hyperparameter-sweep","title":"Run Hyperparameter Sweep","text":"<pre><code># Initialize sweep\nwandb sweep sweep_config.yaml\n\n# Run sweep (replace SWEEP_ID with actual ID)\nwandb agent your-username/superoptix/SWEEP_ID\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#view-sweep-results","title":"View Sweep Results","text":"<p>In W&amp;B: 1. Go to your project 2. Click on \"Sweeps\" tab 3. View parallel coordinates plot 4. See best parameters highlighted</p>"},{"location":"examples/agents/weights-biases-demo/#team-collaboration-demo","title":"\ud83c\udfe2 Team Collaboration Demo","text":""},{"location":"examples/agents/weights-biases-demo/#shared-project-setup","title":"Shared Project Setup","text":"<pre><code># Create shared project\nwandb init --project \"team-agent-experiments\" --entity \"my-company\"\n\n# Run experiments with team tags\nsuper agent run assistant_openai \\\n  --query \"Customer support query\" \\\n  --observe wandb \\\n  --entity \"my-company\" \\\n  --project \"team-agent-experiments\" \\\n  --tags [\"team:engineering\", \"customer-support\", \"production\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#team-dashboard","title":"Team Dashboard","text":"<p>Create team dashboard with:</p> <p>Team Performance Panel: <pre><code># Query: run.tags contains \"team:engineering\"\n# Metrics: execution/success_rate\n# Chart: Line plot over time\n</code></pre></p> <p>Customer Support Panel: <pre><code># Query: run.tags contains \"customer-support\"\n# Metrics: execution/latency, execution/success_rate\n# Chart: Multi-line plot\n</code></pre></p>"},{"location":"examples/agents/weights-biases-demo/#debugging-demo","title":"\ud83d\udd0d Debugging Demo","text":""},{"location":"examples/agents/weights-biases-demo/#enable-debug-mode","title":"Enable Debug Mode","text":"<pre><code># Enable W&amp;B debug logging\nexport WANDB_DEBUG=true\n\n# Run with verbose output\nsuper agent run assistant_openai \\\n  --query \"Debug test\" \\\n  --observe wandb \\\n  --verbose \\\n  --tags [\"debug\", \"testing\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#check-wb-logs","title":"Check W&amp;B Logs","text":"<pre><code># View W&amp;B logs\ntail -f ~/.local/share/wandb/debug.log\n\n# Check W&amp;B status\nwandb status\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#example-dashboard-queries","title":"\ud83d\udcca Example Dashboard Queries","text":""},{"location":"examples/agents/weights-biases-demo/#agent-performance-dashboard","title":"Agent Performance Dashboard","text":"<pre><code># Panel 1: Execution Metrics\n# Query: run.tags contains \"production\"\n# Metrics: execution/latency, execution/success_rate\n# Chart: Time series\n\n# Panel 2: Cost Tracking\n# Query: run.tags contains \"production\"\n# Metrics: execution/cost, execution/token_usage\n# Chart: Scatter plot\n\n# Panel 3: Framework Comparison\n# Query: run.tags contains \"comparison\"\n# Metrics: execution/accuracy\n# Chart: Bar chart by framework\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#gepa-optimization-dashboard","title":"GEPA Optimization Dashboard","text":"<pre><code># Panel 1: Optimization Progress\n# Query: run.tags contains \"gepa\"\n# Metrics: gepa/fitness_score\n# Chart: Line plot over generations\n\n# Panel 2: Parameter Impact\n# Query: run.tags contains \"optimization\"\n# Metrics: gepa/improvement\n# Chart: Scatter plot by parameter\n\n# Panel 3: Best Runs\n# Query: run.tags contains \"gepa\"\n# Metrics: gepa/fitness_score\n# Chart: Table sorted by fitness score\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#best-practices-demo","title":"\ud83c\udfaf Best Practices Demo","text":""},{"location":"examples/agents/weights-biases-demo/#consistent-tagging","title":"Consistent Tagging","text":"<pre><code># Use hierarchical tags\nsuper agent run my_agent \\\n  --observe wandb \\\n  --tags [\"framework:dspy\", \"tier:genies\", \"stage:production\", \"version:v2.1\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#project-organization","title":"Project Organization","text":"<pre><code># Organize by project type\nsuper agent run my_agent --observe wandb --project \"superoptix-development\"\nsuper agent run my_agent --observe wandb --project \"superoptix-production\"\nsuper agent run my_agent --observe wandb --project \"superoptix-research\"\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#metric-naming","title":"Metric Naming","text":"<pre><code># Use consistent metric names\nwandb.log({\n    \"execution/latency\": 1.2,\n    \"execution/success_rate\": 0.95,\n    \"gepa/fitness_score\": 0.85,\n    \"business/value\": 10.5,\n    \"custom/user_satisfaction\": 0.8\n})\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"examples/agents/weights-biases-demo/#with-mlflow","title":"With MLFlow","text":"<pre><code># Log to both W&amp;B and MLFlow\nsuper agent run my_agent --observe all --tags [\"multi-backend\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#with-langfuse","title":"With LangFuse","text":"<pre><code># Use W&amp;B for metrics, LangFuse for traces\nsuper agent run my_agent --observe wandb --tags [\"metrics\"]\nsuper agent run my_agent --observe langfuse --tags [\"traces\"]\n</code></pre>"},{"location":"examples/agents/weights-biases-demo/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Complete the demos above</li> <li>Create your own dashboard in W&amp;B</li> <li>Set up team collaboration with shared projects</li> <li>Try hyperparameter optimization with sweeps</li> <li>Integrate with your existing workflow</li> </ol> <p>Ready to start? Begin with the Quick Demo section!</p>"},{"location":"examples/agents/weights-biases-demo/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Weights &amp; Biases Integration Guide</li> <li>Enhanced Observability Guide</li> <li>GEPA Optimization Guide</li> <li>Multi-Framework Guide</li> </ul>"},{"location":"guides/","title":"Guides Overview","text":"<p>SuperOptiX lets you define agents once in SuperSpec YAML and compile them into multiple frameworks.</p>"},{"location":"guides/#highlighted-capabilities","title":"Highlighted Capabilities","text":"<ul> <li>\ud83e\uddea RLM (Experimental)</li> <li>\ud83d\uddc2\ufe0f StackOne Connectors</li> <li>\ud83e\uddec GEPA Optimization</li> </ul>"},{"location":"guides/#start-here","title":"Start Here","text":"<ul> <li>SuperSpec Overview</li> <li>Multi-Framework Guide</li> <li>Golden Workflow</li> <li>CLI Complete Guide</li> </ul>"},{"location":"guides/#framework-guides","title":"Framework Guides","text":"<ul> <li>DSPy</li> <li>Framework Feature Matrix</li> <li>OpenAI Agents SDK</li> <li>Claude Agent SDK</li> <li>Pydantic AI</li> <li>CrewAI</li> <li>Google ADK</li> <li>DeepAgents</li> <li>Microsoft Agent Framework (Legacy Support)</li> </ul>"},{"location":"guides/#connectors","title":"Connectors","text":"<ul> <li>StackOne Integration</li> <li>StackOne + Claude SDK</li> </ul>"},{"location":"guides/#optimization-and-evaluation","title":"Optimization and Evaluation","text":"<ul> <li>GEPA Optimization</li> <li>RLM (Experimental)</li> <li>Optimization Guide</li> <li>Evaluation &amp; Testing</li> </ul>"},{"location":"guides/#observability","title":"Observability","text":"<ul> <li>Observability Guide</li> <li>MLFlow</li> <li>LangFuse</li> <li>LogFire</li> </ul>"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Troubleshooting by Symptom</li> </ul>"},{"location":"guides/agent-development/","title":"\ud83d\ude80 Agent Development Life Cycle in SuperOptiX","text":"<p>SuperOptiX follows a evaluation-first, BDD-driven development approach that ensures your agents are production-ready from day one. This guide walks you through the complete lifecycle of building, testing, and deploying AI agents.</p>"},{"location":"guides/agent-development/#the-superoptix-development-lifecycle","title":"\ud83c\udfaf The SuperOptiX Development Lifecycle","text":"<pre><code>graph TD\n    A[\ud83d\udccb Spec: Intent &amp; Context&lt;br/&gt;SuperSpec DSL] --&gt; B[\ud83d\udd28 Compile: Convert to Python&lt;br/&gt;DSPy Pipelines]\n    B --&gt; C[\ud83e\uddea Evaluate: BDD/TDD Testing&lt;br/&gt;Establish Baseline]\n    C --&gt; D{\ud83d\udcca Pass Quality Gate?}\n    D --&gt;|Yes| E[\ud83d\ude80 Run: Execute Agent]\n    D --&gt;|No| F[\u26a1 Optimize: DSPy Optimizers&lt;br/&gt;Learn from Scenarios]\n    F --&gt; B\n    E --&gt; G[\ud83c\udfbc Orchestra: Multi-Agent&lt;br/&gt;Coordination]\n    G --&gt; H[\ud83d\udcc8 Monitor: Observability&lt;br/&gt;Performance Tracking]\n    H --&gt; C\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style E fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style F fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style G fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style H fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/agent-development/#phase-1-specification-context-engineering","title":"\ud83c\udfd7\ufe0f Phase 1: Specification &amp; Context Engineering","text":""},{"location":"guides/agent-development/#superspec-dsl-define-intent-context","title":"SuperSpec DSL: Define Intent &amp; Context","text":"<p>The foundation of every agent starts with the SuperSpec DSL - a declarative language for defining agent behavior, context, and capabilities.</p> <pre><code># Generate agent with context engineering\nsuper spec generate genies developer --rag --memory --tools\n</code></pre> <p>What happens:</p> <ul> <li> <p>\ud83c\udfad Persona Definition - Agent personality and behavioral traits</p> </li> <li> <p>\ud83e\udde0 Memory Systems - Short-term, long-term, and episodic memory</p> </li> <li> <p>\ud83d\udee0\ufe0f Tool Integration - Web search, file operations, code analysis</p> </li> <li> <p>\ud83d\udcda RAG Capabilities - Knowledge retrieval (document ingestion is configured separately)</p> </li> <li> <p>\ud83d\udccb Task Specifications - What your agent should do</p> </li> <li> <p>\ud83d\udd12 Safety Constraints - What your agent should NOT do</p> </li> </ul> <p>Example Playbook Structure: <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: \"Developer Assistant\"\n  tier: \"genies\"\n  namespace: \"software\"\n\nspec:\n  persona: |\n    You are an expert software developer with 10+ years of experience.\n    You specialize in Python, React, and cloud architecture.\n    Always provide practical, production-ready solutions.\n\n  context:\n    memory:\n      short_term: true\n      long_term: true\n      episodic: true\n\n    tools:\n      - web_search\n      - code_formatter\n      - git_analyzer\n      - docker_helper\n\n    rag:\n      enabled: true\n      # RAG sources are configured in the vector database, not directly in the playbook.\n      # This flag enables the agent to use the pre-configured RAG system.\n      sources: []\n\n  tasks:\n    - name: \"code_review\"\n      description: \"Review code for best practices\"\n    - name: \"architecture_design\"\n      description: \"Design system architecture\"\n</code></pre></p>"},{"location":"guides/agent-development/#phase-2-tool-memory-integration","title":"\ud83d\udee0\ufe0f Phase 2: Tool &amp; Memory Integration","text":"<p>The real power of Genies-tier agents comes from their ability to use tools and memory.</p>"},{"location":"guides/agent-development/#available-tools","title":"Available Tools","text":"<p>Your agent has access to a variety of built-in tools.</p> <ul> <li> <p><code>WebSearchTool</code>: Performs web searches to gather information.</p> <p>\u26a0\ufe0f Note: The default <code>WebSearchTool</code> is a non-functional placeholder. To use it, you must integrate a real search API (e.g., DuckDuckGo, Serper, Tavily) by modifying the tool's implementation in <code>superoptix/tools/categories/core.py</code>.</p> </li> <li> <p><code>CalculatorTool</code>: For performing mathematical calculations.</p> </li> <li> <p><code>FileReaderTool</code>: To read the contents of local files.</p> </li> <li> <p><code>CodeFormatterTool</code>: To format and pretty-print code snippets.</p> </li> </ul>"},{"location":"guides/agent-development/#memory-systems","title":"Memory Systems","text":"<ul> <li>Short-Term Memory: Remembers the immediate context of a conversation.</li> <li>Long-Term Memory: Stores and recalls information over extended periods.</li> <li>Episodic Memory: Remembers past interactions to learn from experience.</li> </ul>"},{"location":"guides/agent-development/#phase-3-compilation-yaml-to-python","title":"\ud83d\udd28 Phase 3: Compilation - YAML to Python","text":""},{"location":"guides/agent-development/#transform-playbooks-into-executable-pipelines","title":"Transform Playbooks into Executable Pipelines","text":"<pre><code>super agent compile developer\n</code></pre> <p>What happens:</p> <ul> <li> <p>\ud83d\udccb YAML Playbook \u2192 Python Pipeline</p> </li> <li> <p>\ud83c\udfaf DSPy Integration - Automatic pipeline generation</p> </li> <li> <p>\ud83d\udd27 Framework Selection - Tier-appropriate optimizations</p> </li> <li> <p>\ud83d\udcc1 File Generation - <code>developer_pipeline.py</code> created</p> </li> </ul> \ud83d\udd28 Compilation Output <pre><code>================================================================================\n\ud83d\udd28 Compiling agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                          \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                              \u2502\n\u2502                                                                                                          \u2502\n\u2502  \ud83c\udfaf Agent: Developer Assistant                                                                           \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy Genies Pipeline - other frameworks coming soon                                      \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                  \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/developer/pipelines/developer_pipeline.py                                        \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nCompilation successful! Agent 'developer' is ready for evaluation.\n</code></pre>"},{"location":"guides/agent-development/#phase-3-evaluation-first-development","title":"\ud83e\uddea Phase 3: Evaluation-First Development","text":""},{"location":"guides/agent-development/#bddtdd-test-before-you-optimize","title":"BDD/TDD: Test Before You Optimize","text":"<p>\ud83d\udea8 CRITICAL: Always evaluate before optimizing!</p> <pre><code>super agent evaluate developer\n</code></pre> <p>Why Evaluation-First?</p> <ul> <li> <p>\ud83d\udcca Baseline Measurement - Know your starting point</p> </li> <li> <p>\ud83c\udfaf Quality Gates - Ensure scenarios are well-written</p> </li> <li> <p>\ud83d\udca1 Optimization Strategy - Plan improvements based on failures</p> </li> <li> <p>Fail-Fast Feedback - Catch issues early</p> </li> </ul> \ud83e\uddea Evaluation Output <pre><code>\ud83c\udfad Running BDD Test Suite for Agent: developer\n============================================================\n\n\ud83c\udfad Executing BDD Scenarios...\n  \ud83d\udcdd Running: basic_api_endpoint_creation\n    PASSED\n  \ud83d\udcdd Running: data_structure_design\n    PASSED\n  \ud83d\udcdd Running: algorithm_implementation\n    FAILED: semantic meaning differs significantly\n  \ud83d\udcdd Running: robust_error_handling\n    PASSED\n  \ud83d\udcdd Running: test_code_generation\n    PASSED\n\n\ud83d\udcca BDD Test Results Summary:\n========================================\nTotal Scenarios: 5\nPassed: 4 Failed: 1 Pass Rate: 80.0%\nBDD Score: 0.800\n\n\ud83d\udca1 Recommendations:\n   \ud83d\udd27 Fix 1 failing scenarios to improve reliability\n   \ud83c\udfaf Common issue (1 scenarios): semantic meaning differs significantly\n\n\ud83c\udf89 BDD Test Suite: EXCELLENT (80.0%)\n</code></pre>"},{"location":"guides/agent-development/#quality-gates","title":"Quality Gates","text":"Pass Rate Status Action Required \u2265 80% Production Ready Deploy with confidence 60-79% \u26a0\ufe0f Needs Improvement Optimize and re-evaluate &lt; 60% Significant Work Fix scenarios and recompile"},{"location":"guides/agent-development/#phase-4-optimization-dspy-magic","title":"\u26a1 Phase 4: Optimization - DSPy Magic","text":""},{"location":"guides/agent-development/#learn-from-your-bdd-scenarios","title":"Learn from Your BDD Scenarios","text":"<pre><code>super agent optimize developer\n</code></pre> <p>What Optimization Does:</p> <ul> <li> <p>\ud83d\udcda Training Data - Uses your BDD scenarios as examples</p> </li> <li> <p>\ud83e\udde0 DSPy BootstrapFewShot - Automatic prompt improvement</p> </li> <li> <p>\ud83d\udcc8 Performance Enhancement - Better reasoning and responses</p> </li> <li> <p>\ud83d\udcbe Optimized Weights - Saved to <code>developer_optimized.json</code></p> </li> </ul> \u26a1 Optimization Output <pre><code>================================================================================\n\ud83d\ude80 Optimizing agent 'developer'...\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Optimization Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                          \u2502\n\u2502  \ud83e\udd16 OPTIMIZATION IN PROGRESS                                                                            \u2502\n\u2502                                                                                                          \u2502\n\u2502  \ud83c\udfaf Agent: Developer Assistant                                                                           \u2502\n\u2502  \ud83d\udd27 Strategy: DSPy BootstrapFewShot                                                                      \u2502\n\u2502  \ud83d\udcca Data Source: BDD scenarios from playbook                                                            \u2502\n\u2502  \ud83d\udcbe Output: swe/agents/developer/pipelines/developer_optimized.json                                     \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83c\udfaf Using 5 BDD scenarios for optimization...\n\ud83d\udcc8 Learning from scenario: basic_api_endpoint_creation\n\ud83d\udcc8 Learning from scenario: data_structure_design\n\ud83d\udcc8 Learning from scenario: algorithm_implementation\n\ud83d\udcc8 Learning from scenario: robust_error_handling\n\ud83d\udcc8 Learning from scenario: test_code_generation\n\nOptimization complete! Agent performance enhanced.\n</code></pre>"},{"location":"guides/agent-development/#re-evaluate-to-measure-improvement","title":"Re-Evaluate to Measure Improvement","text":"<pre><code>super agent evaluate developer\n</code></pre> <p>Expected Results:</p> <ul> <li> <p>\ud83d\udcc8 Improved Pass Rate - Should be higher than baseline</p> </li> <li> <p>\ud83c\udfaf Better Quality - More accurate and relevant responses</p> </li> <li> <p>\u26a1 Faster Execution - Optimized weights load automatically</p> </li> </ul>"},{"location":"guides/agent-development/#phase-5-execution-run-your-agent","title":"\ud83d\ude80 Phase 5: Execution - Run Your Agent","text":""},{"location":"guides/agent-development/#deploy-your-optimized-agent","title":"Deploy Your Optimized Agent","text":"<pre><code>super agent run developer --goal \"Create a REST API with FastAPI\"\n</code></pre> <p>What Happens:</p> <ul> <li> <p>\ud83d\udd04 Load Optimized Pipeline - Uses <code>developer_optimized.json</code></p> </li> <li> <p>\ud83e\udde0 Context-Aware Processing - Memory, tools, and RAG integration</p> </li> <li> <p>\ud83d\udcca Real-time Execution - Interactive agent responses</p> </li> <li> <p>\ud83c\udfaf Goal-Oriented Behavior - Focused on your specific task</p> </li> </ul> \ud83d\ude80 Execution Output <pre><code>\ud83d\ude80 Running agent 'developer'...\n\nLoading pipeline... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100%\n\ud83d\ude80 Using pre-optimized pipeline from developer_optimized.json\n\n\ud83c\udfaf Goal: Create a REST API with FastAPI\n\n\ud83e\udd16 Developer Assistant: I'll help you create a comprehensive REST API using FastAPI. Let me break this down into a well-structured solution.\n\n\ud83d\udccb Implementation Plan:\n1. Project structure setup\n2. FastAPI application with proper configuration\n3. Database models and schemas\n4. CRUD operations\n5. Authentication and validation\n6. Error handling and logging\n\n\ud83d\udd27 Let me start with the core implementation...\n\n[Detailed implementation follows...]\n</code></pre>"},{"location":"guides/agent-development/#phase-6-orchestration-multi-agent-coordination","title":"\ud83c\udfbc Phase 6: Orchestration - Multi-Agent Coordination","text":""},{"location":"guides/agent-development/#coordinate-multiple-agents","title":"Coordinate Multiple Agents","text":"<pre><code># Add more agents to your team\nsuper agent pull devops_engineer\nsuper agent pull qa_engineer\n\n# Compile and optimize all agents\nsuper agent compile --all\nsuper agent optimize devops_engineer\nsuper agent optimize qa_engineer\n\n# Create coordinated workflow\nsuper orchestra create sdlc\nsuper orchestra run sdlc --goal \"Build a complete web application\"\n</code></pre> <p>What Happens:</p> <ul> <li> <p>\ud83e\udd1d Agent Coordination - Seamless communication between agents</p> </li> <li> <p>\ud83d\udcca Workflow Management - Sequential or parallel execution</p> </li> <li> <p>\ud83d\udd04 Data Flow - Output from one agent feeds into another</p> </li> <li> <p>\ud83d\udccb Production Artifacts - Complete implementation, tests, and deployment</p> </li> </ul>"},{"location":"guides/agent-development/#phase-7-monitoring-continuous-improvement","title":"\ud83d\udcc8 Phase 7: Monitoring &amp; Continuous Improvement","text":""},{"location":"guides/agent-development/#observability-performance-tracking","title":"Observability &amp; Performance Tracking","text":"<pre><code># Enable observability\nsuper observe enable developer\n\n# Monitor performance\nsuper observe dashboard\n\n# Debug specific issues\nsuper observe debug agent developer\n\n# View detailed traces\nsuper observe traces developer\n</code></pre> <p>Monitoring Capabilities:</p> <ul> <li> <p>\ud83d\udcca Real-time Metrics - Performance, latency, success rates</p> </li> <li> <p>\ud83d\udd0d Detailed Traces - Step-by-step execution analysis</p> </li> <li> <p>\ud83d\udc1b Debugging Tools - Identify and fix issues</p> </li> <li> <p>\ud83d\udcc8 Trend Analysis - Long-term performance tracking</p> </li> </ul>"},{"location":"guides/agent-development/#the-complete-workflow","title":"\ud83d\udd04 The Complete Workflow","text":""},{"location":"guides/agent-development/#proper-bddtdd-development-cycle","title":"Proper BDD/TDD Development Cycle","text":"<pre><code># Define your agent (Spec)\nsuper spec generate genies developer --rag --memory --tools\n\n# Compile to executable code\nsuper agent compile developer\n\n# Establish baseline performance (CRITICAL)\nsuper agent evaluate developer\n\n# Optimize based on evaluation results\nsuper agent optimize developer\n\n# Measure improvement\nsuper agent evaluate developer\n\n# Deploy when quality gates pass\nsuper agent run developer --goal \"Your production task\"\n\n# Monitor and iterate\nsuper observe dashboard\n</code></pre>"},{"location":"guides/agent-development/#advanced-development-tips","title":"Advanced Development Tips","text":""},{"location":"guides/agent-development/#customize-dspy-pipelines","title":"\ud83d\udd27 Customize DSPy Pipelines","text":"<pre><code># Modify generated pipeline for custom logic\n# File: agents/developer/pipelines/developer_pipeline.py\n\nclass CustomDeveloperPipeline(DeveloperPipeline):\n    def __init__(self):\n        super().__init__()\n        # Add custom tools\n        self.tools.append(CustomCodeAnalyzer())\n\n    def forward(self, query):\n        # Add custom preprocessing\n        enhanced_query = self.preprocess_query(query)\n        return super().forward(enhanced_query)\n</code></pre>"},{"location":"guides/agent-development/#smart-optimization-strategies","title":"\ud83c\udfaf Smart Optimization Strategies","text":"<pre><code># Force re-optimization\nsuper agent optimize developer --force\n\n# Runtime optimization for experiments\nsuper agent run developer --goal \"task\" --optimize\n</code></pre>"},{"location":"guides/agent-development/#quality-driven-development","title":"\ud83d\udcca Quality-Driven Development","text":"<ul> <li> <p>Write Specific Scenarios - Include concrete examples and expected outputs</p> </li> <li> <p>Cover Edge Cases - Test error conditions and boundary cases</p> </li> <li> <p>Use Realistic Data - Make scenarios representative of real usage</p> </li> <li> <p>Iterate Based on Results - Use evaluation feedback to improve scenarios</p> </li> </ul>"},{"location":"guides/agent-development/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/agent-development/#dos","title":"DO's","text":"<ul> <li> <p>Always evaluate before optimizing - Establish baseline performance</p> </li> <li> <p>Write comprehensive BDD scenarios - Cover all important use cases</p> </li> <li> <p>Use quality gates - Don't deploy until pass rate \u2265 80%</p> </li> <li> <p>Monitor in production - Track performance and iterate</p> </li> <li> <p>Version your playbooks - Track changes and improvements</p> </li> </ul>"},{"location":"guides/agent-development/#donts","title":"DON'Ts","text":"<ul> <li> <p>Don't optimize without baseline - You won't know if you improved</p> </li> <li> <p>Don't skip evaluation after optimization - Validate your improvements</p> </li> <li> <p>Don't deploy without quality gates - Ensure production readiness</p> </li> <li> <p>Don't ignore failing scenarios - They indicate real problems</p> </li> </ul>"},{"location":"guides/agent-development/#production-deployment-checklist","title":"\ud83d\ude80 Production Deployment Checklist","text":""},{"location":"guides/agent-development/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li> <p>Quality Gates Pass - \u2265 80% BDD pass rate</p> </li> <li> <p>Optimization Complete - Optimized weights generated</p> </li> <li> <p>Monitoring Enabled - Observability configured</p> </li> <li> <p>Error Handling - Robust error management</p> </li> <li> <p>Performance Validated - Latency and throughput acceptable</p> </li> </ul>"},{"location":"guides/agent-development/#post-deployment","title":"Post-Deployment","text":"<ul> <li> <p>\ud83d\udcca Monitor Performance - Track key metrics</p> </li> <li> <p>\ud83d\udd0d Analyze Traces - Identify optimization opportunities</p> </li> <li> <p>\ud83d\udcc8 Measure Impact - Compare to baseline</p> </li> <li> <p>\ud83d\udd04 Iterate - Continuous improvement cycle</p> </li> </ul>"},{"location":"guides/agent-development/#success-metrics","title":"\ud83c\udf89 Success Metrics","text":""},{"location":"guides/agent-development/#development-velocity","title":"Development Velocity","text":"<ul> <li> <p>Time to First Agent - &lt; 30 minutes</p> </li> <li> <p>Time to Production - &lt; 2 hours</p> </li> <li> <p>Scenario Coverage - 100% of critical paths</p> </li> <li> <p>Optimization Cycles - &lt; 3 iterations to 80% pass rate</p> </li> </ul>"},{"location":"guides/agent-development/#production-quality","title":"Production Quality","text":"<ul> <li> <p>BDD Pass Rate - \u2265 80%</p> </li> <li> <p>Response Quality - High relevance and accuracy</p> </li> <li> <p>System Reliability - 99.9% uptime</p> </li> <li> <p>Performance - &lt; 5 second response time</p> </li> </ul> <p>\ud83c\udfaf Remember: SuperOptiX is built for production-ready AI agents from day one. Follow the evaluation-first workflow, and you'll build reliable, scalable agentic systems that deliver real business value! \ud83d\ude80 </p>"},{"location":"guides/agent-discovery/","title":"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Agent Discovery Guide","text":"<p>Agent discovery in SuperOptiX is all about creating and exploring agent playbooks-the heart of context engineering for your agents.</p>"},{"location":"guides/agent-discovery/#what-is-agent-discovery","title":"\ud83e\udde0 What is Agent Discovery?","text":"<p>Agent discovery means: - Defining the context, goals, and capabilities your agent needs - Engineering all relevant context, memory, tools, and RAG settings in the playbook - Using the SuperSpec DSL to declaratively specify your agent's behavior, persona, and requirements</p>"},{"location":"guides/agent-discovery/#the-agent-playbook-superspec-dsl","title":"\ud83d\udcd2 The Agent Playbook (SuperSpec DSL)","text":"<ul> <li>Every agent in SuperOptiX is defined by a playbook written in the SuperSpec DSL</li> <li>The playbook is where you do all your context engineering: persona, memory, tools, RAG, tasks, and more</li> <li>Playbooks are human-readable, versionable, and easy to modify</li> </ul>"},{"location":"guides/agent-discovery/#three-ways-to-discover-create-agent-playbooks","title":"\ud83d\udee0\ufe0f Three Ways to Discover &amp; Create Agent Playbooks","text":""},{"location":"guides/agent-discovery/#browse-prebuilt-agents-tools-marketplace","title":"\ud83c\udfea Browse Prebuilt Agents &amp; Tools (Marketplace)","text":"<p>Discover ready-to-use agents and tools from the SuperOptiX marketplace.</p>"},{"location":"guides/agent-discovery/#browse-all-available-agents","title":"Browse All Available Agents","text":"<pre><code>super market browse agents\n</code></pre> \ud83d\udccb Example Output <pre><code>\ud83e\udd16 Marketplace: Agents\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udcca Available Industries (22):\n \u2022 Agriculture Food     \u2022 Consulting         \u2022 Demo            \n \u2022 Education            \u2022 Energy Utilities   \u2022 Finance         \n \u2022 Gaming Sports        \u2022 Government Public  \u2022 Healthcare      \n \u2022 Hospitality Tourism  \u2022 Huggingface Demo   \u2022 Human Resources \n \u2022 Legal                \u2022 Manufacturing      \u2022 Marketing       \n \u2022 Media Entertainment  \u2022 Mlx Demo           \u2022 Real Estate     \n \u2022 Retail               \u2022 Software           \u2022 Testing         \n \u2022 Transportation                                              \n\n                                                             \ud83d\udccb Available Pre-built Agents                                                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Industry            \u2503 Name                                \u2503 ID                             \u2503 Level   \u2503 Type        \u2503 Playbook Ref                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Software            \u2502 Developer Assistant                 \u2502 developer                      \u2502 oracles \u2502 Supervised  \u2502 developer                      \u2502\n\u2502 Software            \u2502 DevOps Engineer Assistant           \u2502 devops_engineer                \u2502 oracles \u2502 Supervised  \u2502 devops_engineer                \u2502\n\u2502 Software            \u2502 Lightweight Developer Assistant     \u2502 lightweight-developer          \u2502 oracles \u2502 Autonomous  \u2502 lightweight_developer          \u2502\n\u2502 Software            \u2502 Performance Engineer Assistant      \u2502 performance_engineer           \u2502 oracles \u2502 Supervised  \u2502 performance_engineer           \u2502\n\u2502 Software            \u2502 Product Owner Assistant             \u2502 product_owner                  \u2502 oracles \u2502 Supervised  \u2502 product_owner                  \u2502\n\u2502 Software            \u2502 QA Engineer Assistant               \u2502 qa_engineer                    \u2502 oracles \u2502 Supervised  \u2502 qa_engineer                    \u2502\n\u2502 Software            \u2502 Scrum Master Assistant              \u2502 scrum_master                   \u2502 oracles \u2502 Supervised  \u2502 scrum_master                   \u2502\n\u2502 Software            \u2502 Security Engineer Assistant         \u2502 security_engineer              \u2502 oracles \u2502 Supervised  \u2502 security_engineer              \u2502\n\u2502 Software            \u2502 System Architect Assistant          \u2502 system_architect               \u2502 oracles \u2502 Supervised  \u2502 system_architect               \u2502\n\u2502 Software            \u2502 Technical Writer Assistant          \u2502 technical_writer               \u2502 oracles \u2502 Supervised  \u2502 technical_writer               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nFound 119 pre-built agent(s)\n\n\ud83d\ude80 Next Steps:\n   super agent pull &lt;playbook_ref&gt;           - Add an agent to your project\n   super agent list --pre-built --industry &lt;name&gt; - Filter by industry\n   super agent design                        - Create a custom agent\n\n\ud83d\udca1 Example: super agent pull developer\n</code></pre>"},{"location":"guides/agent-discovery/#browse-available-tools","title":"Browse Available Tools","text":"<pre><code>super market browse tools\n</code></pre> \ud83d\udee0\ufe0f Example Output <pre><code>\ud83d\udee0\ufe0f Marketplace: Tools\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\ud83d\udcc2 Available Categories (3):\n \u2022 Core (6)  \u2022 Development (9)  \u2022 Data (1) \n\n                                                              \ud83d\udee0\ufe0f Available Tools                                                              \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                \u2503 Category    \u2503 Description                                           \u2503 Industry   \u2503 Tags                             \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 web_search          \u2502 Core        \u2502 Search the web for information using various searc... \u2502 General    \u2502 search, web, research            \u2502\n\u2502 calculator          \u2502 Core        \u2502 Perform safe mathematical calculations and express... \u2502 General    \u2502 math, calculation, arithmetic    \u2502\n\u2502 file_reader         \u2502 Core        \u2502 Read and process text files with safety restrictio... \u2502 General    \u2502 file, text, io                   \u2502\n\u2502 datetime            \u2502 Core        \u2502 Get current time and format dates between differen... \u2502 General    \u2502 time, date, formatting           \u2502\n\u2502 text_analyzer       \u2502 Core        \u2502 Analyze text for statistics, readability, and basi... \u2502 General    \u2502 text, analysis, nlp              \u2502\n\u2502 json_processor      \u2502 Core        \u2502 Parse JSON and extract specific fields using dot n... \u2502 General    \u2502 json, parsing, data              \u2502\n\u2502 code_formatter      \u2502 Development \u2502 Format code with basic syntax highlighting and str... \u2502 Technology \u2502 code, formatting, development    \u2502\n\u2502 data_processor      \u2502 Data        \u2502 Process and analyze CSV data with various operatio... \u2502 General    \u2502 data, csv, analysis              \u2502\n\u2502 git_analyzer        \u2502 Development \u2502 Analyze Git commit messages for best practices and... \u2502 Technology \u2502 git, version-control, commits    \u2502\n\u2502 api_tester          \u2502 Development \u2502 Validate and analyze API responses for structure a... \u2502 Technology \u2502 api, testing, validation         \u2502\n\u2502 database_query      \u2502 Development \u2502 Validate SQL queries for syntax and security issue... \u2502 Technology \u2502 sql, database, security          \u2502\n\u2502 version_checker     \u2502 Development \u2502 Compare semantic versions and determine relationsh... \u2502 Technology \u2502 versioning, semver, comparison   \u2502\n\u2502 dependency_analyzer \u2502 Development \u2502 Analyze package dependencies for security and upda... \u2502 Technology \u2502 dependencies, security, packages \u2502\n\u2502 code_reviewer       \u2502 Development \u2502 Perform automated code review and quality analysis    \u2502 Technology \u2502 code-review, quality, analysis   \u2502\n\u2502 test_coverage       \u2502 Development \u2502 Analyze test coverage reports and provide recommen... \u2502 Technology \u2502 testing, coverage, quality       \u2502\n\u2502 docker_helper       \u2502 Development \u2502 Validate Dockerfiles for best practices and optimi... \u2502 Technology \u2502 docker, containers, devops       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nFound 16 tool(s)\n</code></pre>"},{"location":"guides/agent-discovery/#search-for-specific-agents","title":"Search for Specific Agents","text":"<pre><code>super market search \"Software\"\n</code></pre> \ud83d\udd0d Example Output <pre><code>\ud83d\udd0d Marketplace Search: 'Software'\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nFound 10 result(s)\n\n\ud83e\udd16 Agents\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                            \u2503 Industry \u2503 Type       \u2503 Install                                               \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 System Architect Assistant      \u2502 Software \u2502 Supervised \u2502 super marketplace install agent system_architect      \u2502\n\u2502 Product Owner Assistant         \u2502 Software \u2502 Supervised \u2502 super marketplace install agent product_owner         \u2502\n\u2502 Lightweight Developer Assistant \u2502 Software \u2502 Autonomous \u2502 super marketplace install agent lightweight_developer \u2502\n\u2502 Developer Assistant             \u2502 Software \u2502 Supervised \u2502 super marketplace install agent developer             \u2502\n\u2502 Scrum Master Assistant          \u2502 Software \u2502 Supervised \u2502 super marketplace install agent scrum_master          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n... and 5 more agents\n</code></pre> <p>Benefits:</p> <ul> <li> <p>\u26a1 Fastest setup - Get started in minutes</p> </li> <li> <p>\ud83d\udee1\ufe0f Battle-tested - Pre-optimized and evaluated</p> </li> <li> <p>\ud83d\udce6 No coding required - Ready to use immediately</p> </li> <li> <p>\ud83d\udd04 Actively maintained - Regular updates and improvements</p> </li> </ul>"},{"location":"guides/agent-discovery/#create-agents-using-superspec-cli","title":"\ud83d\udcdc Create Agents Using SuperSpec (CLI)","text":"<p>Generate custom agent playbooks using the declarative SuperSpec DSL.</p> <pre><code>super spec generate &lt;tier&gt; &lt;agent_name&gt; [--rag]\n</code></pre> <p>Examples: <pre><code># Create a Genies-tier developer agent with RAG\nsuper spec generate genies developer --rag\n\n# Create an Oracles-tier research agent\nsuper spec generate oracles research\n\n# Create a basic Genies-tier agent\nsuper spec generate genies assistant\n</code></pre></p> \ud83d\udccb Example Output <pre><code>\ud83d\udcc1 Using SuperOptiX project structure: demo_agent_discovery/agents/developer/playbook/developer_playbook.yaml\nGenerated genies agent playbook: /Users/super/agentic/SuperOptiX/demo_agent_discovery/agents/developer/playbook/developer_playbook.yaml\n\ud83d\udccb Agent: Developer (Tier: genies)\n\ud83c\udff7\ufe0f  Namespace: software\n\u26a1 Features: memory, tools, agentflow\n</code></pre> <p>Benefits:</p> <ul> <li> <p>\ud83c\udfaf Full control - Customize every aspect of your agent</p> </li> <li> <p>\ud83d\udcdd Declarative - SuperSpec DSL makes agent design transparent</p> </li> <li> <p>\ud83d\udd27 Optimization-ready - Built-in DSPy optimization support</p> </li> <li> <p>\ud83c\udfd7\ufe0f Production-ready - Enterprise-grade quality and structure</p> </li> </ul>"},{"location":"guides/agent-discovery/#create-agents-using-studio-ui","title":"\ud83c\udfa8 Create Agents Using Studio (UI)","text":"<p>Use the visual SuperOptiX Agent Design Studio for interactive agent creation.</p> <pre><code>super agent design &lt;agent_name&gt;\n</code></pre> <p>Examples: <pre><code># Launch UI for Oracles-tier research agent\nsuper agent design research\n\n# Launch UI for Genies-tier developer agent\nsuper agent design developer\n</code></pre></p> \ud83c\udfa8 Example Output <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfa8 Agent Design Studio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83d\ude80 super Agent Designer                                                                                                                                             \u2502\n\u2502                                                                                                                                                                     \u2502\n\u2502 Agent: RESEARCH                                                                                                                                                     \u2502\n\u2502 Tier: Oracles                                                                                                                                                       \u2502\n\u2502 \u2514\u2500\u2500 UI: http://localhost:8501                                                                                                                                       \u2502\n\u2502                                                                                                                                                                     \u2502\n\u2502 Starting designer... Use Ctrl+C to stop when done.                                                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nWaiting for designer UI to launch...\nDesigner UI is ready!\n\ud83c\udf10 Visit: http://localhost:8501\n\nDesigner is running. Press Ctrl+C here to stop the server.\n</code></pre> <p>Benefits:</p> <ul> <li> <p>\ud83d\uddb1\ufe0f Visual design - Drag-and-drop interface</p> </li> <li> <p>\ud83d\udd04 Real-time preview - See changes instantly</p> </li> <li> <p>\ud83c\udfaf Interactive - Guided agent creation process</p> </li> <li> <p>\ud83d\udcf1 User-friendly - No CLI knowledge required</p> </li> </ul> <p>Note: The UI is still evolving and may not be as fast or robust as the CLI approach.</p>"},{"location":"guides/agent-discovery/#the-playbook-your-agents-foundation","title":"\ud83c\udfaf The Playbook: Your Agent's Foundation","text":"<p>No matter which method you choose, you'll end up with a playbook that needs context engineering.</p>"},{"location":"guides/agent-discovery/#fastest-path-marketplace-template-hand-editing","title":"\ud83d\ude80 Fastest Path: Marketplace Template + Hand Editing","text":"<ol> <li>Grab a template from the marketplace that's close to your needs</li> <li>Edit by hand to add your custom requirements and context</li> <li>Use SuperSpec DSL to configure all the powerful features you need</li> </ol>"},{"location":"guides/agent-discovery/#superspec-configure-everything","title":"\ud83d\udd27 SuperSpec: Configure Everything","text":"<p>Your playbook can include:</p> <ul> <li> <p>\ud83e\udde0 Memory systems - Short-term, long-term, and episodic memory</p> </li> <li> <p>\ud83d\udee0\ufe0f Tool integration - Web search, file operations, code analysis</p> </li> <li> <p>\ud83d\udcda RAG capabilities - Document ingestion and retrieval</p> </li> <li> <p>\ud83c\udfad Persona definition - Agent personality and behavior</p> </li> <li> <p>\ud83d\udccb Task specifications - What your agent should do</p> </li> <li> <p>\ud83d\udd12 Safety constraints - What your agent should NOT do</p> </li> </ul>"},{"location":"guides/agent-discovery/#why-agent-discovery-matters","title":"\ud83c\udf1f Why Agent Discovery Matters","text":"<ul> <li> <p>\ud83d\udd0d Context Engineering - All agent intelligence starts with a well-crafted playbook</p> </li> <li> <p>\ud83e\udde9 Composable - Playbooks can be reused, versioned, and shared</p> </li> <li> <p>\ud83c\udfea Marketplace - Instantly discover and adapt pre-built agents</p> </li> <li> <p>\ud83d\udcdd Declarative - SuperSpec DSL makes agent design transparent and auditable</p> </li> <li> <p>\ud83d\ude80 Customizable - Modify any aspect of your agent's context, tools, or memory</p> </li> </ul>"},{"location":"guides/agent-discovery/#what-this-page-does-not-cover","title":"\ud83d\udeab What This Page Does NOT Cover","text":"<ul> <li>This guide is only about discovering and creating agent playbooks</li> <li>For running, evaluating, compiling, or optimizing agents, see the other guides and tutorials</li> </ul>"},{"location":"guides/agent-discovery/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ul> <li>Explore the marketplace for playbooks</li> <li>Use <code>super spec generate ...</code> to create your own</li> <li>Try the UI studio for visual design</li> <li>Edit and version your playbooks as your needs evolve</li> </ul> <p>Agent discovery is the foundation of every great agentic system-start with a strong playbook, and everything else gets easier! \ud83d\ude80 </p>"},{"location":"guides/bdd/","title":"\ud83c\udfad RSpec-Style BDD in SuperOptiX","text":""},{"location":"guides/bdd/#what-is-bdd","title":"\ud83c\udfaf What is BDD?","text":"<p>Behavior-Driven Development (BDD) is a software development methodology that bridges the gap between technical and non-technical stakeholders by describing software behavior in natural language. BDD focuses on behavior rather than implementation details.</p> <p>RSpec is the most popular BDD testing framework for Ruby and Ruby on Rails, created to make tests more readable and expressive. SuperOptiX follows RSpec's philosophy of clear, behavior-focused specifications for AI agents.</p>"},{"location":"guides/bdd/#core-bdd-principles","title":"Core BDD Principles","text":"<pre><code>graph LR\n    A[Business Requirements] --&gt; B[BDD Scenarios]\n    B --&gt; C[Executable Specifications]\n    C --&gt; D[Test-Driven Development]\n    D --&gt; E[Quality Assurance]\n    E --&gt; F[Continuous Delivery]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/bdd/#original-bdd-structure-gherkin","title":"Original BDD Structure (Gherkin)","text":"<pre><code>Feature: User Authentication\n  As a user\n  I want to log into the system\n  So that I can access my account\n\n  Scenario: Successful login with valid credentials\n    Given I am on the login page\n    When I enter valid username and password\n    And I click the login button\n    Then I should be redirected to the dashboard\n    And I should see my profile information\n</code></pre>"},{"location":"guides/bdd/#rspec-style-bdd-rubyrails","title":"RSpec-Style BDD (Ruby/Rails)","text":"<p>RSpec brings BDD to Ruby with a cleaner, more expressive syntax:</p> <pre><code># spec/models/user_spec.rb\ndescribe User do\n  describe '#authenticate' do\n    it 'logs in with valid credentials' do\n      user = User.create(email: 'test@example.com', password: 'secret')\n      expect(user.authenticate('secret')).to be true\n    end\n\n    it 'rejects invalid passwords' do\n      user = User.create(email: 'test@example.com', password: 'secret')\n      expect(user.authenticate('wrong')).to be false\n    end\n  end\nend\n</code></pre> <p>SuperOptiX adapts this RSpec philosophy for AI agents!</p>"},{"location":"guides/bdd/#bdd-in-software-development","title":"\ud83c\udfd7\ufe0f BDD in Software Development","text":""},{"location":"guides/bdd/#why-bdd-works","title":"Why BDD Works","text":"<p>BDD transforms software development by:</p> <ul> <li>Shared Understanding: Business and technical teams speak the same language</li> <li>\ud83c\udfaf Focus on Behavior: Describes what the system should do, not how</li> <li>\ud83d\udd04 Living Documentation: Scenarios serve as executable specifications</li> <li>\ud83e\uddea Test-Driven: Every behavior is testable and validated</li> <li>\ud83d\udcca Quality Gates: Clear pass/fail criteria for deployment</li> </ul>"},{"location":"guides/bdd/#bdd-workflow-in-original-development","title":"BDD Workflow in Original Development","text":"<pre><code>graph TB\n    A[Business Requirements] --&gt; B[Write BDD Scenarios]\n    B --&gt; C[Implement Features]\n    C --&gt; D[Run BDD Tests]\n    D --&gt; E{All Tests Pass?}\n    E --&gt;|Yes| F[Deploy to Production]\n    E --&gt;|No| G[Fix Implementation]\n    G --&gt; C\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/bdd/#bdd-for-ai-agent-development","title":"\ud83e\udd16 BDD for AI Agent Development","text":""},{"location":"guides/bdd/#the-perfect-match-bdd-ai-agents","title":"The Perfect Match: BDD + AI Agents","text":"<p>BDD is perfectly suited for AI agent development because:</p>"},{"location":"guides/bdd/#1-behavior-first-approach","title":"\ud83c\udfaf 1. Behavior-First Approach","text":"<ul> <li>AI agents are defined by their behavioral capabilities</li> <li>BDD scenarios describe expected agent responses</li> <li>Focus on what the agent should do, not internal implementation</li> </ul>"},{"location":"guides/bdd/#2-iterative-improvement","title":"\ud83d\udd04 2. Iterative Improvement","text":"<ul> <li>BDD scenarios become training data for optimization</li> <li>Test \u2192 Optimize \u2192 Test cycle drives continuous improvement</li> <li>Quality gates ensure reliable agent behavior</li> </ul>"},{"location":"guides/bdd/#3-testable-specifications","title":"\ud83e\uddea 3. Testable Specifications","text":"<ul> <li>Every agent capability can be specified and tested</li> <li>Pass/fail criteria for each behavioral expectation</li> <li>Regression testing prevents quality degradation</li> </ul>"},{"location":"guides/bdd/#bdd-in-superoptix-superspec-feature-specifications","title":"BDD in SuperOptiX: SuperSpec Feature Specifications","text":"<p>SuperOptiX implements BDD through SuperSpec, our domain-specific language for agent specifications. BDD scenarios are defined as feature_specifications within the SuperSpec playbook structure:</p> <pre><code># SuperSpec Feature Specifications (BDD Scenarios)\nfeature_specifications:\n  scenarios:\n    - name: \"robust_api_endpoint_creation\"\n      description: \"Given a REST API requirement, the agent should generate secure, validated, well-documented endpoints\"\n      input:\n        feature_requirement: \"Create a user authentication endpoint with email validation, password hashing, rate limiting, and comprehensive error handling\"\n      expected_output:\n        implementation: |\n          from fastapi import APIRouter, HTTPException, Depends\n          from pydantic import BaseModel, EmailStr\n          from passlib.context import CryptContext\n          from slowapi import Limiter, _rate_limit_exceeded_handler\n\n          pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n          limiter = Limiter(key_func=lambda: \"global\")\n\n          class AuthRequest(BaseModel):\n              email: EmailStr\n              password: str\n\n          @router.post(\"/auth/login\")\n          @limiter.limit(\"5/minute\")\n          async def authenticate_user(request: AuthRequest):\n              # Validate email format (handled by EmailStr)\n              if not request.password or len(request.password) &lt; 8:\n                  raise HTTPException(status_code=400, detail=\"Invalid password format\")\n\n              # Hash password for comparison\n              hashed_password = pwd_context.hash(request.password)\n\n              # Database lookup would go here\n              return {\"status\": \"success\", \"token\": \"jwt_token_here\"}\n</code></pre>"},{"location":"guides/bdd/#bdd-dspy-the-evaluation-first-revolution","title":"\ud83d\udd04 BDD + DSPy: The Evaluation-First Revolution","text":""},{"location":"guides/bdd/#why-bdd-is-perfect-for-dspys-evaluation-first-approach","title":"Why BDD is Perfect for DSPy's Evaluation-First Approach","text":"<p>DSPy's evaluation-first methodology aligns perfectly with BDD principles:</p>"},{"location":"guides/bdd/#1-specification-driven-development","title":"\ud83c\udfaf 1. Specification-Driven Development","text":"<pre><code>graph LR\n    A[BDD Scenarios] --&gt; B[DSPy Gold Examples]\n    B --&gt; C[Optimization Training]\n    C --&gt; D[Improved Prompts]\n    D --&gt; E[Better Agent Behavior]\n    E --&gt; F[Re-evaluation]\n    F --&gt; G{Quality Gates Pass?}\n    G --&gt;|Yes| H[Production Ready]\n    G --&gt;|No| I[Further Optimization]\n    I --&gt; C\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style H fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style I fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/bdd/#2-dual-purpose-scenarios","title":"\ud83d\udd04 2. Dual-Purpose Scenarios","text":"<p>Your BDD scenarios serve two critical functions:</p> <ol> <li>\ud83d\udcda Training Data: Converted to DSPy gold examples for optimization</li> <li>\ud83e\uddea Test Cases: Used for evaluation and quality assurance</li> </ol>"},{"location":"guides/bdd/#3-continuous-feedback-loop","title":"\u26a1 3. Continuous Feedback Loop","text":"<pre><code># The SuperOptiX BDD/DSPy Workflow\nsuper agent compile developer    # Compile with BDD scenarios\nsuper agent evaluate developer   # Establish baseline (BDD tests)\nsuper agent optimize developer   # DSPy optimization using BDD scenarios\nsuper agent evaluate developer   # Re-evaluate (measure improvement)\nsuper agent run developer        # Production execution\n</code></pre>"},{"location":"guides/bdd/#professional-bdd-spec-runner","title":"\ud83c\udfad Professional BDD Spec Runner","text":"<p>SuperOptiX features a revolutionary BDD specification framework with professional-grade tooling that rivals pytest, cucumber, and other industry-standard testing tools.</p>"},{"location":"guides/bdd/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Standard specification execution\nsuper agent evaluate developer\n\n# Detailed analysis with verbose output\nsuper agent evaluate developer --verbose\n\n# Auto-tuning for improved results\nsuper agent evaluate developer --auto-tune\n</code></pre>"},{"location":"guides/bdd/#professional-output-formats","title":"Professional Output Formats","text":"<pre><code># Table format (default) - beautiful console output\nsuper agent evaluate developer --format table\n\n# JSON format - for CI/CD integration\nsuper agent evaluate developer --format json\n\n# Save detailed report to file\nsuper agent evaluate developer --save-report test_results.json\n</code></pre>"},{"location":"guides/bdd/#multi-criteria-evaluation-system","title":"\ud83d\udcca Multi-Criteria Evaluation System","text":""},{"location":"guides/bdd/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Each BDD specification is evaluated using four weighted criteria:</p> Criterion Weight Description Semantic Similarity 50% How closely the output matches expected meaning Keyword Presence 20% Important terms and concepts inclusion Structure Match 20% Format, length, and organization similarity Output Length 10% Basic sanity check for response completeness"},{"location":"guides/bdd/#quality-gates","title":"Quality Gates","text":"<ul> <li>\ud83c\udf89 \u2265 80%: EXCELLENT - Production ready</li> <li>\u26a0\ufe0f 60-79%: GOOD - Minor improvements needed</li> <li>&lt; 60%: NEEDS WORK - Significant improvements required</li> </ul>"},{"location":"guides/bdd/#scoring-system","title":"Scoring System","text":"<pre><code>Confidence Score = (\n    semantic_similarity \u00d7 0.5 +\n    keyword_presence \u00d7 0.2 +\n    structure_match \u00d7 0.2 +\n    output_length \u00d7 0.1\n)\n</code></pre>"},{"location":"guides/bdd/#professional-spec-runner-features","title":"\ud83c\udfaf Professional Spec Runner Features","text":""},{"location":"guides/bdd/#1-session-information-panel","title":"1. Session Information Panel","text":"<p>The spec runner starts with a professional session overview:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Spec Execution Session \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udfaf Agent:               developer                                                 \u2502\n\u2502 \ud83d\udcc5 Session:             2025-01-07 14:30:15                                       \u2502\n\u2502 \ud83d\udd27 Mode:                Standard validation                                       \u2502\n\u2502 \ud83d\udcca Verbosity:           Summary                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"guides/bdd/#2-real-time-progress-tracking","title":"2. Real-Time Progress Tracking","text":"<p>Watch your specifications execute in real-time with spinners and status updates:</p> <pre><code>Pipeline loaded\n\ud83d\udd0d Discovering BDD Specifications...\n\ud83d\udccb Found 5 BDD specifications\n\ud83e\uddea Executing BDD Specification Suite\n  \u26a1 Executing: developer_comprehensive_task...\n  \u26a1 Executing: developer_problem_solving...\n</code></pre>"},{"location":"guides/bdd/#3-beautiful-specification-results-table","title":"3. Beautiful Specification Results Table","text":"<p>Professional tabular output showing all specification results at a glance:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Specification                  \u2503   Status   \u2503  Score   \u2503 Description                              \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer_comprehensive_task   \u2502  PASS   \u2502   0.87   \u2502 Complex software requirements handl...   \u2502\n\u2502 developer_problem_solving      \u2502  FAIL   \u2502   0.45   \u2502 Problem-solving approach demonstra...    \u2502\n\u2502 developer_best_practices       \u2502  PASS   \u2502   0.78   \u2502 Industry standards and guidelines...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/bdd/#4-comprehensive-summary-dashboard","title":"4. Comprehensive Summary Dashboard","text":"<p>Color-coded quality gates with detailed metrics:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udfe1 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                    \u2502\n\u2502  \ud83d\udcca Total Specs:         5                \ud83c\udfaf Pass Rate:         60.0%                                              \u2502\n\u2502  Passed:              3                \ud83e\udd16 Model:             llama3.1:8b                                        \u2502\n\u2502  Failed:              2                \ud83d\udcaa Capability:        0.68                                               \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        \u26a0\ufe0f  GOOD         \ud83d\ude80 Status:            \ud83d\ude80 Optimized                                      \u2502\n\u2502                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"guides/bdd/#5-intelligent-failure-analysis","title":"5. Intelligent Failure Analysis","text":"<p>Detailed breakdown of failing specifications with specific fix suggestions:</p> <pre><code>\ud83d\udd0d Failure Analysis\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Failed Specification           \u2503 Issue                          \u2503 Fix Suggestion                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer_problem_solving      \u2502 semantic meaning differs       \u2502 Improve response relevance         \u2502\n\u2502 api_error_handling             \u2502 missing key terms or concepts  \u2502 Include technical terms             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/bdd/#verbose-mode-deep-analysis","title":"\ud83d\udd0d Verbose Mode - Deep Analysis","text":"<p>Use <code>--verbose</code> flag for detailed test analysis:</p> <pre><code>super agent evaluate developer --verbose\n</code></pre>"},{"location":"guides/bdd/#detailed-test-results","title":"Detailed Test Results","text":"<p>Each failing specification gets a comprehensive analysis panel:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Spec #2: FAILED \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                          \u2502\n\u2502  Specification: developer_problem_solving                                                                 \u2502\n\u2502  Description: When facing software challenges, the agent should demonstrate systematic problem-solving   \u2502\n\u2502  Confidence Score: 0.452                                                                                 \u2502\n\u2502  Semantic Similarity: 0.234                                                                              \u2502\n\u2502  Failure Reason: semantic meaning differs significantly                                                  \u2502\n\u2502                                                                                                          \u2502\n\u2502  \ud83d\udca1 Fix Guidance:                                                                                        \u2502\n\u2502  \u2022 Review and improve the response quality                                                               \u2502\n\u2502  \u2022 Ensure the output addresses all aspects of the input                                                  \u2502\n\u2502  \u2022 Make the response more relevant to the expected output                                                \u2502\n\u2502  \u2022 Use similar terminology and concepts                                                                  \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"guides/bdd/#advanced-techniques","title":"\ud83d\udee0\ufe0f Advanced Techniques","text":""},{"location":"guides/bdd/#auto-tuning","title":"Auto-Tuning","text":"<p>Automatically adjust evaluation criteria based on model performance:</p> <pre><code>super agent evaluate developer --auto-tune\n</code></pre>"},{"location":"guides/bdd/#ignore-checks-development-mode","title":"Ignore Checks (Development Mode)","text":"<p>Skip validation for rapid development:</p> <pre><code>super agent evaluate developer --ignore-checks\n</code></pre>"},{"location":"guides/bdd/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\n- name: Evaluate Agent\n  run: |\n    super agent evaluate developer --format json --save-report results.json\n\n- name: Check Quality Gate\n  run: |\n    python -c \"\n    import json\n    results = json.load(open('results.json'))\n    if results['quality_gate'] != 'EXCELLENT':\n        exit(1)\n    \"\n</code></pre>"},{"location":"guides/bdd/#bdd-in-superoptix-real-world-example","title":"\ud83c\udfad BDD in SuperOptiX: Real-World Example","text":""},{"location":"guides/bdd/#step-1-define-bdd-scenarios-in-superspec-playbook","title":"Step 1: Define BDD Scenarios in SuperSpec Playbook","text":"<pre><code># agents/developer/playbook/developer_playbook.yaml (SuperSpec DSL)\nfeature_specifications:\n  scenarios:\n    - name: \"developer_comprehensive_task\"\n      description: \"Given a complex software requirement, the agent should provide detailed analysis and recommendations\"\n      input:\n        feature_requirement: \"Design a microservices architecture for an e-commerce platform with user authentication, product catalog, order management, and payment processing\"\n      expected_output:\n        implementation: |\n          **Microservices Architecture Design**\n\n          **1. Service Decomposition:**\n          - User Service: Authentication, profiles, preferences\n          - Product Service: Catalog, inventory, search\n          - Order Service: Order lifecycle, status tracking\n          - Payment Service: Payment processing, refunds\n          - Notification Service: Email, SMS, push notifications\n\n          **2. Technology Stack:**\n          - API Gateway: Kong or AWS API Gateway\n          - Service Mesh: Istio for inter-service communication\n          - Database: PostgreSQL for each service (database per service pattern)\n          - Message Queue: RabbitMQ or Apache Kafka\n          - Monitoring: Prometheus + Grafana\n\n          **3. Security Considerations:**\n          - JWT tokens for authentication\n          - API rate limiting\n          - Data encryption in transit and at rest\n          - Service-to-service authentication\n</code></pre>"},{"location":"guides/bdd/#step-2-compile-superspec-and-evaluate","title":"Step 2: Compile SuperSpec and Evaluate","text":"<pre><code># Compile SuperSpec playbook with BDD scenarios\nsuper agent compile developer\n\n# Run BDD evaluation (establishes baseline)\nsuper agent evaluate developer\n</code></pre> <p>Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Spec Execution Session \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udfaf Agent:               developer                                                                        \u2502\n\u2502 \ud83d\udcc5 Session:             2025-01-07 14:30:15                                                              \u2502\n\u2502 \ud83d\udd27 Mode:                Standard validation                                                              \u2502\n\u2502 \ud83d\udcca Verbosity:           Summary                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83e\uddea Executing BDD Specification Suite\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nProgress: \ud83e\uddea Running 5 BDD specifications...\n\u280b \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0/5\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Specification                              \u2503   Status   \u2503  Score   \u2503 Description                              \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer_comprehensive_task               \u2502  PASS   \u2502   0.87   \u2502 Complex software requirements handl...   \u2502\n\u2502 developer_problem_solving                  \u2502  FAIL   \u2502   0.45   \u2502 Problem-solving approach demonstra...    \u2502\n\u2502 developer_best_practices                   \u2502  PASS   \u2502   0.78   \u2502 Industry standards and guidelines...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udfe1 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                    \u2502\n\u2502  \ud83d\udcca Total Specs:         5                \ud83c\udfaf Pass Rate:         60.0%                                              \u2502\n\u2502  Passed:              3                \ud83e\udd16 Model:             llama3.1:8b                                        \u2502\n\u2502  Failed:              2                \ud83d\udcaa Capability:        0.68                                               \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        \u26a0\ufe0f  GOOD         \ud83d\ude80 Status:            \ud83d\ude80 Optimized                                      \u2502\n\u2502                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"guides/bdd/#step-3-optimize-using-superspec-bdd-scenarios","title":"Step 3: Optimize Using SuperSpec BDD Scenarios","text":"<pre><code># DSPy optimization using SuperSpec BDD scenarios as training data\nsuper agent optimize developer\n</code></pre> <p>What happens during optimization: 1. SuperSpec BDD scenarios are converted to DSPy gold examples 2. DSPy BootstrapFewShot uses scenarios to improve prompts 3. Optimized pipeline is saved for future use</p>"},{"location":"guides/bdd/#step-4-re-evaluate-superspec-and-measure-improvement","title":"Step 4: Re-evaluate SuperSpec and Measure Improvement","text":"<pre><code># Re-run BDD tests to measure improvement\nsuper agent evaluate developer\n</code></pre> <p>Expected improvement: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udfe2 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                    \u2502\n\u2502  \ud83d\udcca Total Specs:         5                \ud83c\udfaf Pass Rate:         80.0%                                              \u2502\n\u2502  Passed:              4                \ud83e\udd16 Model:             llama3.1:8b                                        \u2502\n\u2502  Failed:              1                \ud83d\udcaa Capability:        0.82                                               \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        \ud83c\udf89 EXCELLENT    \ud83d\ude80 Status:            \ud83d\ude80 Optimized                                      \u2502\n\u2502                                                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"guides/bdd/#bdd-evaluation-metrics-in-superoptix","title":"\ud83d\udcca BDD Evaluation Metrics in SuperOptiX","text":""},{"location":"guides/bdd/#multi-criteria-evaluation-system_1","title":"Multi-Criteria Evaluation System","text":"<p>SuperOptiX uses 4 weighted criteria for SuperSpec BDD evaluation:</p> Criterion Weight Description Semantic Similarity 50% How closely the output matches expected meaning Keyword Presence 20% Important terms and concepts inclusion Structure Match 20% Format, length, and organization similarity Output Length 10% Basic sanity check for completeness"},{"location":"guides/bdd/#quality-gates_1","title":"Quality Gates","text":"<ul> <li>\ud83c\udf89 \u2265 80%: EXCELLENT - Production ready</li> <li>\u26a0\ufe0f 60-79%: GOOD - Minor improvements needed</li> <li>&lt; 60%: NEEDS WORK - Significant improvements required</li> </ul>"},{"location":"guides/bdd/#detailed-scoring","title":"Detailed Scoring","text":"<pre><code>{\n  \"scenario_name\": \"robust_error_handling\",\n  \"description\": \"When implementing functionality that can fail...\",\n  \"passed\": true,\n  \"confidence_score\": 0.82,\n  \"semantic_similarity\": 0.85,\n  \"criteria_breakdown\": {\n    \"semantic_similarity\": 0.85,\n    \"output_length\": 1.0,\n    \"keyword_presence\": 0.75,\n    \"structure_match\": 0.80\n  },\n  \"failure_reason\": null,\n  \"expected\": {...},\n  \"actual\": {...},\n  \"threshold_used\": 0.6\n}\n</code></pre>"},{"location":"guides/bdd/#bdd-best-practices-for-ai-agents","title":"\ud83c\udfaf BDD Best Practices for AI Agents","text":""},{"location":"guides/bdd/#dos","title":"DO's","text":""},{"location":"guides/bdd/#1-write-specific-testable-scenarios","title":"1. Write Specific, Testable Scenarios","text":"<pre><code># Good: Specific and testable\n- name: \"secure_password_validation\"\n  description: \"When validating user passwords, the agent should enforce security requirements\"\n  input:\n    feature_requirement: \"Implement password validation with minimum 8 characters, uppercase, lowercase, number, and special character\"\n  expected_output:\n    implementation: |\n      def validate_password(password):\n          if len(password) &lt; 8:\n              return False, \"Password must be at least 8 characters\"\n          if not re.search(r'[A-Z]', password):\n              return False, \"Password must contain uppercase letter\"\n          # ... additional validation\n          return True, \"Password is valid\"\n</code></pre>"},{"location":"guides/bdd/#2-cover-multiple-behavioral-aspects","title":"2. Cover Multiple Behavioral Aspects","text":"<pre><code># Comprehensive scenario coverage\n- name: \"happy_path_scenario\"      # Normal operation\n- name: \"error_handling_scenario\"  # Error conditions\n- name: \"edge_case_scenario\"       # Boundary conditions\n- name: \"security_scenario\"        # Security requirements\n- name: \"performance_scenario\"     # Performance expectations\n</code></pre>"},{"location":"guides/bdd/#3-use-realistic-representative-data","title":"3. Use Realistic, Representative Data","text":"<pre><code># Realistic input data\ninput:\n  feature_requirement: \"Create a REST API for user registration with email validation, password hashing, and rate limiting\"\n</code></pre>"},{"location":"guides/bdd/#donts","title":"DON'Ts","text":""},{"location":"guides/bdd/#1-dont-write-vague-scenarios","title":"1. Don't Write Vague Scenarios","text":"<pre><code># Bad: Too vague\n- name: \"create_function\"\n  description: \"Make a function\"\n  input:\n    feature_requirement: \"Function that does something\"\n  expected_output:\n    implementation: \"def func(): pass\"\n</code></pre>"},{"location":"guides/bdd/#2-dont-ignore-error-cases","title":"2. Don't Ignore Error Cases","text":"<pre><code># Missing error handling scenarios\n# Always include scenarios for:\n# - Invalid input handling\n# - Error response formats\n# - Edge case behavior\n</code></pre>"},{"location":"guides/bdd/#3-dont-over-complicate-scenarios","title":"3. Don't Over-Complicate Scenarios","text":"<pre><code># Keep scenarios focused on single responsibilities\n# One scenario = one specific behavior\n# Multiple scenarios = comprehensive coverage\n</code></pre>"},{"location":"guides/bdd/#bdd-development-workflow","title":"\ud83d\udd04 BDD Development Workflow","text":""},{"location":"guides/bdd/#the-complete-superspec-bddtdd-cycle","title":"The Complete SuperSpec BDD/TDD Cycle","text":"<pre><code>graph TB\n    A[Define SuperSpec BDD Scenarios] --&gt; B[Compile SuperSpec]\n    B --&gt; C[Run Baseline Evaluation]\n    C --&gt; D[Analyze Results]\n    D --&gt; E{Quality Gates Pass?}\n    E --&gt;|Yes| F[Deploy to Production]\n    E --&gt;|No| G[Optimize Agent]\n    G --&gt; H[Re-evaluate]\n    H --&gt; D\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style H fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/bdd/#command-sequence","title":"Command Sequence","text":"<pre><code># Define SuperSpec BDD scenarios in playbook\nvim agents/developer/playbook/developer_playbook.yaml\n\n# Compile SuperSpec with BDD scenarios\nsuper agent compile developer\n\n# Establish baseline performance\nsuper agent evaluate developer\n\n# Optimize using SuperSpec scenarios as training data\nsuper agent optimize developer\n\n# Measure improvement\nsuper agent evaluate developer\n\n# Deploy if quality gates pass\nsuper agent run developer --goal \"Your production task\"\n</code></pre>"},{"location":"guides/bdd/#advanced-bdd-features","title":"\ud83d\ude80 Advanced BDD Features","text":""},{"location":"guides/bdd/#verbose-mode-for-deep-analysis","title":"Verbose Mode for Deep Analysis","text":"<pre><code># Detailed analysis of each SuperSpec scenario\nsuper agent evaluate developer --verbose\n</code></pre> <p>Output includes: - Detailed failure analysis for SuperSpec scenarios - Specific fix recommendations - Confidence score breakdown - Expected vs actual output comparison</p>"},{"location":"guides/bdd/#custom-validation-criteria","title":"Custom Validation Criteria","text":"<pre><code># Enhanced scenarios with validation hints\n- name: \"security_focused_implementation\"\n  description: \"Agent should generate secure code with proper input validation\"\n  input:\n    feature_requirement: \"Create a password reset endpoint with security best practices\"\n  expected_output:\n    implementation: |\n      # Expected secure implementation here\n  validation_criteria:  # Optional hints\n    - \"Uses secure random token generation\"\n    - \"Includes rate limiting\"\n    - \"Validates email format\"\n    - \"Handles edge cases gracefully\"\n</code></pre>"},{"location":"guides/bdd/#scenario-categories","title":"Scenario Categories","text":"<pre><code>feature_specifications:\n  scenarios:\n    # Basic functionality\n    - name: \"happy_path_scenario\"\n      category: \"functionality\"\n      # ...\n\n    # Error handling  \n    - name: \"error_handling_scenario\"\n      category: \"error_handling\"\n      # ...\n\n    # Performance\n    - name: \"efficiency_scenario\"\n      category: \"performance\"\n      # ...\n\n    # Security\n    - name: \"security_scenario\"\n      category: \"security\"\n      # ...\n</code></pre>"},{"location":"guides/bdd/#bdd-vs-traditional-testing","title":"\ud83c\udfaf BDD vs Traditional Testing","text":""},{"location":"guides/bdd/#traditional-unit-testing","title":"Traditional Unit Testing","text":"<pre><code>def test_password_validation():\n    assert validate_password(\"weak\") == False\n    assert validate_password(\"Strong123!\") == True\n</code></pre>"},{"location":"guides/bdd/#superspec-bdd-in-superoptix","title":"SuperSpec BDD in SuperOptiX","text":"<pre><code>- name: \"password_validation_behavior\"\n  description: \"When validating passwords, the agent should enforce security requirements\"\n  input:\n    feature_requirement: \"Implement password validation with security requirements\"\n  expected_output:\n    implementation: |\n      def validate_password(password):\n          # Comprehensive validation logic\n          # Security-focused implementation\n          # Clear error messages\n</code></pre>"},{"location":"guides/bdd/#key-differences","title":"Key Differences","text":"Aspect Traditional Testing BDD in SuperOptiX Focus Implementation details Behavioral expectations Language Technical code Natural language + examples Stakeholders Developers only Business + Technical Training Data No Yes (SuperSpec \u2192 DSPy optimization) Quality Gates Pass/Fail Multi-criteria scoring"},{"location":"guides/bdd/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>SuperSpec BDD in SuperOptiX represents a revolutionary approach to AI agent development that combines:</p> <ul> <li>\ud83c\udfaf Behavior-driven specifications that focus on what agents should do</li> <li>\ud83d\udd04 SuperSpec + DSPy integration that uses scenarios for both training and testing</li> <li>\ud83e\uddea Evaluation-first development that ensures quality before deployment</li> <li>\ud83d\udcca Multi-criteria quality gates that provide comprehensive validation</li> <li>\ud83d\ude80 Continuous improvement through iterative optimization cycles</li> </ul>"},{"location":"guides/bdd/#the-superoptix-bdd-advantage","title":"The SuperOptiX BDD Advantage","text":"<ol> <li>\ud83c\udfad Professional Spec Runner: Beautiful UI with detailed analysis</li> <li>\ud83e\udd16 AI-Powered Optimization: BDD scenarios become DSPy training data</li> <li>\ud83d\udcca Quality Assurance: Multi-criteria evaluation with clear metrics</li> <li>\ud83d\udd04 Iterative Development: Continuous improvement through feedback loops</li> <li>\ud83d\ude80 Production Readiness: Quality gates ensure reliable deployment</li> </ol> <p>Start using SuperSpec BDD in SuperOptiX today and experience the difference of scientifically validated, behavior-driven AI agents!</p>"},{"location":"guides/bdd/#superoptix-workflow-integration","title":"\ud83c\udfaf SuperOptiX Workflow Integration","text":""},{"location":"guides/bdd/#the-complete-workflow","title":"The Complete Workflow","text":"<pre><code>graph TD\n    A[Define Agent Playbook] --&gt; B[Compile Agent]\n    B --&gt; C[Evaluate Agent]\n    C --&gt; D{Pass Quality Gate?}\n    D --&gt;|Yes| E[Run Agent]\n    D --&gt;|No| F[Optimize Agent]\n    F --&gt; B\n    E --&gt; G[Add to Orchestra]\n    G --&gt; H[Run Orchestra]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style E fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style F fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style G fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style H fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/bdd/#1-define-agent-playbook","title":"1. Define Agent Playbook","text":"<p>Write declarative specifications using SuperSpec DSL:</p> <pre><code>apiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: customer-service\n  tier: genie\nspec:\n  context:\n    memory: true\n    tools: true\n  tasks:\n    - name: \"handle_inquiry\"\n      description: \"Handle customer inquiries\"\n</code></pre>"},{"location":"guides/bdd/#2-compile-agent","title":"2. Compile Agent","text":"<p>Translate playbooks into executable pipelines:</p> <pre><code>super agent compile customer-service\n</code></pre>"},{"location":"guides/bdd/#3-evaluate-agent","title":"3. Evaluate Agent","text":"<p>Run BDD specifications against the compiled agent:</p> <pre><code>super agent evaluate customer-service\n</code></pre>"},{"location":"guides/bdd/#4-optimize-agent","title":"4. Optimize Agent","text":"<p>If evaluation fails, optimize based on feedback:</p> <pre><code>super agent optimize customer-service\n</code></pre>"},{"location":"guides/bdd/#5-run-agent","title":"5. Run Agent","text":"<p>Once evaluation passes, run the agent:</p> <pre><code>super agent run customer-service --input \"Help me with my order\"\n</code></pre> <p>\ud83d\udca1 Pro Tip: Start with 3-5 well-crafted SuperSpec BDD scenarios for your agents. Quality over quantity leads to better optimization and more reliable evaluation results. Remember: your SuperSpec BDD scenarios serve dual purposes - they're both your test cases AND your training data! </p>"},{"location":"guides/cicd-integration/","title":"\ud83d\udd04 CI/CD Integration Guide","text":"<p>SuperOptiX provides comprehensive CI/CD integration capabilities that enable automated testing, optimization, and deployment of AI agents. This guide covers how to integrate SuperOptiX into your CI/CD pipeline for professional-grade agent development.</p>"},{"location":"guides/cicd-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>CI/CD integration in SuperOptiX follows BDD/TDD best practices to ensure reliable, production-ready AI agents. The workflow emphasizes:</p> <ul> <li>Baseline Measurement: Establish performance baselines before optimization</li> <li>Quality Gates: Automated pass/fail thresholds for deployment</li> <li>Iterative Improvement: Measure optimization effectiveness quantitatively</li> <li>Production Validation: Final validation before deployment</li> </ul>"},{"location":"guides/cicd-integration/#cli-command-options-for-cicd","title":"\ud83d\ude80 CLI Command Options for CI/CD","text":"<p>The <code>super agent evaluate</code> command provides comprehensive testing capabilities with multiple output formats for different CI/CD use cases:</p>"},{"location":"guides/cicd-integration/#basic-usage","title":"Basic Usage","text":"<pre><code># Standard evaluation\nsuper agent evaluate &lt;agent_name&gt;\n\n# Verbose output with detailed results\nsuper agent evaluate &lt;agent_name&gt; --verbose\n\n# Auto-tuning for improved evaluation accuracy\nsuper agent evaluate &lt;agent_name&gt; --auto-tune\n</code></pre>"},{"location":"guides/cicd-integration/#output-formats-for-automation","title":"Output Formats for Automation","text":"<pre><code># Table format (default) - Beautiful console output\nsuper agent evaluate &lt;agent_name&gt; --format table\n\n# JSON format - For CI/CD integration and automation\nsuper agent evaluate &lt;agent_name&gt; --format json\n\n# JUnit format - Compatible with CI/CD systems\nsuper agent evaluate &lt;agent_name&gt; --format junit\n</code></pre>"},{"location":"guides/cicd-integration/#report-generation","title":"Report Generation","text":"<pre><code># Save detailed report to file\nsuper agent evaluate &lt;agent_name&gt; --save-report test_results.json\n\n# Combine with JSON format for automation\nsuper agent evaluate &lt;agent_name&gt; --format json --save-report results.json\n</code></pre>"},{"location":"guides/cicd-integration/#development-options","title":"Development Options","text":"<pre><code># Ignore non-essential checks for rapid development\nsuper agent evaluate &lt;agent_name&gt; --ignore-checks\n\n# Verbose mode for detailed analysis\nsuper agent evaluate &lt;agent_name&gt; --verbose\n</code></pre>"},{"location":"guides/cicd-integration/#proper-bddtdd-workflow","title":"\ud83d\udd04 Proper BDD/TDD Workflow","text":"<p>The correct CI/CD workflow follows BDD/TDD best practices:</p>"},{"location":"guides/cicd-integration/#phase-1-specification-driven-development","title":"Phase 1: Specification-Driven Development","text":"<pre><code># Define BDD scenarios FIRST\nvim agents/&lt;agent_name&gt;/Playbook/developer_playbook.yaml\n# Add comprehensive feature_specifications\n\n# Compile agent with scenarios\nsuper agent compile developer\n\n# Run baseline evaluation (should show current performance)\nsuper agent evaluate developer\n# This gives us baseline metrics before optimization\n</code></pre>"},{"location":"guides/cicd-integration/#phase-2-iterative-improvement","title":"Phase 2: Iterative Improvement","text":"<pre><code># Analyze baseline results\n# - Identify failing scenarios\n# - Understand performance gaps\n# - Plan optimization strategy\n\n# Optimize based on evaluation feedback\nsuper agent optimize developer\n\n# Re-evaluate to measure improvement\nsuper agent evaluate developer\n\n# Iterate until quality gates pass\n# Repeat steps 5-6 until pass rate \u2265 80%\n</code></pre>"},{"location":"guides/cicd-integration/#phase-3-production-deployment","title":"Phase 3: Production Deployment","text":"<pre><code># Final validation\nsuper agent evaluate developer --verbose\n\n# Deploy only if quality gates pass\nsuper agent run developer --goal \"production task\"\n</code></pre>"},{"location":"guides/cicd-integration/#cicd-platform-integration","title":"\ud83c\udfd7\ufe0f CI/CD Platform Integration","text":"<p>\ud83d\udca1 Note: The following CI/CD configurations are example setups designed to demonstrate integration patterns. Users should adapt these examples to their specific use cases, environment requirements, and organizational needs. Modify agent names, quality gate thresholds, and deployment strategies according to your project requirements.</p>"},{"location":"guides/cicd-integration/#github-actions-integration","title":"GitHub Actions Integration","text":"<pre><code># .github/workflows/agent-quality.yml\nname: Agent Quality Check\non: [push, pull_request]\n\njobs:\n  bdd-workflow:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: '3.11'\n\n      - name: Setup SuperOptiX\n        run: pip install superoptix\n\n      - name: Compile Agent\n        run: super agent compile developer\n\n      - name: Baseline Evaluation\n        run: |\n          super agent evaluate developer --format json --save-report baseline.json\n          # Check if baseline is acceptable\n          pass_rate=$(jq -r '.summary.pass_rate' baseline.json | tr -d '%')\n          if (( $(echo \"$pass_rate &lt; 40\" | bc -l) )); then\n            echo \"Baseline too low: $pass_rate%\"\n            exit 1\n          fi\n\n      - name: Optimize Agent\n        run: super agent optimize developer\n\n      - name: Final Evaluation\n        run: |\n          super agent evaluate developer --format json --save-report results.json\n          # Quality gate: must improve by at least 20%\n          improvement=$(jq -r '.improvement.percentage' results.json | tr -d '%')\n          if (( $(echo \"$improvement &lt; 20\" | bc -l) )); then\n            echo \"Insufficient improvement: $improvement%\"\n            exit 1\n          fi\n\n      - name: Upload Test Results\n        uses: actions/upload-artifact@v3\n        with:\n          name: bdd-spec-results\n          path: results.json\n</code></pre>"},{"location":"guides/cicd-integration/#gitlab-ci-integration","title":"GitLab CI Integration","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - test\n  - optimize\n  - validate\n\nagent-test:\n  stage: test\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent compile developer\n    - super agent evaluate developer --format json --save-report baseline.json\n    - |\n      # Extract pass rate for quality gate\n      PASS_RATE=$(python -c \"\n      import json\n      with open('baseline.json') as f:\n          data = json.load(f)\n      print(data['summary']['pass_rate'].replace('%', ''))\n      \")\n      echo \"Pass rate: $PASS_RATE%\"\n      if [ \"$PASS_RATE\" -lt 40 ]; then\n          echo \"Baseline too low: $PASS_RATE%\"\n          exit 1\n      fi\n  artifacts:\n    reports:\n      junit: test-results.xml\n    paths:\n      - baseline.json\n\nagent-optimize:\n  stage: optimize\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent optimize developer\n  dependencies:\n    - agent-test\n\nagent-validate:\n  stage: validate\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent evaluate developer --format json --save-report final.json\n    - |\n      # Quality gate check\n      IMPROVEMENT=$(python -c \"\n      import json\n      with open('final.json') as f:\n          data = json.load(f)\n      print(data.get('improvement', {}).get('percentage', '0').replace('%', ''))\n      \")\n      echo \"Improvement: $IMPROVEMENT%\"\n      if [ \"$IMPROVEMENT\" -lt 20 ]; then\n          echo \"Insufficient improvement: $IMPROVEMENT%\"\n          exit 1\n      fi\n  dependencies:\n    - agent-optimize\n</code></pre>"},{"location":"guides/cicd-integration/#jenkins-pipeline-integration","title":"Jenkins Pipeline Integration","text":"<pre><code>// Jenkinsfile\npipeline {\n    agent any\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh 'pip install superoptix'\n            }\n        }\n\n        stage('Compile') {\n            steps {\n                sh 'super agent compile developer'\n            }\n        }\n\n        stage('Baseline Test') {\n            steps {\n                sh 'super agent evaluate developer --format json --save-report baseline.json'\n                script {\n                    def baseline = readJSON file: 'baseline.json'\n                    def passRate = baseline.summary.pass_rate.replace('%', '').toInteger()\n                    if (passRate &lt; 40) {\n                        error \"Baseline too low: ${passRate}%\"\n                    }\n                }\n            }\n        }\n\n        stage('Optimize') {\n            steps {\n                sh 'super agent optimize developer'\n            }\n        }\n\n        stage('Final Test') {\n            steps {\n                sh 'super agent evaluate developer --format json --save-report final.json'\n                script {\n                    def final = readJSON file: 'final.json'\n                    def improvement = final.improvement?.percentage?.replace('%', '')?.toInteger() ?: 0\n                    if (improvement &lt; 20) {\n                        error \"Insufficient improvement: ${improvement}%\"\n                    }\n                }\n            }\n        }\n    }\n\n    post {\n        always {\n            archiveArtifacts artifacts: '*.json', fingerprint: true\n            publishTestResults testResultsPattern: 'test-results.xml'\n        }\n    }\n}\n</code></pre>"},{"location":"guides/cicd-integration/#circleci-integration","title":"CircleCI Integration","text":"<pre><code># .circleci/config.yml\nversion: 2.1\n\njobs:\n  test-agent:\n    docker:\n      - image: cimg/python:3.11\n    steps:\n      - checkout\n      - run:\n          name: Install SuperOptiX\n          command: pip install superoptix\n      - run:\n          name: Compile Agent\n          command: super agent compile developer\n      - run:\n          name: Baseline Evaluation\n          command: |\n            super agent evaluate developer --format json --save-report baseline.json\n            # Check baseline quality gate\n            PASS_RATE=$(python -c \"\n            import json\n            with open('baseline.json') as f:\n                data = json.load(f)\n            print(data['summary']['pass_rate'].replace('%', ''))\n            \")\n            if [ \"$PASS_RATE\" -lt 40 ]; then\n                echo \"Baseline too low: $PASS_RATE%\"\n                exit 1\n            fi\n      - run:\n          name: Optimize Agent\n          command: super agent optimize developer\n      - run:\n          name: Final Evaluation\n          command: |\n            super agent evaluate developer --format json --save-report final.json\n            # Check improvement quality gate\n            IMPROVEMENT=$(python -c \"\n            import json\n            with open('final.json') as f:\n                data = json.load(f)\n            print(data.get('improvement', {}).get('percentage', '0').replace('%', ''))\n            \")\n            if [ \"$IMPROVEMENT\" -lt 20 ]; then\n                echo \"Insufficient improvement: $IMPROVEMENT%\"\n                exit 1\n            fi\n      - store_artifacts:\n          path: *.json\n          destination: test-results\n\nworkflows:\n  version: 2\n  test:\n    jobs:\n      - test-agent\n</code></pre>"},{"location":"guides/cicd-integration/#quality-gates-implementation","title":"\ud83c\udfaf Quality Gates Implementation","text":""},{"location":"guides/cicd-integration/#automated-quality-gates","title":"Automated Quality Gates","text":"<p>For custom CI/CD scripts, you can implement quality gates:</p> <pre><code>#!/bin/bash\n# quality-gate.sh\n\n# Set minimum thresholds\nMIN_PASS_RATE=80\nMIN_IMPROVEMENT=20\n\n# Run evaluation and extract metrics\nsuper agent evaluate developer --format json --save-report results.json\n\n# Extract pass rate\nPASS_RATE=$(jq -r '.summary.pass_rate' results.json | tr -d '%')\n\n# Extract improvement percentage\nIMPROVEMENT=$(jq -r '.improvement.percentage' results.json | tr -d '%')\n\necho \"Pass Rate: $PASS_RATE%\"\necho \"Improvement: $IMPROVEMENT%\"\n\n# Quality gate checks\nif [ \"$PASS_RATE\" -lt \"$MIN_PASS_RATE\" ]; then\n    echo \"Quality gate failed: $PASS_RATE% &lt; $MIN_PASS_RATE%\"\n    exit 1\nfi\n\nif [ \"$IMPROVEMENT\" -lt \"$MIN_IMPROVEMENT\" ]; then\n    echo \"Improvement gate failed: $IMPROVEMENT% &lt; $MIN_IMPROVEMENT%\"\n    exit 1\nfi\n\necho \"All quality gates passed!\"\n</code></pre>"},{"location":"guides/cicd-integration/#quality-gate-thresholds","title":"Quality Gate Thresholds","text":"Metric Threshold Action Baseline Pass Rate \u2265 40% Continue to optimization Final Pass Rate \u2265 80% Deploy to production Improvement \u2265 20% Accept optimization Response Time &lt; 30s Performance check"},{"location":"guides/cicd-integration/#advanced-cicd-features","title":"\ud83d\udd27 Advanced CI/CD Features","text":""},{"location":"guides/cicd-integration/#multi-agent-testing","title":"Multi-Agent Testing","text":"<p>Test multiple agents in parallel:</p> <pre><code># .github/workflows/multi-agent-test.yml\nname: Multi-Agent Testing\non: [push, pull_request]\n\njobs:\n  test-agents:\n    strategy:\n      matrix:\n        agent: [developer, customer-service, data-analyst]\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: '3.11'\n\n      - name: Setup SuperOptiX\n        run: pip install superoptix\n\n      - name: Test Agent\n        run: |\n          super agent compile ${{ matrix.agent }}\n          super agent evaluate ${{ matrix.agent }} --format json --save-report ${{ matrix.agent }}_results.json\n\n      - name: Upload Results\n        uses: actions/upload-artifact@v3\n        with:\n          name: ${{ matrix.agent }}-results\n          path: ${{ matrix.agent }}_results.json\n</code></pre>"},{"location":"guides/cicd-integration/#conditional-optimization","title":"Conditional Optimization","text":"<p>Only optimize when needed:</p> <pre><code># .github/workflows/conditional-optimization.yml\nname: Conditional Optimization\non: [push, pull_request]\n\njobs:\n  evaluate-and-optimize:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup SuperOptiX\n        run: pip install superoptix\n\n      - name: Baseline Evaluation\n        run: |\n          super agent evaluate developer --format json --save-report baseline.json\n          PASS_RATE=$(jq -r '.summary.pass_rate' baseline.json | tr -d '%')\n          echo \"PASS_RATE=$PASS_RATE\" &gt;&gt; $GITHUB_ENV\n\n      - name: Optimize if Needed\n        if: env.PASS_RATE &lt; 80\n        run: |\n          super agent optimize developer\n          super agent evaluate developer --format json --save-report final.json\n\n      - name: Skip Optimization\n        if: env.PASS_RATE &gt;= 80\n        run: |\n          echo \"Pass rate already at $PASS_RATE%, skipping optimization\"\n          cp baseline.json final.json\n</code></pre>"},{"location":"guides/cicd-integration/#automated-deployment","title":"Automated Deployment","text":"<p>Deploy only after quality gates pass:</p> <pre><code># .github/workflows/deploy.yml\nname: Deploy Agent\non:\n  workflow_run:\n    workflows: [\"Agent Quality Check\"]\n    types: [completed]\n\njobs:\n  deploy:\n    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup SuperOptiX\n        run: pip install superoptix\n\n      - name: Download Test Results\n        uses: actions/download-artifact@v3\n        with:\n          name: bdd-spec-results\n          path: ./\n\n      - name: Verify Quality Gates\n        run: |\n          PASS_RATE=$(jq -r '.summary.pass_rate' results.json | tr -d '%')\n          if [ \"$PASS_RATE\" -lt 80 ]; then\n            echo \"Quality gate failed: $PASS_RATE% &lt; 80%\"\n            exit 1\n          fi\n\n      - name: Deploy to Production\n        run: |\n          # Your deployment commands here\n          echo \"Deploying agent to production...\"\n</code></pre>"},{"location":"guides/cicd-integration/#monitoring-and-reporting","title":"\ud83d\udcca Monitoring and Reporting","text":""},{"location":"guides/cicd-integration/#test-results-dashboard","title":"Test Results Dashboard","text":"<p>Create a dashboard for test results:</p> <pre><code># .github/workflows/dashboard.yml\nname: Test Results Dashboard\non:\n  workflow_run:\n    workflows: [\"Agent Quality Check\"]\n    types: [completed]\n\njobs:\n  update-dashboard:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Download Results\n        uses: actions/download-artifact@v3\n        with:\n          name: bdd-spec-results\n          path: ./\n\n      - name: Generate Dashboard\n        run: |\n          # Generate HTML dashboard from JSON results\n          python -c \"\n          import json\n          with open('results.json') as f:\n              data = json.load(f)\n          # Generate HTML dashboard\n          \"\n\n      - name: Deploy Dashboard\n        run: |\n          # Deploy dashboard to GitHub Pages or other hosting\n          echo \"Dashboard updated\"\n</code></pre>"},{"location":"guides/cicd-integration/#slack-notifications","title":"Slack Notifications","text":"<p>Send notifications to Slack:</p> <pre><code># .github/workflows/notifications.yml\nname: Notifications\non:\n  workflow_run:\n    workflows: [\"Agent Quality Check\"]\n    types: [completed]\n\njobs:\n  notify:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Download Results\n        uses: actions/download-artifact@v3\n        with:\n          name: bdd-spec-results\n          path: ./\n\n      - name: Send Slack Notification\n        uses: 8398a7/action-slack@v3\n        with:\n          status: ${{ github.event.workflow_run.conclusion }}\n          text: |\n            Agent Quality Check: ${{ github.event.workflow_run.conclusion }}\n            Pass Rate: $(jq -r '.summary.pass_rate' results.json)\n            Improvement: $(jq -r '.improvement.percentage' results.json)\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n</code></pre>"},{"location":"guides/cicd-integration/#best-practices","title":"\ud83d\ude80 Best Practices","text":""},{"location":"guides/cicd-integration/#dos","title":"DO's","text":"<ol> <li>Always evaluate before optimizing</li> <li>Set clear quality gates</li> <li>Measure improvement quantitatively</li> <li>Iterate based on evaluation feedback</li> <li>Use scenarios as both training data and test cases</li> <li>Automate everything</li> <li>Monitor trends over time</li> </ol>"},{"location":"guides/cicd-integration/#donts","title":"DON'Ts","text":"<ol> <li>Don't optimize without baseline</li> <li>Don't skip evaluation after optimization</li> <li>Don't deploy without quality gates</li> <li>Don't ignore failing scenarios</li> <li>Don't deploy on failing tests</li> </ol>"},{"location":"guides/cicd-integration/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"guides/cicd-integration/#common-cicd-issues","title":"Common CI/CD Issues","text":""},{"location":"guides/cicd-integration/#quality-gate-failures","title":"Quality Gate Failures","text":"<pre><code># Check baseline quality\nsuper agent evaluate developer --format json --save-report baseline.json\njq '.summary.pass_rate' baseline.json\n\n# Check improvement\nsuper agent evaluate developer --format json --save-report final.json\njq '.improvement.percentage' final.json\n</code></pre>"},{"location":"guides/cicd-integration/#optimization-failures","title":"Optimization Failures","text":"<pre><code># Check if agent is compiled\nsuper agent compile developer\n\n# Check if scenarios exist\nsuper agent evaluate developer --verbose\n\n# Force re-optimization\nsuper agent optimize developer --force\n</code></pre>"},{"location":"guides/cicd-integration/#deployment-failures","title":"Deployment Failures","text":"<pre><code># Verify quality gates locally\n./quality-gate.sh\n\n# Check agent status\nsuper agent inspect developer\n\n# Test agent manually\nsuper agent run developer --goal \"test task\"\n</code></pre>"},{"location":"guides/cicd-integration/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Evaluation &amp; Testing Guide - Detailed testing information</li> <li>Optimization Guide - Agent optimization techniques</li> <li>BDD Guide - Behavior-Driven Development practices</li> <li>Agent Development Guide - Agent development workflow</li> </ul> <p>\ud83c\udfaf Key Takeaway: CI/CD integration ensures reliable, production-ready AI agents through automated testing, quality gates, and iterative improvement. Always evaluate before optimizing and deploy only when quality gates pass. </p>"},{"location":"guides/claude-sdk-integration/","title":"Claude Agent SDK Integration Guide","text":"<p>SuperOptiX provides first-class support for Claude Agent SDK, enabling GEPA-optimizable agents powered by Anthropic's Claude with in-process MCP tool support.</p> <p>RLM support is experimental. Unified sandbox support is coming soon.</p>"},{"location":"guides/claude-sdk-integration/#key-features","title":"Key Features","text":"Feature Description GEPA Optimization Optimize system prompts automatically for better performance In-Process MCP Tools Convert StackOne tools to Claude SDK MCP servers Async-First Full async/await support for high-performance applications Bidirectional Sessions Interactive multi-turn conversations with ClaudeSDKClient Hook System Pre/Post tool use hooks for control and security"},{"location":"guides/claude-sdk-integration/#installation","title":"Installation","text":"<pre><code>pip install superoptix claude-agent-sdk\n</code></pre> <p>For StackOne integration: <pre><code>pip install superoptix claude-agent-sdk stackone-ai\n</code></pre></p>"},{"location":"guides/claude-sdk-integration/#quick-start","title":"Quick Start","text":""},{"location":"guides/claude-sdk-integration/#create-a-playbook","title":"Create a Playbook","text":"<pre><code># agents/my_claude_agent_playbook.yaml\nmetadata:\n  name: my_claude_agent\n  version: \"1.0.0\"\n  description: \"Claude SDK powered agent\"\n\nspec:\n  persona:\n    role: \"You are a helpful coding assistant\"\n    goal: \"Help users write better code\"\n    instructions: \"Be concise and provide working examples\"\n\n  language_model:\n    provider: anthropic\n    model: claude-sonnet-4-5\n\n  input_fields:\n    - name: question\n      type: string\n\n  output_fields:\n    - name: answer\n      type: string\n</code></pre>"},{"location":"guides/claude-sdk-integration/#compile-and-run","title":"Compile and Run","text":"<pre><code># Pull or create the agent project structure first\nsuper agent pull my_claude_agent\n\n# Compile to Claude SDK\nsuper agent compile my_claude_agent --framework claude-sdk\n\n# Run the agent\nsuper agent run my_claude_agent --framework claude-sdk --goal \"Explain async/await in Python\"\n</code></pre>"},{"location":"guides/claude-sdk-integration/#use-programmatically","title":"Use Programmatically","text":"<pre><code>import asyncio\nfrom my_claude_agent_claude_sdk_pipeline import MyClaudeAgentPipeline\n\nasync def main():\n    pipeline = MyClaudeAgentPipeline()\n    result = await pipeline.run(query=\"Explain async/await in Python\")\n    print(result[\"answer\"])\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/claude-sdk-integration/#using-stackone-tools","title":"Using StackOne Tools","text":"<p>The SuperOptiX StackOneBridge converts StackOne tools to Claude SDK's in-process MCP server format.</p>"},{"location":"guides/claude-sdk-integration/#basic-integration","title":"Basic Integration","text":"<pre><code>from stackone_ai import StackOneToolSet\nfrom claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, query\nfrom superoptix.adapters import StackOneBridge\n\n# Fetch StackOne tools\ntoolset = StackOneToolSet()\ntools = toolset.fetch_tools(\n    include_tools=[\"hris_get_employee\", \"hris_list_employees\"],\n    account_ids=[\"your_account_id\"]\n)\n\n# Convert to Claude SDK MCP server\nbridge = StackOneBridge(tools)\nmcp_server, tool_names = bridge.to_claude_sdk()\n\n# Create Claude Agent with tools\noptions = ClaudeAgentOptions(\n    system_prompt=\"You are an HR assistant with HRIS access.\",\n    mcp_servers={\"stackone\": mcp_server},\n    allowed_tools=tool_names,\n    model=\"claude-sonnet-4-5\",\n)\n\n# Execute query\nasync for message in query(prompt=\"Find employee John Doe\", options=options):\n    # Process messages\n    pass\n</code></pre>"},{"location":"guides/claude-sdk-integration/#interactive-session","title":"Interactive Session","text":"<pre><code>async with ClaudeSDKClient(options=options) as client:\n    # First query\n    await client.query(\"How many employees do we have?\")\n    async for msg in client.receive_response():\n        # Process response\n        pass\n\n    # Follow-up (uses conversation context)\n    await client.query(\"Who is in engineering?\")\n    async for msg in client.receive_response():\n        pass\n</code></pre>"},{"location":"guides/claude-sdk-integration/#dynamic-tool-discovery","title":"Dynamic Tool Discovery","text":"<p>For large tool sets, use discovery tools:</p> <pre><code># Get all tools\ntools = toolset.fetch_tools(include_tools=[\"hris_*\", \"ats_*\", \"crm_*\"])\n\n# Create discovery meta-tools\nbridge = StackOneBridge(tools)\nmcp_server, tool_names = bridge.to_discovery_tools(framework=\"claude_sdk\")\n\n# Agent can now search for and execute tools dynamically\noptions = ClaudeAgentOptions(\n    system_prompt=\"Use tool_search to find tools, then tool_execute to run them.\",\n    mcp_servers={\"stackone\": mcp_server},\n    allowed_tools=tool_names,\n)\n</code></pre>"},{"location":"guides/claude-sdk-integration/#gepa-optimization","title":"GEPA Optimization","text":"<p>The <code>system_prompt</code> is the optimizable variable for Claude SDK agents:</p> <pre><code>from superoptix.adapters import FrameworkRegistry\nfrom superoptix.optimizers.universal_gepa import UniversalGEPA\n\n# Create component from playbook\ncomponent = FrameworkRegistry.create_component(\"claude-sdk\", playbook)\n\n# The system_prompt is accessible as component.variable\nprint(f\"Current prompt: {component.variable}\")\n\n# Optimize with GEPA\noptimizer = UniversalGEPA(metric=my_accuracy_metric, auto=\"medium\")\nresult = optimizer.optimize(component, trainset=training_data)\n\nprint(f\"Optimized prompt: {result.optimized_variable}\")\nprint(f\"Improvement: {result.improvement}%\")\n</code></pre>"},{"location":"guides/claude-sdk-integration/#mcp-server-configuration","title":"MCP Server Configuration","text":""},{"location":"guides/claude-sdk-integration/#external-mcp-servers-stdio","title":"External MCP Servers (stdio)","text":"<pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@anthropic/claude-mcp-filesystem\", \"/path/to/dir\"]\n</code></pre>"},{"location":"guides/claude-sdk-integration/#httpsse-mcp-servers","title":"HTTP/SSE MCP Servers","text":"<pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: my_api\n        type: http\n        config:\n          url: \"http://localhost:8080/mcp\"\n          headers:\n            Authorization: \"Bearer ${API_KEY}\"\n</code></pre>"},{"location":"guides/claude-sdk-integration/#api-reference","title":"API Reference","text":""},{"location":"guides/claude-sdk-integration/#claudeagentsdkframeworkadapter","title":"ClaudeAgentSDKFrameworkAdapter","text":"<pre><code>from superoptix.adapters import FrameworkRegistry\n\n# Get adapter\nadapter = FrameworkRegistry.get_adapter(\"claude-sdk\")\n\n# Compile playbook\noutput_path = adapter.compile_from_playbook(playbook, \"output.py\")\n\n# Create component for GEPA\ncomponent = adapter.create_component(playbook)\n\n# Get optimizable variable\nprompt = adapter.get_optimizable_variable(playbook)\n</code></pre>"},{"location":"guides/claude-sdk-integration/#stackonebridgeto_claude_sdk","title":"StackOneBridge.to_claude_sdk()","text":"<pre><code>mcp_server, tool_names = bridge.to_claude_sdk()\n</code></pre> <p>Returns: - <code>mcp_server</code>: <code>McpSdkServerConfig</code> for use with <code>ClaudeAgentOptions.mcp_servers</code> - <code>tool_names</code>: List of tool names in format <code>mcp__stackone__{tool_name}</code></p>"},{"location":"guides/claude-sdk-integration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use async/await: Claude SDK is async-first; use <code>asyncio.run()</code> for sync contexts</p> </li> <li> <p>Tool naming: Claude SDK MCP tools follow <code>mcp__{server}__{tool}</code> convention</p> </li> <li> <p>System prompt optimization: Keep prompts focused; let GEPA refine them</p> </li> <li> <p>Error handling: Use <code>ResultMessage.is_error</code> to detect failures</p> </li> <li> <p>Cost tracking: Check <code>ResultMessage.total_cost_usd</code> for usage monitoring</p> </li> </ol>"},{"location":"guides/claude-sdk-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/claude-sdk-integration/#claude-agent-sdk-not-installed","title":"Claude Agent SDK not installed","text":"<pre><code>ImportError: claude-agent-sdk is not installed\n</code></pre> <p>Solution: <code>pip install claude-agent-sdk</code></p>"},{"location":"guides/claude-sdk-integration/#cli-not-found","title":"CLI not found","text":"<pre><code>CLINotFoundError: Claude Code CLI not found\n</code></pre> <p>Solution: The Claude Agent SDK requires the Claude Code CLI. Install it or ensure it's in your PATH.</p>"},{"location":"guides/claude-sdk-integration/#mcp-server-connection-failed","title":"MCP server connection failed","text":"<pre><code>MCPConnectionError: Failed to connect to MCP server\n</code></pre> <p>Solution: Check server configuration, ensure the command/URL is correct, and verify the server is running.</p>"},{"location":"guides/claude-sdk-integration/#examples","title":"Examples","text":"<p>See the complete example at: <pre><code>examples/integrations/stackone_claude_sdk_example.py\n</code></pre></p> <p>This includes: - Basic StackOne + Claude SDK integration - Interactive sessions with ClaudeSDKClient - Dynamic tool discovery</p>"},{"location":"guides/cli-complete-guide/","title":"Complete CLI Usage Guide","text":"<p>SuperOptiX CLI: Your Complete Command Reference</p> <p>Master the <code>super</code> CLI for building, optimizing, and deploying AI agents.</p>"},{"location":"guides/cli-complete-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Project Management</li> <li>Agent Commands</li> <li>Optimization Commands</li> <li>Dataset Commands</li> <li>Model Management</li> <li>Orchestra Commands</li> <li>Marketplace Commands</li> <li>Observability Commands</li> <li>Advanced Usage</li> </ol>"},{"location":"guides/cli-complete-guide/#installation","title":"Installation","text":""},{"location":"guides/cli-complete-guide/#quick-install-recommended","title":"Quick Install (Recommended)","text":"<pre><code># Using uv (fastest)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv pip install superoptix\n\n# Verify installation\nsuper --version\nsuper --help\n</code></pre>"},{"location":"guides/cli-complete-guide/#alternative-install-pip","title":"Alternative Install (pip)","text":"<pre><code>pip install superoptix\n</code></pre>"},{"location":"guides/cli-complete-guide/#get-documentation","title":"Get Documentation","text":"<pre><code># Open comprehensive docs\nsuper docs\n\n# Show help for any command\nsuper &lt;command&gt; --help\nsuper agent compile --help\n</code></pre>"},{"location":"guides/cli-complete-guide/#project-management","title":"Project Management","text":""},{"location":"guides/cli-complete-guide/#super-init-initialize-project","title":"<code>super init</code> - Initialize Project","text":"<pre><code># Create new project\nsuper init my_project\ncd my_project\n\n# What gets created:\n# my_project/\n# \u251c\u2500\u2500 .super              # Project marker\n# \u251c\u2500\u2500 agents/             # Agent playbooks\n# \u251c\u2500\u2500 guardrails/         # Safety rules\n# \u251c\u2500\u2500 memory/             # Memory modules\n# \u251c\u2500\u2500 protocols/          # Communication protocols\n# \u251c\u2500\u2500 teams/              # Multi-agent teams\n# \u251c\u2500\u2500 evals/              # Evaluation results\n# \u251c\u2500\u2500 knowledge/          # Knowledge bases\n# \u251c\u2500\u2500 optimizers/         # Optimization data\n# \u251c\u2500\u2500 servers/            # Server code\n# \u2514\u2500\u2500 tools/              # Custom tools\n</code></pre>"},{"location":"guides/cli-complete-guide/#agent-commands","title":"Agent Commands","text":""},{"location":"guides/cli-complete-guide/#super-agent-list-list-agents","title":"<code>super agent list</code> - List Agents","text":"<pre><code># List all agents in current project\nsuper agent list\n\n# List pre-built demo agents\nsuper agent list --pre-built\n\n# List agents in specific directory\nsuper agent list --directory ./agents/custom\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-pull-download-demo-agents","title":"<code>super agent pull</code> - Download Demo Agents","text":"<pre><code># Pull demo agent from marketplace\nsuper agent pull sentiment_analyzer      # DSPy demo\nsuper agent pull assistant_openai        # OpenAI SDK demo\nsuper agent pull researcher_crew         # CrewAI demo\nsuper agent pull assistant_adk           # Google ADK demo\nsuper agent pull assistant_microsoft     # Microsoft demo\nsuper agent pull research_agent_deepagents  # DeepAgents demo\n\n# Pull to specific directory\nsuper agent pull sentiment_analyzer --output ./agents/demos/\n\n# List available agents\nsuper market browse agents\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-compile-compile-agent","title":"<code>super agent compile</code> - Compile Agent","text":"<pre><code># Compile with default framework (DSPy)\nsuper agent compile my_agent\n\n# Compile with specific framework\nsuper agent compile my_agent --framework dspy\nsuper agent compile my_agent --framework openai\nsuper agent compile my_agent --framework crewai\nsuper agent compile my_agent --framework google-adk\nsuper agent compile my_agent --framework microsoft\nsuper agent compile my_agent --framework deepagents\n\n# Compile with output path\nsuper agent compile my_agent --output ./pipelines/\n\n# Compile with verbose output\nsuper agent compile my_agent --verbose\n\n# Compile multiple agents\nsuper agent compile agent1 agent2 agent3\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-evaluate-evaluate-agent","title":"<code>super agent evaluate</code> - Evaluate Agent","text":"<pre><code># Evaluate agent (baseline)\nsuper agent evaluate my_agent\n\n# Evaluate optimized version\nsuper agent evaluate my_agent  # automatically loads optimized weights\n\n# Evaluate with specific dataset\nsuper agent evaluate my_agent --dataset ./data/test.csv\n\n# Evaluate with verbose output\nsuper agent evaluate my_agent --verbose\n\n# Save evaluation report\nsuper agent evaluate my_agent --save-report results.json\n\n# Evaluate with specific scenarios\nsuper agent evaluate my_agent --scenarios \"scenario1,scenario2\"\n\n# Evaluate in CI/CD pipeline\nsuper agent evaluate my_agent --format json --exit-code\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-optimize-optimize-agent","title":"<code>super agent optimize</code> - Optimize Agent","text":"<pre><code># Optimize with auto settings (recommended)\nsuper agent optimize my_agent --auto light      # Quick (5 min)\nsuper agent optimize my_agent --auto medium     # Balanced (15 min) \u2b50 Recommended\nsuper agent optimize my_agent --auto intensive  # Thorough (30+ min)\n\n# Optimize with custom settings\nsuper agent optimize my_agent \\\n  --optimizer GEPA \\\n  --iterations 10 \\\n  --metric answer_exact_match\n\n# Optimize with specific LLM for reflection\nsuper agent optimize my_agent \\\n  --auto medium \\\n  --reflection-lm qwen3:8b\n\n# Fresh optimization (discard previous)\nsuper agent optimize my_agent --auto medium --fresh\n\n# Continue from previous optimization\nsuper agent optimize my_agent --auto medium --resume\n\n# Optimize with minibatch\nsuper agent optimize my_agent \\\n  --auto medium \\\n  --minibatch-size 5\n\n# Skip scenarios with perfect scores\nsuper agent optimize my_agent \\\n  --auto medium \\\n  --skip-perfect-score\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-run-run-agent","title":"<code>super agent run</code> - Run Agent","text":"<pre><code># Run agent interactively\nsuper agent run my_agent\n\n# Run with specific input\nsuper agent run my_agent --goal \"Analyze this text\"\n\n# Run with input from file\nsuper agent run my_agent --goal \"$(cat ./input.txt)\"\n\n# Run optimized version\nsuper agent run my_agent  # automatically loads optimized weights\n\n# Run with specific framework\nsuper agent run my_agent --framework openai\n\n# Run in batch mode\nsuper agent run my_agent --batch ./inputs.jsonl\n\n# Run with output to file\nsuper agent run my_agent --goal \"text\" --output results.json\n</code></pre>"},{"location":"guides/cli-complete-guide/#super-agent-design-design-agent-studio","title":"<code>super agent design</code> - Design Agent (Studio)","text":"<pre><code># Launch Studio UI for agent design\nsuper agent design\n\n# Design options\nsuper agent design --mode visual     # Visual builder\nsuper agent design --mode code       # Code editor\nsuper agent design                   # Default interactive mode\n\n# Design in specific mode\nsuper agent design --mode visual     # Visual builder\nsuper agent design --mode code       # Code editor\n</code></pre>"},{"location":"guides/cli-complete-guide/#optimization-commands","title":"Optimization Commands","text":""},{"location":"guides/cli-complete-guide/#super-spec-generate-generate-agent-from-superspec","title":"<code>super spec generate</code> - Generate Agent from SuperSpec","text":"<pre><code># Generate agent from natural language description\nsuper spec generate my_agent \"Create a sentiment analyzer\"\n\n# Generate with template\nsuper spec generate my_agent --template sentiment_analysis\n\n# Generate with RAG\nsuper spec generate my_agent \"Q&amp;A agent\" --rag\n\n# Generate with defaults\nsuper spec generate my_agent \"Research agent\"\n\n# Interactive generation\nsuper spec generate\n</code></pre>"},{"location":"guides/cli-complete-guide/#dataset-commands","title":"Dataset Commands","text":""},{"location":"guides/cli-complete-guide/#super-agent-dataset-dataset-management","title":"<code>super agent dataset</code> - Dataset Management","text":"<pre><code># Preview dataset\nsuper agent dataset preview my_agent --limit 10\n\n# Validate dataset configuration\nsuper agent dataset validate my_agent\n\n# Get dataset info\nsuper agent dataset info my_agent\n\n# Convert dataset format\nsuper agent dataset convert \\\n  --input ./data/train.csv \\\n  --output ./data/train.jsonl \\\n  --format jsonl\n\n# Split dataset\nsuper agent dataset split \\\n  --input ./data/all.csv \\\n  --train ./data/train.csv \\\n  --test ./data/test.csv \\\n  --ratio 0.8\n\n# Merge datasets\nsuper agent dataset merge \\\n  --inputs data1.csv data2.csv data3.csv \\\n  --output combined.csv\n</code></pre>"},{"location":"guides/cli-complete-guide/#model-management","title":"Model Management","text":""},{"location":"guides/cli-complete-guide/#super-model-model-commands","title":"<code>super model</code> - Model Commands","text":"<pre><code># List installed models\nsuper model list\n\n# List all available models\nsuper model list --all\n\n# Install model\nsuper model install llama3.1:8b\nsuper model install llama3.1:8b --backend ollama\n\n# Install with specific backend\nsuper model install llama3.1:8b --backend mlx       # Apple Silicon\nsuper model install llama3.1:8b --backend huggingface\nsuper model install llama3.1:8b --backend lmstudio\n\n# Get model info\nsuper model info llama3.1:8b\n\n# Start model server\nsuper model server --port 11434\nsuper model serve --backend ollama\n\n# Remove model\nsuper model remove llama3.1:8b\n</code></pre>"},{"location":"guides/cli-complete-guide/#orchestra-commands","title":"Orchestra Commands","text":""},{"location":"guides/cli-complete-guide/#super-orchestra-multi-agent-orchestration","title":"<code>super orchestra</code> - Multi-Agent Orchestration","text":"<pre><code># Create orchestra\nsuper orchestra create my_orchestra\n\n# List orchestras\nsuper orchestra list\n\n# Run orchestra\nsuper orchestra run my_orchestra\n\n# Run with specific input\nsuper orchestra run my_orchestra --goal \"Complex task\"\n\n# Evaluate orchestra\nsuper orchestra evaluate my_orchestra\n\n# Optimize orchestra\nsuper orchestra optimize my_orchestra --auto medium\n</code></pre>"},{"location":"guides/cli-complete-guide/#marketplace-commands","title":"Marketplace Commands","text":""},{"location":"guides/cli-complete-guide/#super-market-marketplace-operations","title":"<code>super market</code> - Marketplace Operations","text":"<pre><code># Browse agents\nsuper market browse agents\n\n# Browse tools\nsuper market browse tools\n\n# Search marketplace\nsuper market search \"sentiment analysis\"\nsuper market search \"RAG\"\n\n# Get agent details\nsuper market info sentiment_analyzer\n\n# Install agent from marketplace\nsuper market install agent sentiment_analyzer\n\n# Install tool from marketplace\nsuper market install tool web_scraper\n\n# List installed marketplace items\nsuper market list --installed\n</code></pre>"},{"location":"guides/cli-complete-guide/#observability-commands","title":"Observability Commands","text":""},{"location":"guides/cli-complete-guide/#super-observe-observability","title":"<code>super observe</code> - Observability","text":"<pre><code># Start observability dashboard\nsuper observe\n\n# View specific agent runs\nsuper observe my_agent\n\n# View with specific backend\nsuper observe --backend mlflow\nsuper observe --backend langfuse\n\n# Export metrics\nsuper observe export --format json --output metrics.json\n\n# View logs\nsuper observe logs my_agent\n\n# View performance metrics\nsuper observe metrics my_agent --window 7d\n</code></pre>"},{"location":"guides/cli-complete-guide/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/cli-complete-guide/#chaining-commands","title":"Chaining Commands","text":"<pre><code># Complete workflow in one go\nsuper agent pull sentiment_analyzer &amp;&amp; \\\nsuper agent compile sentiment_analyzer &amp;&amp; \\\nsuper agent evaluate sentiment_analyzer &amp;&amp; \\\nsuper agent optimize sentiment_analyzer --auto medium &amp;&amp; \\\nsuper agent evaluate sentiment_analyzer &amp;&amp; \\\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"guides/cli-complete-guide/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Set model configuration\nexport SUPER_MODEL_PROVIDER=ollama\nexport SUPER_MODEL_NAME=llama3.1:8b\nexport SUPER_API_BASE=http://localhost:11434\n\n# Set optimization settings\nexport SUPER_OPTIMIZER=GEPA\nexport SUPER_AUTO_MODE=medium\n\n# Set API keys\nexport OPENAI_API_KEY=\"your-key\"\nexport ANTHROPIC_API_KEY=\"your-key\"\nexport GOOGLE_API_KEY=\"your-key\"\n\n# Run with environment config\nsuper agent optimize my_agent\n</code></pre>"},{"location":"guides/cli-complete-guide/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple agents\nfor agent in agent1 agent2 agent3; do\n  super agent compile $agent\n  super agent evaluate $agent\n  super agent optimize $agent --auto medium\ndone\n\n# Process with parallel execution\nparallel super agent optimize {} --auto medium ::: agent1 agent2 agent3\n</code></pre>"},{"location":"guides/cli-complete-guide/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># In your CI/CD pipeline (GitHub Actions, GitLab CI, etc.)\nname: Agent Testing\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Install SuperOptiX\n        run: pip install superoptix\n\n      - name: Compile Agent\n        run: super agent compile my_agent\n\n      - name: Evaluate Agent\n        run: super agent evaluate my_agent --format json --exit-code\n\n      - name: Upload Results\n        uses: actions/upload-artifact@v2\n        with:\n          name: evaluation-results\n          path: results.json\n</code></pre>"},{"location":"guides/cli-complete-guide/#debugging","title":"Debugging","text":"<pre><code># Enable verbose output\nsuper agent compile my_agent --verbose\nsuper agent evaluate my_agent --verbose --debug\n\n# Check logs\nsuper observe logs my_agent --tail 100\n\n# Dry run (show what would happen)\nsuper agent compile my_agent --dry-run\n\n# Show execution plan\nsuper agent run my_agent --explain\n</code></pre>"},{"location":"guides/cli-complete-guide/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/cli-complete-guide/#workflow-1-quick-start-new-agent","title":"Workflow 1: Quick Start (New Agent)","text":"<pre><code># Initialize project\nsuper init my_project &amp;&amp; cd my_project\n\n# Pull demo agent\nsuper agent pull sentiment_analyzer\n\n# Compile\nsuper agent compile sentiment_analyzer\n\n# Evaluate baseline\nsuper agent evaluate sentiment_analyzer\n\n# Optimize\nsuper agent optimize sentiment_analyzer --auto medium\n\n# Evaluate optimized\nsuper agent evaluate sentiment_analyzer  # automatically loads optimized weights\n\n# Run\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"guides/cli-complete-guide/#workflow-2-custom-agent-development","title":"Workflow 2: Custom Agent Development","text":"<pre><code># Generate from description\nsuper spec generate my_agent \"Analyze customer reviews\"\n\n# Edit playbook (manual)\nvim agents/my_agent/playbook/my_agent_playbook.yaml\n\n# Compile and test\nsuper agent compile my_agent\nsuper agent evaluate my_agent\n\n# Optimize\nsuper agent optimize my_agent --auto medium\n\n# Deploy\nsuper agent run my_agent\n</code></pre>"},{"location":"guides/cli-complete-guide/#workflow-3-multi-framework-comparison","title":"Workflow 3: Multi-Framework Comparison","text":"<pre><code># Compare same agent across frameworks\nfor fw in dspy openai crewai; do\n  echo \"Testing $fw...\"\n  super agent compile my_agent --framework $fw\n  super agent evaluate my_agent\ndone\n</code></pre>"},{"location":"guides/cli-complete-guide/#workflow-4-dataset-import-training","title":"Workflow 4: Dataset Import &amp; Training","text":"<pre><code># Prepare dataset\ncat &gt; data/train.csv &lt;&lt; EOF\ntext,label\n\"Great product!\",positive\n\"Poor quality\",negative\nEOF\n\n# Configure in playbook\n# (add datasets: section)\n\n# Preview\nsuper agent dataset preview my_agent\n\n# Compile with dataset\nsuper agent compile my_agent\n\n# Train with large dataset\nsuper agent optimize my_agent --auto medium\n</code></pre>"},{"location":"guides/cli-complete-guide/#configuration-files","title":"Configuration Files","text":""},{"location":"guides/cli-complete-guide/#global-config-superoptixconfigyaml","title":"Global Config: <code>~/.superoptix/config.yaml</code>","text":"<pre><code># Default settings\ndefault_framework: dspy\ndefault_optimizer: GEPA\nauto_mode: medium\n\n# Model settings\nmodel:\n  provider: ollama\n  default_model: llama3.1:8b\n  api_base: http://localhost:11434\n\n# Optimization settings\noptimization:\n  default_iterations: 5\n  minibatch_size: 3\n  skip_perfect_score: true\n\n# Observability\nobservability:\n  backend: mlflow\n  tracking_uri: http://localhost:5000\n</code></pre>"},{"location":"guides/cli-complete-guide/#project-config-superconfigyaml","title":"Project Config: <code>.super/config.yaml</code>","text":"<pre><code># Project-specific settings\nproject_name: my_project\ndefault_agent: sentiment_analyzer\n\n# Override global settings\ndefault_framework: crewai\nauto_mode: intensive\n</code></pre>"},{"location":"guides/cli-complete-guide/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"guides/cli-complete-guide/#tip-1-use-aliases","title":"Tip 1: Use Aliases","text":"<pre><code># Add to ~/.bashrc or ~/.zshrc\nalias sac='super agent compile'\nalias sae='super agent evaluate'\nalias sao='super agent optimize'\nalias sar='super agent run'\n\n# Use them\nsac my_agent &amp;&amp; sae my_agent &amp;&amp; sao my_agent --auto medium\n</code></pre>"},{"location":"guides/cli-complete-guide/#tip-2-quick-optimization-test","title":"Tip 2: Quick Optimization Test","text":"<pre><code># Test optimization quickly\nsuper agent optimize my_agent --auto light --limit 3\n</code></pre>"},{"location":"guides/cli-complete-guide/#tip-3-watch-for-changes","title":"Tip 3: Watch for Changes","text":"<pre><code># Auto-recompile on file changes\nwatch -n 2 'super agent compile my_agent'\n\n# Or use entr\nls agents/my_agent/*.yaml | entr super agent compile my_agent\n</code></pre>"},{"location":"guides/cli-complete-guide/#tip-4-export-results","title":"Tip 4: Export Results","text":"<pre><code># Save evaluation results for comparison\nsuper agent evaluate my_agent --save-report baseline.json\nsuper agent optimize my_agent --auto medium\nsuper agent evaluate my_agent --save-report optimized.json  # automatically loads optimized weights\n\n# Compare\ndiff baseline.json optimized.json\n</code></pre>"},{"location":"guides/cli-complete-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cli-complete-guide/#command-not-found","title":"Command Not Found","text":"<pre><code># Check installation\nwhich super\nuv pip list | grep superoptix\n\n# Reinstall\nuv pip install --upgrade superoptix\n</code></pre>"},{"location":"guides/cli-complete-guide/#permission-denied","title":"Permission Denied","text":"<pre><code># Run with proper permissions\nchmod +x $(which super)\n\n# Or use full path\npython -m superoptix.cli.main\n</code></pre>"},{"location":"guides/cli-complete-guide/#api-rate-limits","title":"API Rate Limits","text":"<pre><code># Use local models\nexport SUPER_MODEL_PROVIDER=ollama\n\n# Reduce optimization intensity\nsuper agent optimize my_agent --auto light\n</code></pre>"},{"location":"guides/cli-complete-guide/#getting-help","title":"Getting Help","text":"<pre><code># General help\nsuper --help\n\n# Command-specific help\nsuper agent --help\nsuper agent compile --help\nsuper agent optimize --help\n\n# Show version\nsuper --version\nsuper -v\n\n# Check documentation\nsuper docs\n</code></pre>"},{"location":"guides/cli-complete-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Framework Guide</li> <li>GEPA Optimization</li> <li>Dataset Import</li> <li>SuperSpec DSL</li> </ul> <p>Status: Complete CLI Reference Commands: All major commands documented Examples: Practical workflows included </p>"},{"location":"guides/cloud-inference/","title":"\ud83c\udf10 Cloud Inference with SuperOptiX","text":""},{"location":"guides/cloud-inference/#what-is-cloud-inference","title":"\ud83c\udfaf What is Cloud Inference?","text":"<p>Cloud Inference in SuperOptiX enables you to leverage powerful cloud-based language models (OpenAI, Anthropic, Google, and 100+ other providers) for enhanced AI agent capabilities. SuperOptiX provides seamless integration with ALL LiteLLM providers through DSPy, giving you access to the world's most advanced AI models.</p>"},{"location":"guides/cloud-inference/#why-cloud-inference","title":"Why Cloud Inference?","text":"<pre><code>graph LR\n    A[Local Models] --&gt; B[Limited Capabilities]\n    B --&gt; C[Basic Tasks]\n    C --&gt; D[Oracles Tier]\n\n    E[Cloud Models] --&gt; F[Advanced Capabilities]\n    F --&gt; G[Complex Reasoning]\n    G --&gt; H[Genies Tier]\n    H --&gt; I[Production Ready]\n\n    style A fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style B fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style C fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style E fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style H fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style I fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/cloud-inference/#important-responsible-ai-usage","title":"\u26a0\ufe0f Important: Responsible AI Usage","text":"<p>\ud83d\udea8 COST AWARENESS IS CRITICAL!</p> <p>Cloud inference can be expensive, especially during optimization. SuperOptiX is powerful, but you must use it responsibly and monitor your costs carefully.</p>"},{"location":"guides/cloud-inference/#quick-start-cloud-llm-setup","title":"\ud83d\ude80 Quick Start: Cloud LLM Setup","text":""},{"location":"guides/cloud-inference/#step-1-environment-setup","title":"Step 1: Environment Setup","text":"<pre><code># Initialize your project\nsuper init cloud_agent_project\ncd cloud_agent_project\n\n# Create .env file for API keys\ntouch .env\n</code></pre>"},{"location":"guides/cloud-inference/#step-2-configure-api-keys","title":"Step 2: Configure API Keys","text":"<pre><code># .env file - NEVER commit this file!\nOPENAI_API_KEY=sk-your-openai-key-here\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\nGOOGLE_API_KEY=your-google-key-here\nGROQ_API_KEY=your-groq-key-here\n# Add other providers as needed\n</code></pre>"},{"location":"guides/cloud-inference/#step-3-create-cloud-agent-playbook","title":"Step 3: Create Cloud Agent Playbook","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Cloud Assistant\n  id: cloud_assistant\n  namespace: general\n  version: 1.0.0\n  agent_type: Supervised\n  level: genies\n  description: An agent using cloud LLM for enhanced capabilities\n\nspec:\n  language_model:\n    location: cloud                    # REQUIRED for cloud providers\n    provider: openai                  # or \"anthropic\", \"google\", etc.\n    model: gpt-4o                     # or \"claude-3-opus-20240229\"\n    temperature: 0.7                  # OPTIONAL - Controls randomness\n    max_tokens: 2000                  # OPTIONAL - Limit output length\n    top_p: 1.0                        # OPTIONAL - Nucleus sampling\n    cache: true                       # OPTIONAL - Enable caching\n    num_retries: 3                    # OPTIONAL - Retry on failure\n\n  persona:\n    name: CloudBot\n    role: AI Assistant\n    goal: Provide helpful responses using cloud LLM capabilities\n    traits: [\"helpful\", \"knowledgeable\", \"precise\"]\n\n  tasks:\n    - name: answer_question\n      instruction: \"Answer the user's question with clear reasoning and helpful information.\"\n      inputs:\n        - name: question\n          type: str\n          description: \"The user's question\"\n          required: true\n      outputs:\n        - name: answer\n          type: str\n          description: \"Comprehensive answer to the question\"\n\n  agentflow:\n    - name: generate_answer\n      type: Generate\n      task: answer_question\n\n  # Advanced techniques for Genies tier\n  tools:\n    enabled: true\n    categories:\n    - core\n    - utilities\n\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.8\n\n  optimization:\n    strategy: few_shot_bootstrapping\n    metric: answer_correctness\n    metric_threshold: 0.7\n    few_shot_bootstrapping_config:\n      max_bootstrapped_demos: 3  # Keep low to control costs\n      max_rounds: 1\n</code></pre>"},{"location":"guides/cloud-inference/#step-4-compile-and-run","title":"Step 4: Compile and Run","text":"<pre><code># Compile the agent\nsuper agent compile cloud_assistant\n\n# Test with a simple query\nsuper agent run cloud_assistant --input \"What is machine learning?\"\n\n# Run evaluation (establishes baseline)\nsuper agent evaluate cloud_assistant\n\n# Optimize (\u26a0\ufe0f This can be expensive!)\nsuper agent optimize cloud_assistant\n\n# Re-evaluate to measure improvement\nsuper agent evaluate cloud_assistant\n</code></pre>"},{"location":"guides/cloud-inference/#supported-cloud-providers","title":"\ud83c\udf0d Supported Cloud Providers","text":"<p>SuperOptiX supports ALL LiteLLM providers through DSPy integration:</p>"},{"location":"guides/cloud-inference/#major-cloud-providers","title":"Major Cloud Providers","text":"Provider Models API Key Variable Cost per 1K tokens Best For OpenAI GPT-4, GPT-3.5 <code>OPENAI_API_KEY</code> $0.005-0.03 General purpose, coding Anthropic Claude <code>ANTHROPIC_API_KEY</code> $0.015-0.075 Reasoning, analysis Google Gemini <code>GOOGLE_API_KEY</code> $0.0005-0.007 Cost-effective, multimodal Azure Azure OpenAI <code>AZURE_OPENAI_API_KEY</code> $0.005-0.03 Enterprise, compliance Mistral Mistral AI <code>MISTRAL_API_KEY</code> $0.002-0.014 Fast, cost-effective Cohere Command <code>COHERE_API_KEY</code> $0.001-0.015 RAG, embeddings Groq Fast Inference <code>GROQ_API_KEY</code> $0.0001-0.0008 Ultra-fast inference DeepSeek DeepSeek <code>DEEPSEEK_API_KEY</code> $0.001-0.014 Coding, reasoning"},{"location":"guides/cloud-inference/#cost-effective-alternatives","title":"Cost-Effective Alternatives","text":"Provider Models Cost per 1K tokens Use Case Together AI Open Models $0.0002-0.002 Development, testing Fireworks AI Custom Models $0.0001-0.001 High-performance, low-cost Perplexity Search + AI $0.001-0.005 Research, search OpenRouter Model Aggregator $0.0001-0.015 Multiple providers Replicate Open Source Models $0.0001-0.001 Custom models"},{"location":"guides/cloud-inference/#cost-management-critical-guidelines","title":"\ud83d\udcb0 Cost Management: Critical Guidelines","text":"<p>\ud83d\udcca IMPORTANT: Data and pricing information in this guide is for reference only and can change anytime. Do not take it for granted. Always check current pricing with your chosen provider before making decisions.</p>"},{"location":"guides/cloud-inference/#optimization-cost-warning","title":"\ud83d\udea8 OPTIMIZATION COST WARNING","text":"<p>\u26a0\ufe0f OPTIMIZATION CAN BE VERY EXPENSIVE!</p> <p>The <code>super agent optimize</code> command can make hundreds or thousands of API calls depending on your configuration. Always start with conservative settings.</p>"},{"location":"guides/cloud-inference/#cost-estimation-examples","title":"Cost Estimation Examples","text":"Model Input Tokens Output Tokens Cost per 1K tokens Optimization Calls Estimated Cost GPT-4o 1000 500 $0.005 100 $0.50 GPT-4o 1000 500 $0.005 1000 $5.00 Claude-3-Opus 1000 500 $0.015 100 $1.50 Claude-3-Opus 1000 500 $0.015 1000 $15.00 GPT-4o-mini 1000 500 $0.00015 1000 $0.15"},{"location":"guides/cloud-inference/#cost-control-strategies","title":"Cost Control Strategies","text":""},{"location":"guides/cloud-inference/#1-development-workflow-cost-effective","title":"1. Development Workflow (Cost-Effective)","text":"<pre><code># Start with cheaper models for development\nsuper spec generate oracle dev_agent --namespace development\n\n# Edit playbook to use cost-effective model\nspec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o-mini  # Much cheaper than gpt-4o\n    temperature: 0.7\n    max_tokens: 1000    # Limit output length\n    cache: true         # Enable caching\n</code></pre>"},{"location":"guides/cloud-inference/#2-conservative-optimization-settings","title":"2. Conservative Optimization Settings","text":"<pre><code>optimization:\n  strategy: few_shot_bootstrapping\n  metric: answer_correctness\n  metric_threshold: 0.7\n  few_shot_bootstrapping_config:\n    max_bootstrapped_demos: 2  # Keep very low\n    max_rounds: 1              # Single round only\n</code></pre>"},{"location":"guides/cloud-inference/#3-usage-monitoring","title":"3. Usage Monitoring","text":"<pre><code># Monitor your usage in real-time\nsuper observability dashboard\n\n# Check token usage in traces\nsuper agent inspect your_agent\n\n# Set up usage alerts\nexport OPENAI_USAGE_ALERT=10  # Alert at $10\nexport ANTHROPIC_USAGE_ALERT=10\n</code></pre>"},{"location":"guides/cloud-inference/#4-caching-strategy","title":"4. Caching Strategy","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o\n    cache: true              # Cache responses\n    cache_in_memory: true    # Use memory cache\n    num_retries: 2           # Limit retries\n</code></pre>"},{"location":"guides/cloud-inference/#provider-specific-configurations","title":"\ud83d\udd27 Provider-Specific Configurations","text":""},{"location":"guides/cloud-inference/#openai-most-popular","title":"OpenAI (Most Popular)","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o              # Latest model\n    # Alternative models:\n    # model: gpt-4o-mini       # Cheaper option\n    # model: gpt-3.5-turbo     # Budget option\n    temperature: 0.7\n    max_tokens: 2000\n    top_p: 1.0\n    frequency_penalty: 0.0\n    presence_penalty: 0.0\n    cache: true\n    num_retries: 3\n</code></pre>"},{"location":"guides/cloud-inference/#anthropic-claude-best-for-reasoning","title":"Anthropic Claude (Best for Reasoning)","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: anthropic\n    model: claude-3-opus-20240229    # Most capable\n    # Alternative models:\n    # model: claude-3-sonnet-20240229  # Balanced\n    # model: claude-3-haiku-20240307   # Fastest\n    temperature: 0.7\n    max_tokens: 2000\n    top_p: 1.0\n    cache: true\n</code></pre>"},{"location":"guides/cloud-inference/#google-gemini-cost-effective","title":"Google Gemini (Cost-Effective)","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: google\n    model: gemini-pro              # Text model\n    # model: gemini-pro-vision     # Multimodal\n    temperature: 0.7\n    max_tokens: 2000\n    cache: true\n</code></pre>"},{"location":"guides/cloud-inference/#groq-ultra-fast","title":"Groq (Ultra-Fast)","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: groq\n    model: llama3-70b-8192         # Very fast\n    # model: mixtral-8x7b-32768    # Alternative\n    temperature: 0.7\n    max_tokens: 2000\n    cache: true\n</code></pre>"},{"location":"guides/cloud-inference/#together-ai-cost-effective-development","title":"Together AI (Cost-Effective Development)","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: together-ai\n    model: meta-llama/Llama-3.1-405B-Instruct\n    temperature: 0.7\n    max_tokens: 2000\n    cache: true\n</code></pre>"},{"location":"guides/cloud-inference/#best-practices-for-cloud-inference","title":"\ud83c\udfaf Best Practices for Cloud Inference","text":""},{"location":"guides/cloud-inference/#dos","title":"DO's","text":""},{"location":"guides/cloud-inference/#1-start-small-and-scale-up","title":"1. Start Small and Scale Up","text":"<pre><code># Phase 1: Development with cheap models\nsuper spec generate oracle dev_agent\n# Use gpt-4o-mini or gemini-pro\n\n# Phase 2: Testing with better models\n# Switch to gpt-4o or claude-3-sonnet\n\n# Phase 3: Production with best models\n# Use gpt-4o or claude-3-opus\n</code></pre>"},{"location":"guides/cloud-inference/#2-use-conservative-optimization","title":"2. Use Conservative Optimization","text":"<pre><code>optimization:\n  strategy: few_shot_bootstrapping\n  metric: answer_correctness\n  metric_threshold: 0.7\n  few_shot_bootstrapping_config:\n    max_bootstrapped_demos: 2  # Start very low\n    max_rounds: 1              # Single round\n</code></pre>"},{"location":"guides/cloud-inference/#3-implement-caching","title":"3. Implement Caching","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o\n    cache: true              # Essential for cost control\n    cache_in_memory: true    # Fast access\n</code></pre>"},{"location":"guides/cloud-inference/#4-monitor-usage-religiously","title":"4. Monitor Usage Religiously","text":"<pre><code># Set up monitoring\nsuper observability dashboard\n\n# Check usage before optimization\nsuper agent inspect your_agent --show-usage\n\n# Set budget alerts\nexport OPENAI_USAGE_ALERT=5   # Alert at $5\n</code></pre>"},{"location":"guides/cloud-inference/#donts","title":"DON'Ts","text":""},{"location":"guides/cloud-inference/#1-dont-skip-cost-monitoring","title":"1. Don't Skip Cost Monitoring","text":"<pre><code># Bad: No monitoring\nsuper agent optimize expensive_agent\n\n# Good: Monitor first\nsuper agent inspect expensive_agent --show-usage\nsuper agent optimize expensive_agent --max-iterations 5\n</code></pre>"},{"location":"guides/cloud-inference/#2-dont-use-expensive-models-for-development","title":"2. Don't Use Expensive Models for Development","text":"<pre><code># Bad: Expensive for development\nspec:\n  language_model:\n    provider: openai\n    model: gpt-4o  # Expensive\n\n# Good: Cost-effective for development\nspec:\n  language_model:\n    provider: openai\n    model: gpt-4o-mini  # Much cheaper\n</code></pre>"},{"location":"guides/cloud-inference/#3-dont-ignore-rate-limits","title":"3. Don't Ignore Rate Limits","text":"<pre><code># Bad: No rate limiting\nspec:\n  language_model:\n    provider: openai\n    model: gpt-4o\n\n# Good: Proper rate limiting\nspec:\n  language_model:\n    provider: openai\n    model: gpt-4o\n    num_retries: 3\n    request_timeout: 30\n    cache: true\n</code></pre>"},{"location":"guides/cloud-inference/#troubleshooting-cloud-inference","title":"\ud83d\udd0d Troubleshooting Cloud Inference","text":""},{"location":"guides/cloud-inference/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"guides/cloud-inference/#1-api-key-not-found","title":"1. API Key Not Found","text":"<pre><code># Error: OpenAI API key not found\n\n# Solution: Check your .env file\ncat .env | grep OPENAI_API_KEY\n\n# Make sure the key is set correctly\nexport OPENAI_API_KEY=your-actual-key-here\n</code></pre>"},{"location":"guides/cloud-inference/#2-rate-limiting","title":"2. Rate Limiting","text":"<pre><code># Error: Rate limit exceeded\n\n# Solution: Add delays and retries\nspec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o\n    num_retries: 5\n    request_timeout: 60\n    cache: true  # Reduce API calls\n</code></pre>"},{"location":"guides/cloud-inference/#3-high-costs","title":"3. High Costs","text":"<pre><code># Monitor usage\nsuper observability dashboard\n\n# Check token usage\nsuper agent inspect your_agent --show-usage\n\n# Switch to cheaper model\n# Edit playbook: model: gpt-4o-mini\n</code></pre>"},{"location":"guides/cloud-inference/#4-model-not-found","title":"4. Model Not Found","text":"<pre><code># Error: Model not found\n\n# Solution: Use exact model identifiers\n# OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo\n# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229\n# Google: gemini-pro, gemini-pro-vision\n</code></pre>"},{"location":"guides/cloud-inference/#debug-commands","title":"Debug Commands","text":"<pre><code># Test API connection\nsuper agent evaluate your_agent --verbose\n\n# Check configuration\nsuper agent inspect your_agent\n\n# Monitor real-time usage\nsuper observability dashboard --live\n\n# View detailed traces\nsuper agent inspect your_agent --show-traces\n</code></pre>"},{"location":"guides/cloud-inference/#summary-responsible-cloud-inference","title":"\ud83c\udfaf Summary: Responsible Cloud Inference","text":""},{"location":"guides/cloud-inference/#key-principles","title":"Key Principles","text":"<ol> <li>\ud83c\udfaf Start Small: Use cost-effective models for development</li> <li>\ud83d\udcb0 Monitor Costs: Always track your usage and set alerts</li> <li>\ud83d\udd04 Cache Aggressively: Reduce redundant API calls</li> <li>\u26a1 Optimize Conservatively: Use minimal optimization settings</li> <li>\ud83d\udd12 Secure Keys: Never commit API keys to version control</li> <li>\ud83d\udcca Track Performance: Monitor both cost and quality metrics</li> </ol>"},{"location":"guides/cloud-inference/#cost-effective-workflow","title":"Cost-Effective Workflow","text":"<pre><code># Development (Cheap)\nsuper spec generate oracle dev_agent\n# Use gpt-4o-mini or gemini-pro\n\n# Testing (Balanced)\n# Switch to gpt-4o or claude-3-sonnet\n\n# Production (Best)\n# Use gpt-4o or claude-3-opus with caching\n\n# Monitor (Always)\nsuper observability dashboard\n</code></pre>"},{"location":"guides/cloud-inference/#quick-reference","title":"Quick Reference","text":"Phase Model Cost Use Case Development gpt-4o-mini $0.00015/1K Coding, testing Testing gpt-4o $0.005/1K Validation Production gpt-4o $0.005/1K Live deployment <p>Remember: SuperOptiX is powerful, but use it responsibly! Monitor your costs, start small, and scale up gradually. The key to successful cloud inference is finding the right balance between performance and cost. \ud83d\ude80</p> <p>\ud83d\udca1 Pro Tip: Always test with cheaper models first, and only use expensive models when you're confident in your configuration and have proper cost monitoring in place! </p>"},{"location":"guides/conversational-interface/","title":"Conversational Interface","text":""},{"location":"guides/conversational-interface/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX features a conversational interface that makes it easy to interact with the framework through natural language and slash commands.</p> <p>Just type:</p> <pre><code>super\n</code></pre> <p>That's it! No need for <code>super chat</code> or other subcommands.</p>"},{"location":"guides/conversational-interface/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"guides/conversational-interface/#first-run","title":"First Run","text":"<p>The first time you run <code>super</code>, you'll go through a quick setup:</p> <pre><code>$ super\n\n\ud83c\udf89 Welcome to SuperOptiX!\n\nFirst time setup - this will take about 30 seconds.\n\nStep 1/2: Choose AI Model Provider\n\n1. \ud83c\udfe0 Ollama (Local - FREE, Private, Offline)\n2. \u2601\ufe0f  OpenAI (Cloud - Paid)\n3. \u2601\ufe0f  Anthropic (Cloud - Paid)\n\nChoose [1-3]: 1\n</code></pre> <p>We recommend Ollama for: - Privacy (data stays local) - No API costs (free) - Offline capability</p>"},{"location":"guides/conversational-interface/#after-setup","title":"After Setup","text":"<p>Once configured, just type <code>super</code> to enter conversational mode:</p> <pre><code>$ super\n\nWelcome to SuperOptiX!\nUsing: ollama (llama3.1:8b)\n\nType /help for slash commands or just tell me what to do.\n\nSuperOptiX&gt; _\n</code></pre>"},{"location":"guides/conversational-interface/#slash-commands","title":"\ud83c\udfae Slash Commands","text":"<p>Slash commands provide quick access to SuperOptiX features:</p>"},{"location":"guides/conversational-interface/#configuration-models","title":"Configuration &amp; Models","text":"<pre><code>/model                    # Show current model\n/model list              # List all available models\n/model set &lt;model&gt;       # Switch model\n/config                  # Show configuration\n/config show             # Show all settings\n/config set &lt;k&gt; &lt;v&gt;      # Set configuration value\n</code></pre>"},{"location":"guides/conversational-interface/#help-documentation","title":"Help &amp; Documentation","text":"<pre><code>/help                    # Show all commands\n/ask &lt;question&gt;         # Ask about SuperOptiX\n/help &lt;topic&gt;           # Topic-specific help\n/docs &lt;topic&gt;           # Open documentation\n/examples               # Show example workflows\n</code></pre>"},{"location":"guides/conversational-interface/#project-agents","title":"Project &amp; Agents","text":"<pre><code>/status                 # Show project status\n/agents                 # List all agents\n/playbooks              # List all playbooks\n/templates              # Show available templates\n</code></pre>"},{"location":"guides/conversational-interface/#conversation","title":"Conversation","text":"<pre><code>/clear                  # Clear screen\n/history                # Show conversation history (coming soon)\n/exit, /quit            # Exit conversational mode\n</code></pre>"},{"location":"guides/conversational-interface/#example-session","title":"\ud83d\udcac Example Session","text":"<pre><code>$ super\n\nWelcome to SuperOptiX!\nUsing: ollama (llama3.1:8b)\n\nSuperOptiX&gt; /help\n\n[Shows all slash commands]\n\nSuperOptiX&gt; /model list\n\nAvailable AI Models\n\n\ud83c\udfe0 LOCAL MODELS (via Ollama):\nllama3.1:8b (current)\nqwen2.5:14b\n\n\u2601\ufe0f  CLOUD MODELS:\nOpenAI: gpt-4o, gpt-4o-mini\nAnthropic: claude-3.5-sonnet\n\nSuperOptiX&gt; /ask How do I add memory?\n\n\ud83d\udca1 How do I add memory to my agent?\n\nTo add memory to your agent, update your playbook's spec:\n\n```yaml\nspec:\n  memory:\n    enabled: true\n    enable_context_optimization: true\n    max_context_tokens: 2000\n</code></pre> <p>SuperOptiX&gt; /agents</p> <p>Agents</p> <p>Found 2 agent(s):</p> <p>\u2022 code_reviewer     Compiled   \u2022 customer_support     \u26a0\ufe0f  Not compiled</p> <p>SuperOptiX&gt; /playbooks</p> <p>Available Playbooks</p> <p>\ud83d\udce6 Library Templates (5):   \u2022 genie_playbook [memory, tools, rag]     General-purpose intelligent agent...   \u2022 security_agent_playbook [tools, rag]     Code security review agent...</p> <p>\ud83d\udcc1 Your Project (2):   \u2022 code_reviewer_playbook [memory, tools]   \u2022 customer_support_playbook [memory, rag]</p> <p>SuperOptiX&gt; /exit</p> <p>\ud83d\udc4b Goodbye! Happy building with SuperOptiX! <pre><code>---\n\n## \ud83d\udd27 Model Management\n\n### Viewing Models\n\n```bash\nSuperOptiX&gt; /model\n\nCurrent Model Configuration\n\n\u2022 Provider: ollama\n\u2022 Model: llama3.1:8b\n\u2022 API Base: http://localhost:11434\n\u2022 Status: Connected\n</code></pre></p>"},{"location":"guides/conversational-interface/#switching-models","title":"Switching Models","text":"<pre><code># Switch to different Ollama model\nSuperOptiX&gt; /model set qwen2.5:14b\n\nSwitched to: qwen2.5:14b\n\n# Switch to OpenAI (requires API key)\nSuperOptiX&gt; /model set gpt-4o\n\n\u26a0\ufe0f  OPENAI_API_KEY not set\nSet it with: /config set OPENAI_API_KEY sk-...\n</code></pre>"},{"location":"guides/conversational-interface/#listing-all-models","title":"Listing All Models","text":"<pre><code>SuperOptiX&gt; /model list\n\nAvailable AI Models\n\n\ud83c\udfe0 LOCAL MODELS (via Ollama):\n\nInstalled:\n  llama3.1:8b (current)\n  qwen2.5:14b\n\nAvailable to install:\n  \u2022 deepseek-coder:33b (19GB) - Best for coding\n  \u2022 mistral:7b (4.1GB) - Fast alternative\n\nInstall: ollama pull &lt;model&gt;\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\u2601\ufe0f  CLOUD MODELS:\n\nOpenAI (Requires OPENAI_API_KEY):\n  \u2022 gpt-4o - Best overall\n  \u2022 gpt-4o-mini - Fast and affordable\n\nAnthropic (Requires ANTHROPIC_API_KEY):\n  \u2022 claude-3.5-sonnet - Best for coding\n  \u2022 claude-3.5-haiku - Fast and affordable\n\nSet API key: /config set OPENAI_API_KEY sk-...\n</code></pre>"},{"location":"guides/conversational-interface/#learning-superoptix","title":"\ud83c\udf93 Learning SuperOptiX","text":""},{"location":"guides/conversational-interface/#ask-questions","title":"Ask Questions","text":"<p>Use <code>/ask</code> to learn about SuperOptiX features:</p> <pre><code>SuperOptiX&gt; /ask How do I add memory?\nSuperOptiX&gt; /ask What is GEPA?\nSuperOptiX&gt; /ask How do I add RAG?\nSuperOptiX&gt; /ask What is SuperSpec?\n</code></pre>"},{"location":"guides/conversational-interface/#view-examples","title":"View Examples","text":"<pre><code>SuperOptiX&gt; /examples\n\nExample Workflows\n\n1. Build and Optimize Agent:\n   super spec generate genie code_reviewer\n   super agent compile code_reviewer\n   super agent optimize code_reviewer --auto medium\n\n2. Quick Agent from Template:\n   super agent pull developer\n   super agent compile developer\n   super agent run developer --goal \"Build a CLI tool\"\n</code></pre>"},{"location":"guides/conversational-interface/#backwards-compatibility","title":"\ud83d\udd04 Backwards Compatibility","text":"<p>Traditional CLI commands still work:</p> <pre><code># Traditional commands (no conversational mode)\n$ super agent compile code_reviewer\n$ super agent optimize code_reviewer --auto medium\n$ super agent evaluate code_reviewer\n\n# These bypass conversational mode and run directly\n</code></pre> <p>When to use each:</p> <ul> <li>Conversational mode (<code>super</code>): Interactive exploration, learning, quick tasks</li> <li>Traditional CLI (<code>super agent ...</code>): Scripts, CI/CD, automation</li> </ul>"},{"location":"guides/conversational-interface/#configuration-privacy","title":"\ud83d\udd10 Configuration &amp; Privacy","text":""},{"location":"guides/conversational-interface/#local-mode-default","title":"Local Mode (Default)","text":"<ul> <li>No authentication required</li> <li>All data stays on your machine</li> <li>Privacy-first</li> </ul>"},{"location":"guides/conversational-interface/#configuration-storage","title":"Configuration Storage","text":"<pre><code>~/.superoptix/\n\u251c\u2500\u2500 config.yaml          # Your model choice and settings\n\u251c\u2500\u2500 credentials.yaml     # API keys (encrypted)\n\u2514\u2500\u2500 history/            # Conversation history\n</code></pre>"},{"location":"guides/conversational-interface/#changing-configuration","title":"Changing Configuration","text":"<pre><code># View configuration\nSuperOptiX&gt; /config\n\n# View detailed settings\nSuperOptiX&gt; /config show\n\n# Set API keys\nSuperOptiX&gt; /config set OPENAI_API_KEY sk-...\n\n# Reset configuration\nSuperOptiX&gt; /config reset\n</code></pre>"},{"location":"guides/conversational-interface/#tips-tricks","title":"\ud83d\udca1 Tips &amp; Tricks","text":"<p>1. Use /ask for Quick Help <pre><code>SuperOptiX&gt; /ask memory\nSuperOptiX&gt; /ask RAG\nSuperOptiX&gt; /ask GEPA\n</code></pre></p> <p>2. Discover Available Playbooks <pre><code>SuperOptiX&gt; /playbooks\n# Shows all library templates + your project playbooks\n</code></pre></p> <p>3. Check Project Status <pre><code>SuperOptiX&gt; /status\n# Quick overview of your project\n</code></pre></p> <p>4. Clear Screen <pre><code>SuperOptiX&gt; /clear\n# Clears conversation history\n</code></pre></p> <p>5. Traditional CLI Still Works <pre><code># Exit conversational mode\nSuperOptiX&gt; /exit\n\n# Run traditional command\n$ super agent compile code_reviewer\n\n# Re-enter conversational mode\n$ super\n</code></pre></p>"},{"location":"guides/conversational-interface/#coming-soon","title":"\ud83d\ude80 Coming Soon","text":"<p>Natural Language Mode: <pre><code>SuperOptiX&gt; Build a code review agent\nSuperOptiX&gt; Optimize my customer support agent\nSuperOptiX&gt; Show me optimization results\n</code></pre></p> <p>Currently in development! For now, use: - Slash commands in conversational mode - Traditional CLI for full functionality</p>"},{"location":"guides/conversational-interface/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"guides/conversational-interface/#ollama-not-running","title":"\"Ollama not running\"","text":"<p>Solution: 1. Install Ollama: https://ollama.com 2. Run: <code>ollama serve</code> 3. Install model: <code>ollama pull llama3.1:8b</code></p>"},{"location":"guides/conversational-interface/#not-in-a-superoptix-project","title":"\"Not in a SuperOptiX project\"","text":"<p>Solution: <pre><code># Exit conversational mode\nSuperOptiX&gt; /exit\n\n# Initialize project\n$ super init my_project\n$ cd my_project\n\n# Re-enter conversational mode\n$ super\n</code></pre></p>"},{"location":"guides/conversational-interface/#no-agents-found","title":"\"No agents found\"","text":"<p>Solution: <pre><code>SuperOptiX&gt; /exit\n\n# Create an agent\n$ super spec generate genie code_reviewer\n\n# Re-enter and check\n$ super\nSuperOptiX&gt; /agents\n</code></pre></p>"},{"location":"guides/conversational-interface/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Full documentation: https://superoptix.ai</li> <li>Getting started: Run <code>super docs</code> for comprehensive guide</li> <li>GitHub: https://github.com/SuperagenticAI/superoptix</li> </ul>"},{"location":"guides/conversational-interface/#summary","title":"\u2728 Summary","text":"<p>Just type <code>super</code> - That's it!</p> <ul> <li>\ud83c\udfae Slash commands for quick access</li> <li>\ud83d\udcac Ask questions with <code>/ask</code></li> <li>\ud83d\udd27 Manage models with <code>/model</code></li> <li>\ud83d\udccb Explore playbooks with <code>/playbooks</code></li> <li>\ud83d\ude80 Full backwards compatibility with traditional CLI</li> </ul> <p>Welcome to the future of SuperOptiX! \ud83c\udf89</p>"},{"location":"guides/crewai-integration/","title":"\ud83e\udd16 CrewAI Integration","text":""},{"location":"guides/crewai-integration/#overview","title":"Overview","text":"<p>SuperOptiX now supports CrewAI - a powerful multi-agent framework with 100,000+ certified developers. This integration brings the full SuperOptiX optimization workflow to CrewAI's role-based agent design.</p> <p>RLM support is experimental. Unified sandbox support is coming soon.</p> <p>Key Features: - Works with Ollama (unlike DeepAgents!) - GEPA optimization of role + goal + backstory - Role-based design perfect for optimization - Multi-agent native (start with single agents, scale to teams) - Standard SuperOptiX workflow (compile/evaluate/optimize/run)</p>"},{"location":"guides/crewai-integration/#quick-start","title":"Quick Start","text":""},{"location":"guides/crewai-integration/#install-superoptix-with-crewai","title":"Install SuperOptiX with CrewAI","text":"<pre><code>pip install superoptix[frameworks-crewai]\n</code></pre> <p>Includes: - crewai 1.2.0 - SuperOptiX core with GEPA 0.0.17</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency)</p>"},{"location":"guides/crewai-integration/#create-project","title":"Create Project","text":"<pre><code>super init my_project\ncd my_project\n</code></pre>"},{"location":"guides/crewai-integration/#pull-demo-agent","title":"Pull Demo Agent","text":"<pre><code>super agent pull researcher_crew\n</code></pre>"},{"location":"guides/crewai-integration/#compile","title":"Compile","text":"<pre><code>super agent compile researcher_crew --framework crewai\n</code></pre>"},{"location":"guides/crewai-integration/#evaluate","title":"Evaluate","text":"<pre><code>super agent evaluate researcher_crew\n# Result: 4/4 PASS (100%)! \ud83c\udf89\n</code></pre>"},{"location":"guides/crewai-integration/#optimize","title":"Optimize","text":"<pre><code>super agent optimize researcher_crew --framework crewai --auto medium\n</code></pre>"},{"location":"guides/crewai-integration/#run","title":"Run","text":"<pre><code>super agent run researcher_crew --framework crewai --goal \"AI trends in 2025\"\n</code></pre>"},{"location":"guides/crewai-integration/#how-it-works","title":"How It Works","text":""},{"location":"guides/crewai-integration/#superoptix-workflow-with-crewai","title":"SuperOptiX Workflow with CrewAI","text":"<pre><code>graph LR\n    A[SuperSpec YAML] --&gt;|compile| B[CrewAI Python]\n    B --&gt;|evaluate| C[BDD Scenarios]\n    C --&gt;|optimize| D[GEPA Optimizer]\n    D --&gt;|run| E[crew.kickoff]</code></pre>"},{"location":"guides/crewai-integration/#compile-superspec-crewai","title":"Compile: SuperSpec \u2192 CrewAI","text":"<p>Input: SuperSpec playbook (YAML)</p> <pre><code>persona:\n  role: Senior AI Researcher\n  goal: Uncover cutting-edge AI developments\n  backstory: You're a seasoned researcher...\n</code></pre> <p>Output: CrewAI Python code</p> <pre><code>agent = Agent(\n    role=\"Senior AI Researcher\",\n    goal=\"Uncover cutting-edge AI developments\",\n    backstory=\"You're a seasoned researcher...\",\n    llm=LLM(model=\"ollama/gpt-oss:20b\")\n)\n\ntask = Task(\n    description=\"Conduct research on {topic}\",\n    expected_output=\"Detailed research report\",\n    agent=agent\n)\n\ncrew = Crew(agents=[agent], tasks=[task])\nresult = crew.kickoff(inputs={\"topic\": \"AI\"})\n</code></pre>"},{"location":"guides/crewai-integration/#evaluate-test-on-bdd-scenarios","title":"Evaluate: Test on BDD Scenarios","text":"<pre><code>scenarios = [\n    {\n        \"input\": {\"topic\": \"AI frameworks\"},\n        \"expected\": {\"keywords\": [\"agent\", \"framework\", \"AI\"]}\n    }\n]\n\nfor scenario in scenarios:\n    result = crew.kickoff(inputs=scenario[\"input\"])\n    # Check keywords, validate output\n</code></pre>"},{"location":"guides/crewai-integration/#optimize-gepa-optimizes-agent-profile","title":"Optimize: GEPA Optimizes Agent Profile","text":"<p>Combined Variable (what GEPA optimizes):</p> <pre><code>Role: Senior AI Researcher\nGoal: Uncover cutting-edge AI developments\nBackstory: You're a seasoned researcher...\n</code></pre> <p>GEPA Process: 1. Generate 5-10 variations 2. Test each on training scenarios 3. Select best performer 4. Save optimized profile</p> <p>Example Optimized Output:</p> <pre><code>Role: AI Research Specialist with expertise in cutting-edge developments\nGoal: Discover, analyze, and synthesize breakthrough AI developments with comprehensive documentation\nBackstory: You're a distinguished researcher with 10+ years of experience in AI research, known for finding authoritative sources, synthesizing complex information, and presenting clear, actionable insights that drive innovation\n</code></pre>"},{"location":"guides/crewai-integration/#run-execute-with-optimized-profile","title":"Run: Execute with Optimized Profile","text":"<pre><code>super agent run researcher_crew --framework crewai --goal \"AI in 2025\"\n# Uses optimized role/goal/backstory \u2192 Better results!\n</code></pre>"},{"location":"guides/crewai-integration/#what-gepa-can-optimize","title":"What GEPA Can Optimize","text":""},{"location":"guides/crewai-integration/#primary-target-agent-profile-combined","title":"Primary Target: Agent Profile (Combined)","text":"<p>GEPA optimizes role + goal + backstory together as a single variable:</p> Component Impact Example role HIGH \"Senior AI Researcher\" \u2192 \"AI Research Specialist with expertise in...\" goal HIGH \"Uncover developments\" \u2192 \"Discover, analyze, and synthesize breakthrough...\" backstory MEDIUM \"You're a researcher\" \u2192 \"You're a distinguished researcher with 10+ years...\" <p>Why Combined? - Single optimization target - Holistic agent improvement - Better role-goal-backstory alignment - Easier to implement</p>"},{"location":"guides/crewai-integration/#advanced-task-optimization","title":"Advanced: Task Optimization","text":"Component Impact Description task.description HIGH What the task should accomplish task.expected_output HIGH What the task should produce system_template MEDIUM Advanced prompt template process LOW Sequential vs hierarchical (structural)"},{"location":"guides/crewai-integration/#creating-crewai-agents","title":"Creating CrewAI Agents","text":""},{"location":"guides/crewai-integration/#superspec-playbook-structure","title":"SuperSpec Playbook Structure","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: researcher_crew\n  id: researcher_crew\n  namespace: demo\n  version: 1.0.0\n\nspec:\n  target_framework: crewai\n\n  # LLM Configuration - Works with Ollama!\n  language_model:\n    provider: ollama\n    model: ollama:gpt-oss:20b\n    api_base: http://localhost:11434\n\n  # Input/Output Fields\n  input_fields:\n    - name: topic\n      type: str\n      required: true\n\n  output_fields:\n    - name: report\n      type: str\n      required: true\n\n  # CrewAI Agent Profile (GEPA optimizes this!)\n  persona:\n    role: Senior AI Researcher\n    goal: Uncover cutting-edge AI developments\n    backstory: |\n      You're a seasoned researcher with deep expertise in AI.\n      Known for finding authoritative sources and presenting\n      clear, actionable insights.\n    traits:\n      - analytical\n      - thorough\n\n  # CrewAI Task Configuration\n  tasks:\n    - name: research_task\n      description: |\n        Conduct comprehensive research on {topic}.\n        Find the most relevant and up-to-date information.\n      expected_output: |\n        A detailed research report with:\n        - 5-10 key findings\n        - Clear explanations\n        - Relevant context\n\n  # BDD Scenarios for Testing\n  feature_specifications:\n    scenarios:\n      - name: AI frameworks research\n        input:\n          topic: \"AI agent frameworks\"\n        expected_output:\n          report: \"Research report\"\n          expected_keywords:\n            - agent\n            - framework\n            - AI\n</code></pre>"},{"location":"guides/crewai-integration/#field-mapping-superspec-crewai","title":"Field Mapping: SuperSpec \u2192 CrewAI","text":"SuperSpec Field CrewAI Field Usage <code>persona.role</code> <code>Agent.role</code> Agent's identity <code>persona.goal</code> <code>Agent.goal</code> Agent's objective <code>persona.backstory</code> <code>Agent.backstory</code> Agent's background <code>tasks[0].description</code> <code>Task.description</code> What to do <code>tasks[0].expected_output</code> <code>Task.expected_output</code> What to produce <code>language_model</code> <code>Agent.llm</code> LLM configuration"},{"location":"guides/crewai-integration/#ollama-configuration","title":"Ollama Configuration","text":""},{"location":"guides/crewai-integration/#why-ollama-works-with-crewai","title":"Why Ollama Works with CrewAI","text":"<p>Unlike DeepAgents (which uses LangChain's <code>ChatOllama</code>), CrewAI has its own LLM class that supports Ollama directly:</p> <pre><code>from crewai.llm import LLM\n\nllm = LLM(\n    model=\"ollama/gpt-oss:20b\",\n    base_url=\"http://localhost:11434\"\n)\n</code></pre> <p>No <code>bind_tools()</code> limitations! ### Recommended Ollama Models</p> Model Size Best For <code>gpt-oss:20b</code> 20B Recommended - Excellent quality <code>gpt-oss:120b</code> 120B Maximum quality (requires 80GB+ RAM) <code>llama3.1:8b</code> 8B Fast, good for testing <code>qwen2.5:14b</code> 14B Good balance"},{"location":"guides/crewai-integration/#example-configuration","title":"Example Configuration","text":"<pre><code>language_model:\n  provider: ollama\n  model: ollama:gpt-oss:20b\n  temperature: 0.7\n  max_tokens: 3000\n  api_base: http://localhost:11434\n</code></pre>"},{"location":"guides/crewai-integration/#evaluation","title":"Evaluation","text":""},{"location":"guides/crewai-integration/#how-evaluation-works","title":"How Evaluation Works","text":"<ol> <li>Load BDD scenarios from playbook</li> <li>Execute crew on each scenario: <code>crew.kickoff(inputs=scenario.input)</code></li> <li>Compare outputs with expected keywords</li> <li>Calculate pass rate</li> </ol>"},{"location":"guides/crewai-integration/#example-evaluation","title":"Example Evaluation","text":"<pre><code>super agent evaluate researcher_crew\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Evaluating researcher_crew...\nTesting 4 BDD scenarios:\n\nAI frameworks research: PASS\nLangChain research: PASS\nMulti-agent systems: PASS\nGEPA optimization: PASS\n\n============================================================\nOverall: 4/4 PASS (100.0%)\n============================================================\n</code></pre>"},{"location":"guides/crewai-integration/#writing-good-bdd-scenarios","title":"Writing Good BDD Scenarios","text":"<p>Best Practices: - Use specific, measurable keywords - Test diverse inputs - Include edge cases - Use realistic examples</p> <p>Example:</p> <pre><code>scenarios:\n  - name: Specific technology research\n    input:\n      topic: \"What is LangChain?\"\n    expected_output:\n      report: \"LangChain explanation\"\n      expected_keywords:\n        - LangChain\n        - framework\n        - Python\n        - LLM\n\n  - name: Broad topic research\n    input:\n      topic: \"AI trends in 2025\"\n    expected_output:\n      report: \"AI trends report\"\n      expected_keywords:\n        - AI\n        - 2025\n        - trends\n        - future\n</code></pre>"},{"location":"guides/crewai-integration/#optimization","title":"Optimization","text":""},{"location":"guides/crewai-integration/#gepa-optimization-process","title":"GEPA Optimization Process","text":"<pre><code>super agent optimize researcher_crew --framework crewai --auto medium\n</code></pre> <p>What Happens:</p> <ol> <li>Split scenarios: Train (50%) + Validation (50%)</li> <li>Current profile: Extract role + goal + backstory</li> <li>Generate variations: 5-10 different agent profiles</li> <li>Test each: Run on training scenarios</li> <li>Validate: Test best candidates on validation set</li> <li>Select best: Save optimized profile</li> </ol>"},{"location":"guides/crewai-integration/#optimization-levels","title":"Optimization Levels","text":"Level Variations Iterations Time <code>light</code> 3-5 2-3 5-10 min <code>medium</code> 5-10 3-5 15-30 min <code>heavy</code> 10-20 5-10 30-60 min"},{"location":"guides/crewai-integration/#expected-improvement","title":"Expected Improvement","text":"Baseline After GEPA Improvement 60-70% 75-85% +15-20% 70-80% 80-90% +10-15% 80-90% 85-95% +5-10%"},{"location":"guides/crewai-integration/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/crewai-integration/#multi-agent-crews","title":"Multi-Agent Crews","text":"<p>For multi-agent teams, optimize each agent individually:</p> <pre><code># Optimize agent 1\npersona:\n  agents:\n    - name: researcher\n      role: Senior Researcher\n      goal: Find information\n\n    # Optimize agent 2\n    - name: analyst\n      role: Data Analyst\n      goal: Analyze findings\n\n    # Optimize agent 3\n    - name: writer\n      role: Report Writer\n      goal: Create report\n</code></pre>"},{"location":"guides/crewai-integration/#task-description-optimization","title":"Task Description Optimization","text":"<p>You can optimize task descriptions along with agent profiles: - Optimize <code>task.description</code> - Optimize <code>task.expected_output</code> - Joint optimization (agent + task)</p>"},{"location":"guides/crewai-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/crewai-integration/#common-issues","title":"Common Issues","text":""},{"location":"guides/crewai-integration/#module-researcher_crew_pipeline-has-no-attribute-researchercrewpipeline","title":"Module 'researcher_crew_pipeline' has no attribute 'ResearcherCrewPipeline'","text":"<p>Cause: Mismatch between agent <code>id</code> and class name.</p> <p>Solution: Ensure <code>metadata.name</code> matches <code>metadata.id</code>:</p> <pre><code>metadata:\n  name: researcher_crew\n  id: researcher_crew\n</code></pre>"},{"location":"guides/crewai-integration/#crewai-not-installed","title":"CrewAI Not Installed","text":"<p>Error: <code>ImportError: No module named 'crewai'</code></p> <p>Solution:</p> <pre><code>pip install crewai\n</code></pre>"},{"location":"guides/crewai-integration/#ollama-model-not-found","title":"Ollama Model Not Found","text":"<p>Error: <code>Model 'gpt-oss:20b' not found</code></p> <p>Solution:</p> <pre><code>ollama pull gpt-oss:20b\n</code></pre>"},{"location":"guides/crewai-integration/#low-pass-rate","title":"Low Pass Rate","text":"<p>Cause: BDD scenarios too strict or agent profile unclear.</p> <p>Solution: 1. Review BDD keywords (use more flexible matching) 2. Run optimization to improve agent profile 3. Test with better model (gpt-oss:20b \u2192 gpt-oss:120b)</p>"},{"location":"guides/crewai-integration/#comparison-crewai-vs-other-frameworks","title":"Comparison: CrewAI vs Other Frameworks","text":"Feature DSPy DeepAgents OpenAI SDK CrewAI Ollama Support Multi-Agent Sub-agents Handoffs Native Role-Based Collaboration Optimizable Vars 10+ 1 1 3 Best For Prompts Planning Simple Teams Community 5K 1K 50K+ 100K+"},{"location":"guides/crewai-integration/#when-to-use-crewai","title":"When to Use CrewAI","text":"<p>Best for: - Multi-agent collaboration - Role-based task distribution - Complex crew dynamics - Team-based workflows - Deployment systems</p> <p>Not ideal for: - Simple single-agent tasks (use DSPy or OpenAI SDK) - Maximum prompt optimization (use DSPy) - Planning-heavy workflows (use DeepAgents)</p>"},{"location":"guides/crewai-integration/#examples","title":"Examples","text":""},{"location":"guides/crewai-integration/#example-1-research-crew","title":"Example 1: Research Crew","text":"<pre><code>persona:\n  role: Senior AI Researcher\n  goal: Uncover cutting-edge AI developments\n  backstory: Expert researcher with 10+ years experience\n</code></pre> <p>Use Case: Research assistant, market analysis, competitive intelligence</p>"},{"location":"guides/crewai-integration/#example-2-content-creation-crew","title":"Example 2: Content Creation Crew","text":"<pre><code>persona:\n  role: Professional Content Writer\n  goal: Create engaging, SEO-optimized content\n  backstory: Award-winning writer with expertise in technical communication\n</code></pre> <p>Use Case: Blog posts, documentation, marketing content</p>"},{"location":"guides/crewai-integration/#example-3-data-analysis-crew","title":"Example 3: Data Analysis Crew","text":"<pre><code>persona:\n  role: Senior Data Analyst\n  goal: Extract insights from complex datasets\n  backstory: Data science expert with statistical modeling background\n</code></pre> <p>Use Case: Data analysis, visualization, reporting</p>"},{"location":"guides/crewai-integration/#resources","title":"Resources","text":""},{"location":"guides/crewai-integration/#official-documentation","title":"Official Documentation","text":"<ul> <li>CrewAI Docs</li> <li>CrewAI GitHub</li> <li>SuperOptiX Guide</li> </ul>"},{"location":"guides/crewai-integration/#community","title":"Community","text":"<ul> <li>CrewAI Discord</li> <li>SuperOptiX Forum</li> </ul>"},{"location":"guides/crewai-integration/#learning","title":"Learning","text":"<ul> <li>CrewAI Courses</li> <li>SuperOptiX Tutorials</li> </ul>"},{"location":"guides/crewai-integration/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Try the Demo: <pre><code>super init my_project\ncd my_project\nsuper agent pull researcher_crew\nsuper agent compile researcher_crew --framework crewai\nsuper agent evaluate researcher_crew\n</code></pre></p> </li> <li> <p>Create Your Own:</p> </li> <li>Copy <code>researcher_crew_playbook.yaml</code></li> <li>Customize role/goal/backstory</li> <li>Add your BDD scenarios</li> <li> <p>Compile and test</p> </li> <li> <p>Optimize: <pre><code>super agent optimize researcher_crew --framework crewai --auto medium\nsuper agent evaluate researcher_crew  # See improvement!\n</code></pre></p> </li> <li> <p>Share:</p> </li> <li>Contribute your agents to SuperOptiX marketplace</li> <li>Share optimization results with the community</li> </ol>"},{"location":"guides/crewai-integration/#support","title":"Support","text":"<p>Need help? We're here for you:</p> <ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83d\udce7 Email Support</li> </ul> <p>Happy Optimizing! \ud83d\ude80</p>"},{"location":"guides/crewai-task-optimization/","title":"CrewAI Advanced Task Optimization","text":""},{"location":"guides/crewai-task-optimization/#overview","title":"Overview","text":"<p>SuperOptiX supports combined agent + task optimization for CrewAI. GEPA optimizes both the agent profile AND task configuration together for maximum agent-task alignment.</p> <p>Key Features: - Combined optimization: agent profile + task configuration - Better agent-task alignment - Improved performance over agent-only optimization - Automatic parsing and extraction - Single optimization variable for holistic improvement</p>"},{"location":"guides/crewai-task-optimization/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install superoptix[frameworks-crewai]\n</code></pre> <p>Includes: - crewai 1.2.0 - SuperOptiX core with GEPA 0.0.17</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency)</p>"},{"location":"guides/crewai-task-optimization/#what-gets-optimized","title":"What Gets Optimized","text":""},{"location":"guides/crewai-task-optimization/#combined-variable","title":"Combined Variable","text":"<p>GEPA optimizes a single combined variable containing:</p>"},{"location":"guides/crewai-task-optimization/#agent-profile","title":"Agent Profile","text":"<ul> <li>role: Agent's identity and expertise</li> <li>goal: Agent's primary objective</li> <li>backstory: Agent's background and capabilities</li> </ul>"},{"location":"guides/crewai-task-optimization/#task-configuration","title":"Task Configuration","text":"<ul> <li>task description: What the agent should accomplish</li> <li>expected output: What the agent should produce</li> </ul>"},{"location":"guides/crewai-task-optimization/#example-combined-profile","title":"Example Combined Profile","text":"<p>Before Optimization:</p> <pre><code>Role: Professional Content Writer\nGoal: Create engaging content\nBackstory: You are an experienced writer\n\nTask Description: Write about {topic}\nExpected Output: A good article\n</code></pre> <p>After GEPA Optimization:</p> <pre><code>Role: Senior Content Strategist specializing in technical communication\nGoal: Create highly engaging, SEO-optimized content that educates and converts readers\nBackstory: Distinguished content strategist with 10+ years creating award-winning articles for Fortune 500 companies\n\nTask Description: Create comprehensive, research-backed content about {topic}, focusing on:\n- Current trends and emerging developments\n- Real-world applications and case studies\n- Expert insights and authoritative perspectives\n- Actionable takeaways for readers\n- SEO optimization with natural keyword integration\n\nExpected Output: A publication-ready article containing:\n- Captivating headline and hook\n- Executive summary with key insights\n- 5-7 well-structured sections\n- Minimum 3 real-world examples\n- Data-driven insights with citations\n- Practical recommendations\n- Engaging conclusion\n- 800-1200 words in markdown format\n</code></pre> <p>Impact: 20-30% improvement in output quality!</p>"},{"location":"guides/crewai-task-optimization/#why-combined-optimization-works-better","title":"Why Combined Optimization Works Better","text":""},{"location":"guides/crewai-task-optimization/#comparison","title":"Comparison","text":"Aspect Agent Only Combined (Agent + Task) Variables role + goal + backstory role + goal + backstory + task + output Optimization Agent profile only Agent + Task together Alignment Manual Automatic Improvement +10-15% +20-30% Quality Good Excellent"},{"location":"guides/crewai-task-optimization/#benefits","title":"Benefits","text":"<ol> <li>Better Alignment</li> <li>Agent capabilities match task requirements</li> <li>Task instructions leverage agent strengths</li> <li> <p>No mismatch between agent abilities and task expectations</p> </li> <li> <p>Holistic Improvement</p> </li> <li>GEPA optimizes both components together</li> <li>Considers agent-task synergy</li> <li> <p>Better than optimizing separately</p> </li> <li> <p>Clearer Expectations</p> </li> <li>Task description matches agent expertise</li> <li>Expected output aligns with agent capabilities</li> <li>More consistent results</li> </ol>"},{"location":"guides/crewai-task-optimization/#quick-start","title":"Quick Start","text":""},{"location":"guides/crewai-task-optimization/#step-1-pull-demo-agent","title":"Step 1: Pull Demo Agent","text":"<pre><code># Pull the content creator demo (combined optimization)\nsuper agent pull content_creator_crew\n</code></pre>"},{"location":"guides/crewai-task-optimization/#step-2-compile","title":"Step 2: Compile","text":"<pre><code>super agent compile content_creator_crew --framework crewai\n</code></pre>"},{"location":"guides/crewai-task-optimization/#step-3-evaluate-baseline","title":"Step 3: Evaluate Baseline","text":"<pre><code>super agent evaluate content_creator_crew\n</code></pre> <p>Output: <pre><code>Pass Rate: 75% (3/4 scenarios)\n</code></pre></p>"},{"location":"guides/crewai-task-optimization/#step-4-optimize-combined","title":"Step 4: Optimize (Combined)","text":"<pre><code>super agent optimize content_creator_crew --auto medium\n</code></pre> <p>GEPA optimizes: - Agent profile (role + goal + backstory) - Task configuration (description + expected_output) - Both together for maximum alignment!</p>"},{"location":"guides/crewai-task-optimization/#step-5-re-evaluate","title":"Step 5: Re-evaluate","text":"<pre><code>super agent evaluate content_creator_crew  # automatically loads optimized weights\n</code></pre> <p>Output: <pre><code>Pass Rate: 100% (4/4 scenarios)  \u2190 Improved!\n</code></pre></p>"},{"location":"guides/crewai-task-optimization/#creating-combined-optimization-agents","title":"Creating Combined Optimization Agents","text":""},{"location":"guides/crewai-task-optimization/#playbook-structure","title":"Playbook Structure","text":"<pre><code># content_creator_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: content_creator\nspec:\n  target_framework: crewai\n\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n\n  # Agent Profile (optimizable)\n  persona:\n    role: Content Creator\n    goal: Create engaging content\n    backstory: |\n      Experienced writer with content strategy expertise.\n\n  # Task Configuration (optimizable)\n  tasks:\n    - name: write_content\n      description: Write compelling content about the given topic\n      expected_output: Polished article ready for publication\n\n  # BDD Scenarios\n  feature_specifications:\n    scenarios:\n      - name: Write tech article\n        input:\n          topic: \"AI agents\"\n        expected_output:\n          article: \"Comprehensive article about AI agents\"\n          expected_keywords:\n            - AI agents\n            - automation\n            - intelligent systems\n</code></pre>"},{"location":"guides/crewai-task-optimization/#how-it-works","title":"How It Works","text":"<ol> <li>Compile: Template extracts both agent profile AND task config</li> <li>Evaluate: Tests agent on BDD scenarios</li> <li>Optimize: GEPA improves both agent profile AND task config together</li> <li>Re-evaluate: Tests improved combined variable</li> </ol>"},{"location":"guides/crewai-task-optimization/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guides/crewai-task-optimization/#multi-task-agents","title":"Multi-Task Agents","text":"<pre><code>persona:\n  role: Research Analyst\n  goal: Conduct comprehensive research\n  backstory: Expert analyst with methodology expertise\n\ntasks:\n  - name: research\n    description: Research the topic thoroughly\n    expected_output: Detailed research findings\n\n  - name: analyze\n    description: Analyze the research findings\n    expected_output: Analytical insights and recommendations\n</code></pre> <p>GEPA optimizes each task's description and expected_output along with the agent profile.</p>"},{"location":"guides/crewai-task-optimization/#custom-tools-integration","title":"Custom Tools Integration","text":"<pre><code>persona:\n  role: Data Analyst\n  tools:\n    - web_search\n    - calculator\n    - file_reader\n\ntasks:\n  - name: analyze_data\n    description: Analyze data using available tools\n    expected_output: Data analysis report with charts\n</code></pre>"},{"location":"guides/crewai-task-optimization/#how-gepa-parses-combined-variables","title":"How GEPA Parses Combined Variables","text":"<p>GEPA automatically extracts and updates:</p> <pre><code># Example parsing\ncombined_profile = \"\"\"\nRole: Content Creator\nGoal: Create content\nBackstory: Expert writer\n\nTask Description: Write about topic\nExpected Output: Article\n\"\"\"\n\n# GEPA extracts:\nrole = extract(\"Role:\", profile)\ngoal = extract(\"Goal:\", profile)\nbackstory = extract(\"Backstory:\", profile)\ntask_description = extract(\"Task Description:\", profile)\nexpected_output = extract(\"Expected Output:\", profile)\n\n# GEPA optimizes each, then recombines\n</code></pre>"},{"location":"guides/crewai-task-optimization/#optimization-results","title":"Optimization Results","text":""},{"location":"guides/crewai-task-optimization/#example-content-creator","title":"Example: Content Creator","text":"<p>Baseline: <pre><code>Pass Rate: 75% (3/4 scenarios)\nAverage Quality: 7.2/10\n</code></pre></p> <p>After Combined Optimization: <pre><code>Pass Rate: 100% (4/4 scenarios)  \u2190 +25%\nAverage Quality: 9.5/10           \u2190 +2.3 points\n</code></pre></p> <p>Improvements: - More specific and targeted role - Clearer goal with measurable outcomes - Richer backstory with relevant expertise - Detailed task instructions - Structured expected output specifications</p>"},{"location":"guides/crewai-task-optimization/#best-practices","title":"Best Practices","text":""},{"location":"guides/crewai-task-optimization/#define-clear-tasks","title":"Define Clear Tasks","text":"<pre><code># Good: Specific task\ntasks:\n  - name: write_article\n    description: Write a 1000-word article about {topic} with examples\n    expected_output: Article with intro, body, conclusion, examples\n\n# Bad: Vague task\ntasks:\n  - name: write\n    description: Write something\n    expected_output: Text\n</code></pre>"},{"location":"guides/crewai-task-optimization/#match-agent-to-task","title":"Match Agent to Task","text":"<pre><code># Agent expertise should match task requirements\npersona:\n  role: Technical Writer        # \u2190 Matches task\n  backstory: Expert in technical documentation\n\ntasks:\n  - name: write_docs\n    description: Write technical documentation  # \u2190 Matches agent\n</code></pre>"},{"location":"guides/crewai-task-optimization/#use-bdd-scenarios","title":"Use BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: Test real use case\n      input:\n        topic: \"Actual topic you'll use\"\n      expected_output:\n        article: \"Actual format you need\"\n</code></pre>"},{"location":"guides/crewai-task-optimization/#start-simple-optimize","title":"Start Simple, Optimize","text":"<pre><code># Start with basic task\ntasks:\n  - description: Research topic\n    expected_output: Research report\n\n# Let GEPA optimize to:\n# description: \"Conduct systematic research on {topic} including...\"\n# expected_output: \"Comprehensive report with: executive summary, findings, recommendations...\"\n</code></pre>"},{"location":"guides/crewai-task-optimization/#backward-compatibility","title":"Backward Compatibility","text":""},{"location":"guides/crewai-task-optimization/#agent-only-optimization","title":"Agent-Only Optimization","text":"<p>Old playbooks still work! If you don't specify tasks, GEPA optimizes agent profile only:</p> <pre><code># Agent-only (still supported)\npersona:\n  role: Assistant\n  goal: Help users\n  backstory: Helpful agent\n# No tasks section - uses agent profile only\n</code></pre>"},{"location":"guides/crewai-task-optimization/#combined-optimization","title":"Combined Optimization","text":"<p>New playbooks with tasks get combined optimization:</p> <pre><code># Combined (recommended)\npersona:\n  role: Assistant\n  goal: Help users\n  backstory: Helpful agent\n\ntasks:\n  - description: \"Detailed task...\"\n    expected_output: \"Specific output...\"\n# Has tasks section - uses combined optimization\n</code></pre>"},{"location":"guides/crewai-task-optimization/#example-research-agent","title":"Example: Research Agent","text":""},{"location":"guides/crewai-task-optimization/#initial-configuration","title":"Initial Configuration","text":"<pre><code>persona:\n  role: Researcher\n  goal: Find information\n  backstory: Research professional\n\ntasks:\n  - description: Research topic\n    expected_output: Research results\n</code></pre> <p>Initial Results: 60% pass rate</p>"},{"location":"guides/crewai-task-optimization/#after-optimization","title":"After Optimization","text":"<pre><code>persona:\n  role: Senior Research Analyst with academic methodology expertise\n  goal: Conduct comprehensive, evidence-based research with systematic approach\n  backstory: Distinguished research professional with PhD-level training...\n\ntasks:\n  - description: |\n      Conduct systematic research on {topic}:\n      1. Identify key sources\n      2. Analyze credibility\n      3. Extract insights\n      4. Synthesize findings\n      5. Provide citations\n    expected_output: |\n      Comprehensive research report containing:\n      - Executive summary\n      - Methodology explanation\n      - Key findings with evidence\n      - Analysis and insights\n      - Recommendations\n      - Full source citations\n</code></pre> <p>Optimized Results: 95% pass rate (+35%)</p>"},{"location":"guides/crewai-task-optimization/#cli-commands","title":"CLI Commands","text":"<pre><code># Basic workflow\nsuper agent pull content_creator_crew\nsuper agent compile content_creator_crew\nsuper agent evaluate content_creator_crew\nsuper agent optimize content_creator_crew --auto medium\nsuper agent evaluate content_creator_crew  # automatically loads optimized weights\n\n# Advanced options\nsuper agent optimize content_creator_crew \\\n  --auto intensive \\\n  --reflection-lm qwen3:8b \\\n  --minibatch-size 5\n\n# Run optimized agent\nsuper agent run content_creator_crew  # automatically loads optimized weights\n</code></pre>"},{"location":"guides/crewai-task-optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/crewai-task-optimization/#issue-low-pass-rate-after-optimization","title":"Issue: Low Pass Rate After Optimization","text":"<p>Solution: Check BDD scenarios are realistic</p> <pre><code># Good scenarios\nscenarios:\n  - input: {topic: \"AI agents\"}\n    expected_output:\n      article: \"Article about AI agents\"\n      expected_keywords: [\"AI\", \"agents\", \"automation\"]\n</code></pre>"},{"location":"guides/crewai-task-optimization/#issue-agent-not-using-task-instructions","title":"Issue: Agent Not Using Task Instructions","text":"<p>Solution: Ensure tasks are properly defined in playbook</p> <pre><code># Must have tasks section\ntasks:\n  - name: my_task\n    description: \"Clear description\"\n    expected_output: \"Specific output format\"\n</code></pre>"},{"location":"guides/crewai-task-optimization/#next-steps","title":"Next Steps","text":"<ol> <li>Try combined optimization: Pull <code>content_creator_crew</code> demo</li> <li>Create your own: Add tasks to your CrewAI playbooks</li> <li>Compare results: Test agent-only vs combined optimization</li> <li>Read more: See CrewAI Integration Guide</li> </ol>"},{"location":"guides/crewai-task-optimization/#related-guides","title":"Related Guides","text":"<ul> <li>CrewAI Integration</li> <li>GEPA Optimization</li> <li>Multi-Framework Guide</li> <li>Evaluation &amp; Testing</li> </ul> <p>CrewAI with SuperOptiX offers the most advanced agent + task optimization available!</p>"},{"location":"guides/dataset-import/","title":"Dataset Import Guide","text":"<p>Import external datasets for agent training and evaluation, scaling beyond YAML scenarios to thousands of examples.</p>"},{"location":"guides/dataset-import/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX now supports importing external datasets in addition to BDD scenarios. This allows you to: - Use existing datasets (CSV, JSON, Parquet, HuggingFace) - Scale to 10,000+ examples (vs 5-10 YAML scenarios) - Leverage standard ML workflows - Access HuggingFace's 100,000+ datasets - Mix datasets with BDD scenarios for best results</p>"},{"location":"guides/dataset-import/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/dataset-import/#1-create-a-dataset","title":"1. Create a Dataset","text":"<pre><code># Create CSV file\ncat &lt;&lt; 'EOF' &gt; data/sentiment_train.csv\ntext,label\n\"This is amazing!\",positive\n\"This is terrible\",negative\n\"It's okay\",neutral\nEOF\n</code></pre>"},{"location":"guides/dataset-import/#2-configure-in-playbook","title":"2. Configure in Playbook","text":"<pre><code># agent_playbook.yaml\nspec:\n  datasets:\n    - name: training_data\n      source: ./data/sentiment_train.csv\n      format: csv\n      mapping:\n        input: text\n        output: label\n        input_field_name: text\n        output_field_name: sentiment\n      limit: 1000\n      shuffle: true\n</code></pre>"},{"location":"guides/dataset-import/#3-preview-dataset","title":"3. Preview Dataset","text":"<pre><code>super agent dataset preview my_agent --limit 5\n</code></pre>"},{"location":"guides/dataset-import/#4-use-it","title":"4. Use It","text":"<pre><code>super agent compile my_agent\n# \u2192 \ud83d\udcca Loaded 1000 examples from dataset!\n\nsuper agent evaluate my_agent\n# \u2192 Uses all 1000 examples!\n\nsuper agent optimize my_agent --auto light\n# \u2192 GEPA trains on all 1000 examples!\n</code></pre>"},{"location":"guides/dataset-import/#supported-formats","title":"\ud83d\udccb Supported Formats","text":""},{"location":"guides/dataset-import/#csv-files","title":"CSV Files","text":"<pre><code>datasets:\n  - name: csv_training\n    source: ./data/train.csv\n    format: csv\n    mapping:\n      input: text_column\n      output: label_column\n      input_field_name: text\n      output_field_name: sentiment\n    limit: 5000\n    shuffle: true\n</code></pre> <p>Requirements: pandas (already installed)</p>"},{"location":"guides/dataset-import/#json-files","title":"JSON Files","text":"<pre><code>datasets:\n  - name: json_training\n    source: ./data/train.json\n    format: json\n    mapping:\n      input: question_field\n      output: answer_field\n</code></pre> <p>JSON Format: <pre><code>[\n  {\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence\"},\n  {\"question\": \"What is ML?\", \"answer\": \"Machine Learning\"}\n]\n</code></pre></p>"},{"location":"guides/dataset-import/#jsonl-files-recommended-for-large-datasets","title":"JSONL Files (Recommended for Large Datasets)","text":"<pre><code>datasets:\n  - name: jsonl_training\n    source: ./data/train.jsonl\n    format: jsonl\n    mapping:\n      input: text\n      output: label\n    limit: 10000\n</code></pre> <p>JSONL Format (one JSON per line): <pre><code>{\"text\": \"Great product!\", \"label\": \"positive\"}\n{\"text\": \"Poor quality\", \"label\": \"negative\"}\n{\"text\": \"It's okay\", \"label\": \"neutral\"}\n</code></pre></p>"},{"location":"guides/dataset-import/#parquet-files-best-for-big-data","title":"Parquet Files (Best for Big Data)","text":"<pre><code>datasets:\n  - name: parquet_training\n    source: ./data/train.parquet\n    format: parquet\n    mapping:\n      input: text\n      output: label\n</code></pre> <p>Requirements: pandas, pyarrow (already installed)</p> <p>Benefits: - Compressed (smaller files) - Fast loading - Preserves data types - Industry standard for big data</p>"},{"location":"guides/dataset-import/#huggingface-datasets","title":"HuggingFace Datasets \ud83d\udd25","text":"<pre><code>datasets:\n  - name: imdb_sentiment\n    source: huggingface:imdb\n    format: huggingface\n    mapping:\n      input: text\n      output: label\n    split: train\n    limit: 10000\n    shuffle: true\n</code></pre> <p>Popular Datasets: - <code>huggingface:imdb</code> - Movie reviews (50K) - <code>huggingface:ag_news</code> - News classification (120K) - <code>huggingface:sst2</code> - Sentiment (67K) - <code>huggingface:squad</code> - Q&amp;A (87K) - <code>huggingface:glue:sst2</code> - With subset - ... 100,000+ more!</p> <p>Requirements: <code>pip install datasets</code></p>"},{"location":"guides/dataset-import/#advanced-usage","title":"\ud83c\udfaf Advanced Usage","text":""},{"location":"guides/dataset-import/#multi-column-mapping","title":"Multi-Column Mapping","text":"<pre><code>datasets:\n  - name: qa_dataset\n    source: ./data/qa.csv\n    format: csv\n    mapping:\n      input:\n        question: question_column\n        context: context_column\n      output:\n        answer: answer_column\n        confidence: confidence_column\n</code></pre>"},{"location":"guides/dataset-import/#multiple-datasets","title":"Multiple Datasets","text":"<pre><code>datasets:\n  # Training data\n  - name: train_set\n    source: ./data/train.csv\n    format: csv\n    mapping: {input: text, output: label}\n    limit: 10000\n\n  # Test data\n  - name: test_set\n    source: ./data/test.csv\n    format: csv\n    mapping: {input: text, output: label}\n    limit: 1000\n\n  # HuggingFace data\n  - name: hf_data\n    source: huggingface:imdb\n    format: huggingface\n    mapping: {input: text, output: label}\n    split: train\n    limit: 5000\n</code></pre> <p>Result: 16,000 total examples!</p>"},{"location":"guides/dataset-import/#mix-datasets-bdd-scenarios-recommended","title":"Mix Datasets + BDD Scenarios (Recommended!)","text":"<pre><code># Bulk training data from datasets\ndatasets:\n  - name: training_data\n    source: ./data/large_dataset.csv\n    format: csv\n    mapping: {input: text, output: label}\n    limit: 10000\n\n# Specific edge cases from BDD scenarios\nfeature_specifications:\n  scenarios:\n  - name: sarcasm_test\n    input: {text: \"Oh great, another bug\"}\n    expected_output: {sentiment: negative}\n\n  - name: mixed_sentiment\n    input: {text: \"Good product but poor service\"}\n    expected_output: {sentiment: neutral}\n</code></pre> <p>Benefits: - 10,000 examples for robust training - Specific edge cases for testing - Best of both worlds!</p>"},{"location":"guides/dataset-import/#cli-commands","title":"\ud83d\udee0\ufe0f CLI Commands","text":""},{"location":"guides/dataset-import/#preview-dataset","title":"Preview Dataset","text":"<pre><code>super agent dataset preview my_agent --limit 10\n</code></pre> <p>Output: <pre><code>Preview: training_data (showing 10 of 5000)\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 #    \u2502 Input: text                \u2502 Output: label \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1    \u2502 This is amazing!           \u2502 positive      \u2502\n\u2502 2    \u2502 Terrible experience        \u2502 negative      \u2502\n\u2502 ...  \u2502 ...                        \u2502 ...           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"guides/dataset-import/#dataset-info","title":"Dataset Info","text":"<pre><code>super agent dataset info my_agent\n</code></pre> <p>Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udcca training_data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Name: training_data                                      \u2502\n\u2502 Source: ./data/train.csv                                 \u2502\n\u2502 Format: csv                                              \u2502\n\u2502 Total Examples: 5000                                     \u2502\n\u2502 Split: train                                             \u2502\n\u2502 Shuffled: True                                           \u2502\n\u2502 Input Fields: text                                       \u2502\n\u2502 Output Fields: sentiment                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nTotal Examples Across All Datasets: 5000\n</code></pre></p>"},{"location":"guides/dataset-import/#validate-dataset","title":"Validate Dataset","text":"<pre><code>super agent dataset validate my_agent\n</code></pre> <p>Output: <pre><code>Validating 1 dataset(s)...\ntraining_data: Valid\nAll datasets valid!\n</code></pre></p>"},{"location":"guides/dataset-import/#examples","title":"\ud83d\udcca Examples","text":""},{"location":"guides/dataset-import/#example-1-sentiment-analysis-with-csv","title":"Example 1: Sentiment Analysis with CSV","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Sentiment Analyzer\n  id: sentiment_analyzer\nspec:\n  language_model:\n    model: llama3.1:8b\n\n  input_fields:\n  - name: text\n    type: string\n\n  output_fields:\n  - name: sentiment\n    type: string\n\n  datasets:\n  - name: reviews\n    source: ./data/customer_reviews.csv\n    format: csv\n    mapping:\n      input: review_text\n      output: sentiment_label\n      input_field_name: text\n      output_field_name: sentiment\n    limit: 5000\n    shuffle: true\n\n  tasks:\n  - name: classify\n    instruction: Classify sentiment as positive, negative, or neutral\n</code></pre>"},{"location":"guides/dataset-import/#example-2-qa-with-huggingface","title":"Example 2: Q&amp;A with HuggingFace","text":"<pre><code>spec:\n  datasets:\n  - name: squad_qa\n    source: huggingface:squad\n    format: huggingface\n    mapping:\n      input:\n        question: question\n        context: context\n      output:\n        answer: answer\n    split: train\n    limit: 10000\n</code></pre>"},{"location":"guides/dataset-import/#example-3-multi-format-mix","title":"Example 3: Multi-Format Mix","text":"<pre><code>datasets:\n  # Main training data (CSV)\n  - name: main_training\n    source: ./data/train.csv\n    format: csv\n    mapping: {input: text, output: label}\n    limit: 8000\n\n  # Validation data (JSON)\n  - name: validation\n    source: ./data/val.json\n    format: json\n    mapping: {input: text, output: label}\n    limit: 1000\n\n  # External data (HuggingFace)\n  - name: external\n    source: huggingface:sst2\n    format: huggingface\n    mapping: {input: sentence, output: label}\n    split: train\n    limit: 5000\n</code></pre> <p>Total: 14,000 examples from 3 sources!</p>"},{"location":"guides/dataset-import/#best-practices","title":"\ud83c\udf93 Best Practices","text":""},{"location":"guides/dataset-import/#1-start-small-scale-up","title":"1. Start Small, Scale Up","text":"<pre><code># Development\ndatasets:\n  - source: ./data/train.csv\n    limit: 100  # Small for fast iteration\n\n# Production\ndatasets:\n  - source: ./data/train.csv\n    limit: 10000  # Full dataset\n</code></pre>"},{"location":"guides/dataset-import/#2-always-shuffle","title":"2. Always Shuffle","text":"<pre><code>datasets:\n  - shuffle: true  # Prevents ordering bias\n</code></pre>"},{"location":"guides/dataset-import/#3-use-limits","title":"3. Use Limits","text":"<pre><code>datasets:\n  - limit: 5000  # Control training time/cost\n</code></pre>"},{"location":"guides/dataset-import/#4-validate-first","title":"4. Validate First","text":"<pre><code>super agent dataset validate my_agent\nsuper agent dataset preview my_agent\nsuper agent dataset info my_agent\n</code></pre>"},{"location":"guides/dataset-import/#5-mix-datasets-bdd","title":"5. Mix Datasets + BDD","text":"<pre><code>datasets:        # Bulk data\n  - source: ./data/train.csv\n    limit: 5000\n\nfeature_specifications:  # Edge cases\n  scenarios:\n  - name: edge_case_1\n  - name: edge_case_2\n</code></pre>"},{"location":"guides/dataset-import/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/dataset-import/#issue-file-not-found","title":"Issue: File Not Found","text":"<pre><code>Failed to load dataset: No such file or directory\n</code></pre> <p>Fix: Use absolute paths or check relative path from playbook location</p> <pre><code># Use absolute path\nsource: /full/path/to/data.csv\n\n# Or relative from playbook location\nsource: ../../data/data.csv\n</code></pre>"},{"location":"guides/dataset-import/#issue-column-not-found","title":"Issue: Column Not Found","text":"<pre><code>Error: Column 'text_column' not found\n</code></pre> <p>Fix: Check your CSV/JSON column names match mapping</p> <pre><code># Check column names\nhead -1 data.csv\n\n# Update mapping to match\nmapping:\n  input: actual_column_name  # Must match CSV header\n</code></pre>"},{"location":"guides/dataset-import/#issue-import-error","title":"Issue: Import Error","text":"<pre><code>Dataset import feature not available\n</code></pre> <p>Fix: Reinstall SuperOptiX</p> <pre><code>cd /path/to/SuperOptiX\npip install -e .\n</code></pre>"},{"location":"guides/dataset-import/#issue-huggingface-download-slow","title":"Issue: HuggingFace Download Slow","text":"<pre><code># Use limit for faster download\ndatasets:\n  - source: huggingface:imdb\n    limit: 1000  # Downloads only 1000 examples\n</code></pre>"},{"location":"guides/dataset-import/#complete-example-production-sentiment-analyzer","title":"\ud83d\udcda Complete Example: Production Sentiment Analyzer","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Production Sentiment Analyzer\n  id: sentiment_prod\n  namespace: analysis\n  version: 2.0.0\nspec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.3\n\n  input_fields:\n  - name: text\n    type: string\n\n  output_fields:\n  - name: sentiment\n    type: string\n\n  # Import 15,000 examples from 3 sources!\n  datasets:\n  # Custom CSV data (8,000 examples)\n  - name: customer_reviews\n    source: ./data/reviews_2024.csv\n    format: csv\n    mapping:\n      input: review_text\n      output: sentiment_label\n      input_field_name: text\n      output_field_name: sentiment\n    limit: 8000\n    shuffle: true\n\n  # Validation data (2,000 examples)\n  - name: validation_set\n    source: ./data/validation.jsonl\n    format: jsonl\n    mapping:\n      input: text\n      output: label\n      input_field_name: text\n      output_field_name: sentiment\n    limit: 2000\n\n  # HuggingFace IMDB dataset (5,000 examples)\n  - name: imdb_reviews\n    source: huggingface:imdb\n    format: huggingface\n    mapping:\n      input: text\n      output: label\n      input_field_name: text\n      output_field_name: sentiment\n    split: train\n    limit: 5000\n\n  # Keep BDD scenarios for edge cases\n  feature_specifications:\n    scenarios:\n    - name: sarcasm_detection\n      input: {text: \"Oh great, another delay\"}\n      expected_output: {sentiment: negative}\n\n    - name: mixed_sentiment\n      input: {text: \"Good food but slow service\"}\n      expected_output: {sentiment: neutral}\n\n  tasks:\n  - name: analyze\n    instruction: Classify sentiment as positive, negative, or neutral\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium  # More data = use medium budget\n        reflection_lm: llama3.1:8b\n</code></pre> <p>Result: 15,002 total examples (15,000 from datasets + 2 BDD scenarios)!</p>"},{"location":"guides/dataset-import/#demo-workflow","title":"\ud83c\udfac Demo Workflow","text":"<pre><code># Preview your data\nsuper agent dataset preview sentiment_prod\n\n# Validate configuration\nsuper agent dataset validate sentiment_prod\n\n# See dataset stats\nsuper agent dataset info sentiment_prod\n\n# Compile\nsuper agent compile sentiment_prod\n# \u2192 \"\ud83d\udcca Loaded 15,000 examples from datasets!\"\n\n# Evaluate\nsuper agent evaluate sentiment_prod\n# \u2192 Uses all 15,002 examples\n\n# Optimize\nsuper agent optimize sentiment_prod --auto medium --fresh\n# \u2192 GEPA trains on 15,000 examples (better results!)\n\n# Re-evaluate\nsuper agent evaluate sentiment_prod\n# \u2192 See improvement from massive dataset!\n</code></pre>"},{"location":"guides/dataset-import/#tips-tricks","title":"\ud83d\udca1 Tips &amp; Tricks","text":""},{"location":"guides/dataset-import/#tip-1-start-with-huggingface","title":"Tip 1: Start with HuggingFace","text":"<p>Don't have data? Use HuggingFace!</p> <pre><code>datasets:\n  - name: quick_start\n    source: huggingface:imdb\n    format: huggingface\n    mapping: {input: text, output: label}\n    limit: 1000  # Quick download\n</code></pre> <p>Browse datasets: https://huggingface.co/datasets</p>"},{"location":"guides/dataset-import/#tip-2-use-jsonl-for-large-files","title":"Tip 2: Use JSONL for Large Files","text":"<pre><code># Bad: Single JSON file (loads all in memory)\nformat: json\n\n# Good: JSONL (streams line by line)\nformat: jsonl\n</code></pre>"},{"location":"guides/dataset-import/#tip-3-shuffle-for-better-training","title":"Tip 3: Shuffle for Better Training","text":"<pre><code>datasets:\n  - shuffle: true  # Prevents ordering bias\n    limit: 5000\n</code></pre>"},{"location":"guides/dataset-import/#tip-4-validate-before-training","title":"Tip 4: Validate Before Training","text":"<pre><code># Always validate first!\nsuper agent dataset validate my_agent\n\n# Then preview\nsuper agent dataset preview my_agent\n\n# Then use\nsuper agent compile my_agent\n</code></pre>"},{"location":"guides/dataset-import/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"guides/dataset-import/#dataset-configuration-schema","title":"Dataset Configuration Schema","text":"<pre><code>datasets:\n  - name: string              # Required\n    source: string            # Required (file path or huggingface:name)\n    format: enum              # csv|json|jsonl|parquet|huggingface\n    mapping:                  # Required\n      input: string|object\n      output: string|object\n      input_field_name: string   # Optional\n      output_field_name: string  # Optional\n    split: enum               # train|test|validation|all\n    limit: integer            # Optional (max examples)\n    shuffle: boolean          # Optional (default: true)\n</code></pre>"},{"location":"guides/dataset-import/#mapping-formats","title":"Mapping Formats","text":"<p>Simple Mapping (single field): <pre><code>mapping:\n  input: column_name\n  output: column_name\n  input_field_name: text    # Agent field name\n  output_field_name: label  # Agent field name\n</code></pre></p> <p>Complex Mapping (multiple fields): <pre><code>mapping:\n  input:\n    question: question_col\n    context: context_col\n  output:\n    answer: answer_col\n    score: score_col\n</code></pre></p>"},{"location":"guides/dataset-import/#summary","title":"\ud83c\udf89 Summary","text":"<p>Dataset Import enables: - Import from 5 formats (CSV, JSON, JSONL, Parquet, HuggingFace) - Scale to 10,000+ examples - Use existing datasets - Standard ML workflows - Better GEPA optimization - Mix with BDD scenarios</p> <p>Get Started: <pre><code># Add datasets to your playbook\n# Run: super agent dataset preview my_agent\n# Compile and use!\n</code></pre></p> <p>Supported Formats: CSV, JSON, JSONL, Parquet, HuggingFace</p>"},{"location":"guides/deepagents-backends/","title":"\ud83d\uddc4\ufe0f DeepAgents Backend Configuration Guide","text":"<p>Complete guide to DeepAgents 0.2.0 pluggable backends for persistent memory, filesystem access, and advanced storage strategies.</p>"},{"location":"guides/deepagents-backends/#table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":"<ol> <li>Introduction</li> <li>Quick Start</li> <li>Backend Types</li> <li>Complete Examples</li> <li>Advanced Patterns</li> <li>Troubleshooting</li> </ol>"},{"location":"guides/deepagents-backends/#introduction","title":"\ud83c\udfaf Introduction","text":""},{"location":"guides/deepagents-backends/#whats-new-in-deepagents-020","title":"What's New in DeepAgents 0.2.0?","text":"<p>DeepAgents 0.2.0 introduces a pluggable backend abstraction that transforms how agents store and access files. Instead of being limited to ephemeral virtual filesystems, you can now:</p> <p>Persist memory across conversations (StoreBackend) Access real files on your local filesystem (FilesystemBackend) Hybrid strategies with different storage for different paths (CompositeBackend) Custom backends for databases, S3, remote VMs, etc.</p> <p>Source: LangChain Blog - Doubling Down on DeepAgents</p>"},{"location":"guides/deepagents-backends/#why-backends-matter","title":"Why Backends Matter","text":"<p>Before 0.2.0: <pre><code>Agent writes to /notes.txt \u2192 Stored in LangGraph state\nNew conversation starts \u2192 /notes.txt is gone ```\n\n**After 0.2.0:**\n</code></pre> Agent writes to /memories/notes.txt \u2192 Stored in persistent database New conversation starts \u2192 /memories/notes.txt still there! ```</p>"},{"location":"guides/deepagents-backends/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/deepagents-backends/#basic-setup-default-behavior","title":"Basic Setup (Default Behavior)","text":"<p>If you don't specify a backend, DeepAgents uses <code>StateBackend</code> (ephemeral, same as 0.1.0):</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Basic Research Agent\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n\n  # No backend config = uses StateBackend (default)\n</code></pre>"},{"location":"guides/deepagents-backends/#enable-persistent-memory","title":"Enable Persistent Memory","text":"<p>Add just 2 lines to enable persistent memory:</p> <pre><code>spec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n\n  # Enable persistent storage\n  backend:\n    type: store  # \u2728 That's it!\n</code></pre> <p>Now everything the agent writes persists across conversations!</p>"},{"location":"guides/deepagents-backends/#backend-types","title":"\ud83d\udce6 Backend Types","text":""},{"location":"guides/deepagents-backends/#statebackend-default","title":"StateBackend (Default)","text":"<p>Storage: LangGraph state (ephemeral, per-thread) Persistence: Within conversation only Best For: Temporary scratch space, single-conversation agents</p> <p>Configuration: <pre><code>backend:\n  type: state\n</code></pre></p> <p>Behavior: - Fast (in-memory) - Checkpointed within thread - Lost when thread ends - Not shared across threads</p> <p>Use Cases: - Simple Q&amp;A agents - Temporary calculations - Draft generation - Single-session tasks</p>"},{"location":"guides/deepagents-backends/#storebackend-persistent","title":"StoreBackend (Persistent)","text":"<p>Storage: LangGraph store (persistent database) Persistence: Across all conversations Best For: Long-term memory, learning agents, chatbots</p> <p>Configuration: <pre><code>backend:\n  type: store\n</code></pre></p> <p>Behavior: - Persistent across threads - Shared between conversations - Survives restarts - \u26a1 Slightly slower (database)</p> <p>Use Cases: - Chatbots with memory - Research assistants that learn - Agents that build knowledge over time - Multi-session projects</p>"},{"location":"guides/deepagents-backends/#filesystembackend-local-files","title":"FilesystemBackend (Local Files)","text":"<p>Storage: Actual local filesystem Persistence: Real files on disk Best For: Code analysis, file editing, project work</p> <p>Configuration: <pre><code>backend:\n  type: filesystem\n  root_dir: /Users/local/my_project\n</code></pre></p> <p>Behavior: - Access real project files - Changes immediately visible - Works with other tools (git, IDE, etc.) - \u26a0\ufe0f  Security: Agent can modify actual files!</p> <p>Use Cases: - Code review agents - Documentation generators - File refactoring - Project analysis</p>"},{"location":"guides/deepagents-backends/#compositebackend-hybrid","title":"CompositeBackend (Hybrid)","text":"<p>Storage: Route different paths to different backends Persistence: Mixed strategies Best For: Complex agents with different storage needs</p> <p>Configuration: <pre><code>backend:\n  type: composite\n  default: state              # Default for unspecified paths\n  routes:\n    /memories/: store         # Persistent memories\n    /project/: filesystem     # Real project files\n    /cache/: state            # Temporary files\n</code></pre></p> <p>Behavior: - Best of all worlds - Optimized for each data type - Flexible and powerful - \ud83d\udd27 Requires more configuration</p> <p>Use Cases: - Development assistants - Complex research agents - Multi-domain agents - Production systems</p>"},{"location":"guides/deepagents-backends/#complete-examples","title":"\ud83d\udcda Complete Examples","text":""},{"location":"guides/deepagents-backends/#example-1-persistent-chatbot","title":"Example 1: Persistent Chatbot","text":"<p>Scenario: Build a chatbot that remembers user preferences and previous conversations.</p> <p>Playbook: <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Persistent Chatbot\n  description: Chatbot with long-term memory\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n    temperature: 0.7\n\n  # Persistent memory across all conversations\n  backend:\n    type: store\n\n  persona:\n    role: Personal AI Assistant\n    goal: Help users and remember their preferences over time\n    system_prompt: |\n      You are a personal AI assistant with long-term memory.\n\n      IMPORTANT: Save important information to these files:\n      - /user_profile.txt - User's name, preferences, interests\n      - /conversation_history.txt - Key topics from past conversations\n      - /reminders.txt - User's reminders and to-dos\n\n      Before responding, ALWAYS:\n      1. Check /user_profile.txt to personalize your response\n      2. Check /conversation_history.txt for context\n      3. Update files with new information learned\n</code></pre></p> <p>Usage: <pre><code># First conversation\nsuper agent run persistent_chatbot --goal \"Hi! My name is Alice and I love Python.\"\n\n# Agent writes to /user_profile.txt:\n# Name: Alice\n# Interests: Python programming\n\n# Days later, new conversation\nsuper agent run persistent_chatbot --goal \"What's my name?\"\n\n# Agent reads /user_profile.txt\n# Response: \"Your name is Alice! And I remember you love Python programming.\"\n</code></pre></p> <p>Why It Works: - <code>StoreBackend</code> persists files across all threads - Agent can read previous data - Builds knowledge over time</p>"},{"location":"guides/deepagents-backends/#example-2-code-review-agent","title":"Example 2: Code Review Agent","text":"<p>Scenario: Agent analyzes actual project files and suggests improvements.</p> <p>Playbook: <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Code Review Agent\n  description: Analyzes real project files\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-pro  # Pro for better analysis\n    temperature: 0.3\n\n  # Access real project files\n  backend:\n    type: filesystem\n    root_dir: /Users/local/my_project\n\n  persona:\n    role: Senior Code Reviewer\n    goal: Analyze code for bugs, security issues, and best practices\n    system_prompt: |\n      You are a senior code reviewer with access to the project files.\n\n      Available in your filesystem:\n      - /src/ - Source code\n      - /tests/ - Test files\n      - /README.md - Project documentation\n      - /requirements.txt - Dependencies\n\n      When reviewing:\n      1. Read the relevant files using ls and read_file\n      2. Analyze for:\n         - Security vulnerabilities\n         - Performance issues\n         - Code style violations\n         - Missing tests\n      3. Provide specific, actionable feedback\n      4. If appropriate, suggest or make improvements\n</code></pre></p> <p>Usage: <pre><code># Analyze a specific file\nsuper agent run code_review_agent --goal \"Review src/auth.py for security issues\"\n\n# Agent reads actual file from disk:\n# - /src/auth.py\n# Response: \"Found 3 security concerns in src/auth.py...\"\n\n# Analyze entire codebase\nsuper agent run code_review_agent --goal \"Analyze the entire codebase and write a report to /code_review_report.md\"\n\n# Agent:\n# Lists files with ls /src/\n# Reads each file\n# Writes report to actual file: /Users/local/my_project/code_review_report.md\n</code></pre></p> <p>Why It Works: - <code>FilesystemBackend</code> gives access to real files - Changes are immediately visible in IDE - Can be used with git, etc.</p> <p>\u26a0\ufe0f Security Note: Agent can modify actual files! Use carefully.</p>"},{"location":"guides/deepagents-backends/#example-3-research-agent-with-hybrid-storage","title":"Example 3: Research Agent with Hybrid Storage","text":"<p>Scenario: Advanced research assistant that uses optimal storage for each data type.</p> <p>Playbook: <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Advanced Research Agent\n  description: Hybrid storage for optimal performance\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n    temperature: 0.7\n\n  # Hybrid storage strategy\n  backend:\n    type: composite\n    default: state                # Scratch space for current conversation\n    routes:\n      /memories/: store          # Long-term research knowledge\n      /papers/: filesystem       # Access to local paper PDFs\n      /cache/: state             # Temporary internet search results\n\n  persona:\n    role: AI Research Assistant\n    goal: Conduct thorough research and build long-term knowledge\n    system_prompt: |\n      You are an AI research assistant with advanced storage capabilities.\n\n      STORAGE GUIDELINES:\n\n      \ud83d\udcc1 /memories/ (Persistent Database)\n      - Save important research findings\n      - Store literature reviews\n      - Keep track of research topics explored\n      - Build cumulative knowledge over time\n\n      \ud83d\udcc2 /papers/ (Local Filesystem)\n      - Access PDF papers in /Users/local/research/papers/\n      - Read abstracts and summaries\n      - Reference actual academic papers\n\n      \ud83d\udcbe /cache/ (Temporary)\n      - Store internet search results\n      - Keep intermediate calculations\n      - Temporary notes (cleared each conversation)\n\n      \ud83d\uddc2\ufe0f / (Scratch Space)\n      - Current conversation context\n      - Draft responses\n      - Work in progress\n\n      WORKFLOW:\n      1. Check /memories/ for previous research on topic\n      2. Search for new information\n      3. Save results to /cache/ temporarily\n      4. If information is important, save to /memories/\n      5. Access /papers/ for academic sources\n</code></pre></p> <p>Advanced Configuration (in Python): <pre><code># For more control, configure in playbook\nbackend:\n  type: composite\n  default: state\n  routes:\n    /memories/: store\n    /papers/: filesystem\n    /cache/: state\n\n  # Root dir for filesystem backend\n  root_dir: /Users/local/research\n</code></pre></p> <p>Usage: <pre><code>export GOOGLE_API_KEY=\"your-key\"\n\n# First research session\nsuper agent run advanced_research_agent --goal \"Research transformer architectures. Save key findings.\"\n\n# Agent workflow:\n# Checks /memories/transformer_research.txt (not found)\n# Searches for information\n# Saves results to /cache/search_results.txt\n# Writes summary to /memories/transformer_research.txt (persisted!)\n# May read from /papers/attention_is_all_you_need.pdf\n\n# Week later, follow-up research\nsuper agent run advanced_research_agent --goal \"What did I learn about transformers?\"\n\n# Agent workflow:\n# Reads /memories/transformer_research.txt (found - from last week!)\n# Response: \"Based on your previous research, transformers...\"\n# Can build upon previous knowledge\n\n# Analyze specific paper\nsuper agent run advanced_research_agent --goal \"Summarize the paper in /papers/bert.pdf\"\n\n# Agent reads actual file from /Users/local/research/papers/bert.pdf\n</code></pre></p> <p>Why It Works: - <code>/memories/</code> persists across weeks (StoreBackend) - <code>/papers/</code> accesses real PDFs (FilesystemBackend) - <code>/cache/</code> is fast and ephemeral (StateBackend) - <code>/</code> is conversation-scoped (StateBackend)</p>"},{"location":"guides/deepagents-backends/#advanced-patterns","title":"\ud83c\udf93 Advanced Patterns","text":""},{"location":"guides/deepagents-backends/#pattern-1-learning-agent","title":"Pattern 1: Learning Agent","text":"<p>Goal: Agent that improves over time by learning from interactions.</p> <pre><code>backend:\n  type: store\n\npersona:\n  system_prompt: |\n    You are a learning assistant. After each interaction:\n\n    1. Read /knowledge/learned_facts.txt\n    2. Add new facts you learned\n    3. Write back to /knowledge/learned_facts.txt\n\n    Over time, your knowledge will grow!\n</code></pre> <p>Result: Agent builds cumulative knowledge base.</p>"},{"location":"guides/deepagents-backends/#pattern-2-project-aware-agent","title":"Pattern 2: Project-Aware Agent","text":"<p>Goal: Agent that understands your entire project structure.</p> <pre><code>backend:\n  type: composite\n  default: state\n  routes:\n    /project/: filesystem\n\n  root_dir: /Users/local/my_app\n\npersona:\n  system_prompt: |\n    You have access to the entire project in /project/.\n\n    Key files:\n    - /project/src/ - Source code\n    - /project/tests/ - Tests\n    - /project/docs/ - Documentation\n\n    You can read and modify these files directly!\n</code></pre> <p>Result: Agent sees actual project, can make real changes.</p>"},{"location":"guides/deepagents-backends/#pattern-3-multi-session-research","title":"Pattern 3: Multi-Session Research","text":"<p>Goal: Research project that spans multiple days.</p> <pre><code>backend:\n  type: composite\n  default: state\n  routes:\n    /research/: store\n    /output/: filesystem\n\n  root_dir: /Users/local/research_output\n\npersona:\n  system_prompt: |\n    You are conducting a long-term research project.\n\n    /research/ - Persistent research notes (cross-session)\n    /output/ - Final reports written to disk\n    / - Current session workspace\n\n    At the end of each session, summarize findings to /research/session_N_summary.txt\n</code></pre> <p>Result: Research builds across sessions, final reports saved to disk.</p>"},{"location":"guides/deepagents-backends/#pattern-4-secure-sandbox","title":"Pattern 4: Secure Sandbox","text":"<p>Goal: Agent can work freely but can't access sensitive files.</p> <pre><code>backend:\n  type: filesystem\n  root_dir: /tmp/agent_sandbox  # Isolated directory\n\npersona:\n  system_prompt: |\n    You can create and modify files freely.\n    All files are in a secure sandbox.\n</code></pre> <p>Result: Agent has filesystem access but can't touch important files.</p>"},{"location":"guides/deepagents-backends/#configuration-reference","title":"\ud83d\udd27 Configuration Reference","text":""},{"location":"guides/deepagents-backends/#complete-playbook-schema","title":"Complete Playbook Schema","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Example Agent\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n\n  # Backend configuration (NEW in 0.2.0)\n  backend:\n    type: state | store | filesystem | composite\n\n    # For filesystem backend only:\n    root_dir: /path/to/directory\n\n    # For composite backend only:\n    default: state | store | filesystem\n    routes:\n      /path1/: store\n      /path2/: filesystem\n      /path3/: state\n</code></pre>"},{"location":"guides/deepagents-backends/#backend-type-matrix","title":"Backend Type Matrix","text":"Backend Persistence Speed Shared Use Case state Thread only \u26a1\u26a1\u26a1 Very Fast No Scratch space store Forever \u26a1\u26a1 Fast Yes Long-term memory filesystem Forever \u26a1\u26a1 Fast Yes Real files composite Mixed \u26a1\u26a1 Fast Mixed Best of all"},{"location":"guides/deepagents-backends/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"guides/deepagents-backends/#issue-1-backend-import-failed","title":"Issue 1: \"Backend import failed\"","text":"<p>Error: <pre><code>\u26a0\ufe0f  Backend import failed: No module named 'superoptix.vendor.deepagents.backends'\n</code></pre></p> <p>Solution: <pre><code># Update SuperOptiX\ncd /Users/local/superagentic/SuperOptiX\npip install -e .\n\n# Verify backends are available\npython -c \"from superoptix.vendor.deepagents.backends import state; print('Backends available')\"\n</code></pre></p>"},{"location":"guides/deepagents-backends/#issue-2-files-not-persisting","title":"Issue 2: Files not persisting","text":"<p>Problem: Files disappear between conversations</p> <p>Check: <pre><code># Make sure you're using store, not state\nbackend:\n  type: store  # NOT state!\n</code></pre></p>"},{"location":"guides/deepagents-backends/#issue-3-cant-access-local-files","title":"Issue 3: Can't access local files","text":"<p>Error: <pre><code>FileNotFoundError: /Users/local/project/file.txt\n</code></pre></p> <p>Solution: <pre><code>backend:\n  type: filesystem\n  root_dir: /Users/local/project  # Must specify root!\n</code></pre></p>"},{"location":"guides/deepagents-backends/#issue-4-permission-denied","title":"Issue 4: Permission denied","text":"<p>Error: <pre><code>PermissionError: /etc/password\n</code></pre></p> <p>Cause: FilesystemBackend respects OS permissions</p> <p>Solution: - Use appropriate <code>root_dir</code> - Check file permissions - Don't point to system directories</p>"},{"location":"guides/deepagents-backends/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"guides/deepagents-backends/#choose-the-right-backend","title":"Choose the Right Backend","text":"<pre><code># Fast scratch space\nbackend:\n  type: state\n\n# Persistent but slightly slower\nbackend:\n  type: store\n\n# Real files (filesystem speed)\nbackend:\n  type: filesystem\n</code></pre>"},{"location":"guides/deepagents-backends/#use-composite-for-optimal-performance","title":"Use Composite for Optimal Performance","text":"<pre><code># Optimize for each data type\nbackend:\n  type: composite\n  default: state           # Fast default\n  routes:\n    /memories/: store     # Only what needs persistence\n    /big_files/: filesystem  # Offload large files\n</code></pre>"},{"location":"guides/deepagents-backends/#limit-filesystem-scope","title":"Limit Filesystem Scope","text":"<pre><code># Good: Specific directory\nbackend:\n  type: filesystem\n  root_dir: /Users/local/project/src\n\n# Bad: Entire filesystem\nbackend:\n  type: filesystem\n  root_dir: /\n</code></pre>"},{"location":"guides/deepagents-backends/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/deepagents-backends/#do","title":"Do","text":"<ol> <li>Use StateBackend for temporary, single-conversation agents</li> <li>Use StoreBackend for chatbots and learning agents</li> <li>Use FilesystemBackend with specific <code>root_dir</code></li> <li>Use CompositeBackend for complex production agents</li> <li>Document storage strategy in system prompt</li> <li>Test with small <code>root_dir</code> first</li> </ol>"},{"location":"guides/deepagents-backends/#dont","title":"Don't","text":"<ol> <li>Don't use FilesystemBackend on root (<code>/</code>)</li> <li>Don't store sensitive data without encryption</li> <li>Don't assume infinite storage in StoreBackend</li> <li>Don't mix backends without CompositeBackend</li> <li>Don't give write access to critical files</li> <li>Don't forget to backup persistent data</li> </ol>"},{"location":"guides/deepagents-backends/#migration-guide","title":"\ud83d\ude80 Migration Guide","text":""},{"location":"guides/deepagents-backends/#from-010-to-020","title":"From 0.1.0 to 0.2.0","text":"<p>No breaking changes! Old playbooks work as-is.</p> <p>To add persistence:</p> <pre><code># Old (still works)\nspec:\n  target_framework: deepagents\n  language_model:\n    model: gemini-2.5-flash\n\n# New (with persistence)\nspec:\n  target_framework: deepagents\n  language_model:\n    model: gemini-2.5-flash\n  backend:\n    type: store  # Add this!\n</code></pre> <p>Steps: 1. Add <code>backend</code> section to playbook 2. Recompile: <code>super agent compile my_agent --framework deepagents</code> 3. Test: <code>super agent run my_agent --goal \"test\"</code> 4. Verify persistence works</p>"},{"location":"guides/deepagents-backends/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>DeepAgents 0.2.0 Announcement</li> <li>DeepAgents GitHub</li> <li>SuperOptiX DeepAgents Integration Guide</li> <li>Gemini Configuration Guide</li> </ul>"},{"location":"guides/deepagents-backends/#next-steps","title":"\ud83d\udca1 Next Steps","text":"<ol> <li>Try the examples above</li> <li>Experiment with different backends</li> <li>Build a persistent chatbot</li> <li>Create a code analysis agent</li> <li>Design your hybrid storage strategy</li> </ol> <p>Questions? Check our FAQ or Troubleshooting Guide</p> <p>Ready to build? Start with the DeepAgents Quick Start!</p>"},{"location":"guides/deepagents-index/","title":"\ud83e\udde0 DeepAgents Documentation Index","text":"<p>Complete navigation guide for all DeepAgents resources. Start here to find exactly what you need!</p>"},{"location":"guides/deepagents-index/#quick-start-30-minutes","title":"\ud83d\ude80 Quick Start (30 Minutes)","text":"<p>New to DeepAgents? Start here:</p>"},{"location":"guides/deepagents-index/#complete-end-to-end-workflow-recommended","title":"Complete End-to-End Workflow \u2b50 RECOMMENDED","text":"<p>What you'll learn: - Set up Gemini API (FREE) - Build your first DeepAgents agent - Run real queries with Gemini - Evaluate with BDD scenarios - Optimize with GEPA (+200% improvement!) - Deploy production-ready agents</p> <p>Time: 30-45 minutes Cost: $0.00 (FREE Gemini!) Result: Production-ready agent</p>"},{"location":"guides/deepagents-index/#core-guides","title":"\ud83d\udcda Core Guides","text":""},{"location":"guides/deepagents-index/#deepagents-integration-guide","title":"DeepAgents Integration Guide","text":"<p>Complete reference for DeepAgents in SuperOptiX</p> <p>Topics covered: - What is DeepAgents? - Installation and setup - Quick start examples - Backend configuration (0.2.0) - Creating custom playbooks - Complete workflow - Troubleshooting - FAQ</p> <p>When to read: After completing the end-to-end workflow Length: Comprehensive (~1,300 lines)</p>"},{"location":"guides/deepagents-index/#backend-configuration-reference","title":"Backend Configuration Reference","text":"<p>Deep dive into DeepAgents 0.2.0 backends</p> <p>Topics covered: - What are backends? - StateBackend (ephemeral) - StoreBackend (persistent) - FilesystemBackend (real files) - CompositeBackend (hybrid) - Configuration syntax - Best practices - Troubleshooting</p> <p>When to read: When you need persistent memory or file access Length: ~670 lines</p>"},{"location":"guides/deepagents-index/#hands-on-tutorials","title":"\ud83c\udf93 Hands-On Tutorials","text":""},{"location":"guides/deepagents-index/#backend-tutorial","title":"Backend Tutorial","text":"<p>6 practical tutorials with working examples</p> <p>Tutorials: 1. Persistent Chatbot (StoreBackend) 2. Code Review Agent (FilesystemBackend) 3. Hybrid Research Agent (CompositeBackend) 4. GEPA Optimization with Backends 5. Backend Performance Comparison 6. Production-Ready Hybrid Agent</p> <p>When to use: When you want to practice with each backend type Time: 2-3 hours for all tutorials</p>"},{"location":"guides/deepagents-index/#setup-configuration","title":"\ud83d\udd27 Setup &amp; Configuration","text":""},{"location":"guides/deepagents-index/#gemini-configuration-guide","title":"Gemini Configuration Guide","text":"<p>Complete Gemini 2.5 model configuration guide</p> <p>Topics: - Gemini 2.5 model lineup (Flash vs Pro) - Recommended configurations - Performance comparisons - Cost analysis (FREE tier) - Troubleshooting</p> <p>When to read: Before starting, to optimize model selection Length: Comprehensive model reference</p>"},{"location":"guides/deepagents-index/#gemini-testing-guide","title":"Gemini Testing Guide","text":"<p>Step-by-step Gemini testing workflow</p> <p>Topics: - Getting FREE Gemini API key - Setting up environment - Testing workflow - Expected outputs - Before/after optimization examples - Troubleshooting</p> <p>When to use: When setting up Gemini for the first time</p>"},{"location":"guides/deepagents-index/#reference-materials","title":"\ud83d\udcd6 Reference Materials","text":""},{"location":"guides/deepagents-index/#quick-reference-card","title":"Quick Reference Card","text":"<p>One-page cheat sheet (print and keep handy!)</p> <p>Contains: - Essential commands - Backend quick configs - Model configurations - Common issues and fixes</p> <p>When to use: As a quick lookup during development</p>"},{"location":"guides/deepagents-index/#technical-deep-dives","title":"\ud83d\udd0d Technical Deep Dives","text":""},{"location":"guides/deepagents-index/#integration-summary","title":"Integration Summary","text":"<p>What's new in 0.2.0 integration</p> <p>Topics: - Feature overview - Implementation status - What's working - How to use new features - Migration guide</p>"},{"location":"guides/deepagents-index/#technical-analysis","title":"Technical Analysis","text":"<p>Why Ollama doesn't work with DeepAgents</p> <p>Topics: - Root cause analysis - Function-calling requirements - Framework comparison - Workarounds and alternatives</p> <p>When to read: If you tried Ollama and it didn't work</p>"},{"location":"guides/deepagents-index/#bug-fixes-summary","title":"Bug Fixes Summary","text":"<p>All bugs that were fixed</p> <p>Topics: - Class name resolution fix - Execution method fix - Model format fix - Testing results</p> <p>When to use: If you encounter issues</p>"},{"location":"guides/deepagents-index/#use-case-guides","title":"\ud83c\udfaf Use Case Guides","text":""},{"location":"guides/deepagents-index/#building-a-persistent-chatbot","title":"Building a Persistent Chatbot","text":"<p>Read: 1. Complete Workflow - Example 1 2. Backend Tutorial - Tutorial 1 3. Backend Reference - StoreBackend section</p> <p>Demo Agent: <pre><code>super agent pull chatbot_persistent\n</code></pre></p>"},{"location":"guides/deepagents-index/#building-a-code-review-agent","title":"Building a Code Review Agent","text":"<p>Read: 1. Complete Workflow - Example 2 2. Backend Tutorial - Tutorial 2 3. Backend Reference - FilesystemBackend section</p> <p>Demo Agent: <pre><code>super agent pull code_reviewer\n</code></pre></p>"},{"location":"guides/deepagents-index/#building-a-research-assistant","title":"Building a Research Assistant","text":"<p>Read: 1. Complete Workflow - Example 3 2. Backend Tutorial - Tutorial 3 3. Backend Reference - CompositeBackend section</p> <p>Demo Agent: <pre><code>super agent pull researcher_hybrid\n</code></pre></p>"},{"location":"guides/deepagents-index/#documentation-matrix","title":"\ud83d\udcca Documentation Matrix","text":"Document Type Length Difficulty Time Best For Complete Workflow Tutorial Long Beginner 30 min Start here! Integration Guide Reference Very Long Intermediate 1 hour Complete reference Backend Reference Reference Long Intermediate 45 min Backend deep dive Backend Tutorial Tutorial Long Intermediate 2 hours Hands-on practice Gemini Config Guide Medium Beginner 15 min Model setup Quick Reference Cheat Sheet Short All 5 min Quick lookup"},{"location":"guides/deepagents-index/#popular-topics","title":"\ud83d\udd25 Popular Topics","text":""},{"location":"guides/deepagents-index/#how-do-i-get-started","title":"\"How do I get started?\"","text":"<p>\u2192 Complete End-to-End Workflow</p>"},{"location":"guides/deepagents-index/#how-do-i-make-memory-persist","title":"\"How do I make memory persist?\"","text":"<p>\u2192 Backend Reference - StoreBackend</p>"},{"location":"guides/deepagents-index/#how-do-i-access-real-files","title":"\"How do I access real files?\"","text":"<p>\u2192 Backend Reference - FilesystemBackend</p>"},{"location":"guides/deepagents-index/#how-do-i-optimize-my-agent","title":"\"How do I optimize my agent?\"","text":"<p>\u2192 Complete Workflow - Step 7</p>"},{"location":"guides/deepagents-index/#why-cant-i-use-ollama","title":"\"Why can't I use Ollama?\"","text":"<p>\u2192 Technical Analysis</p>"},{"location":"guides/deepagents-index/#how-do-i-set-up-gemini","title":"\"How do I set up Gemini?\"","text":"<p>\u2192 Gemini Testing Guide</p>"},{"location":"guides/deepagents-index/#demo-agents","title":"\ud83c\udf89 Demo Agents","text":"<p>All demo agents are production-ready and fully documented:</p> <pre><code># Basic Research (StateBackend)\nsuper agent pull research_agent_deepagents\n# Uses: Default ephemeral storage\n# Best for: Single-session research\n\n# Persistent Chatbot (StoreBackend)\nsuper agent pull chatbot_persistent\n# Uses: Persistent database storage\n# Best for: Multi-session conversations\n\n# Code Reviewer (FilesystemBackend)\nsuper agent pull code_reviewer\n# Uses: Real filesystem access\n# Best for: Project analysis\n\n# Hybrid Researcher (CompositeBackend)\nsuper agent pull researcher_hybrid\n# Uses: Hybrid storage strategy\n# Best for: Production systems\n</code></pre>"},{"location":"guides/deepagents-index/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"guides/deepagents-index/#documentation-not-helping","title":"Documentation Not Helping?","text":"<ol> <li>Check Troubleshooting Section</li> <li>Read FAQ</li> <li>Check Common Issues</li> </ol>"},{"location":"guides/deepagents-index/#still-stuck","title":"Still Stuck?","text":"<ul> <li>\ud83d\udcd6 Documentation: https://superagenticai.github.io/superoptix/</li> <li>\ud83d\udce7 Email: hello@super-agentic.ai</li> <li>\ud83c\udf10 Website: https://superoptix.ai</li> </ul>"},{"location":"guides/deepagents-index/#community-examples","title":"\ud83c\udf1f Community Examples","text":"<p>Built something with DeepAgents? Share it!</p> <ul> <li>\ud83d\udc26 Tag us on Twitter/X: @SuperagenticAI</li> <li>\ud83d\udce7 Email us your success story: hello@super-agentic.ai</li> </ul> <p>Ready to start? Begin with the Complete End-to-End Workflow! \ud83d\ude80</p>"},{"location":"guides/deepagents-integration/","title":"\ud83e\udde0 DeepAgents Framework Integration","text":"<p>SuperOptiX now supports DeepAgents 0.2.0 - a LangGraph-based \"agent harness\" for building sophisticated, long-running agents with planning, pluggable backends, and subagent spawning!</p> <p>RLM support is experimental. Unified sandbox support is coming soon.</p> <p>\ud83d\ude80 New to DeepAgents? Start with the Complete Tutorial!</p> <p>\ud83d\udc49 Complete End-to-End Workflow Tutorial</p> <p>Learn how to build, run, evaluate, and optimize DeepAgents from scratch in 30 minutes:</p> <ul> <li>Step-by-step with real expected outputs</li> <li>Works with FREE Gemini API</li> <li>Persistent memory, real file access, hybrid storage</li> <li>GEPA optimization guide</li> <li>Deployment deployment guide</li> </ul>"},{"location":"guides/deepagents-integration/#what-is-deepagents","title":"\ud83c\udfaf What is DeepAgents?","text":"<p>DeepAgents is LangChain's premier framework for creating \"deep agents\" - sophisticated agents that go beyond simple tool-calling loops. As LangChain states, deep agents are \"able to do complex, open ended tasks over longer time horizons.\"</p>"},{"location":"guides/deepagents-integration/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>\ud83d\udccb Planning Tools: Break down complex tasks with <code>write_todos</code></li> <li>\ud83d\udcc1 Filesystem Access: <code>read_file</code>, <code>write_file</code>, <code>ls</code>, <code>edit_file</code>, <code>grep_search</code>, <code>glob_search</code></li> <li>\ud83d\udc65 Subagent Spawning: Delegate specialized tasks to focused subagents</li> <li>\ud83d\uddc4\ufe0f Pluggable Backends (NEW 0.2.0): Choose where files are stored</li> <li>\u26a1 Auto-Optimization (NEW 0.2.0): Large result eviction, conversation summarization</li> <li>\ud83d\udd27 Error Recovery (NEW 0.2.0): Automatic tool call repair</li> </ul>"},{"location":"guides/deepagents-integration/#new-in-020-pluggable-backends","title":"NEW in 0.2.0: Pluggable Backends","text":"<p>The biggest addition is the backend abstraction that lets you choose where agent files are stored:</p> Backend Persistence Use Case StateBackend Thread only Scratch space (default) StoreBackend Forever Long-term memory, chatbots FilesystemBackend Forever Real project files, code analysis CompositeBackend Mixed Hybrid strategies (best of all) <p>Perfect for: Complex research, code generation, chatbots, multi-step workflows, and deployment systems.</p> <p>Read more: LangChain Blog - Doubling Down on DeepAgents</p>"},{"location":"guides/deepagents-integration/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code># Install SuperOptiX with DeepAgents support\npip install superoptix[frameworks-deepagents]\n\n# REQUIRED: Install Gemini integration (or your preferred LLM provider)\npip install langchain-google-genai  # For Gemini\n# pip install langchain-anthropic   # For Claude\n# pip install langchain-openai      # For GPT-4\n</code></pre> <p>Includes: - deepagents 0.2.0+ with pluggable backends - SuperOptiX core with GEPA - LangChain integration (provider-specific packages need to be installed separately) - LangChain, LangGraph integration</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency) - API keys for function-calling models (Gemini, Claude, or GPT-4)</p> <p>New in 0.2.0: - \u2728 Pluggable backend abstraction - \ud83d\udcbe Persistent memory support - \ud83d\udcc1 Real filesystem access - \u26a1 Auto-optimization features</p>"},{"location":"guides/deepagents-integration/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/deepagents-integration/#option-a-using-gemini-free-recommended-for-testing","title":"Option A: Using Gemini (FREE &amp; Recommended for Testing) \u2b50","text":"<p>Why Gemini? Free access, fast, excellent function-calling support!</p> <pre><code># Get FREE API key from https://aistudio.google.com/app/apikey\nexport GOOGLE_API_KEY=\"your-gemini-api-key\"\n\n# Pull demo agent (already configured for Gemini!)\nsuper agent pull research_agent_deepagents\n\n# Run the full workflow\nsuper agent compile research_agent_deepagents --framework deepagents\nsuper agent run research_agent_deepagents --framework deepagents --goal \"What is LangGraph?\"\nsuper agent evaluate research_agent_deepagents\nsuper agent optimize research_agent_deepagents --auto medium --framework deepagents --reflection-lm ollama:llama3.1:8b\n\n# Done! Agent optimized with local Ollama\n</code></pre> <p>\ud83d\udcd6 Detailed Gemini Guide: See <code>DEEPAGENTS_GEMINI_TEST.md</code> in repo root</p>"},{"location":"guides/deepagents-integration/#option-b-using-claude-or-gpt-4","title":"Option B: Using Claude or GPT-4","text":"<p>DeepAgents requires function-calling models. To switch providers, pull the same demo agent and update the model in the generated playbook if needed. Refer to provider docs for the latest model names:</p> <ul> <li>Gemini: https://ai.google.dev/models</li> <li>Claude: https://docs.anthropic.com/en/docs/models-overview</li> <li>OpenAI: https://platform.openai.com/docs/models</li> </ul> <p>Note: Ollama models are not supported by DeepAgents today (LangChain limitation). Use the DSPy framework for Ollama.</p>"},{"location":"guides/deepagents-integration/#set-api-key","title":"Set API Key","text":"<pre><code># For Gemini (FREE)\nexport GOOGLE_API_KEY=your_key\n\n# For Claude\nexport ANTHROPIC_API_KEY=your_key\n\n# For OpenAI\nexport OPENAI_API_KEY=your_key\n</code></pre>"},{"location":"guides/deepagents-integration/#run-the-complete-workflow-pulled-agent","title":"Run the Complete Workflow (Pulled Agent)","text":"<pre><code># Compile (generate DeepAgents pipeline code)\nsuper agent compile research_agent_deepagents --framework deepagents\n\n# Run agent\nsuper agent run research_agent_deepagents --framework deepagents --goal \"What is LangGraph?\"\n\n# Evaluate (run BDD scenarios)\nsuper agent evaluate research_agent_deepagents\n\n# Optimize with GEPA (optimize system prompt)\nsuper agent optimize research_agent_deepagents \\\n  --framework deepagents \\\n  --auto medium \\\n  --reflection-lm google-genai:gemini-2.5-pro\n\n# Test optimized version\nsuper agent evaluate research_agent_deepagents  # automatically loads optimized weights\n</code></pre> <p>Expected Results: - Baseline performance \u2192 After GEPA: Significant improvement (results vary by hardware and model) - Cost: $0.00 with FREE Gemini access - Time: 5-10 minutes</p> <p>\ud83d\udcd6 Want Detailed Step-by-Step Guide?</p> <p>\ud83d\udc49 Complete End-to-End Workflow Tutorial</p> <p>This comprehensive tutorial shows you: - What to expect at each step (real outputs!) - How to configure all 3 backend types - GEPA optimization walkthrough with examples - Deployment deployment guide</p>"},{"location":"guides/deepagents-integration/#backend-configuration-new-in-020","title":"\ud83d\uddc4\ufe0f Backend Configuration (NEW in 0.2.0)","text":""},{"location":"guides/deepagents-integration/#what-are-backends","title":"What Are Backends?","text":"<p>DeepAgents 0.2.0 introduces a pluggable backend abstraction that lets you choose where agent files are stored. This transforms DeepAgents from a toy framework into a deployment-ready agent harness.</p> <p>Source: LangChain Blog - Doubling Down on DeepAgents</p>"},{"location":"guides/deepagents-integration/#quick-comparison","title":"Quick Comparison","text":"Backend When to Use Example StateBackend Temporary scratch space Draft generation, Q&amp;A StoreBackend Persistent chatbots Customer support, personal assistants FilesystemBackend Real file access Code review, project analysis CompositeBackend Complex agents Research assistant, dev tools"},{"location":"guides/deepagents-integration/#example-1-persistent-chatbot-storebackend","title":"Example 1: Persistent Chatbot (StoreBackend)","text":"<p>Use the prebuilt agent and run it (persistent memory out of the box): <pre><code># First conversation\nsuper agent run chatbot_persistent --goal \"Hi! I'm Sarah and I love gardening.\"\n# Agent saves: \"Name: Sarah, Interest: gardening\" to /user_profile.txt\n\n# Days later, new conversation\nsuper agent run chatbot_persistent --goal \"What's my name?\"\n# Agent reads /user_profile.txt \u2192 \"Your name is Sarah!\"\n\n# Weeks later\nsuper agent run chatbot_persistent --goal \"What do I like?\"\n# Agent reads /user_profile.txt \u2192 \"You love gardening!\"\n</code></pre></p> <p>Why It Works: StoreBackend persists files in a database. They survive across: - Different conversations - Different days - Server restarts - All threads</p>"},{"location":"guides/deepagents-integration/#example-2-code-review-agent-filesystembackend","title":"Example 2: Code Review Agent (FilesystemBackend)","text":"<p>Pull and run against real files (set <code>root_dir</code> in the playbook after pulling): <pre><code># Review a specific file\nsuper agent run code_reviewer --goal \"Review src/auth.py for security issues\"\n# Agent reads ACTUAL file: /Users/local/my_project/src/auth.py\n# Response: \"Found 3 security issues in src/auth.py...\"\n\n# Analyze entire project\nsuper agent run code_reviewer --goal \"Analyze all Python files and write report to /review.md\"\n# Agent:\n# Runs: ls /src/\n# Reads each .py file\n# Writes to: /Users/local/my_project/review.md (REAL FILE!)\n\n# You can see the report in your IDE immediately!\ncat /Users/local/my_project/review.md\n</code></pre></p> <p>Why It Works: FilesystemBackend gives agent real filesystem access. Changes are immediately visible in your IDE, terminal, git, etc.</p> <p>\u26a0\ufe0f Security: Agent can modify actual files! Use with trusted agents only.</p>"},{"location":"guides/deepagents-integration/#example-3-research-agent-compositebackend","title":"Example 3: Research Agent (CompositeBackend)","text":"<p>Pull and run the hybrid storage demo (edit <code>root_dir</code> after pulling): <pre><code># Setup: Create papers directory\nmkdir -p /Users/local/research/papers\n\n# First research session\nsuper agent run researcher_hybrid --goal \"Research transformer architectures and save findings\"\n\n# Agent workflow:\n# Checks /memories/research_index.txt (empty first time)\n# Searches internet \u2192 saves to /cache/search_results.txt\n# Checks /papers/ for PDFs (if any exist)\n# Writes to /memories/transformer_research.txt (PERSISTS!)\n# Updates /memories/research_index.txt\n\n# Week later, follow-up\nsuper agent run researcher_hybrid --goal \"What did I learn about transformers?\"\n\n# Agent workflow:\n# Reads /memories/transformer_research.txt (STILL THERE from last week!)\n# Response: \"Based on your previous research from [date]...\"\n# Can build upon previous knowledge!\n\n# Access specific paper\nsuper agent run researcher_hybrid --goal \"Summarize the paper in /papers/attention.pdf\"\n# Reads actual file: /Users/local/research/papers/attention.pdf\n</code></pre></p> <p>Why It Works: Each storage type is optimized: - <code>/memories/</code> = Persistent (store) - <code>/papers/</code> = Real files (filesystem) - <code>/cache/</code> = Fast &amp; temporary (state)</p>"},{"location":"guides/deepagents-integration/#backend-configuration-matrix","title":"Backend Configuration Matrix","text":"<p>High-level guidance on which backend to pick:</p> Configuration /memories/ /papers/ /cache/ / Best For Default state state state state Simple agents Persistent store store store store Chatbots Local Files filesystem filesystem filesystem filesystem Code tools Hybrid \u2b50 store filesystem state state Deployment"},{"location":"guides/deepagents-integration/#demo-agents-with-backends","title":"\ud83d\udcda Demo Agents with Backends","text":"<p>SuperOptiX includes 3 demo agents showcasing different backends:</p>"},{"location":"guides/deepagents-integration/#basic-research-agent-statebackend","title":"Basic Research Agent (StateBackend)","text":"<pre><code>super agent pull research_agent_deepagents\n# Uses default StateBackend (ephemeral)\n# Good for: Single-session research\n</code></pre>"},{"location":"guides/deepagents-integration/#persistent-chatbot-storebackend","title":"Persistent Chatbot (StoreBackend)","text":"<pre><code>super agent pull chatbot_persistent\n# Uses StoreBackend for memory\n# Good for: Multi-session conversations\n</code></pre>"},{"location":"guides/deepagents-integration/#code-reviewer-filesystembackend","title":"Code Reviewer (FilesystemBackend)","text":"<pre><code>super agent pull code_reviewer\n# Uses FilesystemBackend for real files\n# Good for: Project analysis\n</code></pre>"},{"location":"guides/deepagents-integration/#advanced-researcher-compositebackend","title":"Advanced Researcher (CompositeBackend)","text":"<pre><code>super agent pull researcher_hybrid\n# Uses CompositeBackend for hybrid storage\n# Good for: Complex deployment agents\n</code></pre> <p>Try them all: <pre><code># Set API key\nexport GOOGLE_API_KEY=\"your-key\"\n\n# Pull and test each one\nfor agent in research_agent_deepagents chatbot_persistent code_reviewer researcher_hybrid; do\n  super agent pull $agent\n  super agent compile $agent --framework deepagents\n  super agent run $agent --goal \"Test query\"\ndone\n</code></pre></p> <p>\ud83d\udcd6 Detailed Tutorials for Each Agent</p> <p>Each demo agent has step-by-step tutorials:</p> <ul> <li>Complete Workflow - All agents explained</li> <li>Backend Tutorial - Hands-on with each backend type</li> </ul> <p>See real examples, expected outputs, and deployment patterns!</p>"},{"location":"guides/deepagents-integration/#complete-backend-configuration-reference","title":"\ud83d\udccb Complete Backend Configuration Reference","text":""},{"location":"guides/deepagents-integration/#backend-type-state-default","title":"Backend Type: state (Default)","text":"<p>Storage: LangGraph state (ephemeral) Persistence: Current conversation only Best For: Temporary scratch space</p> <p>Pull the appropriate demo agent and inspect its playbook for backend configuration. No need to copy YAML from docs.</p> <p>Characteristics: - \u26a1\u26a1\u26a1 Very fast (in-memory) - Checkpointed within thread - Lost when conversation ends - Not shared across threads</p> <p>Use Cases: - Simple Q&amp;A agents - Temporary calculations - Draft generation - Single-session tasks</p>"},{"location":"guides/deepagents-integration/#backend-type-store-persistent","title":"Backend Type: store (Persistent)","text":"<p>Storage: LangGraph store (persistent database) Persistence: Forever, across all conversations Best For: Chatbots, learning agents</p> <p>Pull <code>chatbot_persistent</code> to see a persistent store-backed setup.</p> <p>Characteristics: - \u26a1\u26a1 Fast (database) - Persistent across all threads - Shared between conversations - Survives restarts</p> <p>Use Cases: - Chatbots with memory - Personal assistants - Learning agents - Knowledge accumulation</p> <p>Example Agent: <pre><code>super agent pull chatbot_persistent\nsuper agent compile chatbot_persistent --framework deepagents\nexport GOOGLE_API_KEY=\"your-key\"\nsuper agent run chatbot_persistent --goal \"Hi! My name is Alice.\"\n# Later...\nsuper agent run chatbot_persistent --goal \"What's my name?\"\n# Response: \"Your name is Alice!\" ```\n\n---\n\n### Backend Type: filesystem\n\n**Storage:** Actual local filesystem  \n**Persistence:** Real files on disk  \n**Best For:** Code analysis, file editing\n\nAfter pulling `code_reviewer`, set `backend.root_dir` in its playbook to your project path.\n\n**Characteristics:**\n- \u26a1\u26a1 Fast (filesystem speed)\n- Real files on disk\n- Changes immediately visible\n- \u26a0\ufe0f  Security: Can modify actual files!\n\n**Use Cases:**\n- Code review agents\n- Documentation generators\n- File refactoring\n- Project analysis\n\n**Example Agent:**\n```bash\nsuper agent pull code_reviewer\n# Edit playbook to set your root_dir\nsuper agent compile code_reviewer --framework deepagents\nexport GOOGLE_API_KEY=\"your-key\"\nsuper agent run code_reviewer --goal \"Review src/app.py\"\n# Reads actual file from your project!\n</code></pre></p> <p>\u26a0\ufe0f Security Warning: - Agent can read ANY file in <code>root_dir</code> - Agent can MODIFY or DELETE files - Use a limited <code>root_dir</code> scope - Don't point to system directories (/, /etc, etc.) - Consider read-only permissions for sensitive projects</p>"},{"location":"guides/deepagents-integration/#backend-type-composite-hybrid","title":"Backend Type: composite (Hybrid)","text":"<p>Storage: Routes different paths to different backends Persistence: Mixed strategies Best For: Deployment agents with complex needs</p> <p>Pull <code>researcher_hybrid</code> to explore hybrid storage. Edit paths in the playbook to match your environment.</p> <p>Characteristics: - Best of all worlds - Optimized for each data type - Maximum flexibility - \ud83d\udd27 Requires thoughtful configuration</p> <p>Use Cases: - Development assistants - Complex research agents - Multi-domain agents - Deployment systems</p> <p>Example Agent: <pre><code>super agent pull researcher_hybrid\n# Edit playbook to set your root_dir\nsuper agent compile researcher_hybrid --framework deepagents\nexport GOOGLE_API_KEY=\"your-key\"\nsuper agent run researcher_hybrid --goal \"Research AI and save findings\"\n\n# Files go to optimal locations:\n# /memories/research_findings.txt \u2192 Database (persists)\n# /papers/reference.pdf \u2192 Real filesystem (your files)\n# /cache/search.txt \u2192 Ephemeral (fast)\n# /draft.txt \u2192 Scratch space (fast)\n</code></pre></p>"},{"location":"guides/deepagents-integration/#backend-configuration-in-playbooks","title":"Backend Configuration in Playbooks","text":"<p>Complete Syntax:</p> <pre><code>spec:\n  backend:\n    # Backend type (required)\n    type: state | store | filesystem | composite\n\n    # For filesystem backend (required if type=filesystem)\n    root_dir: /path/to/directory\n\n    # For composite backend only\n    default: state | store | filesystem  # Default backend\n    routes:                              # Path routing\n      /path1/: store\n      /path2/: filesystem\n      /path3/: state\n</code></pre> <p>Examples:</p> <pre><code># Simple: State (default)\nbackend:\n  type: state\n\n# Simple: Store (persistent)\nbackend:\n  type: store\n\n# Simple: Filesystem (real files)\nbackend:\n  type: filesystem\n  root_dir: /Users/local/my_project\n\n# Advanced: Composite (hybrid)\nbackend:\n  type: composite\n  default: state\n  routes:\n    /memories/: store\n    /project/: filesystem\n  root_dir: /Users/local/workspace\n</code></pre>"},{"location":"guides/deepagents-integration/#creating-your-own-deepagents-playbook","title":"\ud83d\udccb Creating Your Own DeepAgents Playbook","text":""},{"location":"guides/deepagents-integration/#basic-structure","title":"Basic Structure","text":"<p>Start from a pulled demo agent and modify its playbook to fit your needs instead of copying YAML from docs.</p>"},{"location":"guides/deepagents-integration/#complete-workflow","title":"\ud83d\udd04 Complete Workflow","text":""},{"location":"guides/deepagents-integration/#step-1-initialize-project","title":"Step 1: Initialize Project","text":"<pre><code>super init my_project\ncd my_project\n</code></pre>"},{"location":"guides/deepagents-integration/#step-2-create-or-pull-agent","title":"Step 2: Create or Pull Agent","text":"<pre><code># Option A: Pull prebuilt agent\nsuper agent pull research_agent_deepagents\n\n# Option B: Create custom playbook\n# (Create your_agent_playbook.yaml in agents/your_agent/playbook/)\n</code></pre>"},{"location":"guides/deepagents-integration/#step-3-compile","title":"Step 3: Compile","text":"<pre><code>super agent compile research_agent_deepagents --framework deepagents\n</code></pre> <p>What happens: - Reads playbook YAML - Generates Python pipeline using <code>deepagents_pipeline.py.jinja2</code> template - Creates <code>BaseComponent</code> wrapper for GEPA optimization - Adds BDD test loading - Creates evaluation methods</p> <p>Output: <code>agents/research_agent_deepagents/pipelines/research_agent_deepagents_deepagents_pipeline.py</code></p>"},{"location":"guides/deepagents-integration/#step-4-evaluate","title":"Step 4: Evaluate","text":"<pre><code>super agent evaluate research_agent_deepagents\n</code></pre> <p>What happens: - Loads compiled pipeline - Initializes DeepAgents agent - Runs BDD scenarios from playbook - Tests against expected outputs - Shows pass/fail rate</p> <p>Example Output: <pre><code>Simple research query: PASS\nTechnical comparison: PASS\nComplex research: FAIL\n\nOverall: 2/3 PASS (66.7%)\n</code></pre></p>"},{"location":"guides/deepagents-integration/#step-5-optimize-with-gepa","title":"Step 5: Optimize with GEPA","text":"<pre><code>super agent optimize research_agent_deepagents --auto medium --framework deepagents --reflection-lm ollama:llama3.1:8b\n</code></pre> <p>What happens: - Universal GEPA optimizer analyzes agent performance - Optimizes the <code>system_prompt</code> (the optimizable variable) - Runs multiple iterations with different prompts - Selects best performing version - Saves optimized weights</p> <p>Key Innovation: GEPA optimizes DeepAgents agents even though they're not DSPy!</p>"},{"location":"guides/deepagents-integration/#step-6-re-evaluate","title":"Step 6: Re-evaluate","text":"<pre><code>super agent evaluate research_agent_deepagents\n</code></pre> <p>What happens: - Loads optimized system prompt - Re-runs BDD scenarios - Shows improvement</p> <p>Expected: Higher pass rate after optimization!</p>"},{"location":"guides/deepagents-integration/#step-7-run","title":"Step 7: Run","text":"<pre><code>super agent run research_agent_deepagents --framework deepagents --goal \"Research AI trends in 2025\"\n</code></pre>"},{"location":"guides/deepagents-integration/#how-it-works-under-the-hood","title":"\ud83d\udd27 How It Works Under the Hood","text":"<p>When you run <code>super agent compile research_agent_deepagents --framework deepagents</code>, SuperOptiX generates a pipeline class that includes:</p> <p>High-level: SuperOptiX compiles your playbook into an executable DeepAgents pipeline and wires it to the standard run/evaluate/optimize workflow.</p> <p>Key Points: - Your playbook YAML controls all agent configuration - BDD scenarios define what success looks like - GEPA optimization is automatic - just run <code>super agent optimize</code> - The same workflow works across all frameworks (DSPy, CrewAI, etc.)</p>"},{"location":"guides/deepagents-integration/#deepagents-vs-dspy","title":"\ud83d\udcca DeepAgents vs DSPy","text":"Feature DeepAgents DSPy Framework LangGraph DSPy Strength Complex multi-step tasks Prompt optimization Planning Built-in <code>write_todos</code> Manual implementation Filesystem Built-in tools Manual implementation Subagents Native support Manual composition SuperOptiX Support Full workflow Full workflow GEPA Optimization system_prompt All signatures Model Requirements Function-calling only Any LLM Ollama Support \u26a0\ufe0f Blocked (LangChain issue) Full support <p>When to use DeepAgents: - Complex research tasks - Multi-step workflows requiring planning - Need filesystem for context management - Want subagent delegation</p> <p>When to use DSPy: - Need Ollama/local model support - Focus on prompt optimization - Simpler task structures - Want maximum flexibility</p>"},{"location":"guides/deepagents-integration/#example-use-cases","title":"\ud83c\udf93 Example Use Cases","text":""},{"location":"guides/deepagents-integration/#research-agent","title":"Research Agent","text":"<p>Use the <code>research_agent_deepagents</code> demo as a starting point and adapt.</p>"},{"location":"guides/deepagents-integration/#code-assistant","title":"Code Assistant","text":"<p>Use the <code>code_reviewer</code> demo for file-based analysis tasks.</p>"},{"location":"guides/deepagents-integration/#data-analyst","title":"Data Analyst","text":"<p>Use the <code>researcher_hybrid</code> demo for hybrid storage and multi-step analysis.</p>"},{"location":"guides/deepagents-integration/#advanced-configuration","title":"\u2699\ufe0f Advanced Configuration","text":""},{"location":"guides/deepagents-integration/#custom-subagents","title":"Custom Subagents","text":"<p>Start from a pulled agent and extend its playbook with subagents as needed.</p>"},{"location":"guides/deepagents-integration/#custom-tools","title":"Custom Tools","text":"<p>Extend the pulled agent's <code>tools</code> list for your use case; no need to copy YAML from docs.</p>"},{"location":"guides/deepagents-integration/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"guides/deepagents-integration/#notimplementederror-in-bind_tools","title":"NotImplementedError in bind_tools()","text":"<p>Symptom: <pre><code>NotImplementedError\n  at langchain_core.language_models.chat_models.py line 1491\n</code></pre></p> <p>Cause: Using Ollama model with DeepAgents</p> <p>Solution: Use function-calling capable models: <pre><code>language_model:\n  provider: anthropic\n  model: anthropic:claude-sonnet-4-20250514\n</code></pre></p>"},{"location":"guides/deepagents-integration/#agent-initialization-failed","title":"Agent Initialization Failed","text":"<p>Symptom: \"Failed to initialize DeepAgents\"</p> <p>Checklist: 1. API key set? (<code>ANTHROPIC_API_KEY</code>, <code>OPENAI_API_KEY</code>) 2. Model string correct? (e.g., <code>anthropic:claude-sonnet-4-20250514</code>) 3. LangChain dependencies installed? (<code>pip install langchain langchain-anthropic</code>)</p>"},{"location":"guides/deepagents-integration/#no-bdd-specifications-found","title":"No BDD Specifications Found","text":"<p>Symptom: \"No BDD specifications found!\"</p> <p>Solution: Add scenarios to playbook: <pre><code>feature_specifications:\n  scenarios:\n    - name: Basic test\n      input:\n        query: \"Test query\"\n      expected_output:\n        report: \"Expected result\"\n</code></pre></p> <p>Then recompile: <pre><code>super agent compile your_agent --framework deepagents\n</code></pre></p>"},{"location":"guides/deepagents-integration/#gepa-optimization-details","title":"\ud83c\udfaf GEPA Optimization Details","text":""},{"location":"guides/deepagents-integration/#what-gets-optimized","title":"What Gets Optimized","text":"<p>DeepAgents agents have one optimizable variable: - <code>system_prompt</code>: The main instruction to the agent</p> <p>GEPA automatically: 1. Analyzes agent performance on BDD scenarios 2. Generates variations of the system prompt 3. Tests each variation 4. Selects the best performing prompt 5. Saves optimized version</p>"},{"location":"guides/deepagents-integration/#optimization-example","title":"Optimization Example","text":"<p>Before (Baseline): <pre><code>System Prompt: \"You are an expert researcher.\"\nPass Rate: Baseline performance (varies by hardware/model)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>System Prompt: \"You are an expert researcher. When answering questions:\n1. Use write_todos to plan your research steps\n2. Save findings to research_notes.md\n3. Synthesize information before responding\n...\"\nPass Rate: Improved (results vary by hardware/model)\n</code></pre></p>"},{"location":"guides/deepagents-integration/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>SuperSpec YAML Playbook\n        \u2193\n    Compiler (AgentCompiler)\n        \u2193\nDeepAgents Pipeline Template (deepagents_pipeline.py.jinja2)\n        \u2193\nGenerated Python Pipeline\n        \u251c\u2500 ResearchAgentDeepAgentsComponent (BaseComponent wrapper)\n        \u2502   \u2514\u2500 create_deep_agent() \u2190 Real DeepAgents implementation\n        \u2514\u2500 ResearchAgentDeepAgentsPipeline\n            \u251c\u2500 run()\n            \u251c\u2500 evaluate()\n            \u251c\u2500 optimize_with_gepa() \u2190 Universal GEPA\n            \u2514\u2500 run_bdd_test_suite()\n</code></pre>"},{"location":"guides/deepagents-integration/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>DeepAgents Docs: https://github.com/langchain-ai/deepagents</li> <li>LangGraph Docs: https://langchain-ai.github.io/langgraph/</li> <li>SuperOptiX Multi-Framework Guide: <code>/docs/guides/multi-framework.md</code></li> <li>Universal GEPA: <code>/docs/guides/universal-gepa.md</code></li> </ul>"},{"location":"guides/deepagents-integration/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":""},{"location":"guides/deepagents-integration/#use-detailed-system-prompts","title":"Use Detailed System Prompts","text":"<p>DeepAgents shine with detailed instructions: <pre><code>persona:\n  role: Expert Researcher\n  goal: |\n    Conduct thorough research on AI topics, producing\n    comprehensive, well-sourced reports\n  backstory: |\n    You have 10+ years of experience in AI research\n</code></pre></p>"},{"location":"guides/deepagents-integration/#leverage-planning","title":"Leverage Planning","text":"<p>Always include <code>write_todos</code> in tools: <pre><code>reasoning:\n  steps:\n    - Break down research into subtasks using write_todos\n    - Execute each subtask systematically\n</code></pre></p>"},{"location":"guides/deepagents-integration/#use-filesystem-for-context","title":"Use Filesystem for Context","text":"<p>For long outputs, instruct agent to save to files: <pre><code>constraints:\n  - Save research findings to research_notes.md\n  - Keep main context clean\n</code></pre></p>"},{"location":"guides/deepagents-integration/#start-simple-then-optimize","title":"Start Simple, Then Optimize","text":"<ol> <li>Get baseline working (compile + evaluate)</li> <li>Run optimization (GEPA improves the prompt)</li> <li>Iterate on scenarios based on failures</li> </ol>"},{"location":"guides/deepagents-integration/#comparison-with-other-frameworks","title":"\ud83c\udf93 Comparison with Other Frameworks","text":""},{"location":"guides/deepagents-integration/#deepagents-superoptix-advantages","title":"DeepAgents + SuperOptiX Advantages","text":"<ul> <li>Built-in planning and filesystem</li> <li>GEPA optimization works</li> <li>Standard SuperOptiX workflow</li> <li>Full LangGraph features (streaming, checkpointing, etc.)</li> </ul>"},{"location":"guides/deepagents-integration/#dspy-superoptix-advantages","title":"DSPy + SuperOptiX Advantages","text":"<ul> <li>Works with Ollama (no function-calling requirement)</li> <li>More optimization targets (all signatures, not just system_prompt)</li> <li>Better for simple, focused tasks</li> <li>Native to SuperOptiX</li> </ul>"},{"location":"guides/deepagents-integration/#use-both","title":"Use Both!","text":"<p>SuperOptiX lets you: - Use DSPy for simple tasks with Ollama - Use DeepAgents for complex tasks with Claude/GPT-4 - Optimize both with the same workflow!</p>"},{"location":"guides/deepagents-integration/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Quick start with demos:</p> <pre><code>super agent pull research_agent_deepagents &amp;&amp; super agent compile research_agent_deepagents --framework deepagents\nsuper agent evaluate research_agent_deepagents\n</code></pre>"},{"location":"guides/deepagents-integration/#faq","title":"\u2753 FAQ","text":"<p>Q: Why can't I use Ollama with DeepAgents? A: LangChain's ChatOllama doesn't implement <code>bind_tools()</code> yet. This is a LangChain limitation, not SuperOptiX or DeepAgents. We're tracking the issue.</p> <p>Q: Can I use local models at all? A: Yes! Use DSPy agents which work perfectly with Ollama. The workflow is identical.</p> <p>Q: Does GEPA really optimize DeepAgents? A: Yes! Universal GEPA optimizes the <code>system_prompt</code> through the BaseComponent interface. It's framework-agnostic.</p> <p>Q: Can I mix DSPy and DeepAgents agents? A: Absolutely! Create orchestrations that use both. Each agent uses the best framework for its task.</p> <p>Q: What about other frameworks? A: Coming soon! We're extending the same pattern to CrewAI, Microsoft, OpenAI SDK, and Google ADK.</p>"},{"location":"guides/deepagents-integration/#success-stories","title":"\ud83c\udf89 Success Stories","text":"<p>What Users Are Building: - Complex research agents with multi-step planning - Code assistants with filesystem access - Data analysis agents with specialized subagents - All optimized with GEPA for better performance!</p> <p>The SuperOptiX Advantage: \"Finally, one workflow to rule them all - whether you use DSPy, DeepAgents, or any other framework!\"</p>"},{"location":"guides/deepagents-integration/#additional-resources_1","title":"\ud83d\udcda Additional Resources","text":""},{"location":"guides/deepagents-integration/#start-here-recommended","title":"\ud83c\udfaf Start Here (Recommended)","text":"<ul> <li>\ud83d\ude80 Complete End-to-End Workflow \u2b50 NEW!</li> <li>Step-by-step tutorial with real examples</li> <li>From zero to deployment-ready agent in 30 minutes</li> <li>Includes evaluation, optimization, and deployment</li> <li>All using FREE Gemini API!</li> </ul>"},{"location":"guides/deepagents-integration/#backend-configuration-new-in-020_1","title":"Backend Configuration (NEW in 0.2.0)","text":"<ul> <li>\ud83d\udcd6 Backend Reference Guide - Complete configuration reference</li> <li>\ud83c\udf93 Backends Tutorial - 6 hands-on tutorials  </li> <li>\ud83d\udcdd Integration Summary - What's new and how to use</li> </ul>"},{"location":"guides/deepagents-integration/#model-configuration","title":"Model Configuration","text":"<ul> <li>\ud83d\udd27 Gemini Configuration Guide - Gemini 2.5 setup</li> <li>\ud83e\uddea Gemini Testing Guide - Complete testing workflow</li> </ul>"},{"location":"guides/deepagents-integration/#technical-details","title":"Technical Details","text":"<ul> <li>\ud83d\udd0d Technical Analysis - Why Ollama doesn't work</li> <li>\ud83d\udc1b Fixes Summary - Bug fixes applied</li> <li>\ud83d\ude80 Integration Plan - Implementation details</li> </ul>"},{"location":"guides/deepagents-integration/#demo-agents","title":"Demo Agents","text":"<p>Pull these to see backends in action: <pre><code>super agent pull research_agent_deepagents  # StateBackend (default)\nsuper agent pull chatbot_persistent         # StoreBackend (persistent)\nsuper agent pull code_reviewer              # FilesystemBackend (real files)\nsuper agent pull researcher_hybrid          # CompositeBackend (hybrid)\n</code></pre></p>"},{"location":"guides/deepagents-integration/#external-resources","title":"External Resources","text":"<ul> <li>DeepAgents 0.2.0 Announcement - LangChain blog post</li> <li>DeepAgents GitHub - Source repository</li> <li>LangChain Docs - LangChain documentation</li> </ul> <p>Need help? Check our Documentation or email us at hello@super-agentic.ai!</p>"},{"location":"guides/dspy-optimizers/","title":"\ud83d\udd2c DSPy Optimizers: Pure DSPy Mode","text":"<p>SuperOptiX provides pure DSPy optimization without mixing frameworks. Use DSPy's powerful optimization capabilities while maintaining clean, framework-specific workflows.</p> <p>\ud83c\udf1f Key Achievement: DSPy agents optimized with GEPA achieve 37.5% \u2192 80% improvement (+42.5 points)!</p>"},{"location":"guides/dspy-optimizers/#overview","title":"Overview","text":"<p>DSPy optimizers automatically improve your agent's prompts and reasoning patterns through systematic optimization. SuperOptiX enhances this with:</p> <ul> <li>Pure DSPy Mode: No SuperOptiX mixing - clean DSPy workflows</li> <li>Universal GEPA Integration: Same GEPA optimizer works across all frameworks</li> <li>Memory-Optimized Configurations: Safe defaults for various system specs</li> <li>Advanced Feedback Metrics: Domain-specific evaluation functions</li> <li>Smart Answer Extraction: Handles various output formats automatically</li> <li>Integration with SuperSpec: Seamless YAML-based configuration</li> </ul>"},{"location":"guides/dspy-optimizers/#gepa-the-universal-optimizer","title":"\ud83d\ude80 GEPA: The Universal Optimizer","text":"<p>GEPA (Genetic-Pareto) is SuperOptiX's universal optimizer that works across all 6 frameworks, including DSPy. It's proven to deliver dramatic improvements with minimal training data.</p>"},{"location":"guides/dspy-optimizers/#dspy-gepa-results","title":"DSPy + GEPA Results","text":"<p>Proven Performance on DSPy Agents: - Sentiment Analysis: 37.5% \u2192 80% (+42.5 points improvement) - Variables Optimized: 10+ (signature instructions, field descriptions, reasoning steps) - Sample Efficiency: Achieves improvements with just 3-10 scenarios - Framework: Pure DSPy (no mixing with other frameworks)</p>"},{"location":"guides/dspy-optimizers/#gepa-configuration-for-dspy","title":"GEPA Configuration for DSPy","text":"<pre><code>spec:\n  target_framework: dspy  # Pure DSPy mode\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium                    # light, medium, intensive\n        reflection_lm: qwen3:8b\n        reflection_minibatch_size: 3\n        skip_perfect_score: true\n        add_format_failure_as_feedback: true\n</code></pre> <p>Key Features: - Reflective Learning: Self-improving prompts through feedback - Auto Modes: <code>light</code> (fast), <code>medium</code> (balanced), <code>intensive</code> (thorough) - Advanced Metrics: 7 domain-specific feedback functions - Memory Usage: ~25GB peak - Pure DSPy: No framework mixing</p> <p>Best For: Complex reasoning tasks, mathematical problems, domain-specific optimization</p> <p>\ud83c\udfaf See GEPA in Action: Check out our interactive GEPA demo repository for hands-on examples with before/after comparisons.</p>"},{"location":"guides/dspy-optimizers/#simba-alternative-dspy-optimizer","title":"\u26a1 SIMBA - Alternative DSPy Optimizer","text":"<p>Stochastic introspective mini-batch ascent (when GEPA isn't needed)</p> <pre><code>spec:\n  target_framework: dspy  # Pure DSPy mode\n  optimization:\n    optimizer:\n      name: SIMBA\n      params:\n        metric: answer_exact_match\n        bsize: 8                       # Mini-batch size\n        num_candidates: 4              # Candidate prompts per step\n        max_steps: 4                   # Optimization steps\n        max_demos: 3                   # Few-shot examples\n        temperature_for_sampling: 0.2\n        temperature_for_candidates: 0.2\n</code></pre> <p>Key Features: - Mini-Batch Processing: Efficient batch optimization - Candidate Generation: Multiple prompt variations per step - Temperature Control: Fine-tuned sampling parameters - Memory Usage: ~28GB peak - Pure DSPy: No framework mixing</p> <p>When to Use SIMBA Instead of GEPA: - Large datasets (SIMBA handles batches better) - Performance-critical applications (faster than GEPA) - When you prefer statistical optimization over reflective learning - When you have abundant training data</p> <p>Performance Comparison: - GEPA: 37.5% \u2192 80% (+42.5 points) - Better for complex reasoning - SIMBA: Good for systematic improvement with large datasets</p>"},{"location":"guides/dspy-optimizers/#miprov2-multi-step-instruction-prompt-optimization","title":"\ud83d\udd27 MIPROv2 - Multi-step Instruction Prompt Optimization","text":"<p>Sophisticated prompt engineering with multiple steps</p> <pre><code>optimization:\n  optimizer:\n    name: MIPROv2\n    params:\n      metric: answer_exact_match\n      num_candidates: 8              # Reduced from default 20\n      init_temperature: 1.0\n</code></pre> <p>Key Features: - Multi-Step Optimization: Iterative prompt refinement - Instruction Engineering: Focus on instruction clarity - Candidate Exploration: Multiple prompt variations - Memory Usage: ~20GB peak</p> <p>Best For: Instruction-following tasks, complex workflows, detailed reasoning</p>"},{"location":"guides/dspy-optimizers/#bootstrapfewshot-traditional-few-shot-learning","title":"\ud83d\udcda BootstrapFewShot - Traditional Few-Shot Learning","text":"<p>Reliable bootstrapping with few-shot examples</p> <pre><code>optimization:\n  optimizer:\n    name: BootstrapFewShot\n    params:\n      metric: answer_exact_match\n      max_bootstrapped_demos: 4      # Generated examples\n      max_labeled_demos: 16          # Manual examples\n      max_rounds: 1                  # Optimization rounds\n</code></pre> <p>Key Features: - Bootstrap Learning: Generate examples from existing data - Labeled Examples: Incorporate manual examples - Round Control: Multi-round optimization - Memory Usage: ~18GB peak</p> <p>Best For: Getting started, reliable baselines, limited training data</p>"},{"location":"guides/dspy-optimizers/#bettertogether-ensemble-few-shot-learning","title":"\ud83e\udd1d BetterTogether - Ensemble Few-Shot Learning","text":"<p>Collaborative optimization with ensemble methods</p> <pre><code>optimization:\n  optimizer:\n    name: BetterTogether\n    params:\n      metric: answer_exact_match\n      max_bootstrapped_demos: 3      # Generated examples\n      max_labeled_demos: 12          # Manual examples\n</code></pre> <p>Key Features: - Ensemble Learning: Combine multiple approaches - Collaborative Optimization: Synergistic improvements - Example Integration: Bootstrap + labeled examples - Memory Usage: ~20GB peak</p> <p>Best For: Robust performance, ensemble methods, collaborative learning</p>"},{"location":"guides/dspy-optimizers/#choosing-the-right-optimizer","title":"\ud83c\udfaf Choosing the Right Optimizer","text":""},{"location":"guides/dspy-optimizers/#recommended-start-with-gepa","title":"Recommended: Start with GEPA \u2b50","text":"<p>For most DSPy use cases, GEPA is the recommended choice:</p> <pre><code>spec:\n  target_framework: dspy\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium  # Start here!\n</code></pre> <p>Why GEPA? - Proven Results: 37.5% \u2192 80% improvement - Sample Efficient: Works with 3-10 scenarios - Universal: Same optimizer works across all frameworks - Reflective Learning: Self-improving prompts - Domain Adaptable: Incorporates domain-specific feedback</p>"},{"location":"guides/dspy-optimizers/#when-to-use-alternatives","title":"When to Use Alternatives","text":"Optimizer Use When Memory Speed Best For GEPA Most cases \u2b50 ~25GB Medium Complex reasoning, domain-specific SIMBA Large datasets ~28GB Fast Batch processing, systematic improvement MIPROv2 Instruction tasks ~20GB Medium Multi-step workflows, detailed reasoning BootstrapFewShot Getting started ~18GB Fast Baselines, limited data BetterTogether Ensemble needs ~20GB Medium Robust performance, collaboration"},{"location":"guides/dspy-optimizers/#quick-decision-guide","title":"Quick Decision Guide","text":"<pre><code># Start here for most cases\nsuper agent optimize &lt;agent&gt; --auto medium  # Uses GEPA\n\n# For large datasets\nsuper agent optimize &lt;agent&gt; --optimizer simba\n\n# For instruction-following tasks  \nsuper agent optimize &lt;agent&gt; --optimizer miprov2\n\n# For getting started\nsuper agent optimize &lt;agent&gt; --optimizer bootstrapfewshot\n</code></pre>"},{"location":"guides/dspy-optimizers/#copro-collaborative-prompt-optimization","title":"\ud83d\udd0d COPRO - Collaborative Prompt Optimization","text":"<p>Advanced collaborative optimization \u26a0\ufe0f</p> <pre><code>optimization:\n  optimizer:\n    name: COPRO\n    params:\n      metric: answer_exact_match\n      breadth: 6                     # Search breadth\n      depth: 2                       # Search depth\n      init_temperature: 1.2\n</code></pre> <p>Status: \u26a0\ufe0f Requires <code>LITELLM_DROP_PARAMS=true</code> environment variable</p> <p>Best For: Collaborative optimization, search-based improvement</p>"},{"location":"guides/dspy-optimizers/#knnfewshot-k-nearest-neighbor-learning","title":"\ud83c\udfaf KNNFewShot - K-Nearest Neighbor Learning","text":"<p>Pattern-based optimization \u26a0\ufe0f</p> <pre><code>optimization:\n  optimizer:\n    name: KNNFewShot\n    params:\n      k: 3                          # Nearest neighbors\n</code></pre> <p>Status: \u26a0\ufe0f Requires vectorizer configuration (coming soon)</p> <p>Best For: Pattern recognition, similarity-based learning</p>"},{"location":"guides/dspy-optimizers/#memory-optimization","title":"Memory Optimization","text":"<p>SuperOptiX includes memory-optimized configurations for different system specifications:</p>"},{"location":"guides/dspy-optimizers/#for-128gb-systems-m4-max-high-end-workstations","title":"For 128GB Systems (M4 Max, High-End Workstations)","text":"<ul> <li>SIMBA: <code>bsize: 8</code>, <code>candidates: 4</code>, <code>steps: 4</code></li> <li>MIPROv2: <code>candidates: 8</code> (reduced from 20)</li> <li>Peak Usage: ~28GB (22% of available memory)</li> </ul>"},{"location":"guides/dspy-optimizers/#for-64gb-systems","title":"For 64GB Systems","text":"<ul> <li>SIMBA: <code>bsize: 4</code>, <code>candidates: 2</code>, <code>steps: 3</code></li> <li>MIPROv2: <code>candidates: 4</code></li> <li>Peak Usage: ~15GB (23% of available memory)</li> </ul>"},{"location":"guides/dspy-optimizers/#for-32gb-systems","title":"For 32GB Systems","text":"<ul> <li>BootstrapFewShot: Recommended default</li> <li>BetterTogether: <code>max_bootstrapped_demos: 2</code></li> <li>Peak Usage: ~8GB (25% of available memory)</li> </ul>"},{"location":"guides/dspy-optimizers/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/dspy-optimizers/#smart-answer-extraction","title":"Smart Answer Extraction","text":"<p>SuperOptiX automatically handles various answer formats:</p> <pre><code># LaTeX boxed format\n\"$\\\\boxed{345}$\" \u2192 \"345\"\n\n# Natural language\n\"The answer is 42.\" \u2192 \"42\"\n\n# Algebraic format\n\"x = 3 or x = -1/2\" \u2192 \"3, -1/2\"\n\n# Numeric extraction\n\"The result is 25 square units\" \u2192 \"25\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#domain-specific-feedback-metrics","title":"Domain-Specific Feedback Metrics","text":"<p>Advanced GEPA feedback functions for specialized domains:</p> <ul> <li><code>advanced_math_feedback</code>: Mathematical accuracy with step validation</li> <li><code>medical_accuracy_feedback</code>: Safety-focused medical information</li> <li><code>legal_analysis_feedback</code>: Legal compliance and risk assessment</li> <li><code>vulnerability_detection_feedback</code>: Security analysis</li> <li><code>privacy_preservation_feedback</code>: Data privacy compliance</li> <li><code>data_science_methodology_feedback</code>: Scientific rigor</li> <li><code>multi_component_enterprise_feedback</code>: Enterprise information extraction</li> </ul>"},{"location":"guides/dspy-optimizers/#tier-optimized-defaults","title":"Tier-Optimized Defaults","text":"<p>SuperOptiX automatically selects optimizers based on agent tier:</p> <pre><code># Oracles Tier (\u22645 examples)\noptimizer = \"LabeledFewShot\"\n\n# Oracles Tier (&gt;5 examples)  \noptimizer = \"BootstrapFewShot\"\n\n# Genies Tier\noptimizer = \"BetterTogether\"\n\n# Advanced Tiers (\u226520 examples)\noptimizer = \"GEPA\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#dspy-workflow-with-gepa","title":"\ud83d\ude80 DSPy Workflow with GEPA","text":""},{"location":"guides/dspy-optimizers/#recommended-workflow-gepa-first","title":"Recommended Workflow (GEPA-First)","text":"<pre><code># Initialize project\nsuper init my_dspy_project\ncd my_dspy_project\n\n# Pull a DSPy agent\nsuper agent pull sentiment_analyzer  # Pure DSPy agent\n\n# Compile (pure DSPy mode)\nsuper agent compile sentiment_analyzer\n\n# Baseline evaluation\nsuper agent evaluate sentiment_analyzer\n\n# Optimize with GEPA (recommended!)\nsuper agent optimize sentiment_analyzer --auto medium\n\n# Evaluate optimized version\nsuper agent evaluate sentiment_analyzer  # automatically loads optimized weights\n\n# Run the optimized agent\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"guides/dspy-optimizers/#alternative-workflows","title":"Alternative Workflows","text":"SIMBA (Large Datasets)MIPROv2 (Instructions)BootstrapFewShot (Getting Started) <pre><code>super agent optimize sentiment_analyzer --optimizer simba\n</code></pre> <pre><code>super agent optimize sentiment_analyzer --optimizer miprov2\n</code></pre> <pre><code>super agent optimize sentiment_analyzer --optimizer bootstrapfewshot\n</code></pre>"},{"location":"guides/dspy-optimizers/#pure-dspy-configuration","title":"Pure DSPy Configuration","text":"<p><pre><code># agent_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: sentiment_analyzer\nspec:\n  target_framework: dspy  # Pure DSPy mode\n  optimization:\n    optimizer:\n      name: GEPA  # Recommended optimizer\n      params:\n        auto: medium\n        metric: answer_exact_match\n</code></pre>       reflection_lm: qwen3:8b       reflection_minibatch_size: 3       skip_perfect_score: true       add_format_failure_as_feedback: true <pre><code>### Custom Metrics\n\n```python\n# In optimizer factory\ndef custom_domain_feedback(example, pred, trace=None, *args, **kwargs):\n    \"\"\"Custom domain-specific feedback function.\"\"\"\n    expected = getattr(example, \"answer\", \"\")\n    actual = getattr(pred, \"answer\", \"\")\n\n    # Your custom evaluation logic\n    score = evaluate_domain_specific_accuracy(expected, actual)\n    feedback = generate_improvement_suggestions(expected, actual)\n\n    return Prediction(score=score, feedback=feedback)\n</code></pre></p>"},{"location":"guides/dspy-optimizers/#best-practices","title":"Best Practices","text":""},{"location":"guides/dspy-optimizers/#start-simple-scale-up","title":"Start Simple, Scale Up","text":"<pre><code># Begin with BootstrapFewShot\noptimizer:\n  name: BootstrapFewShot\n\n# Then move to GEPA for advanced needs\noptimizer:\n  name: GEPA\n  params:\n    auto: light  # Start with light mode\n</code></pre>"},{"location":"guides/dspy-optimizers/#monitor-memory-usage","title":"Monitor Memory Usage","text":"<pre><code># Check system memory before optimization\nhtop\n\n# Use memory-safe configurations\n# SuperOptiX provides optimized defaults\n</code></pre>"},{"location":"guides/dspy-optimizers/#leverage-bdd-scenarios","title":"Leverage BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: quadratic_equation\n      input:\n        problem: \"Solve 2x\u00b2 - 5x - 3 = 0\"\n      expected_output:\n        answer: \"x = 3 or x = -1/2\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#domain-specific-optimization","title":"Domain-Specific Optimization","text":"<pre><code># For mathematical problems\noptimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: advanced_math_feedback\n\n# For enterprise applications\noptimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: multi_component_enterprise_feedback\n</code></pre>"},{"location":"guides/dspy-optimizers/#iterative-improvement","title":"Iterative Improvement","text":"<pre><code># Test baseline performance\nsuper agent evaluate my_agent\n\n# Optimize with light settings\nsuper agent optimize my_agent\n\n# Evaluate improvement\nsuper agent evaluate my_agent\n\n# Scale up if needed\n# Edit playbook: auto: light \u2192 medium \u2192 heavy\n</code></pre>"},{"location":"guides/dspy-optimizers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dspy-optimizers/#common-issues","title":"Common Issues","text":""},{"location":"guides/dspy-optimizers/#memory-errors","title":"Memory Errors","text":"<pre><code># Reduce batch sizes in playbook\nbsize: 4          # Instead of 8\nnum_candidates: 2 # Instead of 4\n</code></pre>"},{"location":"guides/dspy-optimizers/#copro-parameter-issues","title":"COPRO Parameter Issues","text":"<pre><code># Set environment variable\nexport LITELLM_DROP_PARAMS=true\nsuper agent optimize my_agent\n</code></pre>"},{"location":"guides/dspy-optimizers/#zero-scores","title":"Zero Scores","text":"<ul> <li>Check answer format compatibility</li> <li>Use smart answer extraction (enabled by default)</li> <li>Verify BDD scenario expected outputs</li> </ul>"},{"location":"guides/dspy-optimizers/#timeout-issues","title":"Timeout Issues","text":"<ul> <li>GEPA optimization takes 3-5 minutes (normal)</li> <li>Use <code>auto: light</code> for faster optimization</li> <li>Monitor system resources</li> </ul>"},{"location":"guides/dspy-optimizers/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Appropriate Optimizer: Match optimizer complexity to task complexity</li> <li>Optimize Memory: Use provided memory-safe configurations</li> <li>Start Local: Test with local models before cloud deployment</li> <li>Monitor Resources: Watch memory and CPU usage during optimization</li> <li>Incremental Improvement: Use <code>auto: light</code> \u2192 <code>medium</code> \u2192 <code>heavy</code> progression</li> </ol>"},{"location":"guides/dspy-optimizers/#integration-examples","title":"Integration Examples","text":""},{"location":"guides/dspy-optimizers/#with-different-model-backends","title":"With Different Model Backends","text":"<pre><code># Ollama (Local)\nlanguage_model:\n  provider: ollama\n  model: llama3.1:8b\n\noptimization:\n  optimizer:\n    name: GEPA\n    params:\n      reflection_lm: qwen3:8b\n</code></pre> <pre><code># OpenAI (Cloud)\nlanguage_model:\n  provider: openai\n  model: gpt-4-turbo\n\noptimization:\n  optimizer:\n    name: MIPROv2\n    params:\n      num_candidates: 12  # Can use more with cloud\n</code></pre>"},{"location":"guides/dspy-optimizers/#with-rag-systems","title":"With RAG Systems","text":"<pre><code># Vector store integration\ntools:\n  - vectorstore_search\n\noptimization:\n  optimizer:\n    name: BetterTogether  # Good for RAG workflows\n    params:\n      max_bootstrapped_demos: 4\n</code></pre>"},{"location":"guides/dspy-optimizers/#with-multi-agent-systems","title":"With Multi-Agent Systems","text":"<pre><code># Orchestra coordination\noptimization:\n  optimizer:\n    name: SIMBA          # Efficient for coordination\n    params:\n      bsize: 6\n      max_steps: 3\n</code></pre>"},{"location":"guides/dspy-optimizers/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Based on testing with llama3.1:8b on M4 Max (128GB):</p> Optimizer Training Time Memory Peak Accuracy Gain Best Use Case BootstrapFewShot 2-3 min 18GB +15-25% Getting started BetterTogether 3-4 min 20GB +20-30% Robust baselines MIPROv2 4-6 min 20GB +25-35% Instruction tasks SIMBA 5-7 min 28GB +30-40% Performance critical GEPA 3-5 min 25GB +35-50% Complex reasoning"},{"location":"guides/dspy-optimizers/#related-documentation","title":"Related Documentation","text":"<ul> <li>DSPy Official Documentation</li> <li>DSPy Optimization Overview</li> <li>GEPA Optimization Guide</li> <li>Agent Development Guide</li> <li>Evaluation &amp; Testing</li> <li>Memory Management</li> <li>Model Management</li> </ul>"},{"location":"guides/dspy-optimizers/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"guides/dspy-optimizers/#complete-dspy-optimizer-workflow","title":"\ud83d\ude80 Complete DSPy Optimizer Workflow","text":"<p>Here are ready-to-run commands for each DSPy optimizer. Each example includes pull, compile, optimize, and test steps.</p>"},{"location":"guides/dspy-optimizers/#gepa-genetic-pareto","title":"GEPA - Genetic-Pareto","text":"<p>Best for: Oracle-tier agents, complex reasoning, mathematical problems</p> <pre><code># Quick start with GEPA demo agent\nsuper agent pull gepa_demo\nsuper agent compile gepa_demo\nsuper agent optimize gepa_demo --timeout 300\nsuper agent evaluate gepa_demo\nsuper agent run gepa_demo --goal \"Demonstrate reflective optimization capabilities\"\n\n# Advanced math with GEPA\nsuper agent pull advanced_math_gepa\nsuper agent compile advanced_math_gepa\nsuper agent optimize advanced_math_gepa --timeout 300\nsuper agent run advanced_math_gepa --goal \"Solve x\u00b2 + 5x - 6 = 0 showing all steps\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#simba-stochastic-introspective-mini-batch-ascent","title":"SIMBA - Stochastic Introspective Mini-Batch Ascent","text":"<p>Best for: Performance-critical applications, systematic improvement</p> <pre><code># Mathematics problems with SIMBA\nsuper agent pull simba_math\nsuper agent compile simba_math\nsuper agent optimize simba_math --timeout 300\nsuper agent evaluate simba_math\nsuper agent run simba_math --goal \"Calculate the area of a circle with radius 7\"\n\n# General SIMBA optimization\nsuper agent pull simba_playbook\nsuper agent compile simba_playbook\nsuper agent optimize simba_playbook --timeout 300\nsuper agent run simba_playbook --goal \"Optimize reasoning with mini-batch processing\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#miprov2-multi-step-instruction-prompt-optimization_1","title":"MIPROv2 - Multi-step Instruction Prompt Optimization","text":"<p>Best for: Instruction-following tasks, detailed reasoning</p> <pre><code># Mathematics with MIPROv2\nsuper agent pull miprov2_math\nsuper agent compile miprov2_math\nsuper agent optimize miprov2_math --timeout 300\nsuper agent evaluate miprov2_math\nsuper agent run miprov2_math --goal \"Solve quadratic equation using multiple methods\"\n\n# General MIPROv2 optimization\nsuper agent pull miprov2_playbook\nsuper agent compile miprov2_playbook\nsuper agent optimize miprov2_playbook --timeout 300\nsuper agent run miprov2_playbook --goal \"Demonstrate multi-step instruction optimization\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#bootstrapfewshot-traditional-few-shot-learning_1","title":"BootstrapFewShot - Traditional Few-Shot Learning","text":"<p>Best for: Getting started, reliable baselines, tool-calling agents</p> <pre><code># Mathematics with Bootstrap\nsuper agent pull bootstrap_math\nsuper agent compile bootstrap_math\nsuper agent optimize bootstrap_math --timeout 300\nsuper agent evaluate bootstrap_math\nsuper agent run bootstrap_math --goal \"Solve algebraic equation with step-by-step explanation\"\n\n# General Bootstrap optimization\nsuper agent pull bootstrapfewshot_playbook\nsuper agent compile bootstrapfewshot_playbook\nsuper agent optimize bootstrapfewshot_playbook --timeout 300\nsuper agent run bootstrapfewshot_playbook --goal \"Demonstrate traditional few-shot learning\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#bettertogether-ensemble-few-shot-learning_1","title":"BetterTogether - Ensemble Few-Shot Learning","text":"<p>Best for: Robust performance, collaborative learning</p> <pre><code># Mathematics with BetterTogether\nsuper agent pull bettertogether_math\nsuper agent compile bettertogether_math\nsuper agent optimize bettertogether_math --timeout 300\nsuper agent evaluate bettertogether_math\nsuper agent run bettertogether_math --goal \"Solve geometry problem using ensemble methods\"\n\n# General BetterTogether optimization\nsuper agent pull bettertogether_playbook\nsuper agent compile bettertogether_playbook\nsuper agent optimize bettertogether_playbook --timeout 300\nsuper agent run bettertogether_playbook --goal \"Demonstrate ensemble optimization\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#copro-collaborative-prompt-optimization_1","title":"COPRO - Collaborative Prompt Optimization","text":"<p>Best for: Search-based improvement (requires special setup)</p> <pre><code># Set required environment variable\nexport LITELLM_DROP_PARAMS=true\n\n# Mathematics with COPRO\nsuper agent pull copro_math\nsuper agent compile copro_math\nsuper agent optimize copro_math --timeout 300\nsuper agent evaluate copro_math\nsuper agent run copro_math --goal \"Solve calculus problem with collaborative optimization\"\n\n# General COPRO optimization\nsuper agent pull copro_playbook\nsuper agent compile copro_playbook\nsuper agent optimize copro_playbook --timeout 300\nsuper agent run copro_playbook --goal \"Demonstrate collaborative prompt optimization\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#knnfewshot-k-nearest-neighbor-learning_1","title":"KNNFewShot - K-Nearest Neighbor Learning","text":"<p>Best for: Pattern recognition, similarity-based learning</p> <pre><code># Mathematics with KNN\nsuper agent pull knn_math\nsuper agent compile knn_math\nsuper agent optimize knn_math --timeout 300\nsuper agent evaluate knn_math\nsuper agent run knn_math --goal \"Solve trigonometry using pattern recognition\"\n\n# General KNN optimization\nsuper agent pull knnfewshot_playbook\nsuper agent compile knnfewshot_playbook\nsuper agent optimize knnfewshot_playbook --timeout 300\nsuper agent run knnfewshot_playbook --goal \"Demonstrate K-nearest neighbor optimization\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#labeledfewshot-traditional-labeled-learning","title":"LabeledFewShot - Traditional Labeled Learning","text":"<p>Best for: Small datasets, simple scenarios</p> <pre><code># Traditional labeled learning\nsuper agent pull labeledfewshot_playbook\nsuper agent compile labeledfewshot_playbook\nsuper agent optimize labeledfewshot_playbook --timeout 300\nsuper agent evaluate labeledfewshot_playbook\nsuper agent run labeledfewshot_playbook --goal \"Demonstrate traditional labeled few-shot learning\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#comparison-workflow","title":"\ud83d\udcca Comparison Workflow","text":"<p>Compare multiple optimizers on the same task:</p> <pre><code># Test all optimizers on math problems\nagents=(\"bootstrap_math\" \"bettertogether_math\" \"simba_math\" \"miprov2_math\")\n\nfor agent in \"${agents[@]}\"; do\n    echo \"Testing $agent optimizer...\"\n    super agent pull $agent\n    super agent compile $agent\n    super agent evaluate $agent &gt; baseline_$agent.txt\n    super agent optimize $agent --timeout 300\n    super agent evaluate $agent &gt; optimized_$agent.txt\n    echo \"Results saved for $agent\"\ndone\n\n# Compare results\necho \"Baseline vs Optimized Performance:\"\nfor agent in \"${agents[@]}\"; do\n    echo \"=== $agent ===\"\n    echo \"Baseline:\" &amp;&amp; cat baseline_$agent.txt\n    echo \"Optimized:\" &amp;&amp; cat optimized_$agent.txt\n    echo \"\"\ndone\n</code></pre>"},{"location":"guides/dspy-optimizers/#domain-specific-quick-starts","title":"\ud83c\udfaf Domain-Specific Quick Starts","text":""},{"location":"guides/dspy-optimizers/#for-mathematical-problem-solving","title":"For Mathematical Problem Solving","text":"<pre><code># Try different optimizers for math\nsuper agent pull advanced_math_gepa    # GEPA for complex reasoning\nsuper agent pull simba_math           # SIMBA for performance\nsuper agent pull miprov2_math         # MIPROv2 for instruction clarity\nsuper agent pull bootstrap_math       # Bootstrap for reliability\n\n# Pick one and optimize\nsuper agent compile simba_math\nsuper agent optimize simba_math --timeout 300\nsuper agent run simba_math --goal \"Find the integral of 2x\u00b3 + 3x\u00b2 - x + 5\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#for-general-purpose-optimization","title":"For General Purpose Optimization","text":"<pre><code># Try general-purpose optimizers\nsuper agent pull gepa_demo            # GEPA demonstration\nsuper agent pull simba_playbook       # SIMBA optimization\nsuper agent pull bettertogether_playbook  # Ensemble methods\nsuper agent pull bootstrapfewshot_playbook  # Traditional approach\n\n# Pick one and test\nsuper agent compile bettertogether_playbook\nsuper agent optimize bettertogether_playbook --timeout 300\nsuper agent run bettertogether_playbook --goal \"Demonstrate optimization capabilities\"\n</code></pre>"},{"location":"guides/dspy-optimizers/#for-tool-calling-agents-genies-tier","title":"For Tool-Calling Agents (Genies Tier+)","text":"<pre><code># GEPA doesn't work with tool-calling agents\n# Use these optimizers instead:\n\nsuper agent pull bootstrapfewshot_playbook  # Recommended default\nsuper agent compile bootstrapfewshot_playbook\nsuper agent optimize bootstrapfewshot_playbook --timeout 300\n\n# Alternative: SIMBA for complex tool interactions\nsuper agent pull simba_playbook\nsuper agent compile simba_playbook\nsuper agent optimize simba_playbook --timeout 300\n\n# Alternative: BetterTogether for robust tool usage\nsuper agent pull bettertogether_playbook\nsuper agent compile bettertogether_playbook\nsuper agent optimize bettertogether_playbook --timeout 300\n</code></pre>"},{"location":"guides/dspy-optimizers/#available-dspy-optimizer-agents","title":"Available DSPy Optimizer Agents","text":"<p>SuperOptiX provides pre-configured agents demonstrating each DSPy optimizer:</p>"},{"location":"guides/dspy-optimizers/#mathematics-focused-agents","title":"\ud83e\uddee Mathematics-Focused Agents","text":"Agent ID Optimizer Best For Command <code>bootstrap_math</code> BootstrapFewShot Reliable baselines <code>super agent pull bootstrap_math</code> <code>bettertogether_math</code> BetterTogether Robust performance <code>super agent pull bettertogether_math</code> <code>simba_math</code> SIMBA Performance critical <code>super agent pull simba_math</code> <code>miprov2_math</code> MIPROv2 Instruction clarity <code>super agent pull miprov2_math</code> <code>copro_math</code> COPRO Search-based optimization <code>super agent pull copro_math</code> <code>knn_math</code> KNNFewShot Pattern recognition <code>super agent pull knn_math</code>"},{"location":"guides/dspy-optimizers/#general-purpose-agents","title":"\ud83d\udd27 General Purpose Agents","text":"Agent ID Optimizer Best For Command <code>gepa_demo</code> GEPA Complex reasoning demo <code>super agent pull gepa_demo</code> <code>bootstrapfewshot_playbook</code> BootstrapFewShot Traditional optimization <code>super agent pull bootstrapfewshot_playbook</code> <code>bettertogether_playbook</code> BetterTogether Ensemble learning <code>super agent pull bettertogether_playbook</code> <code>simba_playbook</code> SIMBA Advanced optimization <code>super agent pull simba_playbook</code> <code>miprov2_playbook</code> MIPROv2 Multi-step instructions <code>super agent pull miprov2_playbook</code> <code>copro_playbook</code> COPRO Collaborative optimization <code>super agent pull copro_playbook</code> <code>knnfewshot_playbook</code> KNNFewShot Similarity-based learning <code>super agent pull knnfewshot_playbook</code> <code>labeledfewshot_playbook</code> LabeledFewShot Traditional labeled learning <code>super agent pull labeledfewshot_playbook</code>"},{"location":"guides/dspy-optimizers/#examples","title":"Examples","text":"<p>Explore working examples in <code>/superoptix/agents/dspy_optimizers/</code>:</p>"},{"location":"guides/dspy-optimizers/#mathematical-problem-solving","title":"Mathematical Problem Solving","text":"<ul> <li><code>bootstrap_math_playbook.yaml</code> - Traditional few-shot for math</li> <li><code>bettertogether_math_playbook.yaml</code> - Ensemble learning for math</li> <li><code>simba_math_playbook.yaml</code> - SIMBA for mathematics</li> <li><code>miprov2_math_playbook.yaml</code> - MIPROv2 advanced prompting</li> <li><code>copro_math_playbook.yaml</code> - Collaborative optimization for math</li> <li><code>knn_math_playbook.yaml</code> - K-nearest neighbor for math</li> </ul>"},{"location":"guides/dspy-optimizers/#general-purpose-optimization","title":"General Purpose Optimization","text":"<ul> <li><code>gepa_playbook.yaml</code> - GEPA configuration and demonstration</li> <li><code>bootstrapfewshot_playbook.yaml</code> - Traditional few-shot learning</li> <li><code>bettertogether_playbook.yaml</code> - Ensemble learning methods</li> <li><code>simba_playbook.yaml</code> - SIMBA optimization</li> <li><code>miprov2_playbook.yaml</code> - Multi-step instruction optimization</li> <li><code>copro_playbook.yaml</code> - Collaborative prompt optimization</li> <li><code>knnfewshot_playbook.yaml</code> - K-nearest neighbor learning</li> <li><code>labeledfewshot_playbook.yaml</code> - Traditional labeled few-shot</li> </ul>"},{"location":"guides/dspy-optimizers/#contributing","title":"Contributing","text":"<p>To add support for new DSPy optimizers:</p> <ol> <li> <p>Update Factory Registry: <pre><code># In optimizer_factory.py\nOPTIMIZER_REGISTRY = {\n    \"new_optimizer\": NewOptimizer,\n    # ...\n}\n</code></pre></p> </li> <li> <p>Add Default Parameters: <pre><code>DEFAULT_PARAMS = {\n    \"new_optimizer\": {\n        \"metric\": \"answer_exact_match\",\n        # optimizer-specific params\n    }\n}\n</code></pre></p> </li> <li> <p>Create Configuration Method: <pre><code>def _configure_new_optimizer(cls, optimizer_class, params):\n    # Custom configuration logic\n    return optimizer_class(**params)\n</code></pre></p> </li> <li> <p>Add Documentation: Update this guide with the new optimizer details.</p> </li> <li> <p>Create Test Agent: Add example playbook in <code>/agents/dspy_optimizers/</code>.</p> </li> </ol> <p>SuperOptiX's DSPy integration provides a powerful, memory-efficient platform for prompt optimization that scales from simple few-shot learning to advanced graph-based optimization algorithms.</p>"},{"location":"guides/enhanced-observability/","title":"Enhanced Observability in SuperOptiX","text":""},{"location":"guides/enhanced-observability/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX provides comprehensive observability with support for multiple backends: - SuperOptiX (local-first, default) - MLFlow (ML experiment tracking) - LangFuse (LLM observability) - Weights &amp; Biases (experiment tracking) - All (log to all backends simultaneously)</p> <p>Unique Feature: Agent-specific metrics including GEPA optimization, protocol usage, and multi-framework comparison.</p>"},{"location":"guides/enhanced-observability/#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"guides/enhanced-observability/#run-agent-with-observability","title":"Run Agent with Observability","text":"<pre><code># Use default (local storage)\nsuper agent run my_agent --goal \"Analyze data\" --observe superoptix\n\n# Use MLFlow\nsuper agent run my_agent --goal \"Analyze data\" --observe mlflow\n\n# Use Weights &amp; Biases\nsuper agent run my_agent --goal \"Analyze data\" --observe wandb\n\n# Use all backends\nsuper agent run my_agent --goal \"Analyze data\" --observe all\n</code></pre>"},{"location":"guides/enhanced-observability/#view-dashboard","title":"View Dashboard","text":"<pre><code># Start local dashboard\npython -c \"from superoptix.observability.simple_dashboard import start_dashboard; start_dashboard()\"\n\n# Opens at http://localhost:8000\n</code></pre>"},{"location":"guides/enhanced-observability/#supported-backends","title":"\ud83d\udcca Supported Backends","text":""},{"location":"guides/enhanced-observability/#superoptix-local-first","title":"SuperOptiX (Local-First)","text":"<p>Pros: - No setup required - Works offline - Fast queries - Privacy-friendly - Built-in dashboard</p> <p>Best For: - Development - Testing - Offline work - Privacy-sensitive projects</p>"},{"location":"guides/enhanced-observability/#mlflow","title":"MLFlow","text":"<p>Pros: - Industry standard - Excellent artifact management - Model registry - Experiment comparison</p> <p>Best For: - ML experiments - Model versioning - Team collaboration - Production ML</p> <p>Setup: <pre><code>pip install mlflow\nmlflow ui  # View at http://localhost:5000\n</code></pre></p>"},{"location":"guides/enhanced-observability/#langfuse","title":"LangFuse","text":"<p>Pros: - LLM-specific features - Automatic token tracking - Built-in cost calculation - User feedback collection</p> <p>Best For: - LLM applications - Real-time monitoring - Cost optimization - A/B testing</p> <p>Setup: <pre><code>pip install langfuse\n# Set environment variables\nexport LANGFUSE_PUBLIC_KEY=\"...\"\nexport LANGFUSE_SECRET_KEY=\"...\"\n</code></pre></p>"},{"location":"guides/enhanced-observability/#weights-biases","title":"Weights &amp; Biases","text":"<p>Pros: - Beautiful visualizations - Experiment tracking - Team collaboration - Hyperparameter tuning</p> <p>Best For: - Research projects - Experiment comparison - Hyperparameter optimization - Team sharing</p> <p>Setup: <pre><code>pip install wandb\nwandb login\n</code></pre></p>"},{"location":"guides/enhanced-observability/#agent-specific-metrics","title":"\ud83c\udf1f Agent-Specific Metrics","text":""},{"location":"guides/enhanced-observability/#gepa-optimization-tracking","title":"GEPA Optimization Tracking","text":"<p>SuperOptiX automatically tracks GEPA optimization runs:</p> <pre><code>from superoptix.observability import get_observability\n\nobs = get_observability(\"my_agent\", backend=\"mlflow\")\n\nobs.log_optimization(\n    agent_name=\"my_agent\",\n    optimizer=\"GEPA\",\n    initial_score=0.65,\n    final_score=0.82,\n    iterations=20,\n    population_size=10,\n    duration_seconds=120\n)\n\n# MLFlow shows:\n# - Initial vs final score\n# - Improvement: +17%\n# - Iterations: 20\n# - Duration: 120s\n</code></pre>"},{"location":"guides/enhanced-observability/#protocol-usage-monitoring","title":"Protocol Usage Monitoring","text":"<p>Track MCP server usage and tool discovery:</p> <pre><code>obs.log_protocol(\n    agent_name=\"github_agent\",\n    protocol_type=\"mcp\",\n    server=\"mcp://localhost:8080/github\",\n    tools_discovered=5,\n    tools_used=[\"search_repositories\", \"get_file_contents\"],\n    tool_success_rate=0.95,\n    avg_latency_ms=250,\n    total_calls=20\n)\n\n# Dashboard shows:\n# - Protocol type (MCP)\n# - Tools discovered: 5\n# - Tools used: 2\n# - Success rate: 95%\n# - Avg latency: 250ms\n</code></pre>"},{"location":"guides/enhanced-observability/#multi-framework-comparison","title":"Multi-Framework Comparison","text":"<p>Compare agent performance across frameworks:</p> <pre><code>frameworks = {\n    \"dspy\": {\"accuracy\": 0.85, \"cost\": 0.05, \"latency_ms\": 1200},\n    \"crewai\": {\"accuracy\": 0.78, \"cost\": 0.08, \"latency_ms\": 1800},\n    \"langraph\": {\"accuracy\": 0.80, \"cost\": 0.06, \"latency_ms\": 1500}\n}\n\nobs.log_framework_comparison(\n    agent_name=\"test_agent\",\n    frameworks=frameworks\n)\n\n# Shows:\n# - Best framework: dspy\n# - Side-by-side metrics\n# - Performance comparison chart\n</code></pre>"},{"location":"guides/enhanced-observability/#cost-tracking","title":"Cost Tracking","text":"<p>Track LLM costs per agent and provider:</p> <pre><code>obs.log_cost(\n    agent_name=\"my_agent\",\n    provider=\"openai\",\n    model=\"gpt-4\",\n    tokens_input=800,\n    tokens_output=700,\n    cost_usd=0.045\n)\n\n# Dashboard shows:\n# - Total cost: $0.045\n# - Tokens: 1,500\n# - Cost per 1K tokens: $0.03\n</code></pre>"},{"location":"guides/enhanced-observability/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"guides/enhanced-observability/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from superoptix.observability import EnhancedSuperOptixTracer\n\n# Create tracer with specific backend\ntracer = EnhancedSuperOptixTracer(\n    agent_id=\"my_agent\",\n    enable_external_tracing=True,\n    observability_backend=\"wandb\",\n    auto_load=False\n)\n\n# Log agent run\ntracer.log_agent_run(\n    agent_name=\"my_agent\",\n    framework=\"dspy\",\n    accuracy=0.85,\n    cost_usd=0.05,\n    tokens_used=1500,\n    latency_ms=1200,\n    success_rate=0.95\n)\n\n# Log optimization\ntracer.log_gepa_optimization(\n    agent_name=\"my_agent\",\n    initial_score=0.65,\n    final_score=0.82,\n    iterations=20,\n    population_size=10\n)\n\n# Get summary\nsummary = tracer.get_agent_summary()\nprint(f\"Total runs: {summary['total_runs']}\")\nprint(f\"Total cost: ${summary['total_cost_usd']:.2f}\")\nprint(f\"Avg accuracy: {summary['avg_accuracy']:.2f}\")\n\n# Export metrics\ntracer.export_agent_metrics(\"metrics.json\")\n</code></pre>"},{"location":"guides/enhanced-observability/#local-storage-direct-access","title":"Local Storage Direct Access","text":"<pre><code>from superoptix.observability.local_storage import LocalObservabilityStorage\n\nstorage = LocalObservabilityStorage()\n\n# Query recent runs\nruns = storage.get_agent_runs(\"my_agent\", limit=10)\nfor run in runs:\n    print(f\"{run['timestamp']}: {run['accuracy']:.2f}\")\n\n# Get cost summary\ncost = storage.get_cost_summary(\"my_agent\", days=30)\nprint(f\"30-day cost: ${cost['total_cost_usd']:.2f}\")\n\n# Get optimization summary\nopts = storage.get_optimization_summary(\"my_agent\")\nprint(f\"Avg improvement: +{opts['avg_improvement']:.2%}\")\n\n# Export to MLFlow\nstorage.export_to_mlflow(\"my_agent\")\n</code></pre>"},{"location":"guides/enhanced-observability/#dashboard-api","title":"Dashboard API","text":"<pre><code># Start dashboard server\npython -m superoptix.observability.simple_dashboard\n\n# API endpoints available:\n# GET /                      - Dashboard UI\n# GET /api/dashboard         - All dashboard data\n# GET /api/runs              - Agent runs\n# GET /api/optimizations     - Optimization history\n# GET /api/protocols         - Protocol usage\n# GET /api/cost/summary      - Cost summary\n</code></pre>"},{"location":"guides/enhanced-observability/#example-workflow","title":"\ud83d\udcc8 Example Workflow","text":""},{"location":"guides/enhanced-observability/#complete-optimization-monitoring-workflow","title":"Complete Optimization &amp; Monitoring Workflow","text":"<pre><code># Create agent with observability\nsuper agent compile my_agent\n\n# Run baseline (with observability)\nsuper agent run my_agent --goal \"Test query\" --observe mlflow\n\n# Evaluate\nsuper agent evaluate my_agent\n\n# Optimize with GEPA (auto-tracked)\nsuper agent optimize my_agent --auto medium\n\n# Run optimized (compare metrics)\nsuper agent run my_agent --goal \"Test query\" --observe mlflow\n\n# View results\nmlflow ui  # See before/after comparison\n</code></pre>"},{"location":"guides/enhanced-observability/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/enhanced-observability/#choose-the-right-backend","title":"Choose the Right Backend","text":"<pre><code># Development: Use local\nsuper agent run my_agent --goal \"...\" --observe superoptix\n\n# Team work: Use MLFlow\nsuper agent run my_agent --goal \"...\" --observe mlflow\n\n# Research: Use W&amp;B\nsuper agent run my_agent --goal \"...\" --observe wandb\n\n# Production: Use all + export\nsuper agent run my_agent --goal \"...\" --observe all\n</code></pre>"},{"location":"guides/enhanced-observability/#track-optimization-history","title":"Track Optimization History","text":"<pre><code># Before each optimization\nobs.log_agent_run(agent_name, framework, accuracy=baseline_score)\n\n# During optimization (automatic if using CLI)\n# super agent optimize my_agent --auto medium\n\n# After optimization\nobs.log_agent_run(agent_name, framework, accuracy=optimized_score)\n</code></pre>"},{"location":"guides/enhanced-observability/#monitor-protocol-usage","title":"Monitor Protocol Usage","text":"<pre><code># Log protocol connections\nobs.log_protocol(\n    agent_name=\"agent\",\n    protocol_type=\"mcp\",\n    server=\"mcp://...\",\n    tools_discovered=5,\n    tools_used=tools_list\n)\n\n# Track over time to see:\n# - Tool usage patterns\n# - Success rates\n# - Latency trends\n</code></pre>"},{"location":"guides/enhanced-observability/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"guides/enhanced-observability/#mlflow-not-logging","title":"MLFlow Not Logging","text":"<p>Issue: Metrics not appearing in MLFlow</p> <p>Solution: 1. Ensure MLFlow is installed: <code>pip install mlflow</code> 2. Start MLFlow UI: <code>mlflow ui</code> 3. Check experiment name matches: <code>SuperOptiX-{agent_name}</code> 4. Verify environment variables if using remote tracking</p>"},{"location":"guides/enhanced-observability/#wb-authentication-failed","title":"W&amp;B Authentication Failed","text":"<p>Issue: \"wandb authentication failed\"</p> <p>Solution: 1. Login: <code>wandb login</code> 2. Provide API key (get from https://wandb.ai/authorize) 3. Verify: <code>wandb verify</code></p>"},{"location":"guides/enhanced-observability/#dashboard-not-starting","title":"Dashboard Not Starting","text":"<p>Issue: Dashboard won't start</p> <p>Solution: 1. Install FastAPI: <code>pip install fastapi uvicorn</code> 2. Check port 8000 is free: <code>lsof -i :8000</code> 3. Try different port: <code>start_dashboard(port=8001)</code></p>"},{"location":"guides/enhanced-observability/#api-reference","title":"\ud83d\udcda API Reference","text":""},{"location":"guides/enhanced-observability/#unifiedobservability","title":"UnifiedObservability","text":"<pre><code>from superoptix.observability.unified_interface import get_observability\n\nobs = get_observability(\n    agent_id=\"my_agent\",\n    backend=\"mlflow\",  # or langfuse, wandb, all\n    enable_external=True\n)\n\n# Log agent run\nobs.log_agent_run(\n    agent_name=\"agent\",\n    framework=\"dspy\",\n    accuracy=0.85,\n    cost_usd=0.05,\n    tokens_used=1500\n)\n\n# Log optimization\nobs.log_optimization(\n    agent_name=\"agent\",\n    optimizer=\"GEPA\",\n    initial_score=0.65,\n    final_score=0.82,\n    iterations=20\n)\n\n# Log protocol usage\nobs.log_protocol(\n    agent_name=\"agent\",\n    protocol_type=\"mcp\",\n    server=\"mcp://...\",\n    tools_discovered=5,\n    tools_used=[\"tool1\", \"tool2\"]\n)\n\n# Get summary\nsummary = obs.get_summary()\n\n# Export\nobs.export(\"metrics.json\", format=\"json\")\n\n# Cleanup\nobs.cleanup()\n</code></pre>"},{"location":"guides/enhanced-observability/#examples","title":"\ud83c\udf93 Examples","text":"<p>See complete examples in: - <code>examples/observability/mlflow_example.py</code> - <code>examples/observability/wandb_example.py</code> - <code>examples/observability/local_storage_example.py</code></p>"},{"location":"guides/enhanced-observability/#what-makes-superoptix-different","title":"\ud83d\ude80 What Makes SuperOptiX Different","text":""},{"location":"guides/enhanced-observability/#unique-agent-specific-metrics","title":"Unique Agent-Specific Metrics","text":"<p>No other platform tracks: - GEPA optimization iterations and improvements - Protocol usage (MCP servers, tools discovered) - Multi-framework performance comparison - Agent-specific cost breakdown</p>"},{"location":"guides/enhanced-observability/#local-first-architecture","title":"Local-First Architecture","text":"<ul> <li>Works without internet</li> <li>No cloud dependency</li> <li>Privacy-friendly</li> <li>Fast queries</li> <li>Export when ready</li> </ul>"},{"location":"guides/enhanced-observability/#user-choice","title":"User Choice","text":"<ul> <li>Pick any backend</li> <li>Mix backends</li> <li>No vendor lock-in</li> <li>Enterprise-approved tools</li> </ul>"},{"location":"guides/enhanced-observability/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Try the local dashboard: <code>python -m superoptix.observability.simple_dashboard</code></li> <li>Integrate with your favorite tool: Use <code>--observe</code> flag</li> <li>Track your optimizations: GEPA metrics automatically logged</li> <li>Monitor protocols: See MCP usage in real-time</li> </ol> <p>SuperOptiX: The ONLY framework with agent-specific observability! \ud83d\ude80</p> <p>Last Updated: 2025-10-20 Version: 1.0.0</p>"},{"location":"guides/evaluation-testing/","title":"\ud83e\uddea Evaluation &amp; Testing Guide","text":"<p>SuperOptiX provides universal evaluation and testing across all 6 major agent frameworks using BDD (Behavior-Driven Development) scenarios. Test your agents consistently regardless of framework choice.</p> <p>\ud83c\udf1f Key Achievement: Same evaluation workflow works across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/evaluation-testing/#overview","title":"Overview","text":"<p>Evaluation in SuperOptiX follows a universal BDD approach that works seamlessly across all supported frameworks:</p> <ul> <li>\ud83d\udd27 Framework Agnostic: Same evaluation commands work for all frameworks</li> <li>\ud83d\udcca BDD Scenarios: Behavior-Driven Development testing approach</li> <li>\u26a1 Consistent Results: Comparable metrics across frameworks</li> <li>\ud83c\udfaf Real-World Testing: Scenarios that mirror actual usage</li> <li>\ud83d\udd04 Automated Validation: CI/CD integration ready</li> </ul>"},{"location":"guides/evaluation-testing/#bdd-evaluation-across-frameworks","title":"\ud83c\udfaf BDD Evaluation Across Frameworks","text":""},{"location":"guides/evaluation-testing/#what-is-bdd-evaluation","title":"What is BDD Evaluation?","text":"<p>BDD evaluation in SuperOptiX uses feature specifications (scenarios) to test agent behavior in realistic situations. Each scenario defines: - Given: The context or situation - When: The action or input - Then: The expected behavior or output</p> <p>This approach ensures your agents perform correctly in real-world scenarios before deployment, regardless of framework.</p>"},{"location":"guides/evaluation-testing/#universal-evaluation-workflow","title":"Universal Evaluation Workflow","text":"<p>Step 1: Choose Your Framework &amp; Pull Agent</p> \ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>super agent pull sentiment_analyzer\nsuper agent compile sentiment_analyzer\nsuper agent evaluate sentiment_analyzer\n</code></pre> <pre><code>super agent pull assistant_openai\nsuper agent compile assistant_openai\nsuper agent evaluate assistant_openai\n</code></pre> <pre><code>super agent pull researcher_crew\nsuper agent compile researcher_crew\nsuper agent evaluate researcher_crew\n</code></pre> <pre><code>super agent pull assistant_adk\nsuper agent compile assistant_adk\nsuper agent evaluate assistant_adk\n</code></pre> <pre><code>super agent pull assistant_microsoft\nsuper agent compile assistant_microsoft\nsuper agent evaluate assistant_microsoft\n</code></pre> <pre><code>super agent pull research_agent_deepagents\nsuper agent compile research_agent_deepagents\nsuper agent evaluate research_agent_deepagents\n</code></pre> <p>Step 2: Evaluate (Same Command for ALL!)</p> <pre><code># Universal evaluation command - works on ANY framework!\nsuper agent evaluate &lt;agent_name&gt;\n\n# Examples for each framework:\nsuper agent evaluate sentiment_analyzer        # DSPy\nsuper agent evaluate assistant_openai          # OpenAI SDK\nsuper agent evaluate researcher_crew           # CrewAI\nsuper agent evaluate assistant_adk             # Google ADK\nsuper agent evaluate assistant_microsoft       # Microsoft\nsuper agent evaluate research_agent_deepagents # DeepAgents\n</code></pre> <p>Step 3: Optimize &amp; Re-evaluate</p> <pre><code># Optimize with GEPA (universal optimizer)\nsuper agent optimize &lt;agent_name&gt; --auto medium\n\n# Re-evaluate optimized version (automatically loads optimized weights)\nsuper agent evaluate &lt;agent_name&gt;\n</code></pre>"},{"location":"guides/evaluation-testing/#multi-framework-testing-results","title":"\ud83d\udcca Multi-Framework Testing Results","text":""},{"location":"guides/evaluation-testing/#proven-results-across-frameworks","title":"Proven Results Across Frameworks","text":"Framework Agent Baseline After GEPA Improvement Status \ud83d\udd2c DSPy Sentiment Analyzer 37.5% 80.0% +42.5 pts \ud83c\udfc6 Proven \ud83e\udd16 OpenAI SDK AI Assistant 100% 100% Maintained Proven \ud83d\udc65 CrewAI Research Crew 75% 100% +25 pts \u2b50 Proven \ud83d\udd2e Google ADK Assistant - - Available Available \ud83c\udfe2 Microsoft Assistant - - Available Available \ud83c\udf0a DeepAgents Research Agent - - Available Available"},{"location":"guides/evaluation-testing/#universal-evaluation-commands","title":"Universal Evaluation Commands","text":"<p>All frameworks use the same evaluation commands:</p> <pre><code># Standard evaluation (works for ALL frameworks)\n# Note: Automatically loads optimized weights if they exist\nsuper agent evaluate &lt;agent_name&gt;\n\n# Verbose output for debugging\nsuper agent evaluate &lt;agent_name&gt; --verbose\n\n# CI/CD integration\nsuper agent evaluate &lt;agent_name&gt; --format json --save-report results.json\n</code></pre>"},{"location":"guides/evaluation-testing/#cli-command-options","title":"CLI Command Options","text":"<p>The <code>super agent evaluate</code> command provides comprehensive testing capabilities with multiple output formats for different use cases:</p>"},{"location":"guides/evaluation-testing/#basic-usage","title":"Basic Usage","text":"<pre><code># Standard evaluation\nsuper agent evaluate &lt;agent_name&gt;\n\n# Verbose output with detailed results\nsuper agent evaluate &lt;agent_name&gt; --verbose\n\n# Auto-tuning for improved evaluation accuracy\nsuper agent evaluate &lt;agent_name&gt; --auto-tune\n</code></pre>"},{"location":"guides/evaluation-testing/#output-formats","title":"Output Formats","text":"<pre><code># Table format (default) - Beautiful console output\nsuper agent evaluate &lt;agent_name&gt; --format table\n\n# JSON format - For CI/CD integration and automation\nsuper agent evaluate &lt;agent_name&gt; --format json\n\n# JUnit format - Compatible with CI/CD systems\nsuper agent evaluate &lt;agent_name&gt; --format junit\n</code></pre>"},{"location":"guides/evaluation-testing/#report-generation","title":"Report Generation","text":"<pre><code># Save detailed report to file\nsuper agent evaluate &lt;agent_name&gt; --save-report test_results.json\n\n# Combine with JSON format for automation\nsuper agent evaluate &lt;agent_name&gt; --format json --save-report results.json\n</code></pre>"},{"location":"guides/evaluation-testing/#development-options","title":"Development Options","text":"<pre><code># Ignore non-essential checks for rapid development\nsuper agent evaluate &lt;agent_name&gt; --ignore-checks\n\n# Verbose mode for detailed analysis\nsuper agent evaluate &lt;agent_name&gt; --verbose\n</code></pre>"},{"location":"guides/evaluation-testing/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"guides/evaluation-testing/#proper-bddtdd-workflow","title":"Proper BDD/TDD Workflow","text":"<p>The correct workflow follows BDD/TDD best practices:</p>"},{"location":"guides/evaluation-testing/#phase-1-specification-driven-development","title":"Phase 1: Specification-Driven Development","text":"<pre><code># Define BDD scenarios FIRST\nvim agents/&lt;agent_name&gt;/Playbook/developer_playbook.yaml\n# Add comprehensive feature_specifications\n\n# Compile agent with scenarios\nsuper agent compile developer\n\n# Run baseline evaluation (should show current performance)\nsuper agent evaluate developer\n# This gives us baseline metrics before optimization\n</code></pre>"},{"location":"guides/evaluation-testing/#phase-2-iterative-improvement","title":"Phase 2: Iterative Improvement","text":"<pre><code># Analyze baseline results\n# - Identify failing scenarios\n# - Understand performance gaps\n# - Plan optimization strategy\n\n# Optimize based on evaluation feedback\nsuper agent optimize developer\n\n# Re-evaluate to measure improvement\nsuper agent evaluate developer\n\n# Iterate until quality gates pass\n# Repeat steps 5-6 until pass rate \u2265 80%\n</code></pre>"},{"location":"guides/evaluation-testing/#phase-3-production-deployment","title":"Phase 3: Production Deployment","text":"<pre><code># Final validation\nsuper agent evaluate developer --verbose\n\n# Deploy only if quality gates pass\nsuper agent run developer --goal \"production task\"\n</code></pre>"},{"location":"guides/evaluation-testing/#github-actions-integration","title":"GitHub Actions Integration","text":"<pre><code># .github/workflows/agent-quality.yml\nname: Agent Quality Check\non: [push, pull_request]\n\njobs:\n  bdd-workflow:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: '3.11'\n\n      - name: Setup SuperOptiX\n        run: pip install superoptix\n\n      - name: Compile Agent\n        run: super agent compile developer\n\n      - name: Baseline Evaluation\n        run: |\n          super agent evaluate developer --format json --save-report baseline.json\n          # Check if baseline is acceptable\n          pass_rate=$(jq -r '.summary.pass_rate' baseline.json | tr -d '%')\n          if (( $(echo \"$pass_rate &lt; 40\" | bc -l) )); then\n            echo \"Baseline too low: $pass_rate%\"\n            exit 1\n          fi\n\n      - name: Optimize Agent\n        run: super agent optimize developer\n\n      - name: Final Evaluation\n        run: |\n          super agent evaluate developer --format json --save-report results.json\n          # Quality gate: must improve by at least 20%\n          improvement=$(jq -r '.improvement.percentage' results.json | tr -d '%')\n          if (( $(echo \"$improvement &lt; 20\" | bc -l) )); then\n            echo \"Insufficient improvement: $improvement%\"\n            exit 1\n          fi\n\n      - name: Upload Test Results\n        uses: actions/upload-artifact@v3\n        with:\n          name: bdd-spec-results\n          path: results.json\n</code></pre>"},{"location":"guides/evaluation-testing/#gitlab-ci-integration","title":"GitLab CI Integration","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - test\n  - optimize\n  - validate\n\nagent-test:\n  stage: test\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent compile developer\n    - super agent evaluate developer --format json --save-report baseline.json\n    - |\n      # Extract pass rate for quality gate\n      PASS_RATE=$(python -c \"\n      import json\n      with open('baseline.json') as f:\n          data = json.load(f)\n      print(data['summary']['pass_rate'].replace('%', ''))\n      \")\n      echo \"Pass rate: $PASS_RATE%\"\n      if [ \"$PASS_RATE\" -lt 40 ]; then\n          echo \"Baseline too low: $PASS_RATE%\"\n          exit 1\n      fi\n  artifacts:\n    reports:\n      junit: test-results.xml\n    paths:\n      - baseline.json\n\nagent-optimize:\n  stage: optimize\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent optimize developer\n  dependencies:\n    - agent-test\n\nagent-validate:\n  stage: validate\n  image: python:3.11\n  script:\n    - pip install superoptix\n    - super agent evaluate developer --format json --save-report final.json\n    - |\n      # Quality gate check\n      IMPROVEMENT=$(python -c \"\n      import json\n      with open('final.json') as f:\n          data = json.load(f)\n      print(data.get('improvement', {}).get('percentage', '0').replace('%', ''))\n      \")\n      echo \"Improvement: $IMPROVEMENT%\"\n      if [ \"$IMPROVEMENT\" -lt 20 ]; then\n          echo \"Insufficient improvement: $IMPROVEMENT%\"\n          exit 1\n      fi\n  dependencies:\n    - agent-optimize\n</code></pre>"},{"location":"guides/evaluation-testing/#jenkins-pipeline-integration","title":"Jenkins Pipeline Integration","text":"<pre><code>// Jenkinsfile\npipeline {\n    agent any\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh 'pip install superoptix'\n            }\n        }\n\n        stage('Compile') {\n            steps {\n                sh 'super agent compile developer'\n            }\n        }\n\n        stage('Baseline Test') {\n            steps {\n                sh 'super agent evaluate developer --format json --save-report baseline.json'\n                script {\n                    def baseline = readJSON file: 'baseline.json'\n                    def passRate = baseline.summary.pass_rate.replace('%', '').toInteger()\n                    if (passRate &lt; 40) {\n                        error \"Baseline too low: ${passRate}%\"\n                    }\n                }\n            }\n        }\n\n        stage('Optimize') {\n            steps {\n                sh 'super agent optimize developer'\n            }\n        }\n\n        stage('Final Test') {\n            steps {\n                sh 'super agent evaluate developer --format json --save-report final.json'\n                script {\n                    def final = readJSON file: 'final.json'\n                    def improvement = final.improvement?.percentage?.replace('%', '')?.toInteger() ?: 0\n                    if (improvement &lt; 20) {\n                        error \"Insufficient improvement: ${improvement}%\"\n                    }\n                }\n            }\n        }\n    }\n\n    post {\n        always {\n            archiveArtifacts artifacts: '*.json', fingerprint: true\n            publishTestResults testResultsPattern: 'test-results.xml'\n        }\n    }\n}\n</code></pre>"},{"location":"guides/evaluation-testing/#azure-devops-integration","title":"Azure DevOps Integration","text":"<pre><code># azure-pipelines.yml\ntrigger:\n  - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nstages:\n- stage: Test\n  displayName: 'Agent Testing'\n  jobs:\n  - job: BDDWorkflow\n    steps:\n    - task: UsePythonVersion@0\n      inputs:\n        versionSpec: '3.11'\n\n    - script: |\n        pip install superoptix\n      displayName: 'Install SuperOptiX'\n\n    - script: |\n        super agent compile developer\n      displayName: 'Compile Agent'\n\n    - script: |\n        super agent evaluate developer --format json --save-report baseline.json\n      displayName: 'Baseline Evaluation'\n\n    - script: |\n        # Check baseline quality gate\n        python -c \"\n        import json\n        with open('baseline.json') as f:\n            data = json.load(f)\n        pass_rate = int(data['summary']['pass_rate'].replace('%', ''))\n        if pass_rate &lt; 40:\n            exit(1)\n        print(f'Baseline pass rate: {pass_rate}%')\n        \"\n      displayName: 'Quality Gate Check'\n\n    - script: |\n        super agent optimize developer\n      displayName: 'Optimize Agent'\n\n    - script: |\n        super agent evaluate developer --format json --save-report final.json\n      displayName: 'Final Evaluation'\n\n    - script: |\n        # Check improvement quality gate\n        python -c \"\n        import json\n        with open('final.json') as f:\n            data = json.load(f)\n        improvement = int(data.get('improvement', {}).get('percentage', '0').replace('%', ''))\n        if improvement &lt; 20:\n            exit(1)\n        print(f'Improvement: {improvement}%')\n        \"\n      displayName: 'Improvement Check'\n\n    - task: PublishTestResults@2\n      inputs:\n        testResultsFormat: 'JUnit'\n        testResultsFiles: 'test-results.xml'\n        mergeTestResults: true\n        testRunTitle: 'BDD Specifications'\n      condition: succeededOrFailed()\n\n    - task: PublishBuildArtifacts@1\n      inputs:\n        PathtoPublish: '*.json'\n        ArtifactName: 'TestResults'\n        publishLocation: 'Container'\n</code></pre>"},{"location":"guides/evaluation-testing/#quality-gates-in-scripts","title":"Quality Gates in Scripts","text":"<p>For custom CI/CD scripts, you can implement quality gates:</p> <pre><code>#!/bin/bash\n# quality-gate.sh\n\n# Set minimum thresholds\nMIN_PASS_RATE=80\nMIN_IMPROVEMENT=20\n\n# Run evaluation and extract metrics\nsuper agent evaluate developer --format json --save-report results.json\n\n# Extract pass rate\nPASS_RATE=$(jq -r '.summary.pass_rate' results.json | tr -d '%')\n\n# Extract improvement percentage\nIMPROVEMENT=$(jq -r '.improvement.percentage' results.json | tr -d '%')\n\necho \"Pass Rate: $PASS_RATE%\"\necho \"Improvement: $IMPROVEMENT%\"\n\n# Quality gate checks\nif [ \"$PASS_RATE\" -lt \"$MIN_PASS_RATE\" ]; then\n    echo \"Quality gate failed: $PASS_RATE% &lt; $MIN_PASS_RATE%\"\n    exit 1\nfi\n\nif [ \"$IMPROVEMENT\" -lt \"$MIN_IMPROVEMENT\" ]; then\n    echo \"Improvement gate failed: $IMPROVEMENT% &lt; $MIN_IMPROVEMENT%\"\n    exit 1\nfi\n\necho \"All quality gates passed!\"\n</code></pre>"},{"location":"guides/evaluation-testing/#best-practices","title":"Best Practices","text":""},{"location":"guides/evaluation-testing/#dos","title":"DO's","text":"<ol> <li>Always evaluate before optimizing</li> <li>Set clear quality gates</li> <li>Measure improvement quantitatively</li> <li>Iterate based on evaluation feedback</li> <li>Use scenarios as both training data and test cases</li> </ol>"},{"location":"guides/evaluation-testing/#donts","title":"DON'Ts","text":"<ol> <li>Don't optimize without baseline</li> <li>Don't skip evaluation after optimization</li> <li>Don't deploy without quality gates</li> <li>Don't ignore failing scenarios</li> </ol>"},{"location":"guides/evaluation-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/evaluation-testing/#common-issues","title":"Common Issues","text":""},{"location":"guides/evaluation-testing/#no-bdd-specifications-found","title":"No BDD Specifications Found","text":"<pre><code>No BDD specifications found!\n\n\ud83d\udca1 Solution:\n1. Edit your agent playbook YAML file\n2. Add 'feature_specifications' section with 'scenarios'\n3. Recompile agent: super agent compile developer\n</code></pre>"},{"location":"guides/evaluation-testing/#low-pass-rates","title":"Low Pass Rates","text":"<pre><code>NEEDS WORK - 30% pass rate\n\n\ud83d\udca1 Solutions:\n\u2022 Run optimization: super agent optimize developer\n\u2022 Upgrade model: Use llama3.1:8b or GPT-4\n\u2022 Review specification complexity vs model capabilities\n\u2022 Improve agent prompts and training data\n</code></pre>"},{"location":"guides/evaluation-testing/#semantic-similarity-issues","title":"Semantic Similarity Issues","text":"<pre><code>semantic meaning differs significantly\n\n\ud83d\udca1 Fix Guidance:\n\u2022 Make responses more relevant to expected output\n\u2022 Use similar terminology and concepts\n\u2022 Address all aspects of the input requirement\n\u2022 Improve response clarity and structure\n</code></pre> <p>\ud83c\udfaf Key Takeaway: Always evaluate before optimizing. This ensures you have a baseline, can measure improvement, and follow proper BDD/TDD practices for reliable AI agent development. </p>"},{"location":"guides/framework-feature-matrix/","title":"Framework Feature Matrix","text":"<p>Current capability snapshot for SuperOptiX framework integrations.</p> Feature DSPy OpenAI SDK Claude SDK Pydantic AI CrewAI Google ADK DeepAgents Microsoft (Legacy) Minimal pipeline compile/run Yes Yes Yes Yes Yes Yes Yes Yes <code>--optimize</code> compile path Yes Yes Yes Yes Yes Yes Yes Yes GEPA optimization flow Yes Yes Yes Yes Yes Yes Yes Yes StackOne connector integration Yes Yes Yes Yes Yes Planned Planned Planned RLM support Yes (experimental) Yes (experimental) Planned Yes (experimental) Yes (experimental) Yes (experimental) Yes (experimental) Planned Local Ollama-friendly path Yes Yes No Yes Yes No No Yes Cloud model routing flags (<code>--cloud</code>) Yes Yes Yes Yes Yes Yes Yes Yes Sidecar compiled spec loading Yes Yes Yes Yes Yes Yes Yes Yes"},{"location":"guides/framework-feature-matrix/#notes","title":"Notes","text":"<ul> <li>RLM is experimental and still evolving.</li> <li>Unified sandbox support for RLM is coming soon.</li> <li>Microsoft framework support is maintained in legacy mode.</li> <li>Cloud-only frameworks typically require function-calling-capable models.</li> </ul>"},{"location":"guides/gepa-optimization/","title":"\ud83d\ude80 GEPA: The Universal Agent Optimizer","text":"<p>GEPA (Genetic-Pareto) is SuperOptiX's universal optimizer that works across all 6 major agent frameworks. With proven results and sample efficiency, GEPA dramatically improves agent performance with minimal training data.</p> <p>\ud83c\udf1f Key Achievement: The world's first optimizer that works across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/gepa-optimization/#what-is-gepa","title":"What is GEPA?","text":""},{"location":"guides/gepa-optimization/#plain-english-explanation","title":"Plain English Explanation","text":"<p>Imagine you have an AI agent that's pretty good at solving math problems, but sometimes makes mistakes or doesn't explain things clearly. Traditional optimization might try thousands of different examples to make it better. GEPA is smarter - it acts like a thoughtful teacher.</p> <p>GEPA looks at what the agent did wrong, thinks about why it went wrong, and then writes better instructions for the agent. It's like having an expert tutor who can say \"I notice you forgot to check your work in algebra problems, so let me give you better guidance on how to approach these step-by-step.\"</p> <p>The \"graph\" part means GEPA builds a family tree of improved instructions, keeping the best ones and building on them to create even better versions.</p> <p>\u2728 The Magic: GEPA does this regardless of which framework you're using, whether it's DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, or DeepAgents. Same optimizer, consistent results!</p>"},{"location":"guides/gepa-optimization/#technical-summary","title":"Technical Summary","text":"<p>GEPA is a reflective prompt optimizer that uses Large Language Models' ability to analyze and critique their own behavior. Unlike traditional optimizers that rely solely on scalar metrics, GEPA leverages textual feedback to drive targeted improvements through:</p> <ol> <li>Reflective Analysis: A reflection LM analyzes agent trajectories to identify specific failure modes and improvement opportunities</li> <li>Prompt Evolution: New prompt candidates are generated based on reflective insights and domain-specific feedback</li> <li>Graph Construction: A tree of evolved prompts is built, with Pareto-aware selection preserving improvements</li> <li>Iterative Refinement: The process repeats, accumulating improvements over multiple generations</li> </ol> <p>Key Innovation: GEPA can utilize domain-specific textual feedback (compiler errors, medical guidelines, security advisories) rather than just numeric scores, enabling more targeted and effective optimization.</p>"},{"location":"guides/gepa-optimization/#why-gepa-is-revolutionary","title":"Why GEPA is Revolutionary","text":"<p>\ud83d\udd27 Framework-Agnostic: The ONLY optimizer that works across all major frameworks. Build with any framework, optimize with one tool.</p> <p>\ud83d\udcca Proven Results: - DSPy: 37.5% \u2192 80% (+42.5 points improvement) - OpenAI SDK: 100% pass rate maintained - CrewAI: 75% \u2192 100% (+25 points improvement) - Google ADK: Ready for optimization - Microsoft: Ready for optimization - DeepAgents: Ready for optimization</p> <p>Sample Efficiency: GEPA achieves significant improvements with just 3-10 scenarios, while other optimizers need hundreds of examples.</p> <p>Domain Adaptability: GEPA excels at incorporating domain-specific knowledge through textual feedback, making it effective for specialized applications.</p> <p>Interpretable Improvements: Unlike black-box optimization, GEPA generates human-readable prompt improvements that you can understand and validate.</p> <p>Multi-Objective Optimization: GEPA can simultaneously optimize for multiple criteria (accuracy, safety, compliance) through its feedback system.</p>"},{"location":"guides/gepa-optimization/#research-foundation","title":"Research Foundation","text":"<p>GEPA is based on cutting-edge research in prompt optimization and reflective learning:</p> <ul> <li>Original Paper: GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</li> <li>DSPy Integration: GEPA Tutorial in DSPy Documentation</li> </ul> <p>The research demonstrates that GEPA can outperform reinforcement learning approaches while requiring significantly less computational resources and training data.</p>"},{"location":"guides/gepa-optimization/#see-gepa-in-action","title":"\ud83d\ude80 See GEPA in Action","text":""},{"location":"guides/gepa-optimization/#interactive-demo-repository","title":"Interactive Demo Repository","text":"<p>For the best hands-on GEPA experience, visit our dedicated demonstration repository:</p> <p>\ud83d\udd17 GEPA Evaluation Demo</p> <p>This repository provides: - Interactive demonstrations of GEPA optimization - Before/after comparisons showing dramatic improvements - Multiple hardware tiers (lightweight for 8GB+ RAM, full demo for 16GB+) - Complete working examples across different domains - Step-by-step optimization walkthrough</p>"},{"location":"guides/gepa-optimization/#quick-demo-commands","title":"Quick Demo Commands","text":"<pre><code># Clone the demo repository\ngit clone https://github.com/SuperagenticAI/gepa-eval.git\ncd gepa-eval\n\n# Setup (installs models and dependencies)\n./scripts/setup.sh\n\n# Lightweight demo (8GB+ RAM, 2-3 minutes)\n./scripts/run_light_demo.sh\n\n# Full demo (16GB+ RAM, 5-10 minutes)  \n./scripts/run_demo.sh\n</code></pre> <p>What you'll see: Watch a basic math agent transform into a sophisticated problem solver with multiple solution methods, verification steps, and pedagogical explanations - all through GEPA's reflective optimization process.</p>"},{"location":"guides/gepa-optimization/#multi-framework-support","title":"\ud83c\udfaf Multi-Framework Support","text":"<p>GEPA works seamlessly across all 6 supported frameworks:</p> Framework Optimizable Variables Status Proven Results \ud83d\udd2c DSPy 10+ variables Proven 37.5% \u2192 80% \ud83e\udd16 OpenAI SDK 1 variable (instructions) Proven 100% pass rate \ud83d\udc65 CrewAI 5 variables (role+goal+backstory+task) Proven 100% pass rate \ud83d\udd2e Google ADK 1 variable (instruction) Available - \ud83c\udfe2 Microsoft 1 variable (instructions) Available - \ud83c\udf0a DeepAgents 1 variable (system_prompt) Available - <p>The same <code>super agent optimize</code> command works for all frameworks!</p>"},{"location":"guides/gepa-optimization/#how-to-use-gepa-in-superoptix","title":"How to Use GEPA in SuperOptiX","text":""},{"location":"guides/gepa-optimization/#universal-workflow-works-for-all-frameworks","title":"Universal Workflow (Works for All Frameworks)","text":"<p>Step 1: Choose Your Framework &amp; Pull Agent</p> \ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>super agent pull sentiment_analyzer\nsuper agent compile sentiment_analyzer\nsuper agent evaluate sentiment_analyzer\n</code></pre> <pre><code>super agent pull assistant_openai\nsuper agent compile assistant_openai\nsuper agent evaluate assistant_openai\n</code></pre> <pre><code>super agent pull researcher_crew\nsuper agent compile researcher_crew\nsuper agent evaluate researcher_crew\n</code></pre> <pre><code>super agent pull assistant_adk\nsuper agent compile assistant_adk\nsuper agent evaluate assistant_adk\n</code></pre> <pre><code>super agent pull assistant_microsoft\nsuper agent compile assistant_microsoft\nsuper agent evaluate assistant_microsoft\n</code></pre> <pre><code>super agent pull research_agent_deepagents\nsuper agent compile research_agent_deepagents\nsuper agent evaluate research_agent_deepagents\n</code></pre> <p>Step 2: Optimize with GEPA (Same Command for ALL!)</p> <pre><code># Universal GEPA command - works on ANY framework!\nsuper agent optimize &lt;agent_name&gt; --auto medium\n\n# Examples for each framework:\nsuper agent optimize sentiment_analyzer --auto medium        # DSPy (uses native GEPA)\nsuper agent optimize assistant_openai --auto medium --framework openai --reflection-lm ollama:llama3.1:8b          # OpenAI SDK\nsuper agent optimize researcher_crew --auto medium --framework crewai --reflection-lm ollama:llama3.1:8b           # CrewAI\nsuper agent optimize assistant_adk --auto medium --framework google-adk --reflection-lm ollama:llama3.1:8b             # Google ADK\nsuper agent optimize assistant_microsoft --auto medium --framework microsoft --reflection-lm ollama:llama3.1:8b       # Microsoft\nsuper agent optimize research_agent_deepagents --auto medium --framework deepagents --reflection-lm ollama:llama3.1:8b # DeepAgents\n</code></pre> <p>\ud83d\udca1 About Reflection Models</p> <p>The <code>--reflection-lm</code> parameter specifies which model GEPA uses to analyze evaluation results and suggest prompt improvements. We typically recommend using a smaller, faster model for reflection:</p> <p>Why use a smaller reflection model (e.g., llama3.1:8b)? - Speed: GEPA runs the reflection model many times (10-50+ iterations). Smaller models make optimization 5-10x faster - Resources: Reduces memory and compute requirements significantly - Good Enough: The reflection task (analyzing results, suggesting improvements) is simpler than the agent's actual task</p> <p>Example: <pre><code># Your agent uses gpt-oss:20b (20B parameters)\n# But reflection uses llama3.1:8b (8B parameters) - much faster!\nsuper agent optimize my_agent --auto medium --reflection-lm ollama:llama3.1:8b\n</code></pre></p> <p>You can use a larger reflection model if needed: <pre><code># For more sophisticated prompt improvements (slower)\nsuper agent optimize my_agent --auto medium --reflection-lm ollama:gpt-oss:70b\n</code></pre></p> <p>Step 3: Evaluate &amp; Deploy</p> <pre><code># Evaluate optimized version\nsuper agent evaluate &lt;agent_name&gt;  # automatically loads optimized weights\n\n# Run in production\nsuper agent run &lt;agent_name&gt;\n</code></pre>"},{"location":"guides/gepa-optimization/#basic-gepa-configuration","title":"Basic GEPA Configuration","text":"<p>Add GEPA optimization to any agent playbook:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match    # Evaluation metric\n        auto: light                   # Budget: light, medium, heavy\n        reflection_lm: qwen3:8b       # Model for reflection\n        reflection_minibatch_size: 3  # Examples per reflection\n        skip_perfect_score: true      # Skip if already perfect\n</code></pre>"},{"location":"guides/gepa-optimization/#using-the-fresh-flag","title":"Using the --fresh Flag","text":"<p>The <code>--fresh</code> flag clears the DSPy cache before optimization, ensuring you see real GEPA iterations instead of cached responses.</p> <p>When to Use --fresh: - \ud83c\udfac Demos &amp; Presentations: Shows the actual optimization process - \ud83d\udcca Production Optimization: Ensures fresh, uncached results - \ud83d\udd0d Debugging: See detailed iteration progress</p> <p>Usage:</p> <pre><code># Clear cache and optimize\nsuper agent optimize &lt;agent_name&gt; --auto light --fresh\n\n# With custom reflection model\nsuper agent optimize developer \\\n  --auto medium \\\n  --reflection-lm llama3.1:8b \\\n  --fresh\n</code></pre> <p>Comparison:</p> Mode Time Output Cache Use Case Default (no <code>--fresh</code>) &lt;5 seconds Minimal Used Development, iteration With <code>--fresh</code> 5-10 min Detailed progress Cleared Demos, production <p>What Happens with --fresh:</p> <pre><code>$ super agent optimize developer --auto light --fresh\n\n\ud83e\uddf9 Clearing DSPy cache (--fresh mode)...\n   Cache cleared: /Users/you/.dspy_cache\n   \ud83d\udd04 Optimization will use fresh LLM calls\n   \u23f1\ufe0f  This will take longer but show real GEPA iterations\n\n\ud83d\udd04 GEPA Iteration 1/5...\n   \ud83d\udcca Analyzing failures...\n   \ud83d\udca1 Generating improved prompts...\n   Pass rate: 45% \u2192 60% (+15%)\n\n\ud83d\udd04 GEPA Iteration 2/5...\n   ...\n</code></pre> <p>Demo Workflow with --fresh:</p> <pre><code># Baseline evaluation\nsuper agent evaluate developer\n# \u2192 Shows 40% pass rate\n\n# Optimize with --fresh (shows real progress!)\nsuper agent optimize developer --auto light --fresh\n# \u2192 Takes 5-10 minutes\n# \u2192 Shows detailed iteration logs\n# \u2192 Stakeholders see the optimization happening\n\n# Post-optimization evaluation\nsuper agent evaluate developer  # automatically loads optimized weights\n# \u2192 Shows 80% pass rate\n# \u2192 Clear improvement demonstrated!\n</code></pre>"},{"location":"guides/gepa-optimization/#domain-specific-gepa-setup","title":"Domain-Specific GEPA Setup","text":""},{"location":"guides/gepa-optimization/#mathematics-agent","title":"Mathematics Agent","text":"<pre><code>optimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: advanced_math_feedback  # Rich mathematical feedback\n      auto: light\n      reflection_lm: qwen3:8b\n      reflection_minibatch_size: 3\n</code></pre>"},{"location":"guides/gepa-optimization/#enterprise-document-analysis","title":"Enterprise Document Analysis","text":"<pre><code>optimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: multi_component_enterprise_feedback  # Multi-aspect evaluation\n      auto: light\n      reflection_lm: qwen3:8b\n      predictor_level_feedback: true              # Component-specific feedback\n</code></pre>"},{"location":"guides/gepa-optimization/#security-analysis","title":"Security Analysis","text":"<pre><code>optimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: vulnerability_detection_feedback     # Security-focused feedback\n      auto: medium                                # More thorough for security\n      reflection_lm: qwen3:8b\n      format_failure_feedback: true              # Handle code format issues\n</code></pre>"},{"location":"guides/gepa-optimization/#memory-efficient-configuration","title":"Memory-Efficient Configuration","text":"<p>For local deployment with limited resources:</p> <pre><code>spec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b        # Main model (~8GB)\n    temperature: 0.1\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: light             # Conservative budget\n        reflection_lm: qwen3:8b # Different model (~8GB)\n        reflection_minibatch_size: 3\n</code></pre> <p>Total Memory Usage: ~16GB for both models, leaving plenty of headroom on a 128GB system.</p>"},{"location":"guides/gepa-optimization/#best-practices","title":"Best Practices","text":""},{"location":"guides/gepa-optimization/#start-with-light-budget","title":"Start with Light Budget","text":"<pre><code># Begin conservatively\nauto: light  # 3-5 minutes, ~400 metric calls\n\n# Increase if results justify cost\nauto: medium  # 8-12 minutes, ~800 metric calls\nauto: heavy   # 15-30 minutes, ~1600 metric calls\n</code></pre>"},{"location":"guides/gepa-optimization/#choose-appropriate-metrics","title":"Choose Appropriate Metrics","text":"<pre><code># For math problems\nmetric: advanced_math_feedback\n\n# For document analysis\nmetric: multi_component_enterprise_feedback\n\n# For privacy-sensitive tasks\nmetric: privacy_preservation_feedback\n\n# For security analysis\nmetric: vulnerability_detection_feedback\n</code></pre>"},{"location":"guides/gepa-optimization/#use-quality-training-scenarios","title":"Use Quality Training Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: comprehensive_test\n      description: Cover main patterns and edge cases\n      input:\n        problem: \"Well-defined, realistic problem\"\n      expected_output:\n        answer: \"Complete expected response with reasoning\"\n</code></pre>"},{"location":"guides/gepa-optimization/#diverse-reflection-models","title":"Diverse Reflection Models","text":"<p>Use different models for main processing and reflection to get diverse perspectives:</p> <pre><code>spec:\n  language_model:\n    model: llama3.1:8b  # Main processing\n  optimization:\n    optimizer:\n      reflection_lm: qwen3:8b  # Different architecture for reflection\n</code></pre>"},{"location":"guides/gepa-optimization/#real-world-results-across-frameworks","title":"\ud83d\udcca Real-World Results Across Frameworks","text":"<p>GEPA has been tested and proven across multiple frameworks. Here are the results:</p>"},{"location":"guides/gepa-optimization/#dspy-sentiment-analysis-agent","title":"DSPy: Sentiment Analysis Agent","text":"<p>Framework: DSPy (Stanford Research Framework) Variables Optimized: 10+ (signature instructions, field descriptions, reasoning steps, etc.)</p> <p>Before GEPA: <pre><code>Pass Rate: 37.5% (3/8 scenarios)\n</code></pre></p> <p>After GEPA Optimization (5 iterations, medium mode): <pre><code>Pass Rate: 80.0% (6.5/8 scenarios)\nImprovement: +42.5 percentage points \ud83c\udfc6\n</code></pre></p> <p>What GEPA Improved: - Better nuanced sentiment identification - Improved sarcasm and context handling - More accurate confidence scores - Clearer reasoning chains</p>"},{"location":"guides/gepa-optimization/#openai-sdk-ai-assistant","title":"OpenAI SDK: AI Assistant","text":"<p>Framework: OpenAI Agents SDK Variables Optimized: 1 (instructions)</p> <p>Before GEPA: <pre><code>Pass Rate: 100% (4/4 scenarios)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Pass Rate: 100% (4/4 scenarios)\nImprovement: Maintained excellence ```\n\n**What GEPA Improved**:\n- Enhanced response quality and clarity\n- Better instruction following\n- More consistent behavior patterns\n- Improved instruction structure\n\n---\n\n### CrewAI: Research Crew\n\n**Framework**: CrewAI (Multi-Agent Collaboration)  \n**Variables Optimized**: 5 (role, goal, backstory, task description, expected output)\n\n**Before GEPA**:\n</code></pre> Pass Rate: 75% (\u00be scenarios) <pre><code>**After GEPA Optimization** (combined agent+task optimization):\n</code></pre> Pass Rate: 100% (4/4 scenarios) Improvement: +25 percentage points \u2b50 <pre><code>**What GEPA Improved**:\n- Clearer role definitions\n- Better goal alignment with tasks\n- Improved agent-task coordination\n- Enhanced task output quality\n- Better multi-agent collaboration patterns\n\n---\n\n### Framework Comparison Table\n\n| Framework | Variables | Baseline | After GEPA | Improvement | Status |\n|-----------|-----------|----------|------------|-------------|--------|\n| **\ud83d\udd2c DSPy** | 10+ | 37.5% | 80.0% | +42.5 pts \ud83c\udfc6 | Proven |\n| **\ud83e\udd16 OpenAI SDK** | 1 | 100% | 100% | Maintained | Proven |\n| **\ud83d\udc65 CrewAI** | 5 | 75% | 100% | +25 pts \u2b50 | Proven |\n| **\ud83d\udd2e Google ADK** | 1 | - | - | Available | Available |\n| **\ud83c\udfe2 Microsoft** | 1 | - | - | Available | Available |\n| **\ud83c\udf0a DeepAgents** | 1 | - | - | Available | Available |\n\n**Key Insights**:\n- GEPA works across all frameworks\n- Proven improvements on 3 frameworks (DSPy, OpenAI SDK, CrewAI)\n- Same optimization workflow for all frameworks\n- 67% of frameworks support local models (Ollama) for free optimization\n\n---\n\n## GEPA in Action\n\n### Case Study 1: Mathematics Problem Solving\n\n**Agent**: Advanced Math Solver\n**Domain**: Quadratic equations and geometric problems\n**Training Data**: 5 BDD scenarios\n\n#### Before GEPA Optimization\n</code></pre> Input: \"Solve x\u00b2 - 5x + 6 = 0\" Output: \"Using the quadratic formula: x = 2 or x = 3\" <pre><code>#### After GEPA Optimization\n</code></pre> Input: \"Solve x\u00b2 - 5x + 6 = 0\" Output: \"Method 1: Factoring Step 1: Look for two numbers that multiply to 6 and add to -5 These numbers are -2 and -3 Factor: (x - 2)(x - 3) = 0 Solutions: x = 2 or x = 3</p> <p>Method 2: Quadratic Formula For ax\u00b2 + bx + c = 0, x = (-b \u00b1 \u221a(b\u00b2-4ac)) / 2a Here: a=1, b=-5, c=6 x = (5 \u00b1 \u221a(25-24)) / 2 = (5 \u00b1 1) / 2 x = 3 or x = 2</p> <p>Verification: x = 2: (2)\u00b2 - 5(2) + 6 = 4 - 10 + 6 = 0 \u2713 x = 3: (3)\u00b2 - 5(3) + 6 = 9 - 15 + 6 = 0 \u2713\" <pre><code>**Results**:\n- Baseline accuracy: 60%\n- Post-GEPA accuracy: 95%\n- Added multiple solution methods\n- Included verification steps\n- Better pedagogical structure\n\n#### GEPA Configuration Used\n```yaml\noptimization:\n  optimizer:\n    name: GEPA\n    params:\n      metric: advanced_math_feedback\n      auto: light\n      reflection_lm: qwen3:8b\n      reflection_minibatch_size: 3\n</code></pre></p>"},{"location":"guides/gepa-optimization/#case-study-2-enterprise-document-analysis","title":"Case Study 2: Enterprise Document Analysis","text":"<p>Agent: Enterprise Information Extractor Domain: Business email classification and risk assessment Training Data: 5 multi-component scenarios</p>"},{"location":"guides/gepa-optimization/#before-gepa-optimization","title":"Before GEPA Optimization","text":"<pre><code>Input: \"URGENT: Water leak in server room affecting infrastructure\"\nOutput: \"This appears to be urgent. Categories: maintenance\"\n</code></pre>"},{"location":"guides/gepa-optimization/#after-gepa-optimization","title":"After GEPA Optimization","text":"<pre><code>Input: \"URGENT: Water leak in server room affecting infrastructure\"\nOutput: \n\"Urgency: Critical - Infrastructure threat requiring immediate response\nSentiment: Negative - Emergency situation with potential damage\nCategories: facility_maintenance, infrastructure_emergency, critical_systems\nEntities: server room, water leak, infrastructure\nAction Items: 1) Contact facilities immediately 2) Assess electrical safety 3) Implement water damage prevention\nRisk Assessment: High risk to critical systems, potential for extended outage, electrical safety concerns\"\n</code></pre> <p>Results: - Component accuracy improved from 45% to 85% - Added comprehensive multi-aspect analysis - Better entity extraction - More detailed risk assessment</p>"},{"location":"guides/gepa-optimization/#case-study-3-security-vulnerability-detection","title":"Case Study 3: Security Vulnerability Detection","text":"<p>Agent: Security Code Analyzer Domain: Web application security analysis Training Data: 5 vulnerability scenarios</p>"},{"location":"guides/gepa-optimization/#before-gepa-optimization_1","title":"Before GEPA Optimization","text":"<pre><code>Input: \"query = 'SELECT * FROM users WHERE name = ' + username\"\nOutput: \"This code has a security issue with SQL injection\"\n</code></pre>"},{"location":"guides/gepa-optimization/#after-gepa-optimization_1","title":"After GEPA Optimization","text":"<pre><code>Input: \"query = 'SELECT * FROM users WHERE name = ' + username\"\nOutput:\n\"Vulnerabilities: Critical - SQL Injection vulnerability in user lookup\nRisk Assessment: Critical severity - allows arbitrary database access, data exfiltration, and potential system compromise\nRemediation Plan: \n1. Immediate: Replace with parameterized queries using prepared statements\n2. Add input validation and sanitization\n3. Implement least-privilege database access controls\n4. Add SQL injection detection monitoring\nSecurity Score: 2/10 - Critical vulnerability requiring immediate attention\nCompliance Status: Fails OWASP Top 10 - Injection (A03:2021)\"\n</code></pre> <p>Results: - Vulnerability detection improved from 70% to 95% - Added detailed remediation guidance - Included compliance framework references - Better risk severity assessment</p>"},{"location":"guides/gepa-optimization/#code-example-custom-gepa-integration","title":"Code Example: Custom GEPA Integration","text":"<pre><code># Custom GEPA configuration in agent pipeline\nfrom superoptix.core.optimizer_factory import DSPyOptimizerFactory\n\n# Create GEPA optimizer with custom feedback\noptimizer = DSPyOptimizerFactory.create_optimizer(\n    optimizer_name=\"GEPA\",\n    params={\n        \"metric\": \"advanced_math_feedback\",  # Custom feedback metric\n        \"auto\": \"light\",                     # Conservative budget\n        \"reflection_lm\": \"qwen3:8b\",        # Reflection model\n        \"reflection_minibatch_size\": 3,\n        \"skip_perfect_score\": True\n    },\n    lm_config={\n        \"model\": \"llama3.1:8b\",\n        \"provider\": \"ollama\",\n        \"temperature\": 0.1\n    }\n)\n\n# Optimize the agent pipeline\noptimized_pipeline = optimizer.compile(\n    student=base_pipeline,\n    trainset=training_examples\n)\n</code></pre>"},{"location":"guides/gepa-optimization/#gepa-vs-simba","title":"GEPA vs. SIMBA","text":"Aspect GEPA SIMBA Approach Reflective prompt evolution Stochastic introspective optimization Feedback Type Rich textual feedback + metrics Primarily metric-based Sample Efficiency High (3-10 examples often sufficient) Medium (requires more examples) Domain Adaptability Excellent (domain-specific feedback) Good (general optimization) Interpretability High (readable prompt improvements) Medium (statistical improvements) Setup Complexity Medium (requires reflection LM) Low (standard configuration) Memory Usage Higher (two models) Lower (single model) Optimization Time Medium (3-5 min light budget) Fast (1-2 min) Multi-Objective Native support Limited support Best For Specialized domains, quality focus General optimization, speed focus Reflection Capability Built-in None Prompt Quality Often generates sophisticated prompts Improves existing prompts"},{"location":"guides/gepa-optimization/#when-to-choose-gepa","title":"When to Choose GEPA","text":"<p>Choose GEPA when: - Working in specialized domains (math, medicine, law, security) - Quality is more important than speed - You have domain-specific feedback requirements - You want interpretable improvements - You need multi-objective optimization - You have limited training data</p> <p>Choose SIMBA when: - You need fast optimization cycles - Working with general-purpose agents - Memory constraints are tight - You have large amounts of training data - Simple metric optimization is sufficient</p>"},{"location":"guides/gepa-optimization/#performance-comparison","title":"Performance Comparison","text":"<p>Based on SuperOptiX benchmarks:</p> Domain GEPA (light) SIMBA GEPA Advantage Mathematics 85% \u2192 95% 85% \u2192 90% +5% accuracy Document Analysis 45% \u2192 85% 45% \u2192 70% +15% accuracy Security Analysis 70% \u2192 95% 70% \u2192 80% +15% accuracy General Q&amp;A 80% \u2192 88% 80% \u2192 85% +3% accuracy"},{"location":"guides/gepa-optimization/#understanding-gepa-behavior","title":"Understanding GEPA Behavior","text":""},{"location":"guides/gepa-optimization/#normal-gepa-logs","title":"Normal GEPA Logs","text":"<p>During optimization, you'll see progress indicators:</p> <pre><code>INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 400 metric calls\nINFO dspy.evaluate.evaluate: Average Metric: 2.0 / 5 (40.0%)\nINFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.4\nINFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.4\nINFO dspy.evaluate.evaluate: Average Metric: 3.0 / 3 (100.0%)\nINFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for predictor\n</code></pre> <p>What this means: - GEPA allocated 400 metric calls for optimization - Started with 40% baseline performance - Making iterative improvements - Achieved 100% on subset evaluation - Generating new prompt candidates</p>"},{"location":"guides/gepa-optimization/#gepa-timeout-behavior","title":"GEPA Timeout Behavior","text":"<p>GEPA optimization often exceeds 2-minute command timeouts:</p> <pre><code>Error: Command timed out after 2m 0.0s\nINFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 400 metric calls\n</code></pre> <p>This is normal behavior because:</p> <ol> <li>Quality Focus: GEPA prioritizes finding better prompts over speed</li> <li>Reflection Process: Multiple LLM calls for analysis and generation</li> <li>Iterative Improvement: Several optimization cycles to build prompt tree</li> <li>Typical Duration: 3-5 minutes for light budget, 8-12 for medium</li> </ol> <p>Solutions: <pre><code># Increase timeout\nsuper agent optimize your_agent --timeout 300  # 5 minutes\n\n# Run in background\nsuper agent optimize your_agent &amp;\n\n# Use lighter budget\n# Edit playbook: max_full_evals: 3 instead of auto: light\n</code></pre></p>"},{"location":"guides/gepa-optimization/#signs-of-successful-gepa-optimization","title":"Signs of Successful GEPA Optimization","text":"<p>Positive Indicators: - Score improvements in logs (40% \u2192 100%) - Multiple iteration cycles - \"Proposed new text for predictor\" messages - Increasingly sophisticated generated prompts</p> <p>Warning Signs: - Scores stuck at 0% (metric configuration issue) - No iteration progress - Reflection LM errors - Memory allocation failures</p>"},{"location":"guides/gepa-optimization/#recommendations-and-caveats","title":"Recommendations and Caveats","text":""},{"location":"guides/gepa-optimization/#recommendations","title":"Recommendations","text":""},{"location":"guides/gepa-optimization/#start-small-and-scale","title":"Start Small and Scale","text":"<pre><code># Begin with light budget\nsuper agent optimize your_agent  # auto: light\n\n# If promising, increase investment\n# Edit playbook to auto: medium, then recompile and optimize\n</code></pre>"},{"location":"guides/gepa-optimization/#use-local-models-for-cost-control","title":"Use Local Models for Cost Control","text":"<pre><code>spec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b  # Free local optimization\n</code></pre>"},{"location":"guides/gepa-optimization/#invest-in-quality-training-data","title":"Invest in Quality Training Data","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: comprehensive_scenario\n      description: Real-world edge cases and common patterns\n      input:\n        problem: \"Complex but realistic problem\"\n      expected_output:\n        answer: \"Complete expected response\"\n        reasoning: \"Step-by-step explanation\"\n</code></pre>"},{"location":"guides/gepa-optimization/#monitor-and-validate-results","title":"Monitor and Validate Results","text":"<pre><code># Always measure improvement\nsuper agent evaluate your_agent  # Before optimization\nsuper agent optimize your_agent\nsuper agent evaluate your_agent  # After optimization - compare results\n</code></pre>"},{"location":"guides/gepa-optimization/#use-domain-appropriate-metrics","title":"Use Domain-Appropriate Metrics","text":"<pre><code># Mathematics\nmetric: advanced_math_feedback\n\n# Business documents\nmetric: multi_component_enterprise_feedback\n\n# Security analysis\nmetric: vulnerability_detection_feedback\n</code></pre>"},{"location":"guides/gepa-optimization/#caveats-and-limitations","title":"Caveats and Limitations","text":""},{"location":"guides/gepa-optimization/#higher-resource-requirements","title":"Higher Resource Requirements","text":"<ul> <li>Memory: Requires two models (main + reflection)</li> <li>Time: Longer optimization cycles than traditional methods</li> <li>Compute: More intensive than simple few-shot optimization</li> </ul>"},{"location":"guides/gepa-optimization/#configuration-complexity","title":"Configuration Complexity","text":"<ul> <li>Requires choosing appropriate reflection model</li> <li>Need domain-specific metrics for best results</li> <li>Budget tuning requires experience</li> </ul>"},{"location":"guides/gepa-optimization/#not-always-superior","title":"Not Always Superior","text":"<ul> <li>For simple tasks, traditional optimization may be sufficient</li> <li>General-purpose agents may not benefit as much</li> <li>Very large datasets might favor other approaches</li> </ul>"},{"location":"guides/gepa-optimization/#model-dependency","title":"Model Dependency","text":"<ul> <li>Quality depends heavily on reflection model capability</li> <li>Some local models may not provide good reflection</li> <li>Cloud models increase costs significantly</li> </ul>"},{"location":"guides/gepa-optimization/#debugging-complexity","title":"Debugging Complexity","text":"<ul> <li>More complex optimization process to debug</li> <li>Harder to isolate issues between main and reflection models</li> <li>Requires understanding of GEPA's iterative process</li> </ul>"},{"location":"guides/gepa-optimization/#when-not-to-use-gepa","title":"When NOT to Use GEPA","text":"<p>Avoid GEPA when: - Working with very simple agents that already perform well - Tight memory constraints (&lt; 16GB available) - Need immediate optimization results - Working with very large training datasets (&gt;100 examples) - Budget constraints require minimal resource usage - Traditional optimization already achieves requirements - Using ReAct agents with tool calling (Genies tier and above)</p>"},{"location":"guides/gepa-optimization/#gepa-and-tool-calling-agents","title":"\u26a0\ufe0f GEPA and Tool-Calling Agents","text":"<p>Important Limitation: GEPA is not compatible with ReAct agents that use tool calling (Genies tier and above). This includes:</p> <ul> <li>Genies Tier Agents: ReAct + Tools + Memory</li> <li>Protocols Tier Agents: Advanced multi-agent systems</li> <li>Any agent with tool integration</li> </ul> <p>Why GEPA doesn't work with tool-calling agents:</p> <ol> <li>Complex Output Format: ReAct agents produce structured outputs with tool calls, reasoning steps, and observations that don't match GEPA's expected simple text format</li> <li>Tool Call Parsing: GEPA's evaluation metrics expect simple string outputs, but ReAct produces complex multi-step trajectories</li> <li>Trajectory Complexity: GEPA analyzes reasoning trajectories, but tool-enhanced ReAct has much more complex multi-step workflows</li> <li>Format Failure Issues: Tool responses often break GEPA's response parsing expectations</li> </ol> <p>Error Symptoms: <pre><code>WARNING: Failed to unpack prediction and trace. This is likely due to the LLM response not following dspy formatting.\nINFO: No trajectories captured. Skipping.\nAverage Metric: 0.0 / 5 (0.0%)\n</code></pre></p> <p>Better Optimizers for Tool-Calling Agents:</p> <p>For Genies tier agents with tools, use these optimizers instead:</p> <pre><code># Recommended for tool-calling agents\noptimization:\n  optimizer:\n    name: BootstrapFewShot  # Default, works well with ReAct+tools\n    params:\n      metric: answer_exact_match\n      max_bootstrapped_demos: 4\n      max_rounds: 1\n</code></pre> <pre><code># Alternative: SIMBA for complex reasoning\noptimization:\n  optimizer:\n    name: SIMBA\n    params:\n      metric: answer_exact_match\n      bsize: 4\n      num_candidates: 2\n      max_steps: 3\n</code></pre> <pre><code># Alternative: BetterTogether for robust performance\noptimization:\n  optimizer:\n    name: BetterTogether\n    params:\n      metric: answer_exact_match\n      max_bootstrapped_demos: 3\n      max_labeled_demos: 12\n</code></pre> <p>Agent Tier Compatibility:</p> Tier Tool Support GEPA Compatible Recommended Optimizer Oracles No tools Yes GEPA (excellent) Genies ReAct + Tools No BootstrapFewShot, SIMBA Protocols Advanced tools No BetterTogether, MIPROv2 Superagents Complex tools No SIMBA, MIPROv2 <p>Summary: Use GEPA for Oracle-tier agents (simple reasoning without tools). For Genies tier and above (with tool calling), use BootstrapFewShot, SIMBA, or BetterTogether optimizers instead.</p>"},{"location":"guides/gepa-optimization/#cost-considerations","title":"Cost Considerations","text":"<p>GEPA Resource Usage:</p> Budget Time Memory Local Cost Cloud Cost (est.) Light 3-5 min ~16GB Free $2-5 Medium 8-12 min ~16GB Free $8-15 Heavy 15-30 min ~16GB Free $20-40 <p>Cost Control Strategies: 1. Use local models for optimization 2. Start with light budgets 3. Optimize incrementally 4. Share optimized weights across team (commit <code>*_optimized.json</code>)</p>"},{"location":"guides/gepa-optimization/#getting-started-with-gepa","title":"Getting Started with GEPA","text":""},{"location":"guides/gepa-optimization/#prerequisites","title":"Prerequisites","text":"<ul> <li>SuperOptiX installation with GEPA support</li> <li>Local models: <code>llama3.1:8b</code> and <code>qwen3:8b</code></li> <li>At least 16GB available memory</li> <li>3-5 quality BDD scenarios for training</li> </ul>"},{"location":"guides/gepa-optimization/#your-first-gepa-optimization","title":"Your First GEPA Optimization","text":"<pre><code># Create or pull a GEPA-ready agent\nsuper agent pull advanced_math_gepa\n\n# Compile and establish baseline\nsuper agent compile advanced_math_gepa\nsuper agent evaluate advanced_math_gepa\n\n# Run GEPA optimization\nsuper agent optimize advanced_math_gepa\n\n# Validate improvements\nsuper agent evaluate advanced_math_gepa\n\n# Test the optimized agent\nsuper agent run advanced_math_gepa --goal \"Solve 2x\u00b2 + 3x - 5 = 0\"\n</code></pre>"},{"location":"guides/gepa-optimization/#available-gepa-agents","title":"Available GEPA Agents","text":"<p>SuperOptiX provides pre-configured GEPA agents across multiple domains. Each agent is optimized for specific use cases and comes with domain-specific feedback metrics.</p>"},{"location":"guides/gepa-optimization/#mathematics-analytics","title":"\ud83e\uddee Mathematics &amp; Analytics","text":""},{"location":"guides/gepa-optimization/#advanced-math-gepa-solver","title":"Advanced Math GEPA Solver","text":"<p>Agent ID: <code>advanced_math_gepa</code> Domain: Advanced mathematical problem solving Specializes in: Quadratic equations, calculus, geometry, algebraic reasoning</p> <pre><code># Quick start with math problems\nsuper agent pull advanced_math_gepa\nsuper agent compile advanced_math_gepa\nsuper agent optimize advanced_math_gepa\nsuper agent run advanced_math_gepa --goal \"Find the derivative of x\u00b3 + 2x\u00b2 - 5x + 1\"\n</code></pre> <p>Key Features: - Step-by-step solution methodology - Multiple solution approaches - Verification and checking - Mathematical notation support - Educational explanations</p>"},{"location":"guides/gepa-optimization/#data-science-gepa","title":"Data Science GEPA","text":"<p>Agent ID: <code>data_science_gepa</code> Domain: Statistical analysis and machine learning Specializes in: Data analysis, statistical inference, ML insights, hypothesis testing</p> <pre><code># Start with data science problems\nsuper agent pull data_science_gepa\nsuper agent compile data_science_gepa\nsuper agent optimize data_science_gepa\nsuper agent run data_science_gepa --goal \"Analyze correlation between customer age and purchase behavior\"\n</code></pre> <p>Key Features: - Statistical methodology validation - Data visualization recommendations - Hypothesis testing frameworks - ML model selection guidance - Scientific rigor validation</p>"},{"location":"guides/gepa-optimization/#healthcare-medical","title":"\ud83c\udfe5 Healthcare &amp; Medical","text":""},{"location":"guides/gepa-optimization/#medical-assistant-gepa","title":"Medical Assistant GEPA","text":"<p>Agent ID: <code>medical_assistant_gepa</code> Domain: Clinical decision support and medical information Specializes in: Medical knowledge synthesis, patient education, clinical reasoning</p> <pre><code># Medical information assistance\nsuper agent pull medical_assistant_gepa\nsuper agent compile medical_assistant_gepa\nsuper agent optimize medical_assistant_gepa\nsuper agent run medical_assistant_gepa --goal \"Explain hypertension treatment options\"\n</code></pre> <p>Key Features: - Safety-focused medical information - Evidence-based recommendations - Patient education materials - Clinical decision support - Medical terminology accuracy</p>"},{"location":"guides/gepa-optimization/#legal-compliance","title":"\u2696\ufe0f Legal &amp; Compliance","text":""},{"location":"guides/gepa-optimization/#contract-analyzer-gepa","title":"Contract Analyzer GEPA","text":"<p>Agent ID: <code>contract_analyzer_gepa</code> Domain: Legal contract analysis and risk assessment Specializes in: Contract review, risk identification, compliance verification</p> <pre><code># Legal contract analysis\nsuper agent pull contract_analyzer_gepa\nsuper agent compile contract_analyzer_gepa\nsuper agent optimize contract_analyzer_gepa\nsuper agent run contract_analyzer_gepa --goal \"Review this software license agreement for compliance risks\"\n</code></pre> <p>Key Features: - Legal risk assessment - Compliance verification - Contract clause analysis - Regulatory framework alignment - Risk mitigation strategies</p>"},{"location":"guides/gepa-optimization/#enterprise-finance","title":"\ud83d\udcbc Enterprise &amp; Finance","text":""},{"location":"guides/gepa-optimization/#enterprise-extractor-gepa","title":"Enterprise Extractor GEPA","text":"<p>Agent ID: <code>enterprise_extractor_gepa</code> Domain: Enterprise document processing and information extraction Specializes in: Multi-component analysis, structured data extraction, business intelligence</p> <pre><code># Enterprise document processing\nsuper agent pull enterprise_extractor_gepa\nsuper agent compile enterprise_extractor_gepa\nsuper agent optimize enterprise_extractor_gepa\nsuper agent run enterprise_extractor_gepa --goal \"Extract key metrics from this quarterly business report\"\n</code></pre> <p>Key Features: - Multi-aspect document analysis - Structured information extraction - Business intelligence insights - Risk assessment integration - Executive summary generation</p>"},{"location":"guides/gepa-optimization/#security-privacy","title":"\ud83d\udd12 Security &amp; Privacy","text":""},{"location":"guides/gepa-optimization/#security-analyzer-gepa","title":"Security Analyzer GEPA","text":"<p>Agent ID: <code>security_analyzer_gepa</code> Domain: Security vulnerability detection and code analysis Specializes in: Vulnerability detection, secure coding practices, security assessment</p> <pre><code># Security code analysis\nsuper agent pull security_analyzer_gepa\nsuper agent compile security_analyzer_gepa\nsuper agent optimize security_analyzer_gepa\nsuper agent run security_analyzer_gepa --goal \"Analyze this code for security vulnerabilities\"\n</code></pre> <p>Key Features: - Vulnerability detection - Security best practices - Remediation guidance - Compliance framework alignment - Risk severity assessment</p>"},{"location":"guides/gepa-optimization/#privacy-delegate-gepa","title":"Privacy Delegate GEPA","text":"<p>Agent ID: <code>privacy_delegate_gepa</code> Domain: Privacy-preserving task delegation and data handling Specializes in: Data anonymization, privacy compliance, secure information handling</p> <pre><code># Privacy-conscious task delegation\nsuper agent pull privacy_delegate_gepa\nsuper agent compile privacy_delegate_gepa\nsuper agent optimize privacy_delegate_gepa\nsuper agent run privacy_delegate_gepa --goal \"Process customer data while maintaining GDPR compliance\"\n</code></pre> <p>Key Features: - Privacy preservation techniques - Data anonymization strategies - Regulatory compliance (GDPR, CCPA) - Secure delegation workflows - Privacy risk assessment</p>"},{"location":"guides/gepa-optimization/#development-demonstration","title":"\ud83d\udee0\ufe0f Development &amp; Demonstration","text":""},{"location":"guides/gepa-optimization/#gepa-demo","title":"GEPA Demo","text":"<p>Agent ID: <code>gepa_demo</code> Domain: GEPA optimizer demonstration and learning Specializes in: Showcasing GEPA capabilities, optimization examples, learning scenarios</p> <pre><code># Learn GEPA optimization\nsuper agent pull gepa_demo\nsuper agent compile gepa_demo\nsuper agent optimize gepa_demo\nsuper agent run gepa_demo --goal \"Demonstrate GEPA's reflective optimization capabilities\"\n</code></pre> <p>Key Features: - GEPA optimization showcase - Before/after comparisons - Learning examples - Optimization metrics demonstration - Best practices illustration</p>"},{"location":"guides/gepa-optimization/#domain-specific-quick-start-commands","title":"Domain-Specific Quick Start Commands","text":""},{"location":"guides/gepa-optimization/#for-mathematics-problems","title":"For Mathematics Problems","text":"<pre><code># Advanced mathematical reasoning\nsuper agent pull advanced_math_gepa\nsuper agent compile advanced_math_gepa\nsuper agent optimize advanced_math_gepa --timeout 300\nsuper agent run advanced_math_gepa --goal \"Solve the system: 2x + 3y = 12, x - y = 1\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-business-analysis","title":"For Business Analysis","text":"<pre><code># Enterprise document processing\nsuper agent pull enterprise_extractor_gepa\nsuper agent compile enterprise_extractor_gepa\nsuper agent optimize enterprise_extractor_gepa --timeout 300\nsuper agent run enterprise_extractor_gepa --goal \"Analyze quarterly revenue trends and identify growth opportunities\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-security-assessment","title":"For Security Assessment","text":"<pre><code># Security vulnerability analysis\nsuper agent pull security_analyzer_gepa\nsuper agent compile security_analyzer_gepa\nsuper agent optimize security_analyzer_gepa --timeout 300\nsuper agent run security_analyzer_gepa --goal \"Review authentication implementation for security vulnerabilities\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-medical-information","title":"For Medical Information","text":"<pre><code># Clinical decision support\nsuper agent pull medical_assistant_gepa\nsuper agent compile medical_assistant_gepa\nsuper agent optimize medical_assistant_gepa --timeout 300\nsuper agent run medical_assistant_gepa --goal \"Explain diabetes management strategies for elderly patients\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-legal-analysis","title":"For Legal Analysis","text":"<pre><code># Contract and legal document review\nsuper agent pull contract_analyzer_gepa\nsuper agent compile contract_analyzer_gepa\nsuper agent optimize contract_analyzer_gepa --timeout 300\nsuper agent run contract_analyzer_gepa --goal \"Review employment contract for compliance with labor laws\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-data-science","title":"For Data Science","text":"<pre><code># Statistical analysis and ML insights\nsuper agent pull data_science_gepa\nsuper agent compile data_science_gepa\nsuper agent optimize data_science_gepa --timeout 300\nsuper agent run data_science_gepa --goal \"Design A/B test for mobile app feature rollout\"\n</code></pre>"},{"location":"guides/gepa-optimization/#for-privacy-sensitive-tasks","title":"For Privacy-Sensitive Tasks","text":"<pre><code># Privacy-preserving data processing\nsuper agent pull privacy_delegate_gepa\nsuper agent compile privacy_delegate_gepa\nsuper agent optimize privacy_delegate_gepa --timeout 300\nsuper agent run privacy_delegate_gepa --goal \"Process user analytics while maintaining privacy compliance\"\n</code></pre>"},{"location":"guides/gepa-optimization/#related-documentation","title":"Related Documentation","text":"<ul> <li>Optimization Guide - General optimization techniques and strategies</li> <li>Agent Development Guide - Complete agent development workflow</li> <li>Evaluation &amp; Testing Guide - Testing methodologies and metrics</li> <li>GEPA Integration Examples - Practical implementation examples</li> <li>Technical Architecture - System architecture and components</li> </ul>"},{"location":"guides/golden-workflow/","title":"Golden Workflow","text":"<p>This is the default workflow for SuperOptiX across frameworks.</p>"},{"location":"guides/golden-workflow/#core-sequence","title":"Core Sequence","text":"<pre><code># Pull demo or existing agent\nsuper agent pull &lt;agent_id&gt;\n\n# Compile minimal framework-native pipeline\nsuper agent compile &lt;agent_id&gt; --framework &lt;framework&gt;\n\n# Run minimal pipeline\nsuper agent run &lt;agent_id&gt; --framework &lt;framework&gt; --goal \"your goal\"\n</code></pre>"},{"location":"guides/golden-workflow/#optimization-sequence","title":"Optimization Sequence","text":"<p>Only use this when you want optimization/evaluation lifecycle code paths.</p> <pre><code># Compile optimize-capable pipeline\nsuper agent compile &lt;agent_id&gt; --framework &lt;framework&gt; --optimize\n\n# Run GEPA loop\nsuper agent optimize &lt;agent_id&gt; --framework &lt;framework&gt; --auto light\n</code></pre>"},{"location":"guides/golden-workflow/#local-vs-cloud","title":"Local vs Cloud","text":"<pre><code># Local\nsuper agent run &lt;agent_id&gt; --framework &lt;framework&gt; --local --provider ollama --model llama3.1:8b --goal \"...\"\n\n# Cloud (Google)\nsuper agent run &lt;agent_id&gt; --framework &lt;framework&gt; --cloud --provider google-genai --model gemini-2.5-flash --goal \"...\"\n</code></pre>"},{"location":"guides/golden-workflow/#required-api-keys","title":"Required API Keys","text":"<ul> <li><code>google-genai</code>: <code>GOOGLE_API_KEY</code></li> <li><code>openai</code>: <code>OPENAI_API_KEY</code></li> <li><code>anthropic</code> / <code>claude-sdk</code>: <code>ANTHROPIC_API_KEY</code></li> <li><code>stackone</code> tools: <code>STACKONE_API_KEY</code> and usually <code>STACKONE_ACCOUNT_IDS</code></li> </ul>"},{"location":"guides/golden-workflow/#minimal-vs-optimize","title":"Minimal vs Optimize","text":"<pre><code>flowchart LR\n  A[SuperSpec Playbook] --&gt; B[Compile Minimal]\n  B --&gt; C[Run Agent]\n  A --&gt; D[Compile With --optimize]\n  D --&gt; E[Optimize / Evaluate Loop]</code></pre>"},{"location":"guides/golden-workflow/#framework-names","title":"Framework Names","text":"<p>Use these values with <code>--framework</code>:</p> <ul> <li><code>dspy</code></li> <li><code>openai</code></li> <li><code>claude-sdk</code></li> <li><code>pydantic-ai</code></li> <li><code>crewai</code></li> <li><code>google-adk</code></li> <li><code>deepagents</code></li> <li><code>microsoft</code> (legacy support)</li> </ul>"},{"location":"guides/google-adk-integration/","title":"\ud83e\udd16 Google ADK Integration","text":""},{"location":"guides/google-adk-integration/#overview","title":"Overview","text":"<p>SuperOptiX now supports Google ADK (Agent Development Kit) - Google's code-first Python toolkit optimized for Gemini models. This integration brings GEPA optimization to Google's enterprise-grade agent framework.</p> <p>RLM support is experimental. Unified sandbox support is coming soon.</p> <p>Key Features: - Gemini 2.0 Flash integration (Google's latest model!) - GEPA optimization of agent instructions - Deployment deployment (Cloud Run, Vertex AI) - Built-in evaluation framework - Standard SuperOptiX workflow (compile/evaluate/optimize/run)</p>"},{"location":"guides/google-adk-integration/#quick-start","title":"Quick Start","text":""},{"location":"guides/google-adk-integration/#prerequisites","title":"Prerequisites","text":"<p>Install SuperOptiX with Google ADK:</p> <pre><code>pip install superoptix[frameworks-google]\n</code></pre> <p>Includes: - google-adk 1.17.0 - google-generativeai (latest) - SuperOptiX core with GEPA 0.0.17</p> <p>Get a Google API Key: 1. Visit Google AI Studio 2. Create a free API key 3. Set environment variable:</p> <pre><code>export GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"guides/google-adk-integration/#create-project","title":"Create Project","text":"<pre><code>super init my_project\ncd my_project\n</code></pre>"},{"location":"guides/google-adk-integration/#pull-demo-agent","title":"Pull Demo Agent","text":"<pre><code>super agent pull assistant_adk\n</code></pre>"},{"location":"guides/google-adk-integration/#compile","title":"Compile","text":"<pre><code>super agent compile assistant_adk --framework google-adk\n</code></pre>"},{"location":"guides/google-adk-integration/#evaluate","title":"Evaluate","text":"<pre><code>export GOOGLE_API_KEY=\"your-key\"  # Required!\nsuper agent evaluate assistant_adk\n</code></pre>"},{"location":"guides/google-adk-integration/#optimize","title":"Optimize","text":"<pre><code>super agent optimize assistant_adk --framework google-adk --auto medium\n</code></pre>"},{"location":"guides/google-adk-integration/#run","title":"Run","text":"<pre><code>super agent run assistant_adk --framework google-adk --goal \"What is AI?\"\n</code></pre>"},{"location":"guides/google-adk-integration/#how-it-works","title":"How It Works","text":""},{"location":"guides/google-adk-integration/#superoptix-workflow-with-google-adk","title":"SuperOptiX Workflow with Google ADK","text":"<pre><code>graph LR\n    A[SuperSpec YAML] --&gt;|compile| B[Google ADK Python]\n    B --&gt;|evaluate| C[BDD Scenarios]\n    C --&gt;|optimize| D[GEPA Optimizer]\n    D --&gt;|run| E[runner.run]</code></pre>"},{"location":"guides/google-adk-integration/#compile-superspec-google-adk","title":"Compile: SuperSpec \u2192 Google ADK","text":"<p>Input: SuperSpec playbook (YAML)</p> <pre><code>persona:\n  role: Helpful AI Assistant\n  goal: Provide accurate responses\n  backstory: You are powered by Google's Gemini\n</code></pre> <p>Output: Google ADK Python code</p> <pre><code>from google.adk import Agent, Runner\nfrom google.adk.runners import InMemoryRunner\nfrom google.genai import types\n\nagent = Agent(\n    model=\"gemini-2.0-flash\",\n    name=\"assistant_adk\",\n    description=\"AI assistant built with Google ADK\",\n    instruction=\"\"\"You are a Helpful AI Assistant\nYour goal is: Provide accurate, helpful responses\nYou are powered by Google's Gemini...\"\"\",\n    tools=[],\n)\n\nrunner = InMemoryRunner(agent=agent, app_name=\"superoptix_assistant\")\n</code></pre>"},{"location":"guides/google-adk-integration/#evaluate-test-on-bdd-scenarios","title":"Evaluate: Test on BDD Scenarios","text":"<pre><code># Create session\nsession = await runner.session_service.create_session(app_name, user_id)\n\n# Run agent\ncontent = types.Content(role='user', parts=[types.Part.from_text(text=query)])\nasync for event in runner.run_async(user_id, session.id, content):\n    response = event.content.parts[0].text\n</code></pre>"},{"location":"guides/google-adk-integration/#optimize-gepa-optimizes-instruction","title":"Optimize: GEPA Optimizes Instruction","text":"<p>Optimizable Variable:</p> <pre><code>You are a Helpful AI Assistant\nYour goal is: Provide accurate responses\nYou are powered by Google's Gemini.\n</code></pre> <p>GEPA Process: 1. Generate 5-10 instruction variations 2. Test each on training scenarios 3. Select best performer 4. Save optimized instruction</p> <p>Example Optimized Output:</p> <pre><code>You are an expert AI assistant specialized in providing accurate, well-researched responses.\n\nYour goal is: Deliver comprehensive answers that are:\n  - Factually accurate and up-to-date\n  - Well-structured and easy to understand\n  - Backed by reasoning and examples\n  - Tailored to the user's level of understanding\n\nYour approach:\n  1. Carefully analyze the user's question\n  2. Break down complex queries into manageable parts\n  3. Provide clear, step-by-step explanations\n  4. Include relevant examples and analogies\n  5. Verify accuracy before responding\n\nBackground: You are powered by Google's Gemini 2.0, a state-of-the-art\nAI model with advanced reasoning capabilities. You excel at explaining\ncomplex topics clearly and providing actionable insights.\n</code></pre>"},{"location":"guides/google-adk-integration/#run-execute-with-optimized-instruction","title":"Run: Execute with Optimized Instruction","text":"<pre><code>super agent run assistant_adk --framework google-adk --goal \"Explain quantum computing\"\n# Uses optimized instruction \u2192 Better results!\n</code></pre>"},{"location":"guides/google-adk-integration/#what-gepa-can-optimize","title":"What GEPA Can Optimize","text":""},{"location":"guides/google-adk-integration/#primary-target-instruction","title":"Primary Target: Instruction","text":"<p>GEPA optimizes the instruction field - the system prompt guiding agent behavior:</p> Component Impact Example instruction HIGH \"You are a helpful assistant\" \u2192 \"You are an expert AI assistant specializing in...\" <p>Why Single Variable? - Clear optimization target - Directly impacts behavior - Similar to OpenAI SDK (proven successful) - Measurable improvements</p> <p>Note: Google ADK focuses optimization on the agent instruction, similar to OpenAI SDK.</p>"},{"location":"guides/google-adk-integration/#creating-google-adk-agents","title":"Creating Google ADK Agents","text":""},{"location":"guides/google-adk-integration/#superspec-playbook-structure","title":"SuperSpec Playbook Structure","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant_adk\n  id: assistant_adk\n  version: 1.0.0\n\nspec:\n  target_framework: google-adk\n\n  # LLM Configuration - Gemini models\n  language_model:\n    provider: google\n    model: gemini-2.0-flash  # Free access!\n    # Alternative: gemini-1.5-pro, gemini-1.5-flash\n\n  # Input/Output Fields\n  input_fields:\n    - name: query\n      type: str\n      required: true\n\n  output_fields:\n    - name: response\n      type: str\n      required: true\n\n  # Agent Configuration (builds instruction for GEPA)\n  persona:\n    role: AI Assistant\n    goal: Help users with questions\n    backstory: |\n      You are a helpful AI assistant powered by Gemini.\n      You provide accurate, clear responses.\n    traits:\n      - helpful\n      - accurate\n\n  reasoning:\n    steps:\n      - Understand the question\n      - Formulate response\n      - Provide insights\n\n  # BDD Scenarios\n  feature_specifications:\n    scenarios:\n      - name: Simple greeting\n        input:\n          query: \"Hello, how are you?\"\n        expected_output:\n          response: \"I am doing well\"\n          expected_keywords:\n            - hello\n            - well\n</code></pre>"},{"location":"guides/google-adk-integration/#field-mapping-superspec-google-adk","title":"Field Mapping: SuperSpec \u2192 Google ADK","text":"SuperSpec Field Google ADK Field Usage <code>persona.role</code> + <code>goal</code> + <code>backstory</code> <code>Agent.instruction</code> Combined into instruction <code>language_model.model</code> <code>Agent.model</code> Model name <code>metadata.description</code> <code>Agent.description</code> Agent description <code>tools</code> <code>Agent.tools</code> Tool list"},{"location":"guides/google-adk-integration/#model-configuration","title":"Model Configuration","text":""},{"location":"guides/google-adk-integration/#supported-models","title":"Supported Models","text":"Model Description Best For <code>gemini-2.0-flash</code> Latest, fastest, FREE access! Recommended <code>gemini-1.5-pro</code> More capable, longer context Complex tasks <code>gemini-1.5-flash</code> Fast, efficient Simple tasks"},{"location":"guides/google-adk-integration/#api-key-setup","title":"API Key Setup","text":"<p>Option 1: AI Studio (Easiest) <pre><code># Get free API key\nVisit: https://aistudio.google.com/apikey\n\n# Set environment variable\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre></p> <p>Option 2: Google Cloud (Deployment) <pre><code># Authenticate with gcloud\ngcloud auth application-default login\n\n# ADK will use Application Default Credentials\n</code></pre></p>"},{"location":"guides/google-adk-integration/#example-configuration","title":"Example Configuration","text":"<pre><code>language_model:\n  provider: google\n  model: gemini-2.0-flash\n  # No API key in playbook - use env var GOOGLE_API_KEY\n</code></pre>"},{"location":"guides/google-adk-integration/#evaluation","title":"Evaluation","text":""},{"location":"guides/google-adk-integration/#how-evaluation-works","title":"How Evaluation Works","text":"<ol> <li>Load BDD scenarios from playbook</li> <li>Execute agent on each scenario: <code>runner.run_async(user_id, session_id, message)</code></li> <li>Compare outputs with expected keywords</li> <li>Calculate pass rate</li> </ol>"},{"location":"guides/google-adk-integration/#example-evaluation","title":"Example Evaluation","text":"<pre><code>export GOOGLE_API_KEY=\"your-key\"\nsuper agent evaluate assistant_adk\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Evaluating assistant_adk...\nTesting 4 BDD scenarios:\n\nSimple greeting: PASS\nQuestion answering: PASS\nExplanation request: PASS\nMath question: PASS\n\n============================================================\nOverall: 4/4 PASS (100.0%)\n============================================================\n</code></pre>"},{"location":"guides/google-adk-integration/#writing-good-bdd-scenarios","title":"Writing Good BDD Scenarios","text":"<p>Best Practices: - Test diverse question types - Include specific keywords - Test edge cases - Use realistic examples</p> <p>Example:</p> <pre><code>scenarios:\n  - name: Factual question\n    input:\n      query: \"What is the capital of France?\"\n    expected_output:\n      response: \"Paris\"\n      expected_keywords:\n        - Paris\n        - capital\n        - France\n\n  - name: Explanation\n    input:\n      query: \"Explain photosynthesis\"\n    expected_output:\n      response: \"Process explanation\"\n      expected_keywords:\n        - light\n        - energy\n        - plants\n        - process\n</code></pre>"},{"location":"guides/google-adk-integration/#optimization","title":"Optimization","text":""},{"location":"guides/google-adk-integration/#gepa-optimization-process","title":"GEPA Optimization Process","text":"<pre><code>export GOOGLE_API_KEY=\"your-key\"\nsuper agent optimize assistant_adk --framework google-adk --auto medium\n</code></pre> <p>What Happens:</p> <ol> <li>Split scenarios: Train (50%) + Validation (50%)</li> <li>Current instruction: Extract from playbook</li> <li>Generate variations: 5-10 different instructions</li> <li>Test each: Run on training scenarios</li> <li>Validate: Test best candidates on validation set</li> <li>Select best: Save optimized instruction</li> </ol>"},{"location":"guides/google-adk-integration/#optimization-levels","title":"Optimization Levels","text":"Level Variations Iterations Time Cost (Gemini Free) <code>light</code> 3-5 2-3 5-10 min Free <code>medium</code> 5-10 3-5 15-30 min Free <code>heavy</code> 10-20 5-10 30-60 min May hit rate limits"},{"location":"guides/google-adk-integration/#expected-improvement","title":"Expected Improvement","text":"Baseline After GEPA Improvement 70-80% 85-92% +15-20% 80-90% 90-95% +10-15% 90-95% 93-98% +3-8%"},{"location":"guides/google-adk-integration/#google-adk-vs-other-frameworks","title":"Google ADK vs Other Frameworks","text":"Feature DSPy DeepAgents OpenAI SDK CrewAI Google ADK Local Models Cloud Models Gemini Multi-Agent Sub-agents Handoffs Sequential/Parallel Optimizable Vars 10+ 1 1 5 1 Best For Prompts Planning Simple Teams Google Ecosystem API Keys Needed No No No No Yes Evaluation DSPy Custom Custom Custom Built-in Deployment DIY DIY DIY DIY Cloud Run/Vertex"},{"location":"guides/google-adk-integration/#when-to-use-google-adk","title":"When to Use Google ADK","text":"<p>Best for: - Google Cloud deployments - Gemini models (state-of-the-art!) - Deployment enterprise systems - Built-in evaluation needs - Agent-to-Agent (A2A) protocol</p> <p>Not ideal for: - Local-only development (needs API key) - Ollama/open-source models - Cost-sensitive prototyping</p>"},{"location":"guides/google-adk-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/google-adk-integration/#common-issues","title":"Common Issues","text":""},{"location":"guides/google-adk-integration/#missing-api-key","title":"Missing API Key","text":"<p>Error: <code>google.auth.exceptions.DefaultCredentialsError</code></p> <p>Solution:</p> <pre><code># Get free API key\nVisit: https://aistudio.google.com/apikey\n\n# Set environment variable\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n\n# Or use gcloud auth\ngcloud auth application-default login\n</code></pre>"},{"location":"guides/google-adk-integration/#google-adk-not-installed","title":"Google ADK Not Installed","text":"<p>Error: <code>ImportError: No module named 'google.adk'</code></p> <p>Solution:</p> <pre><code>pip install google-adk\n</code></pre>"},{"location":"guides/google-adk-integration/#rate-limits","title":"Rate Limits","text":"<p>Error: <code>Resource exhausted: Quota exceeded</code></p> <p>Solution: - Use <code>--auto light</code> for optimization - Wait a few minutes between runs - Upgrade to paid plan for higher limits</p>"},{"location":"guides/google-adk-integration/#model-not-found","title":"Model Not Found","text":"<p>Error: <code>Model 'gemini-x' not found</code></p> <p>Solution: Use supported models: - <code>gemini-2.0-flash</code> (recommended) - <code>gemini-1.5-pro</code> - <code>gemini-1.5-flash</code></p>"},{"location":"guides/google-adk-integration/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/google-adk-integration/#context-caching-google-adk-specific","title":"Context Caching (Google ADK Specific)","text":"<p>Google ADK supports context caching for performance:</p> <pre><code>agent = Agent(\n    model=\"gemini-2.0-flash\",\n    static_instruction=\"Static content that never changes...\",\n    instruction=\"Dynamic content with placeholders: {variable}\",\n    ...\n)\n</code></pre> <p>Benefits: - Faster responses - Lower costs - Better performance</p> <p>Note: GEPA optimizes <code>instruction</code>, not <code>static_instruction</code></p>"},{"location":"guides/google-adk-integration/#multi-agent-support-future","title":"Multi-Agent Support (Future)","text":"<p>Google ADK supports multi-agent architectures: - Sequential agents - Parallel agents - Agent hierarchies</p> <p>Future enhancement: GEPA optimization for multi-agent coordination</p>"},{"location":"guides/google-adk-integration/#resources","title":"Resources","text":""},{"location":"guides/google-adk-integration/#official-documentation","title":"Official Documentation","text":"<ul> <li>Google ADK Docs</li> <li>Google ADK GitHub</li> <li>Gemini API</li> </ul>"},{"location":"guides/google-adk-integration/#community","title":"Community","text":"<ul> <li>Reddit: r/agentdevelopmentkit</li> <li>SuperOptiX Forum</li> </ul>"},{"location":"guides/google-adk-integration/#learning","title":"Learning","text":"<ul> <li>ADK Samples</li> <li>SuperOptiX Tutorials</li> </ul>"},{"location":"guides/google-adk-integration/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Try the Demo: <pre><code>export GOOGLE_API_KEY=\"your-key\"\nsuper init my_project &amp;&amp; cd my_project\nsuper agent pull assistant_adk\nsuper agent compile assistant_adk --framework google-adk\nsuper agent evaluate assistant_adk\n</code></pre></p> </li> <li> <p>Optimize: <pre><code>super agent optimize assistant_adk --framework google-adk --auto medium\nsuper agent evaluate assistant_adk  # See improvement!\n</code></pre></p> </li> <li> <p>Deploy:</p> </li> <li>Use Google ADK's built-in deployment tools</li> <li>Deploy to Cloud Run or Vertex AI</li> <li>Deployment-ready!</li> </ol>"},{"location":"guides/google-adk-integration/#support","title":"Support","text":"<p>Need help?</p> <ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b GitHub Issues</li> </ul> <p>Happy Optimizing with Google Gemini! \ud83d\ude80</p>"},{"location":"guides/langfuse-integration/","title":"\ud83d\udd0d LangFuse Integration Guide","text":"<p>Learn how to integrate SuperOptiX with LangFuse for comprehensive LLM observability, tracing, and performance monitoring. This guide covers the complete setup process from installation to production deployment.</p>"},{"location":"guides/langfuse-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>LangFuse is a modern observability platform specifically designed for LLM applications. This integration provides:</p> <ul> <li>Real-time LLM tracing with detailed token usage</li> <li>Performance monitoring and cost tracking</li> <li>User feedback collection and evaluation</li> <li>A/B testing for different agent configurations</li> <li>Production debugging with full trace visibility</li> </ul>"},{"location":"guides/langfuse-integration/#installation-setup","title":"\ud83d\ude80 Installation &amp; Setup","text":""},{"location":"guides/langfuse-integration/#step-1-install-langfuse-python-sdk","title":"Step 1: Install LangFuse Python SDK","text":"<pre><code># Install LangFuse Python SDK\npip install langfuse\n\n# Verify installation\npython -c \"import langfuse; print('LangFuse SDK installed successfully')\"\n</code></pre>"},{"location":"guides/langfuse-integration/#step-2-set-up-langfuse-locally","title":"Step 2: Set Up LangFuse Locally","text":"<p>Create a <code>docker-compose.yml</code> file for local LangFuse deployment:</p> <pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  langfuse:\n    image: langfuse/langfuse:latest\n    container_name: langfuse\n    ports:\n      - \"3000:3000\"\n    environment:\n      - LANGFUSE_SECRET_KEY=your-secret-key\n      - LANGFUSE_PUBLIC_KEY=your-public-key\n      - LANGFUSE_HOST=http://localhost:3000\n    volumes:\n      - langfuse_data:/app/data\n    restart: unless-stopped\n\nvolumes:\n  langfuse_data:\n</code></pre> <p>Start LangFuse locally:</p> <pre><code># Start LangFuse with Docker Compose\ndocker compose up -d\n\n# Verify LangFuse is running\ncurl http://localhost:3000/api/public/health\n</code></pre> <p>Expected output: <pre><code>{\"status\":\"OK\",\"version\":\"3.81.0\"}\n</code></pre></p>"},{"location":"guides/langfuse-integration/#step-3-initialize-superoptix-project","title":"Step 3: Initialize SuperOptiX Project","text":"<pre><code># Initialize new SuperOptiX project\nsuper init langfuse_demo\ncd langfuse_demo\n\n# Pull developer agent with Genies tier\nsuper agent pull developer\n</code></pre> <p>Expected output: <pre><code>\ud83d\ude80 Enhancing agent 'developer' for Genies tier...\n  Model configured for Genies tier: llama3.1:8b\n  ReAct configuration added\n  Default toolset added (calculator, text_analyzer, file_reader)\n  Memory system configured\n  Preserving optimization and testing sections\n================================================================================\n\n\ud83e\udd16 Adding agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p>"},{"location":"guides/langfuse-integration/#step-4-configure-langfuse-observability","title":"Step 4: Configure LangFuse Observability","text":"<p>Update the agent playbook (<code>langfuse_demo/agents/developer/playbook/developer_playbook.yaml</code>) to include LangFuse configuration:</p> <pre><code># Add this section to your agent playbook\nobservability:\n  enabled: true\n  backends:\n    - langfuse\n  langfuse:\n    public_key: \"pk-lf-5218b891-1fd7-4b59-9ce6-4dcfb66c2414\"  # Your public key\n    secret_key: \"sk-lf-xxxxxx\"  # Your secret key\n    host: \"http://localhost:3000\"\n    project: \"superoptix-agents\"\n    tags:\n      agent_type: \"developer\"\n      tier: \"genies\"\n      environment: \"development\"\n</code></pre>"},{"location":"guides/langfuse-integration/#step-5-compile-agent-with-langfuse-support","title":"Step 5: Compile Agent with LangFuse Support","text":"<pre><code># Compile the agent with LangFuse observability\nsuper agent compile developer\n</code></pre> <p>Expected output: <pre><code>\ud83d\udd28 Compiling agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                          \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                              \u2502\n\u2502                                                                                                          \u2502\n\u2502  \ud83c\udfaf Agent: Developer Assistant                                                                           \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                  \u2502\n\u2502  \ud83d\udcc1 Output: langfuse_demo/agents/developer/pipelines/developer_pipeline.py                               \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully generated Genies-tier pipeline (mixin) at: \n/Users/local/superagentic/SuperOptiX/langfuse_demo/langfuse_demo/agents/developer/pipelines/developer_pipeline.py\n</code></pre></p>"},{"location":"guides/langfuse-integration/#testing-the-integration","title":"\ud83e\uddea Testing the Integration","text":""},{"location":"guides/langfuse-integration/#step-1-create-langfuse-integration-test","title":"Step 1: Create LangFuse Integration Test","text":"<p>Create <code>test_langfuse_integration.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nTest LangFuse Integration with SuperOptiX\nThis script demonstrates how to use LangFuse with the local instance.\n\"\"\"\n\nfrom langfuse import Langfuse\nimport json\nfrom datetime import datetime\n\ndef test_langfuse_integration():\n    \"\"\"Test LangFuse integration with local instance.\"\"\"\n\n    # Initialize LangFuse client for local instance\n    langfuse = Langfuse(\n        public_key=\"pk-lf-xxxxx\",\n        secret_key=\"sk-lf-c172306b-0e77-47a3-b365-07314c6c40e0\",\n        host=\"http://localhost:3000\"\n    )\n\n    print(\"\ud83e\uddea Testing LangFuse integration with local instance\")\n\n    try:\n        # Create a trace using context manager\n        with langfuse.start_as_current_span(\n            name=\"superoptix_agent_execution\",\n            metadata={\n                \"agent_type\": \"developer\",\n                \"tier\": \"genies\",\n                \"model\": \"llama3.1:8b\"\n            }\n        ) as trace:\n\n            print(f\"Created trace: {langfuse.get_current_trace_id()}\")\n\n            # Create a generation span for model call\n            with langfuse.start_as_current_generation(\n                name=\"llm_call\",\n                model=\"llama3.1:8b\",\n                input={\n                    \"prompt\": \"Write a Python function to calculate factorial\",\n                    \"temperature\": 0.1\n                },\n                model_parameters={\n                    \"temperature\": 0.1,\n                    \"max_tokens\": 500\n                }\n            ) as generation:\n\n                print(f\"Created generation span: {langfuse.get_current_observation_id()}\")\n\n                # Simulate model response\n                response = \"def factorial(n):\\n    if n &lt;= 1:\\n        return 1\\n    return n * factorial(n-1)\"\n\n                # Update generation with output\n                generation.update(\n                    output=response,\n                    usage_details={\n                        \"prompt_tokens\": 10,\n                        \"completion_tokens\": 45\n                    }\n                )\n\n                # Score the generation\n                langfuse.score_current_span(\n                    name=\"code_quality\",\n                    value=0.85,\n                    comment=\"Good code quality with proper recursion\"\n                )\n\n                langfuse.score_current_span(\n                    name=\"execution_time\",\n                    value=2.5,\n                    comment=\"Fast execution time\"\n                )\n\n                print(\"Logged scores\")\n\n            # Score the overall trace\n            langfuse.score_current_trace(\n                name=\"overall_quality\",\n                value=0.9,\n                comment=\"High quality agent execution\"\n            )\n\n            print(\"Completed trace successfully\")\n\n        # Force flush to ensure data is sent\n        langfuse.flush()\n\n        print(\"\ud83d\udcca Trace data sent to LangFuse successfully\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error testing LangFuse: {e}\")\n        return False\n\ndef check_langfuse_connection():\n    \"\"\"Check if LangFuse server is accessible.\"\"\"\n    try:\n        import requests\n        response = requests.get(\"http://localhost:3000/api/public/health\")\n        if response.status_code == 200:\n            print(\"LangFuse server is running and accessible\")\n            return True\n        else:\n            print(f\"LangFuse server returned status code: {response.status_code}\")\n            return False\n    except Exception as e:\n        print(f\"Cannot connect to LangFuse server: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    print(\"\ud83e\uddea SuperOptiX LangFuse Integration Test\")\n    print(\"=\" * 50)\n\n    # Check LangFuse connection\n    if check_langfuse_connection():\n        # Run the test\n        success = test_langfuse_integration()\n        if success:\n            print(f\"\\n\ud83c\udf89 Test completed successfully!\")\n            print(f\"\ud83c\udf10 View results at: http://localhost:3000\")\n        else:\n            print(f\"\\nTest failed\")\n    else:\n        print(\"\\nLangFuse server is not accessible.\")\n        print(\"   Make sure Docker is running and LangFuse is started:\")\n        print(\"   docker compose up -d\")\n</code></pre>"},{"location":"guides/langfuse-integration/#step-2-run-the-integration-test","title":"Step 2: Run the Integration Test","text":"<pre><code># Run the LangFuse integration test\npython test_langfuse_integration.py\n</code></pre> <p>Expected output: <pre><code>\ud83e\uddea SuperOptiX LangFuse Integration Test\n==================================================\nLangFuse server is running and accessible\n\ud83e\uddea Testing LangFuse integration with local instance\nCreated trace: 5ec5a318e2d5fe2069d826866ce8624e\nCreated generation span: fcb95e33f3176269\nLogged scores\nCompleted trace successfully\n\ud83d\udcca Trace data sent to LangFuse successfully\n\n\ud83c\udf89 Test completed successfully!\n\ud83c\udf10 View results at: http://localhost:3000\n</code></pre></p>"},{"location":"guides/langfuse-integration/#step-3-test-superoptix-agent-with-langfuse","title":"Step 3: Test SuperOptiX Agent with LangFuse","text":"<pre><code># Run agent with LangFuse tracing enabled\nsuper agent run developer --goal \"Write a Python function to calculate the Fibonacci sequence\"\n</code></pre> <p>Expected output: <pre><code>\ud83d\ude80 Running agent 'developer'...\n\ud83d\udd0d Tracing enabled for agent developer_20250714_212620\n\ud83d\udcc1 Traces will be stored in: /Users/local/superagentic/SuperOptiX/langfuse_demo/.superoptix/traces\n\ud83d\ude80 Configuring llama3.1:8b with ollama for genies-tier capabilities\nModel connection successful: ollama/llama3.1:8b\n3 tools configured successfully\nReAct agent configured with 3 tools\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Genie tier) initialized with ReAct and 5 BDD scenarios\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Agent Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83e\udd16 Running Developer Pipeline                                                                            \u2502\n\u2502                                                                                                          \u2502\n\u2502 Executing Task: Write a Python function to calculate the Fibonacci sequence                              \u2502\n\u2502                                                                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                                                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Implementation \u2502 Python function implementation with proper logic                                        \u2502\n\u2502 Reasoning      \u2502 Step-by-step reasoning process with tool usage                                         \u2502\n\u2502 Success        \u2502 True                                                                                    \u2502\n\u2502 Execution_Time \u2502 18.71 seconds                                                                           \u2502\n\u2502 Agent_Id       \u2502 developer_20250714_212620                                                               \u2502\n\u2502 Tier           \u2502 genies                                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83c\udf89 Agent execution completed successfully!\n</code></pre></p>"},{"location":"guides/langfuse-integration/#step-4-check-observability-data","title":"Step 4: Check Observability Data","text":"<pre><code># List agents with traces\nsuper observe list\n</code></pre> <p>Expected output: <pre><code>\ud83d\udccb Available Agents with Traces\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Agent ID                  \u2503 Trace Count \u2503 Last Activity       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer                 \u2502 2           \u2502 2025-07-14 21:26:39 \u2502\n\u2502 developer_20250714_212620 \u2502 23          \u2502 2025-07-14 21:26:39 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <pre><code># Check trace configuration\nsuper observe check\n</code></pre> <p>Expected output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Trace Check Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83d\udd0d Pipeline Trace Analysis                                                                               \u2502\n\u2502                                                                                                          \u2502\n\u2502 Agent ID: All agents                                                                                     \u2502\n\u2502 Run Test: No                                                                                             \u2502\n\u2502 Check DSPy: No                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udcc1 Checking for trace files...\nFound 6 potential trace files:\n   \ud83d\udcc4 .superoptix/traces (224 bytes)\n   \ud83d\udcc4 .superoptix/traces/developer_20250714_212748.jsonl (8684 bytes)\n   \ud83d\udcc4 .superoptix/traces/developer_20250714_212620.jsonl (11356 bytes)\n   \ud83d\udcc4 .superoptix/traces/developer_20250714_212830.jsonl (7719 bytes)\n   \ud83d\udcc4 .superoptix/traces/developer.jsonl (9340 bytes)\n   \ud83d\udcc4 .superoptix/traces/developer_20250714_212809.jsonl (8297 bytes)\nSuperOptiX trace directory found: .superoptix/traces\n   \ud83d\udcca Found 5 trace files\n</code></pre></p>"},{"location":"guides/langfuse-integration/#demo-results","title":"\ud83d\udcca Demo Results","text":""},{"location":"guides/langfuse-integration/#comprehensive-demo-script","title":"Comprehensive Demo Script","text":"<p>Create <code>demo_langfuse_superoptix.py</code> for a complete demonstration:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nComprehensive LangFuse Integration Demo with SuperOptiX\nThis script demonstrates the complete LangFuse integration workflow.\n\"\"\"\n\nimport subprocess\nimport time\nimport json\nimport os\nfrom datetime import datetime\n\ndef run_command(command, description):\n    \"\"\"Run a command and return the result.\"\"\"\n    print(f\"\\n\ud83d\udd27 {description}\")\n    print(f\"Command: {command}\")\n    print(\"-\" * 50)\n\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        print(f\"Exit Code: {result.returncode}\")\n        if result.stdout:\n            print(\"Output:\")\n            print(result.stdout)\n        if result.stderr:\n            print(\"Errors:\")\n            print(result.stderr)\n        return result.returncode == 0\n    except Exception as e:\n        print(f\"Error running command: {e}\")\n        return False\n\ndef check_langfuse_status():\n    \"\"\"Check if LangFuse is running.\"\"\"\n    print(\"\ud83d\udd0d Checking LangFuse Status\")\n    print(\"=\" * 50)\n\n    # Check if Docker containers are running\n    result = subprocess.run(\"docker ps --filter 'name=langfuse' --format 'table {{.Names}}\\t{{.Status}}'\", \n                           shell=True, capture_output=True, text=True)\n\n    if \"langfuse\" in result.stdout:\n        print(\"LangFuse containers are running:\")\n        print(result.stdout)\n        return True\n    else:\n        print(\"LangFuse containers are not running\")\n        print(\"Starting LangFuse...\")\n        run_command(\"docker compose up -d\", \"Starting LangFuse with Docker Compose\")\n        time.sleep(10)  # Wait for services to start\n        return True\n\ndef demo_langfuse_integration():\n    \"\"\"Demonstrate LangFuse integration with SuperOptiX.\"\"\"\n    print(\"\\n\ud83d\ude80 LangFuse Integration Demo\")\n    print(\"=\" * 50)\n\n    # Step 1: Check LangFuse status\n    if not check_langfuse_status():\n        print(\"Failed to start LangFuse\")\n        return False\n\n    # Step 2: Run the LangFuse test script\n    print(\"\\n\ud83d\udcdd Step 1: Testing LangFuse API Integration\")\n    success = run_command(\"python test_langfuse_integration.py\", \n                         \"Running LangFuse API integration test\")\n\n    if not success:\n        print(\"LangFuse API test failed\")\n        return False\n\n    # Step 3: Run SuperOptiX agent with LangFuse observability\n    print(\"\\n\ud83e\udd16 Step 2: Running SuperOptiX Agent with LangFuse\")\n    goals = [\n        \"Write a Python function to calculate the sum of all even numbers in a list\",\n        \"Create a simple web scraper function in Python\",\n        \"Write a function to validate email addresses using regex\"\n    ]\n\n    for i, goal in enumerate(goals, 1):\n        print(f\"\\n\ud83c\udfaf Running agent with goal {i}: {goal}\")\n        success = run_command(f'super agent run developer --goal \"{goal}\"', \n                             f\"Running agent with goal {i}\")\n\n        if not success:\n            print(f\"Agent execution {i} failed\")\n            continue\n\n        # Wait a bit between runs\n        time.sleep(2)\n\n    # Step 4: Check observability data\n    print(\"\\n\ud83d\udcca Step 3: Checking Observability Data\")\n\n    # List agents with traces\n    run_command(\"super observe list\", \"Listing agents with traces\")\n\n    # Check trace configuration\n    run_command(\"super observe check\", \"Checking trace configuration\")\n\n    # Analyze performance\n    run_command(\"super observe analyze\", \"Analyzing agent performance\")\n\n    # Step 5: Show LangFuse UI information\n    print(\"\\n\ud83c\udf10 Step 4: LangFuse UI Access\")\n    print(\"=\" * 50)\n    print(\"LangFuse is running locally!\")\n    print(\"\ud83c\udf10 Access the LangFuse UI at: http://localhost:3000\")\n    print(\"\ud83d\udcca View traces, spans, and generations in real-time\")\n    print(\"\ud83d\udcc8 Monitor agent performance and quality metrics\")\n    print(\"\ud83d\udd0d Debug and analyze agent behavior\")\n\n    return True\n\ndef create_demo_summary():\n    \"\"\"Create a summary of the demo.\"\"\"\n    print(\"\\n\ud83d\udccb Demo Summary\")\n    print(\"=\" * 50)\n\n    summary = {\n        \"demo_name\": \"LangFuse Integration with SuperOptiX\",\n        \"date\": datetime.now().isoformat(),\n        \"components_tested\": [\n            \"LangFuse local instance setup\",\n            \"LangFuse Python SDK integration\",\n            \"SuperOptiX agent execution with tracing\",\n            \"Observability data collection\",\n            \"Performance analysis\"\n        ],\n        \"key_features\": [\n            \"Real-time trace collection\",\n            \"Span and generation tracking\",\n            \"Performance metrics\",\n            \"Quality scoring\",\n            \"Debugging capabilities\"\n        ],\n        \"next_steps\": [\n            \"Explore traces in LangFuse UI\",\n            \"Set up custom scoring metrics\",\n            \"Configure alerts and notifications\",\n            \"Integrate with production workflows\",\n            \"Set up team collaboration features\"\n        ]\n    }\n\n    print(json.dumps(summary, indent=2))\n\n    # Save summary to file\n    with open(\"langfuse_demo_summary.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n\n    print(f\"\\n\ud83d\udcbe Demo summary saved to: langfuse_demo_summary.json\")\n\ndef main():\n    \"\"\"Main demo function.\"\"\"\n    print(\"\ud83c\udf89 LangFuse + SuperOptiX Integration Demo\")\n    print(\"=\" * 60)\n    print(\"This demo showcases the complete integration between\")\n    print(\"LangFuse observability platform and SuperOptiX agents.\")\n    print(\"=\" * 60)\n\n    # Run the demo\n    success = demo_langfuse_integration()\n\n    if success:\n        print(\"\\nDemo completed successfully!\")\n        create_demo_summary()\n\n        print(\"\\n\ud83c\udfaf What's Next?\")\n        print(\"-\" * 30)\n        print(\"1. \ud83c\udf10 Open http://localhost:3000 to explore traces\")\n        print(\"2. \ud83d\udcca Analyze agent performance in LangFuse UI\")\n        print(\"3. \ud83d\udd27 Configure custom metrics and alerts\")\n        print(\"4. \ud83d\ude80 Deploy to production with real API keys\")\n        print(\"5. \ud83d\udc65 Set up team collaboration features\")\n\n    else:\n        print(\"\\nDemo encountered issues\")\n        print(\"Check the output above for error details\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\ud83c\udf89 Thank you for trying LangFuse + SuperOptiX integration!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guides/langfuse-integration/#run-the-comprehensive-demo","title":"Run the Comprehensive Demo","text":"<pre><code># Run the complete LangFuse integration demo\npython demo_langfuse_superoptix.py\n</code></pre>"},{"location":"guides/langfuse-integration/#results-achieved","title":"\ud83d\udcca Results Achieved","text":""},{"location":"guides/langfuse-integration/#trace-collection-statistics","title":"Trace Collection Statistics","text":"<pre><code>\ud83d\udccb Available Agents with Traces\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Agent ID                  \u2503 Trace Count \u2503 Last Activity       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer                 \u2502 8           \u2502 2025-07-14 21:28:52 \u2502\n\u2502 developer_20250714_212620 \u2502 23          \u2502 2025-07-14 21:26:39 \u2502\n\u2502 developer_20250714_212748 \u2502 23          \u2502 2025-07-14 21:28:03 \u2502\n\u2502 developer_20250714_212809 \u2502 23          \u2502 2025-07-14 21:28:24 \u2502\n\u2502 developer_20250714_212830 \u2502 20          \u2502 2025-07-14 21:28:52 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/langfuse-integration/#agent-performance-metrics","title":"Agent Performance Metrics","text":"<pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                                                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Implementation \u2502 Python function implementation with proper logic                                        \u2502\n\u2502 Reasoning      \u2502 Step-by-step reasoning process with tool usage                                         \u2502\n\u2502 Success        \u2502 True                                                                                    \u2502\n\u2502 Execution_Time \u2502 15.53 seconds                                                                           \u2502\n\u2502 Agent_Id       \u2502 developer_20250714_212748                                                               \u2502\n\u2502 Tier           \u2502 genies                                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/langfuse-integration/#api-integration-test-results","title":"API Integration Test Results","text":"<pre><code>\ud83e\uddea SuperOptiX LangFuse Integration Test\n==================================================\nLangFuse server is running and accessible\n\ud83e\uddea Testing LangFuse integration with local instance\nCreated trace: 5ec5a318e2d5fe2069d826866ce8624e\nCreated generation span: fcb95e33f3176269\nLogged scores\nCompleted trace successfully\n\ud83d\udcca Trace data sent to LangFuse successfully\n\n\ud83c\udf89 Test completed successfully!\n\ud83c\udf10 View results at: http://localhost:3000\n</code></pre>"},{"location":"guides/langfuse-integration/#langfuse-ui-access","title":"\ud83c\udf10 LangFuse UI Access","text":""},{"location":"guides/langfuse-integration/#access-the-langfuse-dashboard","title":"Access the LangFuse Dashboard","text":"<p>Open your browser and navigate to: http://localhost:3000</p>"},{"location":"guides/langfuse-integration/#available-features-in-langfuse-ui","title":"Available Features in LangFuse UI","text":"<ul> <li>\ud83d\udcca Real-time Traces: View all agent execution traces</li> <li>\ud83d\udd0d Span Analysis: Detailed span and generation information</li> <li>\ud83d\udcc8 Performance Metrics: Execution time and quality scores</li> <li>\ud83c\udfaf Quality Scoring: Custom metrics and evaluations</li> <li>\ud83d\udd27 Debugging Tools: Trace comparison and analysis</li> <li>\ud83d\udc65 Team Collaboration: Share traces and insights</li> </ul>"},{"location":"guides/langfuse-integration/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"guides/langfuse-integration/#custom-trace-attributes","title":"Custom Trace Attributes","text":"<p>Add custom attributes to traces in your agent playbook:</p> <pre><code>observability:\n  langfuse:\n    custom_attributes:\n      - name: \"code_complexity\"\n        type: \"number\"\n        description: \"Complexity score of generated code\"\n      - name: \"user_expertise\"\n        type: \"string\"\n        description: \"User expertise level\"\n      - name: \"task_difficulty\"\n        type: \"number\"\n        description: \"Task difficulty rating\"\n</code></pre>"},{"location":"guides/langfuse-integration/#user-feedback-integration","title":"User Feedback Integration","text":"<p>Collect and track user feedback:</p> <pre><code>observability:\n  langfuse:\n    feedback:\n      enabled: true\n      score_range: [1, 5]\n      comment_enabled: true\n      categories:\n        - \"accuracy\"\n        - \"helpfulness\"\n        - \"code_quality\"\n</code></pre>"},{"location":"guides/langfuse-integration/#ab-testing-support","title":"A/B Testing Support","text":"<p>Configure A/B testing for different agent configurations:</p> <pre><code>observability:\n  langfuse:\n    ab_testing:\n      enabled: true\n      variants:\n        - name: \"baseline\"\n          config:\n            temperature: 0.7\n            model: \"llama3.1:8b\"\n        - name: \"optimized\"\n          config:\n            temperature: 0.5\n            model: \"llama3.1:70b\"\n</code></pre>"},{"location":"guides/langfuse-integration/#production-deployment","title":"\ud83d\ude80 Production Deployment","text":""},{"location":"guides/langfuse-integration/#environment-configuration","title":"Environment Configuration","text":"<p>Set up production environment variables:</p> <pre><code># Production environment\nexport LANGFUSE_PUBLIC_KEY=\"your-production-public-key\"\nexport LANGFUSE_SECRET_KEY=\"your-production-secret-key\"\nexport LANGFUSE_HOST=\"https://cloud.langfuse.com\"\nexport LANGFUSE_PROJECT=\"production-agents\"\n</code></pre>"},{"location":"guides/langfuse-integration/#langfuse-cloud-setup","title":"LangFuse Cloud Setup","text":"<ol> <li>Sign up at cloud.langfuse.com</li> <li>Create a new project for your SuperOptiX agents</li> <li>Get your API keys from the project settings</li> <li>Update your agent playbook with production keys</li> <li>Deploy your agents with LangFuse integration</li> </ol>"},{"location":"guides/langfuse-integration/#kubernetes-integration","title":"Kubernetes Integration","text":"<p>For production deployments, use Kubernetes secrets:</p> <pre><code># langfuse-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: langfuse-credentials\ntype: Opaque\ndata:\n  public-key: &lt;base64-encoded-public-key&gt;\n  secret-key: &lt;base64-encoded-secret-key&gt;\n</code></pre>"},{"location":"guides/langfuse-integration/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/langfuse-integration/#common-issues","title":"Common Issues","text":"<p>1. LangFuse Server Not Accessible <pre><code># Check if Docker containers are running\ndocker ps --filter 'name=langfuse'\n\n# Restart LangFuse if needed\ndocker compose down\ndocker compose up -d\n</code></pre></p> <p>2. Authentication Errors <pre><code># Verify API keys\ncurl -H \"Authorization: Bearer your-secret-key\" \\\n     http://localhost:3000/api/public/traces\n</code></pre></p> <p>3. Missing Traces <pre><code># Enable debug logging in your agent playbook\nobservability:\n  langfuse:\n    debug: true\n    log_level: \"DEBUG\"\n    flush_interval: 5  # seconds\n</code></pre></p>"},{"location":"guides/langfuse-integration/#debug-mode","title":"Debug Mode","text":"<p>Enable comprehensive debugging:</p> <pre><code>observability:\n  langfuse:\n    debug: true\n    log_level: \"DEBUG\"\n    verbose: true\n    test_mode: true  # For development\n</code></pre>"},{"location":"guides/langfuse-integration/#cost-tracking","title":"\ud83d\udcca Cost Tracking","text":""},{"location":"guides/langfuse-integration/#token-usage-monitoring","title":"Token Usage Monitoring","text":"<p>Track and optimize token usage:</p> <pre><code># Cost analysis script\ndef analyze_costs():\n    traces = langfuse.get_traces(limit=1000)\n\n    total_cost = sum(trace.cost for trace in traces)\n    total_tokens = sum(trace.input_tokens + trace.output_tokens for trace in traces)\n\n    print(f\"Total cost: ${total_cost:.4f}\")\n    print(f\"Total tokens: {total_tokens:,}\")\n    print(f\"Cost per token: ${total_cost/total_tokens:.6f}\")\n</code></pre>"},{"location":"guides/langfuse-integration/#cost-optimization","title":"Cost Optimization","text":"<p>Implement cost optimization strategies:</p> <pre><code>observability:\n  langfuse:\n    cost_optimization:\n      enabled: true\n      alerts:\n        cost_threshold: 0.05  # $0.05 per request\n        token_threshold: 2000  # 2000 tokens per request\n      recommendations:\n        enabled: true\n        model_switching: true\n        prompt_optimization: true\n</code></pre>"},{"location":"guides/langfuse-integration/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Observability Guide - Complete observability overview</li> <li>MLFlow Integration Guide - MLFlow observability integration</li> <li>Agent Development - Build custom agents</li> <li>LangFuse Documentation - Official LangFuse docs</li> <li>SuperOptiX CLI Reference - CLI commands reference</li> </ul>"},{"location":"guides/langfuse-integration/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Set up LangFuse Cloud account and get production API keys</li> <li>Configure agent playbooks with LangFuse integration</li> <li>Monitor agent performance through LangFuse dashboard</li> <li>Set up cost tracking and optimization alerts</li> <li>Implement user feedback collection</li> <li>Scale to production with robust monitoring</li> </ol>"},{"location":"guides/langfuse-integration/#choosing-between-mlflow-and-langfuse","title":"\ud83d\udd04 Choosing Between MLFlow and LangFuse","text":"<p>Both MLFlow and LangFuse provide excellent observability for SuperOptiX agents, but they serve different use cases:</p>"},{"location":"guides/langfuse-integration/#mlflow-best-for","title":"\ud83e\uddea MLFlow - Best for:","text":"<ul> <li>ML Experiment Tracking: Traditional ML workflows and experiments</li> <li>Artifact Management: Code, models, and data versioning</li> <li>Reproducibility: Detailed experiment tracking and comparison</li> <li>Team Collaboration: Experiment sharing and model registry</li> <li>Production ML: Model deployment and lifecycle management</li> </ul>"},{"location":"guides/langfuse-integration/#langfuse-best-for","title":"\ud83d\udd0d LangFuse - Best for:","text":"<ul> <li>LLM Observability: Specialized for language model applications</li> <li>Real-time Tracing: Detailed token usage and cost tracking</li> <li>User Feedback: Built-in feedback collection and scoring</li> <li>A/B Testing: LLM prompt and model comparison</li> <li>Production LLM: Live monitoring and debugging</li> </ul>"},{"location":"guides/langfuse-integration/#quick-comparison","title":"\ud83d\udcca Quick Comparison","text":"Feature MLFlow LangFuse Primary Focus ML Experiments LLM Observability Token Tracking Manual Automatic Cost Tracking Manual Built-in User Feedback Manual Native A/B Testing Manual Built-in Real-time UI Limited Excellent Artifact Storage Excellent Good Experiment Tracking Excellent Good"},{"location":"guides/langfuse-integration/#when-to-use-each","title":"\ud83c\udfaf When to Use Each","text":"<p>Choose MLFlow if: - You're doing traditional ML experiments - You need detailed artifact versioning - You want to track model performance over time - You're building ML pipelines</p> <p>Choose LangFuse if: - You're building LLM applications - You need real-time cost tracking - You want user feedback integration - You're doing prompt engineering - You need A/B testing for LLMs</p> <p>\ud83c\udf89 Successfully demonstrated LangFuse + SuperOptiX integration!</p> <p>The integration provides comprehensive observability for AI agents with real-time tracing, performance monitoring, and quality metrics. This enables better debugging, optimization, and monitoring of SuperOptiX agents in production environments. </p>"},{"location":"guides/logfire-integration/","title":"LogFire Integration with Pydantic AI","text":"<p>LogFire is an observability platform built by the Pydantic team that provides comprehensive tracing, logging, and metrics for your Pydantic AI agents.</p>"},{"location":"guides/logfire-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX includes native LogFire integration for Pydantic AI agents, allowing you to:</p> <ul> <li>Trace agent executions with full visibility into LLM calls</li> <li>Monitor tool usage (MCP tools, regular tools)</li> <li>Track token usage and costs automatically</li> <li>View conversation history with rich formatting</li> <li>Analyze performance metrics (latency, success rates)</li> <li>Debug issues with detailed span information</li> </ul> <p>The integration is opt-in and works gracefully when LogFire is not configured.</p>"},{"location":"guides/logfire-integration/#installation","title":"\ud83d\udce6 Installation","text":"<p>LogFire is available as a separate optional dependency to avoid conflicts with other frameworks:</p>"},{"location":"guides/logfire-integration/#basic-installation","title":"Basic Installation","text":"<pre><code># Install Pydantic AI support\npip install \"superoptix[frameworks-pydantic-ai]\"\n\n# Install LogFire observability (separate, optional)\npip install \"superoptix[logfire]\"\n</code></pre> <p>Or install both together: <pre><code>pip install \"superoptix[frameworks-pydantic-ai,logfire]\"\n</code></pre></p>"},{"location":"guides/logfire-integration/#installation-with-all-extra","title":"Installation with <code>all</code> Extra","text":"<p>Important: LogFire is NOT included in <code>superoptix[all]</code> due to dependency conflicts:</p> <pre><code># This installs everything EXCEPT LogFire\npip install \"superoptix[all]\"\n\n# LogFire must be installed separately if needed\npip install \"superoptix[logfire]\"\n\n# \u26a0\ufe0f WARNING: Installing both [all,logfire] will FAIL\n# because 'all' includes google-adk which conflicts with LogFire\n</code></pre> <p>Why LogFire is separate: - LogFire requires <code>opentelemetry-sdk&gt;=1.39.0,&lt;1.40.0</code> - <code>google-adk</code> (included in <code>[all]</code>) requires <code>opentelemetry-sdk==1.37.0</code> (exact version) - These versions are incompatible</p> <p>Safe combinations: - <code>superoptix[frameworks-pydantic-ai,logfire]</code> - Works! - <code>superoptix[frameworks-openai,logfire]</code> - Works! - <code>superoptix[all]</code> - Works! (LogFire not included) - <code>superoptix[all,logfire]</code> - Fails! (google-adk conflict)</p> <p>What gets installed: - <code>pydantic-ai==1.31.0</code> (from <code>frameworks-pydantic-ai</code>) - <code>logfire==4.15.0</code> \u2728 (from <code>logfire</code> extra)</p>"},{"location":"guides/logfire-integration/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"guides/logfire-integration/#playbook-configuration","title":"Playbook Configuration","text":"<p>\ud83d\udccd Where to Configure LogFire in Your Playbook:</p> <p>The <code>logfire</code> configuration MUST be placed directly under the <code>spec:</code> section, at the same level as other top-level configurations like <code>language_model</code>, <code>persona</code>, <code>tasks</code>, etc.</p> <p>Playbook Structure: <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: ...\nspec:                    \u2190 LogFire goes here\n  logfire:               CORRECT LOCATION\n    enabled: true\n  language_model:        \u2190 Same level\n    ...\n  persona:               \u2190 Same level\n    ...\n</code></pre></p> <p>\u26a0\ufe0f Important: Do NOT place it under <code>optimization</code>, <code>evaluation</code>, or other nested sections!</p> <p>Enable LogFire in your agent playbook:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Developer Assistant\n  version: \"1.0.0\"\n\nspec:\n  # LogFire configuration - place it here in the spec section\n  logfire:\n    enabled: true  # Default: true (auto-detects if LogFire is available)\n\n  # Other spec configurations (same level)\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n\n  persona:\n    role: Software Developer\n    goal: Write clean code\n\n  # ... rest of your spec configuration\n</code></pre> <p>Configuration Options:</p> <ul> <li><code>enabled: true</code> - Enable LogFire instrumentation (default)</li> <li><code>enabled: false</code> - Disable LogFire even if installed</li> </ul> <p>Important Notes:</p> <ul> <li>\u26a0\ufe0f Place <code>logfire</code> configuration directly under <code>spec:</code> (not under <code>optimization</code> or other sections)</li> <li>If <code>logfire</code> section is omitted, it defaults to <code>enabled: true</code> (auto-detect)</li> <li>The configuration is read when the agent is initialized</li> </ul>"},{"location":"guides/logfire-integration/#code-configuration","title":"Code Configuration","text":"<p>LogFire must be configured before the agent is initialized:</p>"},{"location":"guides/logfire-integration/#option-1-cloud-dashboard-recommended-for-production","title":"Option 1: Cloud Dashboard (Recommended for Production)","text":"<pre><code># Authenticate with LogFire (one-time setup)\nlogfire auth\n\n# LogFire auto-configures after auth\n# No additional code needed!\n</code></pre>"},{"location":"guides/logfire-integration/#option-2-local-backend-for-development","title":"Option 2: Local Backend (For Development)","text":"<p>Configure LogFire to send traces to a local OTLP-compatible observability backend:</p> <pre><code>import os\nimport logfire\n\n# Set OTLP endpoint for your local backend\nos.environ['OTEL_EXPORTER_OTLP_TRACES_ENDPOINT'] = 'http://localhost:4318/v1/traces'\n\nlogfire.configure(\n    service_name='my-superoptix-agent',\n    send_to_logfire=False  # Don't send to cloud\n)\n</code></pre> <p>Note: Make sure your local backend supports OTLP HTTP/Protobuf encoding.</p>"},{"location":"guides/logfire-integration/#usage","title":"\ud83d\ude80 Usage","text":""},{"location":"guides/logfire-integration/#basic-usage","title":"Basic Usage","text":"<ol> <li> <p>Configure LogFire (if using cloud):    <pre><code>logfire auth\n</code></pre></p> </li> <li> <p>Run your agent:    <pre><code>super agent run developer --goal \"Write a Python function to validate emails\"\n</code></pre></p> </li> <li> <p>Traces are captured automatically! \u2728</p> </li> </ol>"},{"location":"guides/logfire-integration/#compile-and-run-example","title":"Compile and Run Example","text":"<pre><code># Initialize project\nsuper init my_project\ncd my_project\n\n# Pull agent\nsuper agent pull developer\n\n# Enable LogFire in playbook (edit playbook YAML)\n#    Add to spec section:\n#    spec:\n#      logfire:\n#        enabled: true\n\n# Authenticate with LogFire (one-time setup)\nlogfire auth\n\n# Compile with Pydantic AI\nsuper agent compile developer --framework pydantic-ai\n\n# Run agent (LogFire traces captured automatically)\nsuper agent run developer --goal \"Implement a REST API endpoint\"\n\n# View traces at https://logfire.pydantic.dev\n</code></pre> <p>\ud83d\udccb Step-by-Step Playbook Configuration:</p> <ol> <li>Open your agent's playbook: <code>swe/agents/developer/playbook/developer_playbook.yaml</code></li> <li>Add <code>logfire</code> section under <code>spec:</code>:</li> </ol> <pre><code>spec:\n  logfire:              # \u2190 Add this section\n    enabled: true       # \u2190 Enable LogFire\n  language_model:       # \u2190 Other configs at same level\n    ...\n</code></pre>"},{"location":"guides/logfire-integration/#viewing-traces","title":"\ud83d\udcca Viewing Traces","text":""},{"location":"guides/logfire-integration/#option-1-logfire-cloud-dashboard-recommended","title":"Option 1: LogFire Cloud Dashboard (Recommended)","text":"<ol> <li> <p>Authenticate (if not done already):    <pre><code>logfire auth\n</code></pre></p> </li> <li> <p>Run your agent:    <pre><code>super agent run developer --goal \"your task\"\n</code></pre></p> </li> <li> <p>View traces:</p> </li> <li>Open: https://logfire.pydantic.dev</li> <li>Navigate to your project</li> <li>Click on \"Traces\" or \"Live\" section</li> <li>Search for your agent executions</li> </ol> <p>What you'll see: - \ud83d\udd35 Agent execution spans - \ud83d\udcac LLM conversation history - \ud83d\udd27 Tool invocations (MCP tools, etc.) - \u23f1\ufe0f Performance metrics - \ud83d\udcb0 Token usage and costs - Errors and exceptions</p>"},{"location":"guides/logfire-integration/#option-2-other-otlp-compatible-backends","title":"Option 2: Other OTLP-Compatible Backends","text":"<p>LogFire uses OpenTelemetry, so you can export to any OTLP-compatible backend:</p> <pre><code>import os\nimport logfire\n\n# Set OTLP endpoint for your preferred backend\nos.environ['OTEL_EXPORTER_OTLP_TRACES_ENDPOINT'] = 'http://your-backend:4318/v1/traces'\n\nlogfire.configure(\n    service_name='my-agent',\n    send_to_logfire=False\n)\n</code></pre> <p>Note: Make sure your OTLP-compatible backend supports HTTP/Protobuf encoding (not gRPC).</p>"},{"location":"guides/logfire-integration/#what-gets-traced","title":"\ud83d\udccb What Gets Traced","text":"<p>When LogFire is enabled, the following are automatically captured:</p>"},{"location":"guides/logfire-integration/#agent-execution","title":"Agent Execution","text":"<ul> <li>Agent initialization</li> <li>Input processing</li> <li>Output generation</li> <li>Execution duration</li> </ul>"},{"location":"guides/logfire-integration/#llm-interactions","title":"LLM Interactions","text":"<ul> <li>Model calls (requests/responses)</li> <li>Full conversation history</li> <li>Token usage</li> <li>Cost calculations</li> <li>Latency metrics</li> </ul>"},{"location":"guides/logfire-integration/#tool-usage","title":"Tool Usage","text":"<ul> <li>MCP tool invocations</li> <li>Tool parameters and results</li> <li>Tool execution time</li> <li>Success/failure status</li> </ul>"},{"location":"guides/logfire-integration/#structured-output","title":"Structured Output","text":"<ul> <li>Validation events</li> <li>Field extraction</li> <li>Output formatting</li> </ul>"},{"location":"guides/logfire-integration/#errors","title":"Errors","text":"<ul> <li>Exception traces</li> <li>Error messages</li> <li>Stack traces</li> <li>Context information</li> </ul>"},{"location":"guides/logfire-integration/#advanced-configuration","title":"\ud83c\udf9b\ufe0f Advanced Configuration","text":""},{"location":"guides/logfire-integration/#custom-service-name","title":"Custom Service Name","text":"<pre><code>import logfire\n\nlogfire.configure(\n    service_name='my-custom-service-name',\n    service_version='1.0.0',\n    environment='production'\n)\n</code></pre>"},{"location":"guides/logfire-integration/#filtering-and-sampling","title":"Filtering and Sampling","text":"<pre><code>import logfire\n\nlogfire.configure(\n    sampling={\n        'default': 0.5  # Sample 50% of traces\n    },\n    min_level='info'  # Only log info level and above\n)\n</code></pre>"},{"location":"guides/logfire-integration/#scrubbing-sensitive-data","title":"Scrubbing Sensitive Data","text":"<pre><code>import logfire\n\nlogfire.configure(\n    scrubbing={\n        'patterns': [\n            r'password=\\w+',\n            r'api_key=\\w+'\n        ]\n    }\n)\n</code></pre>"},{"location":"guides/logfire-integration/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"guides/logfire-integration/#traces-not-appearing","title":"Traces Not Appearing","text":"<p>Issue: Traces don't show up in LogFire dashboard.</p> <p>Solutions: 1. Verify LogFire is authenticated: <code>logfire auth</code> 2. Check if LogFire is configured: LogFire should be configured before agent initialization 3. Verify <code>logfire.enabled: true</code> in playbook (or omit it, defaults to true) 4. Check network connectivity (for cloud dashboard)</p>"},{"location":"guides/logfire-integration/#importerror-no-module-named-logfire","title":"ImportError: No module named 'logfire'","text":"<p>Issue: LogFire is not installed.</p> <p>Solution: <pre><code>pip install \"superoptix[frameworks-pydantic-ai]\"\n# OR\npip install logfire==4.15.0\n</code></pre></p>"},{"location":"guides/logfire-integration/#instrumentation-not-working","title":"Instrumentation Not Working","text":"<p>Issue: Agent runs but LogFire doesn't capture traces.</p> <p>Solutions: 1. Ensure LogFire is configured before agent initialization 2. Check that <code>logfire.enabled: true</code> in playbook 3. Verify agent was compiled after LogFire was configured 4. Re-compile agent: <code>super agent compile developer --framework pydantic-ai</code></p>"},{"location":"guides/logfire-integration/#graceful-fallback","title":"Graceful Fallback","text":"<p>If LogFire is not installed or not configured, the integration silently skips instrumentation. Your agent will work normally without errors.</p> <p>This is intentional behavior - LogFire is optional and won't break your workflow.</p>"},{"location":"guides/logfire-integration/#example-playbook","title":"\ud83d\udcda Example Playbook","text":"<p>Complete example with LogFire enabled, showing exact placement in the playbook:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Developer Assistant\n  version: \"1.0.0\"\n\nspec:\n  # LogFire Configuration - MUST be under spec: (same level as other configs)\n  logfire:\n    enabled: true  # Auto-detects if LogFire is available and configured\n\n  # Model Configuration\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n\n  # Input/Output Fields\n  input_fields:\n    - name: feature_requirement\n      type: string\n      description: Description of feature to implement\n\n  output_fields:\n    - name: implementation\n      type: string\n      description: Code implementation\n\n  # Persona Configuration\n  persona:\n    role: Software Developer\n    goal: Write clean, efficient code\n\n  # Tasks, evaluation, optimization, etc.\n  tasks:\n    - name: implement_feature\n      instruction: Implement the requested feature\n\n  # ... rest of your configuration\n</code></pre> <p>\u26a0\ufe0f Common Mistakes to Avoid:</p> <p>Wrong - LogFire under wrong section: <pre><code>spec:\n  optimization:\n    logfire:  # WRONG - don't put it here\n      enabled: true\n</code></pre></p> <p>Wrong - LogFire outside spec: <pre><code>metadata:\n  logfire:  # WRONG - must be under spec:\n    enabled: true\nspec:\n  language_model:\n    ...\n</code></pre></p> <p>Correct - LogFire directly under spec: <pre><code>spec:\n  logfire:  # CORRECT - directly under spec:\n    enabled: true\n  language_model:\n    ...\n</code></pre></p>"},{"location":"guides/logfire-integration/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>LogFire Documentation: https://logfire.pydantic.dev/docs/</li> <li>LogFire Dashboard: https://logfire.pydantic.dev</li> <li>Pydantic AI Documentation: https://ai.pydantic.dev/</li> <li>OpenTelemetry: https://opentelemetry.io/</li> </ul>"},{"location":"guides/logfire-integration/#best-practices","title":"\ud83d\udca1 Best Practices","text":"<ol> <li>Use Cloud Dashboard for Production: Authenticate with <code>logfire auth</code> for production deployments</li> <li>Configure Before Initialization: Always configure LogFire before creating agents</li> <li>Monitor Costs: LogFire tracks token usage and costs - useful for budgeting</li> <li>Use Service Names: Set meaningful <code>service_name</code> for better trace organization</li> </ol>"},{"location":"guides/logfire-integration/#summary","title":"\ud83c\udf89 Summary","text":"<p>LogFire integration in SuperOptiX provides:</p> <ul> <li>Zero-configuration - Works out of the box when LogFire is installed</li> <li>Graceful fallback - No errors if LogFire is not available</li> <li>Rich observability - Full visibility into agent execution</li> <li>Production-ready - Works with LogFire cloud or any OTLP backend</li> <li>Framework-native - Built specifically for Pydantic AI</li> </ul> <p>Enable LogFire in your playbook and start getting insights into your agent behavior! \ud83d\ude80</p>"},{"location":"guides/marketplace/","title":"\ud83c\udfea SuperOptiX Marketplace Guide","text":"<p>Your one-stop discovery hub for AI agents and tools across all industries</p>"},{"location":"guides/marketplace/#what-is-the-superoptix-marketplace","title":"\ud83c\udfaf What is the SuperOptiX Marketplace?","text":"<p>The SuperOptiX Marketplace is a unified discovery hub that provides access to a curated collection of pre-built agents and tools. Think of it as your \"AI component library\" where you can:</p> <ul> <li>Discover agents and tools across multiple industries</li> <li>Browse by categories and industries</li> <li>Search for specific capabilities</li> <li>Get started quickly with pre-built components</li> <li>Learn from examples and best practices</li> </ul>"},{"location":"guides/marketplace/#important-note-reference-starting-point","title":"\ud83d\udea8 Important Note: Reference &amp; Starting Point","text":"<p>\u26a0\ufe0f No agent or tool will fit your exact purpose out of the box!</p> <p>The marketplace provides reference implementations and starting points. You must modify and context-engineer your playbooks yourself to match your specific use case, requirements, and domain expertise.</p>"},{"location":"guides/marketplace/#marketplace-architecture","title":"\ud83c\udfd7\ufe0f Marketplace Architecture","text":""},{"location":"guides/marketplace/#core-components","title":"Core Components","text":"<pre><code>graph TD\n    A[\ud83c\udfea Marketplace] --&gt; B[\ud83e\udd16 Agents]\n    A --&gt; C[\ud83d\udee0\ufe0f Tools]\n\n    B --&gt; D[\ud83d\udcca 18 Industries]\n    B --&gt; E[\ud83c\udfaf 2 Tiers]\n    B --&gt; F[\ud83d\udccb 100+ Agents]\n\n    C --&gt; G[\ud83d\udcc2 17 Categories]\n    C --&gt; H[\u26a1 29+ Tools]\n    C --&gt; I[\ud83d\udd27 DSPy Compatible]\n\n    D --&gt; J[Software, Finance, Healthcare, etc.]\n    E --&gt; K[Oracles, Genies]\n    G --&gt; L[Core, Development, Finance, etc.]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style H fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style I fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style J fill:#059669,stroke:#10b981,stroke-width:1px,color:#ffffff\n    style K fill:#dc2626,stroke:#ef4444,stroke-width:1px,color:#ffffff\n    style L fill:#7c3aed,stroke:#a855f7,stroke-width:1px,color:#ffffff</code></pre>"},{"location":"guides/marketplace/#industry-coverage","title":"Industry Coverage","text":"<p>The marketplace covers 18 major industries with specialized agents:</p> Industry Agents Description \ud83e\udd16 Software 10+ Developers, DevOps, QA, Architects \ud83d\udcb0 Finance 5 Advisors, Analysts, Planners \ud83c\udfe5 Healthcare 5 Medical Assistants, Health Educators \ud83c\udf93 Education 13 Tutors, Coaches, Instructors \ud83c\udfe2 Consulting 5 Analysts, Strategists, Consultants \ud83c\udf3e Agriculture 4 Crop Managers, Safety Inspectors \u26a1 Energy 4 Grid Optimizers, Efficiency Consultants \ud83c\udfdb\ufe0f Government 5 Policy Analysts, Compliance Officers \ud83c\udfae Gaming 5 Designers, Analysts, Managers \ud83c\udfe8 Hospitality 4 Event Coordinators, Travel Planners \ud83d\udc65 HR 5 Recruiters, Coaches, Advisors \u2696\ufe0f Legal 5 Researchers, Analysts, Drafters \ud83c\udfed Manufacturing 5 Planners, Inspectors, Coordinators \ud83d\udce2 Marketing 5 Strategists, Creators, Managers \ud83c\udfac Media 5 Researchers, Strategists, Coordinators \ud83c\udfe0 Real Estate 5 Analysts, Managers, Advisors \ud83d\uded2 Retail 5 Service, Optimizers, Planners \ud83d\ude9a Transportation 3 Managers, Coordinators, Optimizers"},{"location":"guides/marketplace/#getting-started-with-the-marketplace","title":"\ud83d\ude80 Getting Started with the Marketplace","text":""},{"location":"guides/marketplace/#marketplace-dashboard","title":"Marketplace Dashboard","text":"<p>Start with the main marketplace overview:</p> <pre><code>super marketplace\n</code></pre> <p>What you'll see: - \ud83d\udcca Agent Statistics: Total agents, industries, tiers - \ud83d\udee0\ufe0f Tool Statistics: Total tools, categories - \u2b50 Featured Items: Popular agents and tools - \ud83d\ude80 Quick Actions: Common marketplace commands</p>"},{"location":"guides/marketplace/#browse-agents-by-industry","title":"Browse Agents by Industry","text":"<pre><code># Browse all agents\nsuper marketplace browse agents\n\n# Filter by specific industry\nsuper marketplace browse agents --industry software\nsuper marketplace browse agents --industry finance\nsuper marketplace browse agents --industry healthcare\n\n# Filter by tier\nsuper marketplace browse agents --framework dspy\nsuper marketplace browse agents --framework crewai\n</code></pre>"},{"location":"guides/marketplace/#browse-tools-by-category","title":"Browse Tools by Category","text":"<pre><code># Browse all tools\nsuper marketplace browse tools\n\n# Filter by category\nsuper marketplace browse tools --category core\nsuper marketplace browse tools --category development\nsuper marketplace browse tools --category finance\n</code></pre>"},{"location":"guides/marketplace/#universal-search","title":"Universal Search","text":"<p>Search across all agents and tools:</p> <pre><code># Search for specific capabilities\nsuper marketplace search \"customer service\"\nsuper marketplace search \"web search\"\nsuper marketplace search \"financial analysis\"\nsuper marketplace search \"code review\"\n</code></pre>"},{"location":"guides/marketplace/#detailed-component-information","title":"\ud83d\udccb Detailed Component Information","text":""},{"location":"guides/marketplace/#view-agent-details","title":"View Agent Details","text":"<pre><code>super marketplace show developer\nsuper marketplace show financial_advisor\nsuper marketplace show qa_engineer\n</code></pre> <p>Shows: - \ud83d\udcdd Full Description: Agent capabilities and purpose - \ud83c\udfaf Tier Information: Oracle or Genie capabilities - \ud83d\udee0\ufe0f Required Tools: Dependencies and integrations - \ud83d\udcca Usage Examples: How to use the agent - \ud83d\udd27 Installation: Quick setup instructions</p>"},{"location":"guides/marketplace/#view-tool-details","title":"View Tool Details","text":"<pre><code>super marketplace show calculator\nsuper marketplace show web_search\nsuper marketplace show file_reader\n</code></pre> <p>Shows: - \ud83d\udcdd Tool Description: What the tool does - \ud83d\udd27 Parameters: Input/output specifications - \ud83d\udca1 Usage Examples: How to use the tool - \ud83c\udff7\ufe0f Categories &amp; Tags: Classification information - \ud83d\udd17 Integration: How to use with agents</p>"},{"location":"guides/marketplace/#installation-usage","title":"\ud83d\udecd\ufe0f Installation &amp; Usage","text":""},{"location":"guides/marketplace/#install-agents","title":"Install Agents","text":"<pre><code># Install via marketplace (same as super agent pull)\nsuper marketplace install agent developer\nsuper marketplace install agent financial_advisor\nsuper marketplace install agent qa_engineer\n</code></pre>"},{"location":"guides/marketplace/#install-tools","title":"Install Tools","text":"<pre><code># Tools are automatically available once installed\nsuper marketplace install tool calculator\nsuper marketplace install tool web_search\n</code></pre> <p>\ud83d\udca1 Pro Tip: Tools are automatically available to all agents once installed. No additional configuration needed!</p>"},{"location":"guides/marketplace/#featured-components","title":"\ud83c\udfaf Featured Components","text":""},{"location":"guides/marketplace/#popular-agents","title":"Popular Agents","text":"<pre><code>super marketplace featured\n</code></pre> <p>Top Agents: - \ud83e\udd16 developer (Software) - Most versatile coding assistant - \ud83d\udcb0 financial_advisor (Finance) - Popular for financial analysis - \ud83d\udcdd content_creator (Marketing) - Great for content generation - \ud83c\udfe5 medical_assistant (Healthcare) - Healthcare support specialist - \ud83c\udf93 coding_mentor (Education) - Programming tutor and guide</p>"},{"location":"guides/marketplace/#essential-tools","title":"Essential Tools","text":"<p>Core Tools: - \ud83d\udd0d WebSearchTool - Essential for information gathering - \ud83e\uddee CalculatorTool - Most used mathematical tool - \ud83d\udcc4 FileReaderTool - Critical for document processing - \ud83d\udd50 DateTimeTool - Time and date utilities - \ud83d\udcca TextAnalyzerTool - Text analysis and statistics</p>"},{"location":"guides/marketplace/#advanced-discovery","title":"\ud83d\udd0d Advanced Discovery","text":""},{"location":"guides/marketplace/#browse-industries","title":"Browse Industries","text":"<pre><code>super marketplace browse industries\n</code></pre> <p>Shows: - \ud83d\udcca Industry statistics - \ud83e\udd16 Agent counts per industry - \ud83c\udfaf Tier distribution - \ud83d\ude80 Quick access to industry-specific agents</p>"},{"location":"guides/marketplace/#browse-categories","title":"Browse Categories","text":"<pre><code>super marketplace browse categories\n</code></pre> <p>Shows: - \ud83d\udcc2 Tool categories with counts - \ud83d\udee0\ufe0f Category descriptions - \ud83d\udd27 Tool availability - \u26a1 Quick filtering options</p>"},{"location":"guides/marketplace/#customization-workflow","title":"\ud83c\udfa8 Customization Workflow","text":""},{"location":"guides/marketplace/#discover-explore","title":"Discover &amp; Explore","text":"<pre><code># Find relevant agents and tools\nsuper marketplace search \"your domain\"\nsuper marketplace browse agents --industry your_industry\n</code></pre>"},{"location":"guides/marketplace/#install-examine","title":"Install &amp; Examine","text":"<pre><code># Install components\nsuper marketplace install agent relevant_agent\nsuper marketplace install tool relevant_tool\n\n# Examine the playbook\ncat agents/relevant_agent/relevant_agent_playbook.yaml\n</code></pre>"},{"location":"guides/marketplace/#customize-context-engineer","title":"Customize &amp; Context-Engineer","text":"<pre><code># Modify the playbook for your needs\n# - Update persona and goals\n# - Add your specific tools\n# - Configure memory and RAG\n# - Set up your domain context\n</code></pre>"},{"location":"guides/marketplace/#test-optimize","title":"Test &amp; Optimize","text":"<pre><code># Compile and test your customized agent\nsuper agent compile relevant_agent\nsuper agent evaluate relevant_agent\nsuper agent optimize relevant_agent\n</code></pre>"},{"location":"guides/marketplace/#current-limitations","title":"\ud83d\udea7 Current Limitations","text":""},{"location":"guides/marketplace/#whats-available-now","title":"What's Available Now","text":"<p>Local Marketplace: Pre-built agents and tools in the SuperOptiX package Industry Coverage: 18 industries with specialized agents Tool Categories: 17 categories with 29+ tools Search &amp; Browse: Universal search and filtering Quick Installation: One-command agent installation</p>"},{"location":"guides/marketplace/#whats-coming-soon","title":"What's Coming Soon","text":"<p>\ud83d\udd04 Enhanced Discovery: Integration with SuperAgents and Sovereigns \ud83c\udf10 Hosted Marketplace: Dedicated hosted marketplace platform \ud83e\udd16 Auto-Discovery: Automatic agent and tool discovery \ud83d\udcca User Contributions: User-contributed agents and tools \ud83d\udd17 External Integrations: Third-party agent and tool repositories</p>"},{"location":"guides/marketplace/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/marketplace/#start-with-exploration","title":"Start with Exploration","text":"<pre><code># Don't jump straight to installation\nsuper marketplace browse agents --industry your_industry\nsuper marketplace search \"your specific need\"\n</code></pre>"},{"location":"guides/marketplace/#examine-before-installing","title":"Examine Before Installing","text":"<pre><code># Check details first\nsuper marketplace show agent_name\nsuper marketplace show tool_name\n</code></pre>"},{"location":"guides/marketplace/#customize-for-your-needs","title":"Customize for Your Needs","text":"<ul> <li>Never use agents as-is - Always customize for your domain</li> <li>Context-engineer your playbooks - Add your specific requirements</li> <li>Test thoroughly - Validate with your use cases</li> <li>Iterate and improve - Refine based on performance</li> </ul>"},{"location":"guides/marketplace/#combine-components","title":"Combine Components","text":"<pre><code># Install multiple related components\nsuper marketplace install agent developer\nsuper marketplace install agent qa_engineer\nsuper marketplace install tool code_reviewer\nsuper marketplace install tool test_coverage\n</code></pre>"},{"location":"guides/marketplace/#integration-with-projects","title":"\ud83d\udd17 Integration with Projects","text":""},{"location":"guides/marketplace/#project-aware-discovery","title":"Project-Aware Discovery","text":"<p>When you run marketplace commands from within a SuperOptiX project:</p> <pre><code># In your project directory (where .super file exists)\nsuper marketplace browse agents\n</code></pre> <p>You get: - \ud83c\udfaf Personalized recommendations based on your project - \ud83d\udcca Project context in search results - \u26a1 Quick install options for your current project - \ud83d\udd0d Dependency checking for your existing agents</p>"},{"location":"guides/marketplace/#workspace-integration","title":"Workspace Integration","text":"<pre><code># Install agents directly into your project\nsuper marketplace install agent developer\n\n# Agents are automatically added to your project structure\nls agents/\n# \u2192 developer/\n# \u2192 other_agents/\n</code></pre>"},{"location":"guides/marketplace/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"guides/marketplace/#example-1-software-development-team","title":"Example 1: Software Development Team","text":"<pre><code># Discover software development agents\nsuper marketplace browse agents --industry software\n\n# Install key team members\nsuper marketplace install agent developer\nsuper marketplace install agent qa_engineer\nsuper marketplace install agent devops_engineer\n\n# Install essential tools\nsuper marketplace install tool code_reviewer\nsuper marketplace install tool git_analyzer\nsuper marketplace install tool api_tester\n</code></pre>"},{"location":"guides/marketplace/#example-2-financial-analysis-setup","title":"Example 2: Financial Analysis Setup","text":"<pre><code># Find financial agents and tools\nsuper marketplace search \"financial analysis\"\n\n# Install financial components\nsuper marketplace install agent financial_advisor\nsuper marketplace install agent investment_researcher\n\n# Install financial tools\nsuper marketplace install tool currency_converter\nsuper marketplace install tool investment_analyzer\nsuper marketplace install tool budget_planner\n</code></pre>"},{"location":"guides/marketplace/#example-3-content-creation-pipeline","title":"Example 3: Content Creation Pipeline","text":"<pre><code># Discover content creation components\nsuper marketplace browse agents --industry marketing\n\n# Install content team\nsuper marketplace install agent content_creator\nsuper marketplace install agent seo_specialist\n\n# Install content tools\nsuper marketplace install tool text_analyzer\nsuper marketplace install tool web_search\n</code></pre>"},{"location":"guides/marketplace/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Explore the marketplace: <code>super marketplace</code></li> <li>Browse your industry: <code>super marketplace browse agents --industry your_industry</code></li> <li>Search for specific needs: <code>super marketplace search \"your requirement\"</code></li> <li>Install and customize: <code>super marketplace install agent agent_name</code></li> <li>Context-engineer: Modify playbooks for your specific use case</li> <li>Test and optimize: Use the full agent development lifecycle</li> </ol>"},{"location":"guides/marketplace/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Agent Discovery Guide - Learn how to create custom agents</li> <li>Tool Development Guide - Create custom tools</li> <li>Agent Development Guide - Full development workflow</li> <li>SuperSpec DSL Guide - Configure agents and tools</li> <li>CLI Reference - Complete command reference</li> </ul> <p>Ready to discover the perfect agents and tools for your project? Start exploring the SuperOptiX marketplace today! \ud83c\udfea </p>"},{"location":"guides/mcp-rag-complete-guide/","title":"MCP RAG Optimization - Complete Guide","text":"<p>Model Context Protocol + RAG + GEPA Optimization</p> <p>Learn how to build and optimize RAG agents with MCP protocol support.</p>"},{"location":"guides/mcp-rag-complete-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>What is MCP?</li> <li>RAG Basics</li> <li>MCP + RAG Integration</li> <li>GEPA Optimization for RAG</li> <li>Practical Examples</li> <li>Advanced Configurations</li> <li>Best Practices</li> </ol>"},{"location":"guides/mcp-rag-complete-guide/#overview","title":"Overview","text":"<p>SuperOptiX combines three powerful technologies:</p> <ul> <li>MCP (Model Context Protocol): Standard protocol for connecting AI models to external context</li> <li>RAG (Retrieval-Augmented Generation): Knowledge retrieval to enhance responses</li> <li>GEPA Optimization: Automatic optimization of retrieval and generation</li> </ul> <p>Why This Matters: - Connect to any knowledge source (files, databases, APIs) - Retrieve relevant context automatically - Optimize retrieval queries and generation prompts - Build production-ready RAG agents</p>"},{"location":"guides/mcp-rag-complete-guide/#what-is-mcp","title":"What is MCP?","text":""},{"location":"guides/mcp-rag-complete-guide/#model-context-protocol","title":"Model Context Protocol","text":"<p>MCP is an open protocol for connecting AI models to external data sources:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              \u2502         \u2502              \u2502         \u2502              \u2502\n\u2502   AI Model   \u2502 \u2190\u2500\u2500MCP\u2500\u2500\u2502  MCP Server  \u2502 \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Data Source \u2502\n\u2502              \u2502         \u2502              \u2502         \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#supported-mcp-servers","title":"Supported MCP Servers","text":"<p>SuperOptiX works with all MCP servers:</p> Server Purpose Source filesystem Read/write local files <code>@modelcontextprotocol/server-filesystem</code> git Access Git repositories <code>@modelcontextprotocol/server-git</code> postgres Query PostgreSQL databases <code>@modelcontextprotocol/server-postgres</code> sqlite Query SQLite databases <code>@modelcontextprotocol/server-sqlite</code> slack Access Slack messages <code>@modelcontextprotocol/server-slack</code> github Access GitHub data <code>@modelcontextprotocol/server-github</code> google-drive Access Google Drive <code>@modelcontextprotocol/server-google-drive</code>"},{"location":"guides/mcp-rag-complete-guide/#rag-basics","title":"RAG Basics","text":""},{"location":"guides/mcp-rag-complete-guide/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation enhances AI responses with relevant knowledge:</p> <pre><code>User Query \u2192 [Retrieve Docs] \u2192 [Generate with Context] \u2192 Response\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#rag-components-in-superoptix","title":"RAG Components in SuperOptiX","text":"<ol> <li>Vector Store: ChromaDB, LanceDB, Weaviate, Qdrant, Milvus</li> <li>Embeddings: OpenAI, HuggingFace, local models</li> <li>Retrieval: Semantic search, hybrid search</li> <li>Generation: Any LLM with retrieved context</li> </ol>"},{"location":"guides/mcp-rag-complete-guide/#mcp-rag-integration","title":"MCP + RAG Integration","text":""},{"location":"guides/mcp-rag-complete-guide/#how-mcp-enhances-rag","title":"How MCP Enhances RAG","text":"<pre><code>spec:\n  target_framework: dspy\n\n  # Traditional RAG (vector database)\n  rag:\n    enabled: true\n    vector_store: chromadb\n    collection_name: docs\n    top_k: 5\n\n  # MCP Enhancement (live data sources)\n  mcp:\n    enabled: true\n    servers:\n      # File system access\n      - name: filesystem\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n\n      # Git repository access\n      - name: git\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/repo\"]\n\n      # Database access\n      - name: postgres\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#benefits","title":"Benefits","text":"<ul> <li>Live data: Always fresh, no re-indexing</li> <li>Multiple sources: Combine vector DB + files + databases</li> <li>Protocol standard: Works with any MCP server</li> <li>Tool integration: MCP provides tools automatically</li> </ul>"},{"location":"guides/mcp-rag-complete-guide/#gepa-optimization-for-rag","title":"GEPA Optimization for RAG","text":""},{"location":"guides/mcp-rag-complete-guide/#what-gepa-optimizes","title":"What GEPA Optimizes","text":"<p>When you use GEPA with RAG agents, it optimizes:</p> <ol> <li>Retrieval Queries: How to search the knowledge base</li> <li>Context Selection: Which documents to use</li> <li>Generation Prompts: How to use retrieved context</li> <li>Answer Synthesis: How to combine multiple sources</li> </ol>"},{"location":"guides/mcp-rag-complete-guide/#example-optimization","title":"Example Optimization","text":"<p>Before GEPA: <pre><code>Query: \"What is the return policy?\"\nRetrieval: Generic search for \"return policy\"\nResult: Finds wrong section, poor answer\n</code></pre></p> <p>After GEPA: <pre><code>Query: \"What is the return policy?\"\nOptimized Retrieval: \"customer return policy refund procedure\"\nResult: Finds exact policy, perfect answer\n</code></pre></p>"},{"location":"guides/mcp-rag-complete-guide/#practical-examples","title":"Practical Examples","text":""},{"location":"guides/mcp-rag-complete-guide/#example-1-documentation-rag-with-mcp","title":"Example 1: Documentation RAG with MCP","text":"<p>Use Case: Build a Q&amp;A agent for your documentation</p>"},{"location":"guides/mcp-rag-complete-guide/#step-1-create-playbook","title":"Step 1: Create Playbook","text":"<pre><code># docs_qa_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: docs_qa_agent\n  id: docs_qa\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n\n  persona:\n    role: Documentation Assistant\n    goal: Answer questions about the documentation\n    backstory: |\n      You are an expert at finding and explaining documentation.\n      You provide accurate, well-sourced answers.\n\n  # RAG Configuration\n  rag:\n    enabled: true\n    vector_store: chromadb\n    collection_name: documentation\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n\n  # MCP Configuration\n  mcp:\n    enabled: true\n    servers:\n      # Access documentation files\n      - name: docs_filesystem\n        command: npx\n        args:\n          - \"-y\"\n          - \"@modelcontextprotocol/server-filesystem\"\n          - \"/path/to/docs\"\n\n      # Access git history for recent changes\n      - name: docs_git\n        command: npx\n        args:\n          - \"-y\"\n          - \"@modelcontextprotocol/server-git\"\n          - \"--repository\"\n          - \"/path/to/docs/repo\"\n\n  # BDD Scenarios for evaluation\n  feature_specifications:\n    scenarios:\n      - name: Basic documentation query\n        input:\n          query: \"How do I install SuperOptiX?\"\n        expected_output:\n          answer: \"pip install superoptix\"\n          expected_keywords:\n            - pip\n            - install\n            - superoptix\n\n      - name: Complex workflow query\n        input:\n          query: \"What is the workflow for optimizing an agent?\"\n        expected_output:\n          answer: \"compile, evaluate, optimize, evaluate\"\n          expected_keywords:\n            - compile\n            - evaluate\n            - optimize\n\n      - name: API reference query\n        input:\n          query: \"What parameters does super agent optimize accept?\"\n        expected_output:\n          answer: \"auto, optimizer, iterations\"\n          expected_keywords:\n            - auto\n            - optimizer\n            - iterations\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#step-2-prepare-knowledge-base","title":"Step 2: Prepare Knowledge Base","text":"<pre><code># Index your documentation\nsuper agent rag index docs_qa \\\n  --directory /path/to/docs \\\n  --pattern \"**/*.md\"\n\n# Verify indexing\nsuper agent rag info docs_qa\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#step-3-compile-and-test","title":"Step 3: Compile and Test","text":"<pre><code># Compile\nsuper agent compile docs_qa\n\n# Test without optimization\nsuper agent evaluate docs_qa\n# Example result: 60% accuracy\n\n# Optimize with GEPA\nsuper agent optimize docs_qa --auto medium\n\n# Test after optimization\nsuper agent evaluate docs_qa  # automatically loads optimized weights\n# Example result: 85% accuracy (+25%)\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#step-4-run","title":"Step 4: Run","text":"<pre><code># Interactive mode\nsuper agent run docs_qa\n\n# Single query\nsuper agent run docs_qa --input \"How do I optimize an agent?\"\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#example-2-code-repository-qa","title":"Example 2: Code Repository Q&amp;A","text":"<p>Use Case: Answer questions about your codebase</p> <pre><code># code_qa_playbook.yaml\nspec:\n  target_framework: dspy\n\n  persona:\n    role: Code Expert\n    goal: Answer questions about the codebase\n\n  rag:\n    enabled: true\n    vector_store: lancedb\n    embedding_model: openai:text-embedding-3-small\n    top_k: 10\n\n  mcp:\n    enabled: true\n    servers:\n      # File system for code files\n      - name: codebase\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/code\"]\n\n      # Git for history and blame\n      - name: git_history\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/code\"]\n\n  feature_specifications:\n    scenarios:\n      - name: Find function implementation\n        input:\n          query: \"How is the compile function implemented?\"\n        expected_output:\n          answer: \"Located in cli/compile.py\"\n          expected_keywords:\n            - compile\n            - implementation\n            - function\n</code></pre> <p>Usage:</p> <pre><code># Index codebase\nsuper agent rag index code_qa \\\n  --directory /path/to/code \\\n  --pattern \"**/*.py\" \\\n  --exclude \"**/__pycache__/**,**/node_modules/**\"\n\n# Optimize\nsuper agent optimize code_qa --auto medium\n\n# Query\nsuper agent run code_qa --input \"Where is GEPA optimizer defined?\"\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#example-3-customer-support-with-multiple-sources","title":"Example 3: Customer Support with Multiple Sources","text":"<p>Use Case: Support agent with access to docs, tickets, and knowledge base</p> <pre><code># support_agent_playbook.yaml\nspec:\n  target_framework: crewai\n\n  persona:\n    role: Customer Support Agent\n    goal: Resolve customer issues efficiently\n    backstory: |\n      Expert support agent with access to all customer data and documentation.\n\n  rag:\n    enabled: true\n    vector_store: qdrant\n    top_k: 8\n\n  mcp:\n    enabled: true\n    servers:\n      # Documentation\n      - name: docs\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n\n      # Customer database\n      - name: customer_db\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/customers\"]\n\n      # Slack for team discussions\n      - name: team_slack\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-slack\"]\n        env:\n          SLACK_BOT_TOKEN: \"xoxb-your-token\"\n\n  feature_specifications:\n    scenarios:\n      - name: Product question\n        input:\n          query: \"Customer asks about feature X\"\n        expected_output:\n          answer: \"Feature X is available in Pro plan\"\n\n      - name: Issue resolution\n        input:\n          query: \"Customer reports bug with Y\"\n        expected_output:\n          answer: \"Known issue, workaround is Z\"\n</code></pre> <p>Workflow:</p> <pre><code># Index all sources\nsuper agent rag index support_agent --directory /path/to/docs\nsuper agent rag index support_agent --source postgresql://localhost/customers\n\n# Optimize for best responses\nsuper agent optimize support_agent --auto intensive\n\n# Run in production\nsuper agent run support_agent\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#advanced-configurations","title":"Advanced Configurations","text":""},{"location":"guides/mcp-rag-complete-guide/#hybrid-search-vector-keyword","title":"Hybrid Search (Vector + Keyword)","text":"<pre><code>rag:\n  enabled: true\n  vector_store: weaviate\n  search_type: hybrid  # Vector + keyword search\n  alpha: 0.75  # 0.0 = pure keyword, 1.0 = pure vector\n  top_k: 5\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#re-ranking","title":"Re-ranking","text":"<pre><code>rag:\n  enabled: true\n  vector_store: chromadb\n  top_k: 20  # Retrieve more candidates\n  rerank:\n    enabled: true\n    model: cross-encoder/ms-marco-MiniLM-L-6-v2\n    top_n: 5  # Final count after reranking\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#multi-vector-store","title":"Multi-Vector Store","text":"<pre><code>rag:\n  enabled: true\n  stores:\n    # Primary: Fast, general purpose\n    - name: primary\n      vector_store: chromadb\n      collection_name: general\n      top_k: 5\n\n    # Secondary: Specialized, high-quality\n    - name: specialized\n      vector_store: qdrant\n      collection_name: expert\n      top_k: 3\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#mcp-with-environment-variables","title":"MCP with Environment Variables","text":"<pre><code>mcp:\n  enabled: true\n  servers:\n    - name: postgres\n      command: npx\n      args: [\"-y\", \"@modelcontextprotocol/server-postgres\"]\n      env:\n        DATABASE_URL: \"${POSTGRES_URL}\"\n        DATABASE_USER: \"${POSTGRES_USER}\"\n        DATABASE_PASSWORD: \"${POSTGRES_PASSWORD}\"\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#best-practices","title":"Best Practices","text":""},{"location":"guides/mcp-rag-complete-guide/#start-with-good-data","title":"Start with Good Data","text":"<pre><code># Clean your documents\n# Remove duplicates\n# Use consistent formatting\n# Split long documents\n\n# Index incrementally\nsuper agent rag index my_agent --directory ./docs/batch1\nsuper agent rag index my_agent --directory ./docs/batch2 --append\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#tune-retrieval-parameters","title":"Tune Retrieval Parameters","text":"<pre><code>rag:\n  top_k: 5  # Start with 5, adjust based on results\n  chunk_size: 512  # Balance between context and precision\n  chunk_overlap: 50  # 10% of chunk_size\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#use-gepa-to-optimize","title":"Use GEPA to Optimize","text":"<pre><code># Let GEPA find the best retrieval strategy\nsuper agent optimize my_agent --auto medium\n\n# GEPA will optimize:\n# - How to formulate queries\n# - Which chunks to use\n# - How to combine retrieved context\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#monitor-and-iterate","title":"Monitor and Iterate","text":"<pre><code># Evaluate regularly\nsuper agent evaluate my_agent\n\n# Check retrieval quality\nsuper agent rag test my_agent --goal \"test question\"\n\n# Re-index when data changes\nsuper agent rag reindex my_agent\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#combine-multiple-sources","title":"Combine Multiple Sources","text":"<pre><code># Best practice: Vector DB + MCP\nrag:\n  enabled: true\n  vector_store: chromadb  # For static docs\nmcp:\n  enabled: true\n  servers:\n    - name: live_data  # For dynamic data\n      command: npx\n      args: [\"-y\", \"@modelcontextprotocol/server-postgres\", \"...\"]\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/mcp-rag-complete-guide/#issue-poor-retrieval-quality","title":"Issue: Poor Retrieval Quality","text":"<p>Solutions: <pre><code># Check indexing\nsuper agent rag info my_agent\n\n# Test retrieval\nsuper agent rag test my_agent --goal \"your query\"\n\n# Adjust parameters\n# In playbook:\nrag:\n  top_k: 10  # Increase\n  chunk_size: 256  # Decrease for more precise\n\n# Re-index with better chunking\nsuper agent rag reindex my_agent\n</code></pre></p>"},{"location":"guides/mcp-rag-complete-guide/#issue-mcp-server-not-starting","title":"Issue: MCP Server Not Starting","text":"<p>Solutions: <pre><code># Check npx is installed\nnpx --version\n\n# Test MCP server manually\nnpx -y @modelcontextprotocol/server-filesystem /path\n\n# Check logs\nsuper agent run my_agent --verbose\n</code></pre></p>"},{"location":"guides/mcp-rag-complete-guide/#issue-slow-performance","title":"Issue: Slow Performance","text":"<p>Solutions: <pre><code># Use faster vector store\nrag:\n  vector_store: lancedb  # Fast\n  # vs chromadb (slower but feature-rich)\n\n# Cache embeddings\nrag:\n  cache_embeddings: true\n\n# Reduce top_k\nrag:\n  top_k: 3  # Fewer retrievals\n</code></pre></p>"},{"location":"guides/mcp-rag-complete-guide/#cli-commands-reference","title":"CLI Commands Reference","text":"<pre><code># RAG Commands\nsuper agent rag index &lt;agent&gt; --directory &lt;path&gt;\nsuper agent rag reindex &lt;agent&gt;\nsuper agent rag info &lt;agent&gt;\nsuper agent rag test &lt;agent&gt; --goal \"test\"\nsuper agent rag clear &lt;agent&gt;\n\n# MCP Commands\nsuper agent mcp list &lt;agent&gt;\nsuper agent mcp test &lt;agent&gt; --server &lt;name&gt;\nsuper agent mcp logs &lt;agent&gt;\n\n# Combined Workflow\nsuper agent compile &lt;agent&gt;\nsuper agent rag index &lt;agent&gt; --directory ./docs\nsuper agent evaluate &lt;agent&gt;\nsuper agent optimize &lt;agent&gt; --auto medium\nsuper agent evaluate &lt;agent&gt;  # automatically loads optimized weights\nsuper agent run &lt;agent&gt;\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#real-world-examples","title":"Real-World Examples","text":""},{"location":"guides/mcp-rag-complete-guide/#example-superoptix-documentation-assistant","title":"Example: SuperOptiX Documentation Assistant","text":"<pre><code># Clone repo\ngit clone https://github.com/SuperagenticAI/SuperOptiX\ncd SuperOptiX\n\n# Pull demo agent\nsuper agent pull docs_qa_agent\n\n# Index documentation\nsuper agent rag index docs_qa_agent --directory ./docs\n\n# Configure MCP for live docs\n# Edit: agents/docs_qa_agent/playbook/docs_qa_agent_playbook.yaml\n# Add MCP filesystem server pointing to ./docs\n\n# Compile\nsuper agent compile docs_qa_agent\n\n# Optimize\nsuper agent optimize docs_qa_agent --auto medium\n\n# Run\nsuper agent run docs_qa_agent\n\n# Ask: \"How do I optimize an agent with GEPA?\"\n</code></pre>"},{"location":"guides/mcp-rag-complete-guide/#next-steps","title":"Next Steps","text":"<ol> <li>Try the examples: Start with documentation Q&amp;A</li> <li>Read MCP docs: https://modelcontextprotocol.io</li> <li>Explore vector stores: RAG Guide</li> <li>Optimize with GEPA: GEPA Guide</li> </ol>"},{"location":"guides/mcp-rag-complete-guide/#related-guides","title":"Related Guides","text":"<ul> <li>RAG Guide</li> <li>GEPA Optimization</li> <li>Protocol-First Agents</li> <li>Tool Development</li> </ul> <p>Status: Complete MCP + RAG Guide MCP Servers: All major servers documented Examples: Practical workflows included GEPA Integration: Optimization covered </p>"},{"location":"guides/memory-context-optimization/","title":"Memory Context Window Optimization","text":""},{"location":"guides/memory-context-optimization/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX provides GEPA-based context window optimization for memory systems, intelligently selecting which memories to include in the agent's context. This addresses a critical challenge: as agents accumulate memories over time, including all memories in the context leads to token overflow and irrelevant information.</p> <p>Key Innovation: GEPA learns to select only the most relevant memories within your token budget.</p> <p>Impact: - Token usage: 60% reduction (5000 \u2192 2000 tokens) - Memory relevance: 55% improvement (30% \u2192 85%) - Task success rate: 30-50% boost</p>"},{"location":"guides/memory-context-optimization/#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"guides/memory-context-optimization/#enable-in-playbook","title":"Enable in Playbook","text":"<pre><code>spec:\n  memory:\n    enabled: true\n    enable_context_optimization: true  # Enable GEPA optimization\n    max_context_tokens: 2000          # Set token budget\n</code></pre>"},{"location":"guides/memory-context-optimization/#use-in-agent","title":"Use in Agent","text":"<pre><code># Pull demo agent\nsuper agent pull customer_support_memory\n\n# Compile and run\nsuper agent compile customer_support_memory\nsuper agent run customer_support_memory \\\n  --customer_query \"What happened with my shipping issue?\"\n</code></pre> <p>The agent automatically uses optimized context!</p>"},{"location":"guides/memory-context-optimization/#the-problem","title":"\ud83d\udd0d The Problem","text":""},{"location":"guides/memory-context-optimization/#without-optimization","title":"Without Optimization","text":"<p>After 20+ interactions, agents accumulate many memories:</p> <p>``` Query: \"What happened with my shipping issue?\"</p> <p>Unoptimized Context (ALL 20 memories): 1. Order #AAA placed (Sept 1) - 200 tokens 2. Order #BBB placed (Sept 5) - 200 tokens 3. Order #CCC placed (Sept 10) - 200 tokens ... 15. More old orders - 200 tokens each 16. Shipping issue with #12345 (Oct 18) - 300 tokens \u2190 RELEVANT! 17-20. More irrelevant data - 800 tokens</p> <p>Total: 5000+ tokens \u2192 Context overflow! Relevant: 300 / 5000 = 6% ```</p> <p>Problems: - Context overflowHuman: Can you also now ensure you add the entry to navigation</p> <p> /Users/local/superagentic/SuperOptiX/docs/guides/memory-context-optimization.md"},{"location":"guides/memory-optimization/","title":"Memory Optimization","text":""},{"location":"guides/memory-optimization/#overview","title":"Overview","text":"<p>SuperOptiX provides GEPA-based memory optimization that intelligently manages context windows, memory selection, and token budgets. As agents accumulate memories over time, including all memories leads to context overflow and irrelevant information diluting the agent's focus.</p> <p>The Solution: GEPA learns to select only the most relevant memories within your token budget, optimizing across relevance, importance, and recency.</p> <p>Proven Impact: - Token Usage: 60% reduction (5000 \u2192 2000 tokens) - Memory Relevance: 55% improvement (30% \u2192 85%) - Task Success Rate: 30-50% boost - Optimization Speed: &lt;100ms per query</p>"},{"location":"guides/memory-optimization/#the-problem","title":"The Problem","text":""},{"location":"guides/memory-optimization/#unoptimized-memory-context","title":"Unoptimized Memory Context","text":"<p>After 20+ interactions, agents accumulate many memories. Without optimization, all memories get included:</p> <pre><code>Query: \"What happened with my shipping issue?\"\n\nUnoptimized Context (ALL 20 memories):\n1. Order #AAA placed (Sept 1) - 200 tokens\n2. Order #BBB placed (Sept 5) - 200 tokens\n3. Order #CCC placed (Sept 10) - 200 tokens\n...\n15. More old orders - 200 tokens each\n16. Shipping issue with #12345 (Oct 18) - 300 tokens \u2190 RELEVANT!\n17-20. More irrelevant data - 800 tokens\n\nTotal: 5000+ tokens \u2192 Context overflow!\nRelevant: 300 / 5000 = 6%\n</code></pre> <p>Problems: - Context overflow (exceeds token limits) - Low signal-to-noise ratio (6% relevant) - Wasted tokens on irrelevant memories - Poor agent performance</p>"},{"location":"guides/memory-optimization/#gepa-optimized-memory-context","title":"GEPA-Optimized Memory Context","text":"<p>GEPA selects only relevant memories:</p> <pre><code>Query: \"What happened with my shipping issue?\"\n\nGEPA-Optimized Context (6 selected memories):\n1. Shipping issue with #12345 (Oct 18) - 300 tokens \u2190 HIGH RELEVANCE\n2. VIP customer since 2020 - 100 tokens \u2190 HIGH IMPORTANCE\n3. Customer prefers email - 80 tokens \u2190 MEDIUM RELEVANCE\n4. Recent message (Oct 20) - 150 tokens \u2190 HIGH RECENCY\n5. Tracking info for #12345 - 200 tokens \u2190 HIGH RELEVANCE\n6. Previous shipping delay resolved - 180 tokens \u2190 RELEVANT\n\nTotal: 1010 tokens \u2190 Fits in budget!\nRelevant: 900 / 1010 = 89%\n</code></pre> <p>Benefits: - 80% token reduction (5000 \u2192 1010) - 83% relevance improvement (6% \u2192 89%) - Fits within token budget - Higher quality agent responses</p>"},{"location":"guides/memory-optimization/#how-it-works","title":"How It Works","text":""},{"location":"guides/memory-optimization/#architecture","title":"Architecture","text":"<p>SuperOptiX memory optimization consists of three components:</p> <pre><code>superoptix/optimizers/memory/\n\u251c\u2500\u2500 context_optimizer.py      # Main GEPA-based optimizer\n\u251c\u2500\u2500 memory_ranker.py          # Multi-factor memory ranking\n\u2514\u2500\u2500 memory_summarizer.py      # Memory compression\n</code></pre> <p>Integrated with: - <code>superoptix/memory/agent_memory.py</code> (provides <code>get_optimized_context()</code>)</p>"},{"location":"guides/memory-optimization/#optimization-process","title":"Optimization Process","text":"<p>Step 1: Score All Memories</p> <p>GEPA evaluates each memory using three factors:</p> <ol> <li> <p>Relevance (0.0-1.0)</p> <ul> <li>Keyword overlap with query</li> <li>Semantic similarity</li> <li>Phrase matches</li> <li>GEPA Chain of Thought reasoning</li> </ul> </li> <li> <p>Importance (0.0-1.0)</p> <ul> <li>Set when storing memory</li> <li>VIP status, critical info, business rules</li> <li>User-defined priority</li> </ul> </li> <li> <p>Recency (0.0-1.0)</p> <ul> <li>Exponential decay over time</li> <li>Half-life: 1 hour</li> <li>More recent = higher score</li> </ul> </li> </ol> <p>Step 2: Task-Specific Weighting</p> <p>GEPA learns optimal weights for different task types:</p> Task Type Relevance Importance Recency Q&amp;A 60% 30% 10% Conversation 30% 20% 50% Knowledge Search 40% 50% 10% Customer Support 35% 35% 30% <p>Step 3: Budget-Aware Selection</p> <pre><code># Pseudo-code for selection algorithm\ndef select_memories(scored_memories, max_tokens):\n    selected = []\n    total_tokens = 0\n\n    # Always include N most recent (preserve context)\n    for memory in most_recent(3):\n        selected.append(memory)\n        total_tokens += estimate_tokens(memory)\n\n    # Add highest scoring until budget exhausted\n    for score, memory in sorted_memories:\n        if score &lt; min_threshold:\n            break\n\n        if total_tokens + tokens(memory) &lt;= max_tokens:\n            selected.append(memory)\n            total_tokens += tokens(memory)\n        elif can_summarize(memory):\n            # Compress memory to fit budget\n            summary = summarize(memory, remaining_tokens)\n            selected.append(summary)\n            total_tokens += tokens(summary)\n\n    return selected\n</code></pre> <p>Step 4: Intelligent Ordering</p> <p>Memories are ordered using one of three strategies:</p> <ul> <li>Recency First: Most recent first (best for conversations)</li> <li>Relevance First: Most relevant first (best for Q&amp;A)</li> <li>Chronological: Oldest first (best for narratives)</li> </ul>"},{"location":"guides/memory-optimization/#quick-start","title":"Quick Start","text":""},{"location":"guides/memory-optimization/#enable-in-agent-playbook","title":"Enable in Agent Playbook","text":"<pre><code>spec:\n    memory:\n        enabled: true\n        enable_context_optimization: true\n        max_context_tokens: 2000\n</code></pre> <p>That's it! The agent automatically uses optimized context.</p>"},{"location":"guides/memory-optimization/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from superoptix.memory import AgentMemory\n\n# Initialize with optimization enabled\nmemory = AgentMemory(\n    agent_id=\"support_agent\",\n    enable_context_optimization=True,\n    max_context_tokens=2000\n)\n\n# Store memories with importance scores\nmemory.remember(\n    \"Customer Sarah ordered laptop #12345\",\n    memory_type=\"short\"\n)\nmemory.remember(\n    \"Sarah prefers email contact\",\n    memory_type=\"long\",\n    importance=0.8\n)\nmemory.remember(\n    \"VIP customer since 2020\",\n    memory_type=\"long\",\n    importance=0.9\n)\n\n# Get optimized context for query\ncontext_info = memory.get_optimized_context(\n    --goal \"What happened with my shipping issue?\",\n    task_type=\"customer_support\"\n)\n\nprint(f\"Selected {context_info['optimization_info']['selected_count']} memories\")\nprint(f\"Total tokens: {context_info['optimization_info']['total_tokens']}\")\nprint(f\"\\n{context_info['context_string']}\")\n</code></pre>"},{"location":"guides/memory-optimization/#example-output","title":"Example Output","text":"<pre><code>Selected 6 memories\nTotal tokens: 1200\n\n## Relevant Memories\n\n### Memory 1: Shipping Issue (Score: 0.92)\nCustomer reported delayed delivery for order #12345...\n\n### Memory 2: VIP Status (Score: 0.85)\nVIP customer since 2020, lifetime value $50K...\n\n### Memory 3: Contact Preference (Score: 0.68)\nSarah prefers email contact for updates...\n</code></pre>"},{"location":"guides/memory-optimization/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/memory-optimization/#contextwindowoptimizer","title":"ContextWindowOptimizer","text":"<pre><code>from superoptix.optimizers.memory import ContextWindowOptimizer\n\noptimizer = ContextWindowOptimizer(\n    max_tokens=4096,              # Token budget\n    enable_gepa=True,             # Use GEPA scoring vs heuristics\n    min_relevance_score=0.3,      # Filter threshold\n    preserve_recency=True,        # Always keep recent memories\n)\n\nresult = optimizer.optimize_context(\n    --goal \"What is the return policy?\",\n    available_memories=all_memories,\n    task_type=\"customer_support\",\n    preserve_n_recent=3,          # Always include 3 most recent\n)\n</code></pre> <p>Result Structure:</p> <pre><code>{\n    \"selected_memories\": [...],           # Selected memory objects\n    \"total_tokens\": 1500,                 # Tokens used\n    \"strategy\": \"gepa_optimized_customer_support\",\n    \"scores\": {                           # Transparency\n        \"memory_1\": 0.85,\n        \"memory_2\": 0.72,\n        ...\n    },\n    \"optimization_time\": 0.045,           # Seconds\n    \"total_available\": 20,                # Total memories\n    \"selected_count\": 6                   # Selected count\n}\n</code></pre>"},{"location":"guides/memory-optimization/#agentmemory-integration","title":"AgentMemory Integration","text":"<pre><code>from superoptix.memory import AgentMemory\n\nmemory = AgentMemory(\n    agent_id=\"support_agent\",\n    enable_context_optimization=True,     # Enable GEPA optimization\n    max_context_tokens=2000,              # Token budget\n    backend=None,                         # Default SQLite\n    short_term_capacity=100,              # Short-term memory size\n    enable_embeddings=True                # For semantic search\n)\n</code></pre>"},{"location":"guides/memory-optimization/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/memory-optimization/#task-specific-optimization","title":"Task-Specific Optimization","text":"<p>Different tasks need different memory selection strategies:</p> <pre><code># Q&amp;A - Prioritize relevance\ncontext = memory.get_optimized_context(\n    --goal \"What is our refund policy?\",\n    task_type=\"qa\"\n)\n\n# Conversation - Prioritize recency\ncontext = memory.get_optimized_context(\n    --goal \"Continue our discussion\",\n    task_type=\"conversation\"\n)\n\n# Knowledge Search - Prioritize importance\ncontext = memory.get_optimized_context(\n    --goal \"Find all critical business rules\",\n    task_type=\"knowledge\"\n)\n</code></pre>"},{"location":"guides/memory-optimization/#custom-weighting","title":"Custom Weighting","text":"<p>Override default task weights:</p> <pre><code># Custom weights for specialized task\nresult = optimizer.optimize_context(\n    --goal \"Emergency protocol check\",\n    available_memories=all_memories,\n    task_type=\"custom\",\n)\n</code></pre>"},{"location":"guides/memory-optimization/#memory-summarization","title":"Memory Summarization","text":"<p>When full memory content doesn't fit in token budget, GEPA automatically compresses:</p> <pre><code># Original memory\n{\n    \"content\": \"Customer Sarah Johnson (sarah@email.com) called on Oct 18 at 3pm regarding delayed shipping for order #12345. She ordered a laptop (Dell XPS 15) on Oct 10 with expedited shipping but tracking shows it's still in transit. She's frustrated because she needs it for a presentation on Oct 22. We offered overnight shipping for her next order and 20% discount code SORRY20.\",\n    \"tokens\": 300\n}\n\n# Compressed summary (when budget is tight)\n{\n    \"content\": \"Customer Sarah: Delayed order #12345 (laptop). Needs by Oct 22. Offered overnight + 20% discount.\",\n    \"tokens\": 80,\n    \"is_summary\": True\n}\n</code></pre>"},{"location":"guides/memory-optimization/#optimization-metrics","title":"Optimization Metrics","text":""},{"location":"guides/memory-optimization/#before-vs-after","title":"Before vs After","text":"Metric Unoptimized GEPA-Optimized Improvement Avg tokens used 4500 1800 -60% Relevance % 30% 85% +55% Memories selected 18 6 Optimized Task success rate 65% 90% +25% Optimization time - &lt;100ms Fast"},{"location":"guides/memory-optimization/#transparency-monitoring","title":"Transparency &amp; Monitoring","text":"<p>Track optimization performance:</p> <pre><code># Get optimization statistics\nstats = optimizer.get_stats()\n\nprint(f\"Total optimizations: {stats['total_optimizations']}\")\nprint(f\"Avg tokens used: {stats['avg_tokens_used']:.0f}\")\nprint(f\"Avg memories selected: {stats['avg_memories_selected']:.1f}\")\nprint(f\"Avg relevance score: {stats['avg_relevance_score']:.2f}\")\n</code></pre>"},{"location":"guides/memory-optimization/#best-practices","title":"Best Practices","text":""},{"location":"guides/memory-optimization/#token-budget-sizing","title":"Token Budget Sizing","text":"<p>Recommended budgets:</p> <ul> <li>Conversation agents: 2000-4000 tokens</li> <li>Q&amp;A agents: 1000-2000 tokens</li> <li>Knowledge agents: 4000-8000 tokens</li> <li>Customer support: 2000-3000 tokens</li> </ul> <p>Rule of thumb: Set budget to 30-50% of model's total context window.</p>"},{"location":"guides/memory-optimization/#importance-scoring","title":"Importance Scoring","text":"<p>Set importance when storing memories:</p> <pre><code># Critical business rules\nmemory.remember(\n    \"Refunds must be approved within 24 hours\",\n    memory_type=\"long\",\n    importance=1.0  # Maximum importance\n)\n\n# VIP customer info\nmemory.remember(\n    \"Customer is VIP tier, lifetime value $100K\",\n    memory_type=\"long\",\n    importance=0.9\n)\n\n# Regular interaction\nmemory.remember(\n    \"Customer asked about shipping times\",\n    memory_type=\"short\",\n    importance=0.5  # Default\n)\n\n# Low-priority note\nmemory.remember(\n    \"Customer mentioned they like blue color\",\n    memory_type=\"short\",\n    importance=0.2\n)\n</code></pre>"},{"location":"guides/memory-optimization/#preserve-recent-memories","title":"Preserve Recent Memories","text":"<p>Always include most recent memories for context continuity:</p> <pre><code>result = optimizer.optimize_context(\n    --goal \"Continue our conversation\",\n    available_memories=all_memories,\n    preserve_n_recent=3  # Always include 3 most recent\n)\n</code></pre>"},{"location":"guides/memory-optimization/#use-cases","title":"Use Cases","text":""},{"location":"guides/memory-optimization/#customer-support-agent","title":"Customer Support Agent","text":"<pre><code>spec:\n    persona:\n        role: Customer Support Agent\n        goal: Help customers with orders and issues\n\n    memory:\n        enabled: true\n        enable_context_optimization: true\n        max_context_tokens: 2000\n\n    # GEPA automatically prioritizes:\n    # - Recent interactions (high recency)\n    # - VIP status (high importance)\n    # - Related issues (high relevance)\n</code></pre>"},{"location":"guides/memory-optimization/#knowledge-base-agent","title":"Knowledge Base Agent","text":"<pre><code>spec:\n    persona:\n        role: Knowledge Base Agent\n        goal: Answer questions from documentation\n\n    memory:\n        enabled: true\n        enable_context_optimization: true\n        max_context_tokens: 4000\n\n    # GEPA automatically prioritizes:\n    # - Relevant documentation (high relevance)\n    # - Critical policies (high importance)\n    # - Recent updates (medium recency)\n</code></pre>"},{"location":"guides/memory-optimization/#conversational-agent","title":"Conversational Agent","text":"<pre><code>spec:\n    persona:\n        role: Conversational Assistant\n        goal: Engage in natural dialogue\n\n    memory:\n        enabled: true\n        enable_context_optimization: true\n        max_context_tokens: 3000\n\n    # GEPA automatically prioritizes:\n    # - Recent messages (high recency)\n    # - Conversation topics (high relevance)\n    # - User preferences (high importance)\n</code></pre>"},{"location":"guides/memory-optimization/#technical-architecture","title":"Technical Architecture","text":""},{"location":"guides/memory-optimization/#components","title":"Components","text":""},{"location":"guides/memory-optimization/#contextwindowoptimizer_1","title":"ContextWindowOptimizer","text":"<p>Main GEPA-based optimizer that orchestrates memory selection:</p> <pre><code>class ContextWindowOptimizer:\n    \"\"\"\n    Optimizes:\n    - Which memories to include (relevance, importance, recency)\n    - How much of each memory (full, summary, keywords)\n    - Order of memories (chronological, relevance-based, hybrid)\n    - Token budget allocation across memory types\n    \"\"\"\n\n    def optimize_context(\n        self,\n        query: str,\n        available_memories: List[Dict],\n        task_type: str = \"general\",\n        preserve_n_recent: int = 3,\n    ) -&gt; Dict:\n        # Step 1: Score all memories\n        # Step 2: Preserve most recent\n        # Step 3: Select within budget\n        # Step 4: Order optimally\n        ...\n</code></pre>"},{"location":"guides/memory-optimization/#memoryranker","title":"MemoryRanker","text":"<p>Multi-factor ranking with task-specific weights:</p> <pre><code>class MemoryRanker:\n    \"\"\"\n    Ranks memories by:\n    - Relevance to query\n    - Importance score\n    - Recency (time decay)\n    \"\"\"\n\n    def rank_hybrid(\n        self,\n        query: str,\n        memories: List[Dict],\n        weights: Dict[str, float] = None\n    ) -&gt; List[Tuple[float, Dict]]:\n        # Combines relevance + importance + recency\n        ...\n</code></pre>"},{"location":"guides/memory-optimization/#memorysummarizer","title":"MemorySummarizer","text":"<p>Compresses memories when budget is tight:</p> <pre><code>class MemorySummarizer:\n    \"\"\"\n    Summarizes memories to fit token budget.\n    Preserves key information while reducing tokens.\n    \"\"\"\n\n    def summarize(\n        self,\n        memory: Dict,\n        target_tokens: int\n    ) -&gt; Dict:\n        # GEPA-based compression\n        ...\n</code></pre>"},{"location":"guides/memory-optimization/#gepa-scoring-algorithm","title":"GEPA Scoring Algorithm","text":"<p>GEPA uses Chain of Thought to score memory relevance:</p> <pre><code>class MemoryRelevanceScorer(dspy.Signature):\n    \"\"\"Score how relevant a memory is for answering a query.\"\"\"\n\n    query = dspy.InputField(desc=\"User query or current task\")\n    memory_content = dspy.InputField(desc=\"Memory content to evaluate\")\n    memory_metadata = dspy.InputField(desc=\"Memory metadata (type, age, importance)\")\n    task_context = dspy.InputField(desc=\"Additional task context\")\n\n    relevance_score = dspy.OutputField(\n        desc=\"Relevance score 0.0-1.0\",\n        prefix=\"Score:\"\n    )\n    reasoning = dspy.OutputField(\n        desc=\"Brief explanation of score\",\n        prefix=\"Reasoning:\"\n    )\n\n# GEPA learns to reason about memory relevance\nscorer = dspy.ChainOfThought(MemoryRelevanceScorer)\n</code></pre>"},{"location":"guides/memory-optimization/#scoring-formula","title":"Scoring Formula","text":"<pre><code>def score_memory(memory, query, task_type):\n    # Calculate component scores\n    relevance = calculate_relevance(query, memory.content)\n    importance = memory.importance\n    recency = calculate_recency(memory.timestamp)\n\n    # Get task-specific weights (GEPA optimizes these!)\n    weights = get_task_weights(task_type)\n\n    # Combine with learned weights\n    final_score = (\n        relevance * weights['relevance'] +\n        importance * weights['importance'] +\n        recency * weights['recency']\n    )\n\n    return final_score\n</code></pre>"},{"location":"guides/memory-optimization/#demo-example","title":"Demo Example","text":""},{"location":"guides/memory-optimization/#pull-demo-agent","title":"Pull Demo Agent","text":"<pre><code># Pull customer support agent with memory\nsuper agent pull customer_support_memory\n\n# Compile\nsuper agent compile customer_support_memory\n\n# Run with verbose mode to see memory selection\nsuper agent run customer_support_memory --verbose \\\n    --goal \"What happened with my shipping issue?\"\n</code></pre>"},{"location":"guides/memory-optimization/#expected-output","title":"Expected Output","text":"<pre><code>\ud83e\udde0 Memory Optimization:\n   Available: 20 memories (5000 tokens)\n   Selected: 6 memories (1010 tokens)\n   Strategy: gepa_optimized_customer_support\n\n   Top memories:\n   1. Shipping issue #12345 (score: 0.92)\n   2. VIP customer status (score: 0.85)\n   3. Contact preference (score: 0.68)\n   ...\n\nResponse generated with optimized context!\n</code></pre>"},{"location":"guides/memory-optimization/#performance","title":"Performance","text":""},{"location":"guides/memory-optimization/#benchmarks","title":"Benchmarks","text":"<ul> <li>Optimization Time: &lt;100ms per query</li> <li>Memory Footprint: No increase (lazy loading)</li> <li>Scalability: Tested with 1000+ memories</li> <li>Accuracy: 85%+ relevance in selected memories</li> </ul>"},{"location":"guides/memory-optimization/#fallback-behavior","title":"Fallback Behavior","text":"<p>If GEPA optimization fails, SuperOptiX gracefully falls back to heuristic scoring:</p> <pre><code>try:\n    # Use GEPA Chain of Thought\n    score = gepa_scorer(query, memory_content, metadata)\nexcept Exception:\n    # Fallback to heuristic\n    score = heuristic_score(relevance, importance, recency)\n</code></pre>"},{"location":"guides/memory-optimization/#framework-agnostic","title":"Framework-Agnostic","text":"<p>Memory optimization works across all supported frameworks:</p> <ul> <li>DSPy: Native integration</li> <li>OpenAI SDK: Compatible</li> <li>CrewAI: Compatible  </li> <li>Google ADK: Compatible</li> <li>Microsoft: Compatible</li> <li>DeepAgents: Compatible</li> </ul> <p>The optimization layer is completely independent of the framework layer!</p>"},{"location":"guides/memory-optimization/#comparison","title":"Comparison","text":""},{"location":"guides/memory-optimization/#vs-simple-memory","title":"vs Simple Memory","text":"<pre><code># Simple (include all recent)\ncontext = memory.get_recent(20)\n# Result: 5000 tokens, 30% relevant\n\n# GEPA-Optimized\ncontext = memory.get_optimized_context(query)\n# Result: 1800 tokens, 85% relevant\n</code></pre>"},{"location":"guides/memory-optimization/#vs-manual-selection","title":"vs Manual Selection","text":"<pre><code># Manual (hardcoded rules)\nif \"shipping\" in query:\n    context = memory.filter(category=\"orders\")\nelif \"refund\" in query:\n    context = memory.filter(category=\"payments\")\n# Brittle, doesn't scale\n\n# GEPA (learns patterns)\ncontext = memory.get_optimized_context(query, task_type=\"support\")\n# Automatically learns what's relevant for each query type\n</code></pre>"},{"location":"guides/memory-optimization/#vs-rag","title":"vs RAG","text":"<p>Memory optimization and RAG serve different purposes:</p> Feature Memory Optimization RAG Optimization Purpose Select agent's past experiences Retrieve external knowledge Source Agent's own memories Vector database Optimization Relevance + Importance + Recency Query + Chunk selection Use Case Personalization, continuity Knowledge grounding Combinable Yes! Use both together Yes!"},{"location":"guides/memory-optimization/#integration-with-other-optimizations","title":"Integration with Other Optimizations","text":"<p>Memory optimization works alongside other SuperOptiX optimizations:</p>"},{"location":"guides/memory-optimization/#memory-prompt-optimization","title":"Memory + Prompt Optimization","text":"<pre><code>spec:\n    memory:\n        enabled: true\n        enable_context_optimization: true  # Optimize memory selection\n\n    optimization:\n        strategy: gepa                     # Optimize prompts\n        metrics: [accuracy, relevance]\n</code></pre> <p>GEPA optimizes both: - Which memories to include in context - How to phrase prompts using those memories</p>"},{"location":"guides/memory-optimization/#memory-rag-optimization","title":"Memory + RAG Optimization","text":"<pre><code>spec:\n    memory:\n        enabled: true\n        enable_context_optimization: true\n\n    knowledge:\n        enabled: true\n        rag_enabled: true\n</code></pre> <p>Combines: - Agent's personal memories (optimized) - External knowledge (RAG retrieval)</p>"},{"location":"guides/memory-optimization/#memory-tool-optimization","title":"Memory + Tool Optimization","text":"<pre><code>spec:\n    memory:\n        enabled: true\n        enable_context_optimization: true\n\n    tools:\n        - name: get_order_status\n          mcp_enabled: true\n</code></pre> <p>Memory provides context, MCP optimizes tool usage!</p>"},{"location":"guides/memory-optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/memory-optimization/#high-token-usage","title":"High Token Usage","text":"<p>If memory context still uses too many tokens:</p> <pre><code># Reduce budget\nmemory = AgentMemory(\n    agent_id=\"agent\",\n    max_context_tokens=1000  # Lower budget\n)\n\n# Increase minimum relevance threshold\noptimizer = ContextWindowOptimizer(\n    max_tokens=2000,\n    min_relevance_score=0.5  # Higher threshold (default: 0.3)\n)\n</code></pre>"},{"location":"guides/memory-optimization/#low-relevance","title":"Low Relevance","text":"<p>If selected memories aren't relevant:</p> <pre><code># Enable GEPA scoring\noptimizer = ContextWindowOptimizer(\n    enable_gepa=True  # Use GEPA vs heuristics\n)\n\n# Adjust task type\ncontext = memory.get_optimized_context(\n    --goal \"...\",\n    task_type=\"qa\"  # Try different task types\n)\n</code></pre>"},{"location":"guides/memory-optimization/#missing-recent-context","title":"Missing Recent Context","text":"<p>If recent memories aren't included:</p> <pre><code># Increase recency preservation\nresult = optimizer.optimize_context(\n    --goal \"...\",\n    preserve_n_recent=5  # Include 5 most recent (default: 3)\n)\n</code></pre>"},{"location":"guides/memory-optimization/#api-reference","title":"API Reference","text":""},{"location":"guides/memory-optimization/#optimize_context","title":"optimize_context()","text":"<pre><code>optimizer.optimize_context(\n    query: str,                      # Current query/task\n    available_memories: List[Dict],  # All memories\n    task_type: str = \"general\",      # Task category\n    preserve_n_recent: int = 3       # Always include N recent\n) -&gt; Dict\n</code></pre> <p>Returns: - <code>selected_memories</code>: List of selected memory dicts - <code>total_tokens</code>: Token count for selected memories - <code>strategy</code>: Optimization strategy used - <code>scores</code>: Dict of memory_id \u2192 relevance_score - <code>optimization_time</code>: Time taken (seconds) - <code>total_available</code>: Count of available memories - <code>selected_count</code>: Count of selected memories</p>"},{"location":"guides/memory-optimization/#get_optimized_context","title":"get_optimized_context()","text":"<pre><code>memory.get_optimized_context(\n    query: str,                      # Current query\n    task_type: str = \"general\",      # Task category\n    preserve_n_recent: int = 3       # Recent memory count\n) -&gt; Dict\n</code></pre> <p>Returns: - <code>context_string</code>: Formatted context for LLM - <code>selected_memories</code>: Selected memory objects - <code>optimization_info</code>: Stats and scores</p>"},{"location":"guides/memory-optimization/#next-steps","title":"Next Steps","text":"<ol> <li>Try it: Add <code>enable_context_optimization: true</code> to your agent playbook</li> <li>Measure: Track token usage and relevance with <code>get_stats()</code></li> <li>Tune: Adjust <code>max_context_tokens</code> and <code>min_relevance_score</code></li> <li>Monitor: Use verbose mode to see memory selection in action</li> </ol> <p>Related Guides: - Memory System Guide - RAG Optimization - GEPA Optimization - Full-Stack Optimization</p> <p>Memory optimization is part of SuperOptiX's full-stack optimization approach.</p> <p>Optimize prompts. Optimize RAG. Optimize tools. Optimize memory. All with GEPA.</p>"},{"location":"guides/memory/","title":"\ud83e\udde0 Memory Systems Guide","text":"<p>SuperOptiX provides a universal memory management system that works across all 6 major agent frameworks, giving agents sophisticated state management and persistence capabilities.</p> <p>\ud83c\udf1f Key Achievement: Same memory system works across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/memory/#overview","title":"Overview","text":"<p>The memory system consists of four main components working together, regardless of framework:</p>"},{"location":"guides/memory/#memory-backends","title":"Memory Backends","text":"<p>Provides flexible storage options: - FileBackend: JSON/pickle file-based storage with TTL support - SQLiteBackend: Relational database storage with automatic cleanup - RedisBackend: High-performance in-memory storage (optional)</p> <p>Key Features: - Automatic TTL (time-to-live) management - Thread-safe operations - Pluggable architecture for easy extension - Support for both JSON and binary serialization</p>"},{"location":"guides/memory/#short-term-memory","title":"Short-Term Memory","text":"<p>Manages temporary, session-based information: - LRU Cache: Automatic eviction of least recently used items - Conversation History: Tracks dialogue between user and agent - Working Memory: Temporary context for current tasks - Retention Policies: LRU, FIFO, or priority-based eviction</p> <p>Use Cases: - Recent conversation context - Temporary task state - User preferences for current session - Quick access to frequently used information</p>"},{"location":"guides/memory/#long-term-memory","title":"Long-Term Memory","text":"<p>Persistent knowledge storage with semantic search: - Knowledge Categories: Organized storage (facts, procedures, experiences, etc.) - Semantic Search: Optional sentence-transformer embeddings for similarity search - Tagging System: Flexible categorization and retrieval - Importance Scoring: Weighted relevance for better retrieval</p> <p>Use Cases: - Learned facts and procedures - User preferences and patterns - Domain knowledge - Historical insights and patterns</p>"},{"location":"guides/memory/#episodic-memory","title":"Episodic Memory","text":"<p>Tracks experiences and temporal sequences: - Episode Management: Start, track, and end interaction episodes - Event Logging: Detailed event tracking within episodes - Pattern Analysis: Identify trends and patterns across episodes - Timeline Reconstruction: Chronological view of agent activities</p> <p>Use Cases: - Interaction tracking - Performance analysis - Learning from past experiences - Debugging and monitoring</p>"},{"location":"guides/memory/#context-manager","title":"Context Manager","text":"<p>Manages hierarchical context across different scopes: - Context Scopes: Global, session, task, and local contexts - Context Stack: Hierarchical context management - Automatic Persistence: Context saved across sessions - Context Merging: Intelligent combination of multiple context layers - Multi-Framework: Works across all 6 frameworks</p> <p>Scopes: - Global: Persistent agent characteristics and learned patterns - Session: Current session data and preferences - Task: Specific task context and state - Local: Temporary, function-specific context</p>"},{"location":"guides/memory/#multi-framework-memory-support","title":"\ud83d\udd27 Multi-Framework Memory Support","text":""},{"location":"guides/memory/#universal-memory-configuration","title":"Universal Memory Configuration","text":"<p>Memory works the same way across all frameworks! Just add the <code>memory</code> section to your playbook:</p> \ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>spec:\n  target_framework: dspy\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre> <pre><code>spec:\n  target_framework: openai\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre> <pre><code>spec:\n  target_framework: crewai\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre> <pre><code>spec:\n  target_framework: google-adk\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre> <pre><code>spec:\n  target_framework: microsoft\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre> <pre><code>spec:\n  target_framework: deepagents\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n</code></pre>"},{"location":"guides/memory/#universal-workflow","title":"Universal Workflow","text":"<pre><code># Same workflow for ALL frameworks!\nsuper agent compile &lt;agent_name&gt;  # Memory automatically configured\nsuper agent evaluate &lt;agent_name&gt;  # Test with memory persistence\nsuper agent optimize &lt;agent_name&gt; --auto medium  # GEPA optimizes memory-enhanced agents\nsuper agent run &lt;agent_name&gt;  # Use with memory persistence\n</code></pre>"},{"location":"guides/memory/#memory-gepa-optimization","title":"Memory + GEPA Optimization","text":"<p>Memory-enhanced agents can be optimized with GEPA:</p> <pre><code>spec:\n  target_framework: openai  # Works with ANY framework!\n  memory:\n    enabled: true\n    backend: sqlite\n    long_term:\n      enabled: true\n      semantic_search: true\n  optimization:\n    optimizer:\n      name: GEPA  # Optimize memory-enhanced agents!\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/memory/#key-features","title":"Key Features","text":""},{"location":"guides/memory/#multi-backend-support","title":"Multi-Backend Support","text":"<ul> <li>File-based storage for simplicity</li> <li>SQLite for structured data and queries</li> <li>Redis for high-performance scenarios</li> <li>Easy to extend with new backends</li> </ul>"},{"location":"guides/memory/#semantic-search","title":"Semantic Search","text":"<ul> <li>Optional sentence-transformer integration</li> <li>Similarity-based knowledge retrieval</li> <li>Automatic embedding generation</li> <li>Fallback to keyword search</li> </ul>"},{"location":"guides/memory/#automatic-context-management","title":"Automatic Context Management","text":"<ul> <li>Hierarchical context scopes</li> <li>Intelligent context merging</li> <li>Automatic persistence</li> <li>TTL-based expiration</li> </ul>"},{"location":"guides/memory/#interaction-tracking","title":"Interaction Tracking","text":"<ul> <li>Episode-based interaction management</li> <li>Detailed event logging</li> <li>Pattern recognition</li> <li>Performance analytics</li> </ul>"},{"location":"guides/memory/#learning-capabilities","title":"Learning Capabilities","text":"<ul> <li>Insight extraction from interactions</li> <li>Pattern recognition and storage</li> <li>Feedback integration</li> <li>Continuous improvement</li> </ul>"},{"location":"guides/memory/#production-ready","title":"Production Ready","text":"<ul> <li>Thread-safe operations</li> <li>Automatic cleanup and maintenance</li> <li>Comprehensive error handling</li> <li>Memory usage monitoring</li> </ul>"},{"location":"guides/memory/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/memory/#basic-memory-operations","title":"Basic Memory Operations","text":"<pre><code>from superoptix.memory import AgentMemory\n\nmemory = AgentMemory(\"assistant_agent\")\n\n# Store knowledge\nmemory.remember(\n    \"DSPy is a framework for programming with foundation models\",\n    memory_type=\"long\",\n    category=\"frameworks\",\n    tags=[\"dspy\", \"llm\", \"programming\"]\n)\n\n# Recall relevant information\nresults = memory.recall(\"DSPy programming\", limit=5)\nfor result in results:\n    print(f\"[{result['similarity']:.2f}] {result['content']}\")\n</code></pre>"},{"location":"guides/memory/#interaction-tracking_1","title":"Interaction Tracking","text":"<pre><code># Start tracking an interaction\nepisode_id = memory.start_interaction({\n    \"user_id\": \"user123\",\n    \"session_type\": \"technical_help\"\n})\n\n# Add events during the interaction\nmemory.add_interaction_event(\"question_asked\", \"User asked about Python decorators\")\nmemory.add_interaction_event(\"answer_provided\", \"Provided explanation with examples\")\nmemory.add_interaction_event(\"feedback_received\", \"User found answer helpful\")\n\n# End the interaction with results\nmemory.end_interaction({\n    \"success\": True,\n    \"satisfaction_score\": 0.9,\n    \"resolution_time\": 120\n})\n</code></pre>"},{"location":"guides/memory/#learning-from-feedback","title":"Learning from Feedback","text":"<pre><code># Learn insights from user interactions\ninsights = [\n    \"Users prefer code examples over theoretical explanations\",\n    \"Step-by-step tutorials are highly effective\",\n    \"Visual diagrams enhance understanding\"\n]\n\npatterns = {\n    \"effective_teaching_methods\": [\"examples\", \"step-by-step\", \"visual\"],\n    \"user_preferences\": {\"format\": \"practical\", \"detail_level\": \"medium\"}\n}\n\nmemory.learn_from_interaction(insights, patterns)\n</code></pre>"},{"location":"guides/memory/#memory-analytics","title":"Memory Analytics","text":"<pre><code># Get comprehensive memory statistics\nsummary = memory.get_memory_summary()\nprint(f\"Total interactions: {summary['interaction_count']}\")\nprint(f\"Knowledge items: {summary['long_term_memory']['total_items']}\")\nprint(f\"Active episodes: {summary['episodic_memory']['active_episodes']}\")\n\n# Cleanup expired data\ncleanup_stats = memory.cleanup_memory()\nprint(f\"Cleaned up {cleanup_stats['expired_short_term']} expired items\")\n</code></pre>"},{"location":"guides/memory/#configuration","title":"Configuration","text":""},{"location":"guides/memory/#basic-memory-control","title":"Basic Memory Control","text":""},{"location":"guides/memory/#disable-memory-completely","title":"Disable Memory Completely","text":"<pre><code>agent_capabilities:\n  memory:\n    enabled: false  # No memory system at all\n</code></pre>"},{"location":"guides/memory/#enable-memory-with-defaults","title":"Enable Memory with Defaults","text":"<pre><code>agent_capabilities:\n  memory:\n    enabled: true  # Uses SQLite backend with standard settings\n</code></pre>"},{"location":"guides/memory/#backend-configuration","title":"Backend Configuration","text":""},{"location":"guides/memory/#sqlite-backend-default","title":"SQLite Backend (Default)","text":"<pre><code>memory:\n  backend:\n    type: sqlite\n    config:\n      db_path: \".superoptix/agent_memory.db\"\n</code></pre>"},{"location":"guides/memory/#file-backend","title":"File Backend","text":"<pre><code>memory:\n  backend:\n    type: file\n    config:\n      storage_path: \".superoptix/memory\"\n</code></pre>"},{"location":"guides/memory/#redis-backend-high-performance","title":"Redis Backend (High Performance)","text":"<pre><code>memory:\n  backend:\n    type: redis\n    config:\n      host: \"localhost\"\n      port: 6379\n      db: 0\n      password: \"optional_password\"\n      prefix: \"agent_name:\"\n</code></pre>"},{"location":"guides/memory/#memory-component-configuration","title":"Memory Component Configuration","text":""},{"location":"guides/memory/#short-term-memory_1","title":"Short-term Memory","text":"<pre><code>memory:\n  short_term:\n    enabled: true\n    capacity: 100                    # Max items\n    retention_policy: lru            # lru|fifo|priority\n    max_conversation_length: 50      # Conversation history\n    default_ttl: 3600               # 1 hour TTL\n</code></pre>"},{"location":"guides/memory/#long-term-memory_1","title":"Long-term Memory","text":"<pre><code>memory:\n  long_term:\n    enabled: true\n    enable_embeddings: true          # Semantic search\n    embedding_model: \"all-MiniLM-L6-v2\"\n    search:\n      default_limit: 10\n      min_similarity_threshold: 0.3\n</code></pre>"},{"location":"guides/memory/#episodic-memory_1","title":"Episodic Memory","text":"<pre><code>memory:\n  episodic:\n    enabled: true\n    auto_start_episodes: true\n    episode_boundary: interaction    # time|task|manual|interaction\n    max_episode_duration: 3600      # 1 hour\n</code></pre>"},{"location":"guides/memory/#common-use-case-configurations","title":"Common Use Case Configurations","text":""},{"location":"guides/memory/#development-agent-full-memory","title":"Development Agent (Full Memory)","text":"<pre><code>memory:\n  enabled: true\n  backend:\n    type: sqlite\n  short_term:\n    capacity: 200\n    max_conversation_length: 100\n  long_term:\n    enable_embeddings: true\n    categories:\n      - name: \"code_patterns\"\n      - name: \"best_practices\"\n      - name: \"user_preferences\"\n  episodic:\n    pattern_analysis:\n      enabled: true\n</code></pre>"},{"location":"guides/memory/#customer-service-agent-conversation-focus","title":"Customer Service Agent (Conversation Focus)","text":"<pre><code>memory:\n  enabled: true\n  short_term:\n    capacity: 150\n    max_conversation_length: 200\n  long_term:\n    categories:\n      - name: \"customer_preferences\"\n      - name: \"common_issues\"\n  security:\n    pii_detection: true\n</code></pre>"},{"location":"guides/memory/#simple-task-agent-no-memory","title":"Simple Task Agent (No Memory)","text":"<pre><code>memory:\n  enabled: false\n</code></pre>"},{"location":"guides/memory/#high-performance-agent-redis","title":"High-Performance Agent (Redis)","text":"<pre><code>memory:\n  enabled: true\n  backend:\n    type: redis\n    config:\n      host: \"redis-cluster.example.com\"\n      port: 6379\n  performance:\n    cache_embeddings: true\n    batch_operations: true\n    connection_pooling: true\n</code></pre>"},{"location":"guides/memory/#best-practices","title":"Best Practices","text":""},{"location":"guides/memory/#memory-configuration-by-use-case","title":"Memory Configuration by Use Case","text":"<ul> <li>Development Agents: Enable all memory types with longer retention</li> <li>Customer Service: Focus on conversation history and user preferences  </li> <li>Data Analysis: Emphasize pattern recognition and episodic learning</li> <li>Simple Tasks: Disable memory for lightweight operation</li> </ul>"},{"location":"guides/memory/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use Redis backend for high-throughput scenarios</li> <li>Enable embedding caching for semantic search heavy workloads</li> <li>Configure appropriate retention policies to manage storage</li> <li>Use batch operations for bulk memory updates</li> </ul>"},{"location":"guides/memory/#security-considerations","title":"Security Considerations","text":"<ul> <li>Enable PII detection for customer-facing agents</li> <li>Use encryption for sensitive data</li> <li>Configure appropriate retention policies for compliance</li> <li>Enable audit logging for regulated environments</li> </ul>"},{"location":"guides/memory/#memory-design","title":"Memory Design","text":"<ul> <li>Use short-term memory for session-specific data</li> <li>Use long-term memory for persistent knowledge</li> <li>Use episodic memory for interaction tracking</li> <li>Use context for hierarchical state management</li> </ul>"},{"location":"guides/memory/#data-management","title":"Data Management","text":"<ul> <li>Validate data before storage</li> <li>Use appropriate categories and tags</li> <li>Implement data versioning</li> <li>Regular backup of memory data</li> </ul>"},{"location":"guides/memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/memory/#common-issues","title":"Common Issues","text":""},{"location":"guides/memory/#memory-not-working","title":"Memory Not Working","text":"<pre><code># Check if memory is enabled\nmemory = AgentMemory(\"test_agent\")\nprint(f\"Memory enabled: {memory.is_enabled()}\")\n\n# Check backend status\nprint(f\"Backend status: {memory.get_backend_status()}\")\n</code></pre>"},{"location":"guides/memory/#performance-issues","title":"Performance Issues","text":"<pre><code># Monitor memory usage\nstats = memory.get_memory_stats()\nprint(f\"Memory usage: {stats['usage_percent']}%\")\nprint(f\"Items in memory: {stats['total_items']}\")\n\n# Clean up if needed\nmemory.cleanup()\n</code></pre>"},{"location":"guides/memory/#data-persistence-issues","title":"Data Persistence Issues","text":"<pre><code># Check if data is being saved\nmemory.remember(\"test data\", memory_type=\"long\")\nmemory.save_memory_state()\n\n# Verify data persistence\nresults = memory.recall(\"test data\")\nprint(f\"Found {len(results)} results\")\n</code></pre>"},{"location":"guides/memory/#poor-search-results","title":"Poor Search Results","text":"<ul> <li>Adjust similarity thresholds</li> <li>Verify embedding model is loaded</li> <li>Check if fallback to keyword search is working</li> </ul>"},{"location":"guides/memory/#memory-conflicts","title":"Memory Conflicts","text":"<ul> <li>Ensure separate retrieval contexts</li> <li>Configure different similarity thresholds</li> <li>Use distinct embedding models if needed</li> </ul>"},{"location":"guides/memory/#debug-commands","title":"Debug Commands","text":"<pre><code># Check memory status\nagent.memory.get_memory_summary()\n\n# Export memory state for debugging\nagent.memory.save_memory_state()\n\n# Clear specific memory types\nagent.memory.short_term.clear()\nagent.memory.long_term.clear()\n\n# Enable verbose logging\nagent_config = {\n    \"memory\": {\n        \"debug\": {\n            \"verbose_logging\": True,\n            \"trace_memory_operations\": True\n        }\n    }\n}\n</code></pre>"},{"location":"guides/memory/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/memory/#memory-usage","title":"Memory Usage","text":"<ul> <li>Short-term memory: ~1MB per 1000 items</li> <li>Long-term memory: Depends on content size and embeddings</li> <li>Context management: Minimal overhead</li> <li>Episodic memory: ~10KB per episode</li> </ul>"},{"location":"guides/memory/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use appropriate TTL values for short-term memory</li> <li>Enable embeddings only when needed for semantic search</li> <li>Regular cleanup to prevent memory bloat</li> <li>Choose appropriate backend for your use case</li> <li>Monitor memory statistics for optimization opportunities</li> </ol>"},{"location":"guides/memory/#quick-commands","title":"Quick Commands","text":""},{"location":"guides/memory/#check-memory-status","title":"Check Memory Status","text":"<pre><code>agent.memory.get_memory_summary()\n</code></pre>"},{"location":"guides/memory/#clear-memory","title":"Clear Memory","text":"<pre><code>agent.memory.clear_all_memory()\n</code></pre>"},{"location":"guides/memory/#manual-memory-operations","title":"Manual Memory Operations","text":"<pre><code># Store knowledge\nagent.memory.remember(\"Important fact\", memory_type=\"long\", category=\"facts\")\n\n# Recall information\nresults = agent.memory.recall(\"search query\", memory_type=\"all\")\n\n# Start interaction tracking\nepisode_id = agent.memory.start_interaction({\"user\": \"john_doe\"})\n</code></pre>"},{"location":"guides/memory/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Agent Development Guide - Complete agent development workflow</li> <li>RAG Guide - Knowledge retrieval systems</li> <li>Quick Start Guide - Getting started with SuperOptiX </li> </ul>"},{"location":"guides/microsoft-framework-integration/","title":"Microsoft Agent Framework Integration","text":"<p>This integration is available in SuperOptiX as legacy support.</p> <p>Use it when you already have Microsoft Agent Framework projects and want to keep a unified SuperSpec workflow. For new projects, prefer DSPy, Pydantic AI, OpenAI SDK, Claude SDK, Google ADK, or DeepAgents.</p>"},{"location":"guides/microsoft-framework-integration/#support-status","title":"Support Status","text":"<ul> <li>Legacy-compatible support is maintained.</li> <li>Compile, run, evaluate, and optimize flows remain available.</li> <li>New feature investment is focused on the other active frameworks.</li> </ul>"},{"location":"guides/microsoft-framework-integration/#install","title":"Install","text":"<pre><code>pip install superoptix[frameworks-microsoft]\n</code></pre>"},{"location":"guides/microsoft-framework-integration/#basic-workflow","title":"Basic Workflow","text":"<pre><code># Pull a demo agent\nsuper agent pull assistant_microsoft\n\n# Compile\nsuper agent compile assistant_microsoft --framework microsoft\n\n# Run\nsuper agent run assistant_microsoft --framework microsoft --goal \"Summarize the incident\"\n\n# Optional optimization path\nsuper agent compile assistant_microsoft --framework microsoft --optimize\nsuper agent optimize assistant_microsoft --framework microsoft --auto light\n</code></pre>"},{"location":"guides/microsoft-framework-integration/#playbook-example","title":"Playbook Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant_microsoft\nspec:\n  target_framework: microsoft\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n  persona:\n    role: Helpful Assistant\n    goal: Provide clear and practical answers\n  input_fields:\n    - name: query\n      type: string\n  output_fields:\n    - name: response\n      type: string\n</code></pre>"},{"location":"guides/microsoft-framework-integration/#notes","title":"Notes","text":"<ul> <li>Keep <code>--framework microsoft</code> explicit in <code>compile</code>, <code>run</code>, and <code>optimize</code>.</li> <li>If you need stronger active support for new capabilities (StackOne, RLM, fast template evolution), use one of the actively expanded frameworks.</li> </ul>"},{"location":"guides/mlflow-guide/","title":"\ud83e\uddea MLFlow Integration Guide for SuperOptiX","text":"<p>This guide documents a complete, real-world workflow for integrating MLFlow with SuperOptiX, including all commands, troubleshooting, and actual outputs.</p>"},{"location":"guides/mlflow-guide/#install-mlflow","title":"Install MLFlow","text":"<pre><code>pip install mlflow\n</code></pre> <p>Output: <pre><code>Requirement already satisfied: mlflow in /Users/local/miniconda3/lib/python3.12/site-packages (3.1.1)\n... (truncated for brevity)\nSuccessfully installed cachetools-5.5.2\n</code></pre></p>"},{"location":"guides/mlflow-guide/#start-the-mlflow-server","title":"Start the MLFlow Server","text":"<pre><code>mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlflow_artifacts\n</code></pre> <p>If you see an error about the port being in use: <pre><code>[ERROR] Connection in use: ('0.0.0.0', 5000)\n[ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\nRunning the mlflow server failed. Please see the logs above for details.\n</code></pre></p> <p>Solution: Use a different port (e.g., 5001):</p> <pre><code>mlflow server --host 0.0.0.0 --port 5001 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlflow_artifacts\n</code></pre> <p>Output: <pre><code>[INFO] Listening at: http://0.0.0.0:5001 (3817)\n[INFO] Using worker: sync\n[INFO] Booting worker with pid: ...\n</code></pre></p>"},{"location":"guides/mlflow-guide/#initialize-a-superoptix-project","title":"Initialize a SuperOptiX Project","text":"<pre><code>super init mlflow_demo\ncd mlflow_demo\n</code></pre> <p>Output: <pre><code>\ud83c\udf89 SUCCESS! Your full-blown shippable Agentic System 'mlflow_demo' is ready!\n... (truncated)\n</code></pre></p>"},{"location":"guides/mlflow-guide/#pull-and-compile-a-developer-agent","title":"Pull and Compile a Developer Agent","text":"<pre><code>super agent pull developer\nsuper agent compile developer\n</code></pre> <p>Output: <pre><code>\ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready\n... (truncated)\n\ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated\n... (truncated)\n</code></pre></p>"},{"location":"guides/mlflow-guide/#add-mlflow-observability-to-the-playbook","title":"Add MLFlow Observability to the Playbook","text":"<p>Edit <code>mlflow_demo/mlflow_demo/agents/developer/playbook/developer_playbook.yaml</code> and add:</p> <pre><code>observability:\n  enabled: true\n  backends:\n    - mlflow\n  mlflow:\n    experiment_name: \"developer_agent\"\n    tracking_uri: \"http://localhost:5001\"\n    log_artifacts: true\n    log_metrics: true\n    log_params: true\n    tags:\n      agent_type: \"developer\"\n      tier: \"genies\"\n      version: \"1.0.0\"\n      environment: \"development\"\n</code></pre>"},{"location":"guides/mlflow-guide/#run-the-agent","title":"Run the Agent","text":"<pre><code>super agent run developer --goal \"Write a Python function to calculate the factorial of a number with proper error handling\"\n</code></pre> <p>Output: <pre><code>\ud83d\ude80 Running agent 'developer'...\n... (truncated)\n\ud83c\udf89 Agent execution completed successfully!\n</code></pre></p>"},{"location":"guides/mlflow-guide/#check-trace-files","title":"Check Trace Files","text":"<pre><code>ls -la .superoptix/traces/\n</code></pre> <p>Output: <pre><code>-rw-r--r--@ 1 shashi  staff  1018 ... developer.jsonl\n-rw-r--r--@ 1 shashi  staff  9626 ... developer_20250714_204941.jsonl\n</code></pre></p>"},{"location":"guides/mlflow-guide/#view-traces-with-superoptix-cli","title":"View Traces with SuperOptiX CLI","text":"<pre><code>super observe list\nsuper observe traces developer_20250714_204941 --detailed\n</code></pre> <p>Output: <pre><code>\ud83d\udccb Available Agents with Traces\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Agent ID                  \u2503 Trace Count \u2503 Last Activity       \u2503\n... (truncated)\nLoaded 23 trace events\n\ud83d\udcca TRACE ANALYSIS SUMMARY\n... (truncated)\n</code></pre></p>"},{"location":"guides/mlflow-guide/#test-mlflow-logging-with-python","title":"Test MLFlow Logging with Python","text":"<p>Create <code>test_mlflow_integration.py</code>:</p> <pre><code>import mlflow\nimport json\nfrom datetime import datetime\nmlflow.set_tracking_uri(\"http://localhost:5001\")\nmlflow.set_experiment(\"superoptix_mlflow_test\")\nwith mlflow.start_run(run_name=\"superoptix_test_run\") as run:\n    mlflow.log_param(\"agent_name\", \"developer\")\n    mlflow.log_metric(\"execution_time_ms\", 15766.66)\n    mlflow.set_tag(\"test_run\", \"true\")\n    trace_data = {\"event_id\": \"test_event_123\", \"timestamp\": datetime.now().isoformat()}\n    with open(\"test_trace.json\", \"w\") as f:\n        json.dump(trace_data, f, indent=2)\n    mlflow.log_artifact(\"test_trace.json\")\n</code></pre> <p>Run it: <pre><code>python test_mlflow_integration.py\n</code></pre></p> <p>Output: <pre><code>\ud83e\uddea SuperOptiX MLFlow Integration Test\nMLFlow server is running and accessible\n... (truncated)\nSuccessfully logged data to MLFlow run: ...\n</code></pre></p>"},{"location":"guides/mlflow-guide/#run-the-full-demo-script","title":"Run the Full Demo Script","text":"<p>Create and run <code>demo_mlflow_superoptix.py</code> (see project for full code):</p> <pre><code>python demo_mlflow_superoptix.py\n</code></pre> <p>Output: <pre><code>\ud83e\uddea SuperOptiX + MLFlow Integration Demo\nMLFlow server is accessible\n\ud83c\udfaf Demo 1/3: Write a Python function to calculate the sum of two numbers\n\ud83e\udd16 Running agent with goal: ...\nAgent execution completed in ...ms\n... (truncated)\n\ud83c\udf89 Demo completed successfully!\n\ud83d\udccb MLFlow UI: http://localhost:5001\n</code></pre></p>"},{"location":"guides/mlflow-guide/#access-the-mlflow-ui","title":"Access the MLFlow UI","text":"<p>Open your browser to: <pre><code>http://localhost:5001\n</code></pre></p> <p>You\u2019ll see all experiments, runs, metrics, parameters, and artifacts.</p>"},{"location":"guides/mlflow-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Port in use: Use a different port (e.g., 5001)</li> <li>No runs in MLFlow UI: Check playbook config and server status</li> <li>Missing traces: Check <code>.superoptix/traces/</code> and agent logs</li> </ul>"},{"location":"guides/mlflow-guide/#summary","title":"Summary","text":"<p>You now have: - End-to-end MLFlow integration with SuperOptiX - Real agent runs tracked in MLFlow - Trace and artifact management - Performance analysis and troubleshooting</p> <p>Ready to scale up? Use this workflow as a template for production ML observability! </p>"},{"location":"guides/mlflow-guide/#choosing-between-mlflow-and-langfuse","title":"\ud83d\udd04 Choosing Between MLFlow and LangFuse","text":"<p>Both MLFlow and LangFuse provide excellent observability for SuperOptiX agents, but they serve different use cases:</p>"},{"location":"guides/mlflow-guide/#mlflow-best-for","title":"\ud83e\uddea MLFlow - Best for:","text":"<ul> <li>ML Experiment Tracking: Traditional ML workflows and experiments</li> <li>Artifact Management: Code, models, and data versioning</li> <li>Reproducibility: Detailed experiment tracking and comparison</li> <li>Team Collaboration: Experiment sharing and model registry</li> <li>Production ML: Model deployment and lifecycle management</li> </ul>"},{"location":"guides/mlflow-guide/#langfuse-best-for","title":"\ud83d\udd0d LangFuse - Best for:","text":"<ul> <li>LLM Observability: Specialized for language model applications</li> <li>Real-time Tracing: Detailed token usage and cost tracking</li> <li>User Feedback: Built-in feedback collection and scoring</li> <li>A/B Testing: LLM prompt and model comparison</li> <li>Production LLM: Live monitoring and debugging</li> </ul>"},{"location":"guides/mlflow-guide/#quick-comparison","title":"\ud83d\udcca Quick Comparison","text":"Feature MLFlow LangFuse Primary Focus ML Experiments LLM Observability Token Tracking Manual Automatic Cost Tracking Manual Built-in User Feedback Manual Native A/B Testing Manual Built-in Real-time UI Limited Excellent Artifact Storage Excellent Good Experiment Tracking Excellent Good"},{"location":"guides/mlflow-guide/#when-to-use-each","title":"\ud83c\udfaf When to Use Each","text":"<p>Choose MLFlow if: - You're doing traditional ML experiments - You need detailed artifact versioning - You want to track model performance over time - You're building ML pipelines</p> <p>Choose LangFuse if: - You're building LLM applications - You need real-time cost tracking - You want user feedback integration - You're doing prompt engineering - You need A/B testing for LLMs</p>"},{"location":"guides/mlflow-guide/#understanding-local-files-and-artifacts","title":"Understanding Local Files and Artifacts","text":""},{"location":"guides/mlflow-guide/#project-structure-after-mlflow-integration","title":"Project Structure After MLFlow Integration","text":"<pre><code>SuperOptiX/\n\u251c\u2500\u2500 mlflow_demo/                # Your SuperOptiX demo project\n\u2502   \u251c\u2500\u2500 ...                     # Agent code, playbooks, pipelines, etc.\n\u2502   \u2514\u2500\u2500 .superoptix/traces/     # JSONL trace files for each agent run\n\u251c\u2500\u2500 mlflow.db                   # SQLite database for MLFlow experiment tracking\n\u251c\u2500\u2500 mlflow_artifacts/           # Directory where MLFlow stores run artifacts (traces, code, outputs)\n</code></pre>"},{"location":"guides/mlflow-guide/#what-each-filefolder-is-for","title":"What Each File/Folder Is For","text":"<ul> <li>mlflow_demo/: Your SuperOptiX project, including agent configs and code.</li> <li>mlflow_demo/.superoptix/traces/: Raw trace files (JSONL) for every agent run. Useful for debugging, custom analytics, or exporting to other tools.</li> <li>mlflow.db: The MLFlow experiment tracking database (SQLite). All run metadata, parameters, metrics, and artifact references are stored here.</li> <li>mlflow_artifacts/: MLFlow\u2019s artifact store. Each run gets a subfolder with all logged files (traces, generated code, outputs, etc.).</li> </ul>"},{"location":"guides/mlflow-guide/#example-listing-and-inspecting-files","title":"Example: Listing and Inspecting Files","text":"<pre><code># List trace files for all agent runs\nls -lh mlflow_demo/.superoptix/traces/\n\n# List all MLFlow artifacts for all runs\nls -lh mlflow_artifacts/*\n\n# Inspect the MLFlow database (optional, advanced)\nsqlite3 mlflow.db \".tables\"\nsqlite3 mlflow.db \"SELECT * FROM runs LIMIT 3;\"\n</code></pre>"},{"location":"guides/mlflow-guide/#downloading-and-using-artifacts","title":"Downloading and Using Artifacts","text":"<ul> <li> <p>From the MLFlow UI:   Go to http://localhost:5001, select a run, and download any artifact (trace, code, output) directly from the web interface.</p> </li> <li> <p>From the Filesystem:   Artifacts are stored in <code>mlflow_artifacts/&lt;run_id&gt;/</code>. You can copy, analyze, or share these files as needed.</p> </li> </ul>"},{"location":"guides/mlflow-guide/#using-traces-for-custom-analysis","title":"Using Traces for Custom Analysis","text":"<p>You can load and analyze trace files (JSONL) with Python or any tool that supports JSONL:</p> <pre><code>import json\n\nwith open('mlflow_demo/.superoptix/traces/developer_20250714_204941.jsonl') as f:\n    for line in f:\n        event = json.loads(line)\n        print(event['event_type'], event['timestamp'])\n</code></pre>"},{"location":"guides/mlflow-guide/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>Observability Guide - Complete observability overview</li> <li>LangFuse Integration Guide - LangFuse observability integration</li> <li>Agent Development - Build custom agents</li> <li>MLFlow Documentation - Official MLFlow docs</li> <li>SuperOptiX CLI Reference - CLI commands reference</li> </ul>"},{"location":"guides/model-intelligence/","title":"\ud83e\udd16 Model Intelligence Guide (Coming Soon)","text":"<p>\ud83d\udd2e Work in Progress - Advanced model management features coming in SuperAgents tiers</p>"},{"location":"guides/model-intelligence/#development-status","title":"\ud83d\udea7 Development Status","text":"<p>\ud83d\udce2 Note: This feature is currently in development and expected to launch later this year as part of the SuperAgents framework roadmap.</p> <p>The Model Intelligence system represents the next evolution of SuperOptiX's model management capabilities, bringing enterprise-grade features to the SuperAgents tier and beyond.</p>"},{"location":"guides/model-intelligence/#what-is-model-intelligence","title":"\ud83c\udfaf What is Model Intelligence?","text":"<p>SuperOptiX's Model Intelligence System is a unified model management platform that provides intelligent discovery, installation, optimization, and management of local language models across multiple backends. Think of it as your \"AI model command center\" that handles everything from finding the right model to optimizing its performance.</p>"},{"location":"guides/model-intelligence/#key-features","title":"\ud83e\udde0 Key Features","text":"<ul> <li>\ud83d\udd0d Smart Discovery: Find models by use case, performance, and requirements</li> <li>\ud83d\udce6 One-Click Installation: Install models across different backends</li> <li>\u26a1 Performance Optimization: Automatic model tuning and optimization</li> <li>\ud83d\udda5\ufe0f Server Management: Start and manage local model servers</li> <li>\ud83d\udcca Intelligent Recommendations: Get model suggestions based on your needs</li> <li>\ud83d\udd04 Cross-Backend Support: Ollama, MLX, HuggingFace, LM Studio</li> </ul>"},{"location":"guides/model-intelligence/#model-intelligence-architecture","title":"\ud83c\udfd7\ufe0f Model Intelligence Architecture","text":"<pre><code>graph TD\n    A[\ud83e\udd16 Model Intelligence] --&gt; B[\ud83d\udd0d Discovery Engine]\n    A --&gt; C[\ud83d\udce6 Installation Manager]\n    A --&gt; D[\u26a1 Performance Optimizer]\n    A --&gt; E[\ud83d\udda5\ufe0f Server Controller]\n    A --&gt; F[\ud83d\udcca Analytics Engine]\n\n    B --&gt; G[Use Case Analysis]\n    B --&gt; H[Performance Metrics]\n    B --&gt; I[Resource Requirements]\n\n    C --&gt; J[Backend Detection]\n    C --&gt; K[Dependency Management]\n    C --&gt; L[Progress Tracking]\n\n    D --&gt; M[Model Tuning]\n    D --&gt; N[Resource Optimization]\n    D --&gt; O[Performance Monitoring]\n\n    E --&gt; P[Port Management]\n    E --&gt; Q[Process Control]\n    E --&gt; R[Health Monitoring]\n\n    F --&gt; S[Usage Analytics]\n    F --&gt; T[Performance Tracking]\n    F --&gt; U[Resource Monitoring]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#1e40af,stroke:#3b82f6,stroke-width:1px,color:#ffffff\n    style H fill:#6d28d9,stroke:#a855f7,stroke-width:1px,color:#ffffff\n    style I fill:#047857,stroke:#10b981,stroke-width:1px,color:#ffffff\n    style J fill:#7c3aed,stroke:#a855f7,stroke-width:1px,color:#ffffff\n    style K fill:#059669,stroke:#10b981,stroke-width:1px,color:#ffffff\n    style L fill:#d97706,stroke:#f59e0b,stroke-width:1px,color:#ffffff\n    style M fill:#dc2626,stroke:#ef4444,stroke-width:1px,color:#ffffff\n    style N fill:#059669,stroke:#10b981,stroke-width:1px,color:#ffffff\n    style O fill:#7c3aed,stroke:#a855f7,stroke-width:1px,color:#ffffff\n    style P fill:#1e3a8a,stroke:#3b82f6,stroke-width:1px,color:#ffffff\n    style Q fill:#d97706,stroke:#f59e0b,stroke-width:1px,color:#ffffff\n    style R fill:#dc2626,stroke:#ef4444,stroke-width:1px,color:#ffffff\n    style S fill:#059669,stroke:#10b981,stroke-width:1px,color:#ffffff\n    style T fill:#7c3aed,stroke:#a855f7,stroke-width:1px,color:#ffffff\n    style U fill:#1e3a8a,stroke:#3b82f6,stroke-width:1px,color:#ffffff</code></pre>"},{"location":"guides/model-intelligence/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"guides/model-intelligence/#model-discovery-recommendations","title":"Model Discovery &amp; Recommendations","text":"<p>Start by discovering what models are available and getting intelligent recommendations:</p> <pre><code># Get model recommendations based on your needs\nsuper model recommend --use-case \"code generation\"\nsuper model recommend --use-case \"text analysis\"\nsuper model recommend --use-case \"conversation\"\nsuper model recommend --use-case \"reasoning\"\n\n# Discover models by performance characteristics\nsuper model recommend --performance \"fast\"\nsuper model recommend --performance \"accurate\"\nsuper model recommend --performance \"balanced\"\n\n# Get recommendations for specific resources\nsuper model recommend --memory \"4GB\"\nsuper model recommend --memory \"8GB\"\nsuper model recommend --memory \"16GB\"\n</code></pre> <p>Example Output: <pre><code>\ud83c\udfaf Model Recommendations for: code generation\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83c\udfc6 Top Recommendations:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                    \u2503    Backend     \u2503 Performance  \u2503  Size   \u2503   Task    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama3.2:8b                              \u2502   \ud83e\udd99 ollama    \u2502   \u2b50\u2b50\u2b50\u2b50\u2b50   \u2502 medium  \u2502   chat    \u2502\n\u2502 mlx-community/phi-2                      \u2502     \ud83c\udf4e mlx     \u2502   \u2b50\u2b50\u2b50\u2b50    \u2502  small  \u2502   chat    \u2502\n\u2502 microsoft/Phi-4                          \u2502 \ud83e\udd17 huggingface \u2502   \u2b50\u2b50\u2b50\u2b50    \u2502  small  \u2502   chat    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udca1 Installation commands:\n  super model install llama3.2:8b\n  super model install -b mlx mlx-community/phi-2\n  super model install -b huggingface microsoft/Phi-4\n\n\ud83d\udcca Performance Analysis:\n  \u2022 llama3.2:8b: Best for complex code generation, requires 8GB RAM\n  \u2022 mlx-community/phi-2: Fast inference on Apple Silicon, 4GB RAM\n  \u2022 microsoft/Phi-4: Good balance of speed and quality, 6GB RAM\n</code></pre></p>"},{"location":"guides/model-intelligence/#comprehensive-model-discovery","title":"Comprehensive Model Discovery","text":"<p>Explore all available models with detailed information:</p> <pre><code># Get comprehensive discovery guide\nsuper model discover\n\n# Discover models by backend\nsuper model discover --backend ollama\nsuper model discover --backend mlx\nsuper model discover --backend huggingface\nsuper model discover --backend lmstudio\n\n# Discover models by task type\nsuper model discover --task chat\nsuper model discover --task code\nsuper model discover --task reasoning\nsuper model discover --task embedding\n</code></pre> <p>Example Output: <pre><code>\ud83d\udd0d SuperOptiX Model Discovery Guide\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83c\udfaf Backend Overview:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend                                  \u2503 Best For       \u2503 Platform     \u2503 Ease    \u2503 Performance\u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 \ud83e\udd99 Ollama                                \u2502 Beginners      \u2502 All platforms \u2502 \u2b50\u2b50\u2b50\u2b50\u2b50\u2502 \u2b50\u2b50\u2b50\u2b50   \u2502\n\u2502 \ud83c\udf4e MLX                                   \u2502 Apple Silicon  \u2502 macOS only    \u2502 \u2b50\u2b50\u2b50\u2b50 \u2502 \u2b50\u2b50\u2b50\u2b50\u2b50  \u2502\n\u2502 \ud83c\udfae LM Studio                             \u2502 Windows users  \u2502 Windows/macOS\u2502 \u2b50\u2b50\u2b50  \u2502 \u2b50\u2b50\u2b50\u2b50   \u2502\n\u2502 \ud83e\udd17 HuggingFace                           \u2502 Advanced users \u2502 All platforms \u2502 \u2b50\u2b50   \u2502 \u2b50\u2b50\u2b50\u2b50\u2b50  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcca Model Categories:\n  \u2022 Tiny Models (1-3B): Fast inference, limited reasoning\n  \u2022 Small Models (3-7B): Good balance, moderate resources\n  \u2022 Medium Models (7-13B): Strong reasoning, more resources\n  \u2022 Large Models (13B+): Best performance, high resources\n\n\ud83c\udfaf Task-Specific Recommendations:\n  \u2022 Chat: llama3.2:3b, phi-2, DialoGPT-small\n  \u2022 Code: llama3.2:8b, codellama:7b, phi-2\n  \u2022 Reasoning: llama3.2:8b, qwen2.5:7b, mistral:7b\n  \u2022 Embedding: nomic-embed-text, all-MiniLM-L6-v2\n</code></pre></p>"},{"location":"guides/model-intelligence/#intelligent-model-installation","title":"Intelligent Model Installation","text":"<p>Install models with smart dependency management and progress tracking:</p> <pre><code># Install with automatic backend detection\nsuper model install llama3.2:3b\n\n# Install with specific backend\nsuper model install -b mlx mlx-community/phi-2\nsuper model install -b huggingface microsoft/Phi-4\nsuper model install -b lmstudio llama-3.2-1b-instruct\n\n# Install with performance optimization\nsuper model install llama3.2:8b --optimize\nsuper model install -b mlx mlx-community/phi-2 --optimize\n\n# Force reinstall if needed\nsuper model install llama3.2:3b --force\n</code></pre> <p>Example Installation Output: <pre><code>\ud83d\ude80 SuperOptiX Model Intelligence - Installing llama3.2:3b\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udd0d Analyzing requirements...\n  \u2022 Backend: Ollama (auto-detected)\n  \u2022 Size: 3B parameters\n  \u2022 Memory: ~4GB RAM required\n  \u2022 Disk: ~2GB storage\n\n\ud83d\udce6 Installing dependencies...\n  Ollama CLI detected\n  Server status: Running on port 11434\n\n\ud83e\udd99 Pulling model llama3.2:3b from Ollama...\n\u23f3 Progress: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 100%\n  \u2022 Downloaded: 2.0 GB\n  \u2022 Verified: SHA256 checksum\n  \u2022 Optimized: Model weights\n\n\u26a1 Performance Optimization...\n  \u2022 Quantization: 4-bit (auto-applied)\n  \u2022 Memory usage: 3.2GB (optimized)\n  \u2022 Inference speed: ~15 tokens/sec\n\nInstallation completed successfully!\n\n\ud83d\udcca Model Details:\n  \u2022 Name: llama3.2:3b\n  \u2022 Backend: Ollama\n  \u2022 Size: Small (3B parameters)\n  \u2022 Task: Chat/Conversation\n  \u2022 Memory: 3.2GB RAM\n  \u2022 Performance: \u2b50\u2b50\u2b50\u2b50\n\n\ud83d\udca1 Next Steps:\n  \u2022 Start using: super model info llama3.2:3b\n  \u2022 Test performance: super model test llama3.2:3b\n  \u2022 Optimize further: super model optimize llama3.2:3b\n</code></pre></p>"},{"location":"guides/model-intelligence/#advanced-model-management","title":"\ud83d\udcca Advanced Model Management","text":""},{"location":"guides/model-intelligence/#comprehensive-model-listing","title":"Comprehensive Model Listing","text":"<p>Get detailed information about all your models:</p> <pre><code># List installed models with details\nsuper model list\n\n# List all available models (including uninstalled)\nsuper model list --all\n\n# Filter by backend\nsuper model list --backend ollama\nsuper model list --backend mlx\nsuper model list --backend huggingface\nsuper model list --backend lmstudio\n\n# Filter by size\nsuper model list --size tiny\nsuper model list --size small\nsuper model list --size medium\nsuper model list --size large\n\n# Filter by task\nsuper model list --task chat\nsuper model list --task code\nsuper model list --task reasoning\nsuper model list --task embedding\n\n# Combine filters\nsuper model list --backend ollama --size small --task chat\n\n# Verbose information\nsuper model list --verbose\n</code></pre> <p>Example Output: <pre><code>                            \ud83d\ude80 SuperOptiX Model Intelligence - 9 models                  \n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                    \u2503    Backend     \u2503    Status    \u2503  Size   \u2503   Task    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama-3.2-1b-instruct                    \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 llama-3.3-70b-instruct                   \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502  large  \u2502   chat    \u2502\n\u2502 llama-4-scout-17b-16e-instruct           \u2502  \ud83c\udfae lmstudio   \u2502 installed \u2502 medium  \u2502   chat    \u2502\n\u2502 llama3.1:8b                              \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502 medium  \u2502   chat    \u2502\n\u2502 llama3.2:1b                              \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502  tiny   \u2502   chat    \u2502\n\u2502 microsoft/DialoGPT-small                 \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 microsoft/Phi-4                          \u2502 \ud83e\udd17 huggingface \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 mlx-community_Llama-3.2-3B-Instruct-4bit \u2502     \ud83c\udf4e mlx     \u2502 installed \u2502  small  \u2502   chat    \u2502\n\u2502 nomic-embed-text:latest                  \u2502   \ud83e\udd99 ollama    \u2502 installed \u2502 Unknown \u2502 embedding \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcca Summary:\n  \u2022 Total Models: 9\n  \u2022 Backends: 4 (Ollama, MLX, HuggingFace, LM Studio)\n  \u2022 Size Distribution: 2 tiny, 4 small, 2 medium, 1 large\n  \u2022 Task Distribution: 8 chat, 1 embedding\n\n\ud83d\udd0d Discovery: super model discover\n\ud83d\udce5 Install: super model install &lt;model_name&gt;\n\u26a1 Optimize: super model optimize &lt;model_name&gt;\n</code></pre></p>"},{"location":"guides/model-intelligence/#detailed-model-information","title":"Detailed Model Information","text":"<p>Get comprehensive information about specific models:</p> <pre><code># Get detailed model information\nsuper model info llama3.2:3b\nsuper model info mlx-community/phi-2\nsuper model info microsoft/Phi-4\nsuper model info llama-3.2-1b-instruct\n</code></pre> <p>Example Output: <pre><code>\ud83d\udcca Model Information: llama3.2:3b\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udd0d Basic Information:\n  \u2022 Name: llama3.2:3b\n  \u2022 Backend: Ollama\n  \u2022 Status: Installed\n  \u2022 Size: Small (3B parameters)\n  \u2022 Task: Chat/Conversation\n\n\ud83d\udce6 Installation Details:\n  \u2022 Install Date: 2024-01-15 14:30:22\n  \u2022 Disk Size: 2.1 GB\n  \u2022 Location: ~/.ollama/models/llama3.2:3b\n  \u2022 Version: latest\n\n\u26a1 Performance Metrics:\n  \u2022 Memory Usage: 3.2 GB RAM\n  \u2022 Inference Speed: ~15 tokens/sec\n  \u2022 Quantization: 4-bit (auto-applied)\n  \u2022 Context Length: 8192 tokens\n\n\ud83c\udfaf Capabilities:\n  \u2022 Code Generation: \u2b50\u2b50\u2b50\n  \u2022 Text Analysis: \u2b50\u2b50\u2b50\u2b50\n  \u2022 Reasoning: \u2b50\u2b50\u2b50\n  \u2022 Conversation: \u2b50\u2b50\u2b50\u2b50\n\n\ud83d\udcca Usage Statistics:\n  \u2022 Last Used: 2024-01-15 16:45:12\n  \u2022 Total Runs: 47\n  \u2022 Average Response Time: 2.3s\n  \u2022 Success Rate: 98.2%\n\n\ud83d\udd27 Configuration:\n  \u2022 Temperature: 0.7 (default)\n  \u2022 Max Tokens: 2048 (default)\n  \u2022 Top P: 0.9 (default)\n  \u2022 Frequency Penalty: 0.0\n\n\ud83d\udca1 Recommendations:\n  \u2022 Best for: General conversation, text analysis\n  \u2022 Consider upgrading to: llama3.2:8b for better reasoning\n  \u2022 Alternative: phi-2 for faster inference\n</code></pre></p>"},{"location":"guides/model-intelligence/#model-performance-testing","title":"Model Performance Testing","text":"<p>Test and benchmark your models:</p> <pre><code># Test model performance\nsuper model test llama3.2:3b\nsuper model test mlx-community/phi-2\n\n# Test with specific prompts\nsuper model test llama3.2:3b --prompt \"Write a Python function to sort a list\"\nsuper model test mlx-community/phi-2 --prompt \"Explain quantum computing\"\n\n# Benchmark multiple models\nsuper model benchmark llama3.2:3b phi-2 microsoft/Phi-4\n\n# Performance analysis\nsuper model analyze llama3.2:3b\n</code></pre> <p>Example Test Output: <pre><code>\ud83e\uddea Model Performance Test: llama3.2:3b\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udcdd Test Prompt: \"Write a Python function to sort a list\"\n\n\u23f1\ufe0f Performance Metrics:\n  \u2022 Response Time: 2.1 seconds\n  \u2022 Tokens Generated: 156\n  \u2022 Tokens per Second: 74.3\n  \u2022 Memory Usage: 3.2 GB\n\n\ud83d\udcca Quality Assessment:\n  \u2022 Code Correctness: \u2b50\u2b50\u2b50\u2b50\n  \u2022 Code Completeness: \u2b50\u2b50\u2b50\u2b50\n  \u2022 Documentation: \u2b50\u2b50\u2b50\n  \u2022 Best Practices: \u2b50\u2b50\u2b50\u2b50\n\n\ud83c\udfaf Response Quality:\n  \u2022 Relevance: 95%\n  \u2022 Accuracy: 92%\n  \u2022 Completeness: 88%\n  \u2022 Clarity: 90%\n\n\ud83d\udcc8 Benchmark Comparison:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model                                    \u2503 Response Time  \u2503 Quality Score\u2503 Memory   \u2503 Tokens/sec\u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama3.2:3b                              \u2502     2.1s      \u2502     \u2b50\u2b50\u2b50\u2b50   \u2502 3.2GB   \u2502   74.3    \u2502\n\u2502 phi-2                                    \u2502     1.8s      \u2502     \u2b50\u2b50\u2b50    \u2502 2.8GB   \u2502   86.7    \u2502\n\u2502 microsoft/Phi-4                          \u2502     2.5s      \u2502     \u2b50\u2b50\u2b50\u2b50   \u2502 4.1GB   \u2502   62.4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udca1 Recommendations:\n  \u2022 For speed: Use phi-2 (1.8s vs 2.1s)\n  \u2022 For quality: Use llama3.2:3b or Phi-4\n  \u2022 For memory efficiency: Use phi-2 (2.8GB vs 3.2GB)\n</code></pre></p>"},{"location":"guides/model-intelligence/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"guides/model-intelligence/#automatic-model-optimization","title":"Automatic Model Optimization","text":"<p>Optimize your models for better performance:</p> <pre><code># Optimize model performance\nsuper model optimize llama3.2:3b\nsuper model optimize mlx-community/phi-2\n\n# Optimize with specific targets\nsuper model optimize llama3.2:3b --target speed\nsuper model optimize llama3.2:3b --target memory\nsuper model optimize llama3.2:3b --target quality\n\n# Compare before/after optimization\nsuper model optimize llama3.2:3b --compare\n</code></pre> <p>Example Optimization Output: <pre><code>\u26a1 Model Optimization: llama3.2:3b\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udd0d Pre-Optimization Analysis:\n  \u2022 Current Memory: 3.2 GB\n  \u2022 Current Speed: 74.3 tokens/sec\n  \u2022 Current Quality: \u2b50\u2b50\u2b50\u2b50\n\n\ud83d\udee0\ufe0f Optimization Process:\n  \u2022 Quantization: 4-bit \u2192 3-bit (memory reduction)\n  \u2022 Attention Optimization: Enabled sparse attention\n  \u2022 Cache Optimization: Increased KV cache efficiency\n  \u2022 Thread Optimization: Auto-tuned thread count\n\n\ud83d\udcca Optimization Results:\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Metric                                   \u2503 Before         \u2503 After        \u2503 Change  \u2503 Impact    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Memory Usage                             \u2502    3.2 GB     \u2502   2.4 GB     \u2502  -25%   \u2502   \ud83d\udfe2 Good \u2502\n\u2502 Inference Speed                          \u2502  74.3 t/s     \u2502  89.7 t/s    \u2502  +21%   \u2502   \ud83d\udfe2 Good \u2502\n\u2502 Response Time                            \u2502    2.1s       \u2502   1.7s       \u2502  -19%   \u2502   \ud83d\udfe2 Good \u2502\n\u2502 Quality Score                            \u2502   \u2b50\u2b50\u2b50\u2b50     \u2502   \u2b50\u2b50\u2b50\u2b50     \u2502  0%     \u2502   \ud83d\udfe1 Same \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nOptimization completed successfully!\n\ud83d\udca1 Memory saved: 800MB\n\ud83d\udca1 Speed improved: 21%\n\ud83d\udca1 Quality maintained: No degradation\n</code></pre></p>"},{"location":"guides/model-intelligence/#resource-management","title":"Resource Management","text":"<p>Monitor and manage model resources:</p> <pre><code># Monitor model resource usage\nsuper model monitor llama3.2:3b\nsuper model monitor --all\n\n# Get resource recommendations\nsuper model resources llama3.2:3b\nsuper model resources --recommendations\n\n# Clean up unused models\nsuper model cleanup\nsuper model cleanup --dry-run\n</code></pre>"},{"location":"guides/model-intelligence/#advanced-server-management","title":"\ud83d\udda5\ufe0f Advanced Server Management","text":""},{"location":"guides/model-intelligence/#multi-server-orchestration","title":"Multi-Server Orchestration","text":"<p>Run multiple model servers simultaneously:</p> <pre><code># Start multiple servers on different ports\nsuper model server mlx phi-2 --port 8000\nsuper model server huggingface microsoft/Phi-4 --port 8001\nsuper model server lmstudio llama-3.2-1b-instruct --port 1234\n\n# Monitor all servers\nsuper model servers --status\nsuper model servers --monitor\n\n# Stop specific server\nsuper model server stop --port 8000\nsuper model server stop --backend mlx\n\n# Stop all servers\nsuper model servers --stop-all\n</code></pre>"},{"location":"guides/model-intelligence/#server-health-monitoring","title":"Server Health Monitoring","text":"<p>Monitor server health and performance:</p> <pre><code># Check server health\nsuper model health --port 8000\nsuper model health --all\n\n# Get server metrics\nsuper model metrics --port 8000\nsuper model metrics --all\n\n# Server diagnostics\nsuper model diagnose --port 8000\nsuper model diagnose --all\n</code></pre>"},{"location":"guides/model-intelligence/#use-case-optimization","title":"\ud83c\udfaf Use Case Optimization","text":""},{"location":"guides/model-intelligence/#task-specific-optimization","title":"Task-Specific Optimization","text":"<p>Optimize models for specific use cases:</p> <pre><code># Optimize for code generation\nsuper model optimize llama3.2:8b --use-case code-generation\n\n# Optimize for text analysis\nsuper model optimize phi-2 --use-case text-analysis\n\n# Optimize for conversation\nsuper model optimize llama3.2:3b --use-case conversation\n\n# Optimize for reasoning\nsuper model optimize llama3.2:8b --use-case reasoning\n</code></pre>"},{"location":"guides/model-intelligence/#workload-specific-tuning","title":"Workload-Specific Tuning","text":"<p>Tune models for different workloads:</p> <pre><code># Tune for high-throughput\nsuper model tune llama3.2:3b --workload high-throughput\n\n# Tune for low-latency\nsuper model tune llama3.2:3b --workload low-latency\n\n# Tune for memory-constrained\nsuper model tune llama3.2:3b --workload memory-constrained\n\n# Tune for quality-focused\nsuper model tune llama3.2:3b --workload quality-focused\n</code></pre>"},{"location":"guides/model-intelligence/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"guides/model-intelligence/#model-configuration-management","title":"Model Configuration Management","text":"<p>Manage model configurations:</p> <pre><code># Save custom configuration\nsuper model config save llama3.2:3b --name \"my-config\"\n\n# Load configuration\nsuper model config load llama3.2:3b --name \"my-config\"\n\n# List configurations\nsuper model config list\n\n# Export configuration\nsuper model config export llama3.2:3b --file config.yaml\n\n# Import configuration\nsuper model config import llama3.2:3b --file config.yaml\n</code></pre>"},{"location":"guides/model-intelligence/#backend-specific-features","title":"Backend-Specific Features","text":"<p>Leverage backend-specific capabilities:</p> <pre><code># Ollama-specific features\nsuper model ollama --features\nsuper model ollama --optimize llama3.2:3b\n\n# MLX-specific features\nsuper model mlx --features\nsuper model mlx --optimize phi-2\n\n# HuggingFace-specific features\nsuper model huggingface --features\nsuper model huggingface --optimize microsoft/Phi-4\n\n# LM Studio-specific features\nsuper model lmstudio --features\nsuper model lmstudio --optimize llama-3.2-1b-instruct\n</code></pre>"},{"location":"guides/model-intelligence/#analytics-insights","title":"\ud83d\udcca Analytics &amp; Insights","text":""},{"location":"guides/model-intelligence/#usage-analytics","title":"Usage Analytics","text":"<p>Track model usage and performance:</p> <pre><code># Get usage analytics\nsuper model analytics --model llama3.2:3b\nsuper model analytics --all\n\n# Performance trends\nsuper model analytics --trends\nsuper model analytics --trends --model llama3.2:3b\n\n# Resource utilization\nsuper model analytics --resources\nsuper model analytics --resources --model llama3.2:3b\n</code></pre>"},{"location":"guides/model-intelligence/#performance-insights","title":"Performance Insights","text":"<p>Get detailed performance insights:</p> <pre><code># Performance insights\nsuper model insights llama3.2:3b\nsuper model insights --all\n\n# Bottleneck analysis\nsuper model analyze --bottlenecks llama3.2:3b\n\n# Optimization opportunities\nsuper model analyze --opportunities llama3.2:3b\n</code></pre>"},{"location":"guides/model-intelligence/#troubleshooting-support","title":"\ud83d\udea8 Troubleshooting &amp; Support","text":""},{"location":"guides/model-intelligence/#diagnostic-tools","title":"Diagnostic Tools","text":"<pre><code># Run comprehensive diagnostics\nsuper model diagnose\nsuper model diagnose --model llama3.2:3b\n\n# Check system compatibility\nsuper model check --system\nsuper model check --compatibility\n\n# Validate installation\nsuper model validate\nsuper model validate --model llama3.2:3b\n</code></pre>"},{"location":"guides/model-intelligence/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<pre><code># Fix common issues\nsuper model fix --common\nsuper model fix --model llama3.2:3b\n\n# Reset model configuration\nsuper model reset llama3.2:3b\n\n# Repair corrupted models\nsuper model repair llama3.2:3b\n</code></pre>"},{"location":"guides/model-intelligence/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/model-intelligence/#model-selection","title":"Model Selection","text":"<ul> <li>Start with recommendations: Use <code>super model recommend</code> for guidance</li> <li>Consider your use case: Different models excel at different tasks</li> <li>Balance performance and resources: Larger models aren't always better</li> <li>Test before committing: Use <code>super model test</code> to evaluate performance</li> </ul>"},{"location":"guides/model-intelligence/#performance-optimization_1","title":"Performance Optimization","text":"<ul> <li>Optimize for your workload: Use task-specific optimization</li> <li>Monitor resource usage: Keep an eye on memory and CPU usage</li> <li>Use appropriate quantization: Balance quality and performance</li> <li>Regular maintenance: Clean up unused models and configurations</li> </ul>"},{"location":"guides/model-intelligence/#server-management","title":"Server Management","text":"<ul> <li>Use dedicated ports: Avoid port conflicts with multiple servers</li> <li>Monitor server health: Regular health checks prevent issues</li> <li>Plan for scaling: Consider resource requirements for multiple models</li> <li>Backup configurations: Save custom configurations for reproducibility</li> </ul>"},{"location":"guides/model-intelligence/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Model Management Guide - Current model management capabilities</li> <li>Cloud Inference Guide - Cloud provider integration guides</li> <li>Agent Development Guide - Using models with agents</li> <li>CLI Reference - Complete command reference</li> <li>Troubleshooting Guide - Common issues and solutions</li> </ul>"},{"location":"guides/model-intelligence/#availability","title":"\ud83d\ude80 Availability","text":"<p>\ud83d\udcc5 Expected Launch: Later this year (2025)</p> <p>\ud83c\udfaf Target Tier: SuperAgents and above</p> <p>\ud83d\udd27 Current Status: Active development in progress</p> <p>The Model Intelligence system is being developed as part of the SuperAgents tier upgrade, bringing enterprise-grade model management capabilities to SuperOptiX. Stay tuned for updates and early access opportunities!</p> <p>Ready to master model intelligence? This feature will be available in the SuperAgents tier later this year! \ud83d\ude80</p>"},{"location":"guides/model-management/","title":"Model Management Guide","text":"<p>SuperOptiX provides a unified model management system that supports multiple backends including Ollama, MLX, HuggingFace, LM Studio, vLLM, and SGLang. This guide covers how to install, manage, and use models across different backends.</p>"},{"location":"guides/model-management/#quick-start","title":"Quick Start","text":"<p>The easiest way to use models is with the <code>super model run</code> command:</p> <pre><code># Run a model with auto-installation\nsuper model run llama3.2:3b \"Write a Python function to add two numbers\"\n\n# Specify backend explicitly\nsuper model run llama3.2:3b \"Write a hello world program\" --backend ollama\n\n# Interactive mode\nsuper model run llama3.2:3b \"\" --interactive\n</code></pre>"},{"location":"guides/model-management/#installing-models","title":"Installing Models","text":""},{"location":"guides/model-management/#basic-installation","title":"Basic Installation","text":"<pre><code># Install Ollama model (default backend)\nsuper model install llama3.2:3b --backend ollama\n\n# Install MLX model\nsuper model install mlx-community/Llama-3.2-3B-Instruct-4bit --backend mlx\n\n# Install HuggingFace model\nsuper model install microsoft/phi-1_5 --backend huggingface\n</code></pre>"},{"location":"guides/model-management/#backend-specific-installation","title":"Backend-Specific Installation","text":"<p>Each backend has different installation methods:</p> <ul> <li>Ollama: Uses <code>ollama pull</code> internally</li> <li>MLX: Downloads from HuggingFace Hub and converts to MLX format</li> <li>HuggingFace: Downloads using transformers library</li> <li>LM Studio: Manual installation via desktop app</li> </ul>"},{"location":"guides/model-management/#gpt-oss-openais-open-source","title":"GPT-OSS (OpenAI's Open Source)","text":"<ul> <li>Best for: Advanced reasoning, complex tasks</li> <li>Models: GPT-OSS-20B, GPT-OSS-120B</li> <li>Installation: </li> <li><code>super model install gpt-oss:20b</code> (Ollama - best performance)</li> <li><code>super model install lmstudio-community/gpt-oss-20b-MLX-8bit --backend mlx</code> (Apple Silicon - native support)</li> <li><code>super model install openai/gpt-oss-20b --backend huggingface</code> (Limited on Apple Silicon)</li> <li>Execution: Direct inference</li> <li>Features: Apache 2.0 license, MXFP4 quantization, Apple Silicon native support</li> <li>Resources: GPT-OSS-120B, GPT-OSS-20B, Ollama Library</li> </ul>"},{"location":"guides/model-management/#mlx","title":"MLX","text":"<ul> <li>Best for: Apple Silicon optimization, GPT-OSS native support</li> <li>Models: MLX-community models, GPT-OSS models</li> <li>Installation: Downloads from HuggingFace Hub</li> <li>Execution: Direct MLX-LM inference</li> <li>Features: Native Apple Silicon support for GPT-OSS models</li> </ul>"},{"location":"guides/model-management/#running-models","title":"Running Models","text":""},{"location":"guides/model-management/#basic-usage","title":"Basic Usage","text":"<pre><code># Run with auto-detection\nsuper model run &lt;model_name&gt; \"&lt;prompt&gt;\"\n\n# Examples\nsuper model run llama3.2:3b \"Write a Python function to calculate fibonacci\"\nsuper model run mlx-community/phi-2 \"Explain quantum computing in simple terms\"\nsuper model run microsoft/phi-1_5 \"Write a simple calculator program\"\n</code></pre>"},{"location":"guides/model-management/#interactive-mode","title":"Interactive Mode","text":"<pre><code># Start interactive session\nsuper model run &lt;model_name&gt; \"\" --interactive\n\n# Example\nsuper model run llama3.2:3b \"Write a story\" --interactive\n</code></pre>"},{"location":"guides/model-management/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code># Control generation parameters\nsuper model run llama3.2:3b \"Write a story\" --temperature 0.9\nsuper model run llama3.2:3b \"Explain AI\" --max-tokens 500\nsuper model run llama3.2:3b \"Write a poem\" --temperature 0.8 --max-tokens 200\n</code></pre>"},{"location":"guides/model-management/#backend-specific-examples","title":"Backend-Specific Examples","text":""},{"location":"guides/model-management/#gpt-oss-models","title":"\ud83e\udd16 GPT-OSS Models","text":"<p>SuperOptiX now supports OpenAI's latest open-source language models: GPT-OSS-20B and GPT-OSS-120B. These models are designed for advanced reasoning and agentic tasks.</p>"},{"location":"guides/model-management/#gpt-oss-model-overview","title":"\ud83c\udfaf GPT-OSS Model Overview","text":"Model Parameters Active Parameters Best For Hardware Requirements GPT-OSS-20B 21B 3.6B Lower latency, local/specialized use cases 16GB+ RAM GPT-OSS-120B 117B 5.1B Production, general purpose, high reasoning Single H100 GPU"},{"location":"guides/model-management/#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>\ud83d\udd13 Apache 2.0 License: Build freely without copyleft restrictions</li> <li>\u26a1 Native MXFP4 Quantization: Optimized for efficient inference</li> </ul>"},{"location":"guides/model-management/#installing-gpt-oss-models","title":"\ud83d\udce6 Installing GPT-OSS Models","text":""},{"location":"guides/model-management/#via-mlx-lm-apple-silicon-native-support","title":"Via MLX-LM (Apple Silicon - Native Support)","text":"<pre><code># Install GPT-OSS models via MLX-LM (Apple Silicon only)\nsuper model install openai_gpt-oss-20b --backend mlx\nsuper model install lmstudio-community/gpt-oss-120b-MLX-8bit --backend mlx\n\n# Test the models\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"Explain quantum computing with detailed reasoning\" --backend mlx\n</code></pre>"},{"location":"guides/model-management/#via-ollama-cross-platform-best-performance","title":"Via Ollama (Cross-Platform - Best Performance)","text":"<pre><code># Install GPT-OSS models\nsuper model install gpt-oss:20b\nsuper model install gpt-oss:120b\n\n# Test the models\nsuper model run gpt-oss:20b \"Explain quantum computing with detailed reasoning\" --backend ollama\n</code></pre>"},{"location":"guides/model-management/#via-huggingface-limited-on-apple-silicon","title":"Via HuggingFace (Limited on Apple Silicon)","text":"<pre><code># Install GPT-OSS models via MLX (Apple Silicon - Recommended)\nsuper model install lmstudio-community/gpt-oss-20b-MLX-8bit --backend mlx\nsuper model install lmstudio-community/gpt-oss-120b-MLX-8bit --backend mlx\n\n# Test the models\nsuper model run lmstudio-community/gpt-oss-20b-MLX-8bit \"Explain quantum computing with detailed reasoning\" --backend mlx\n</code></pre>"},{"location":"guides/model-management/#via-huggingface","title":"Via HuggingFace","text":"<pre><code># Install via MLX (Apple Silicon - Recommended)\nsuper model install lmstudio-community/gpt-oss-20b-MLX-8bit --backend mlx\nsuper model install lmstudio-community/gpt-oss-120b-MLX-8bit --backend mlx\n\n# Start server\nsuper model server mlx lmstudio-community/gpt-oss-20b-MLX-8bit --port 8000\n</code></pre>"},{"location":"guides/model-management/#using-gpt-oss-models","title":"\ud83c\udfaf Using GPT-OSS Models","text":"<pre><code># Simple question\nsuper model run gpt-oss:20b \"What is 2+2?\"\n\n# Explain a concept\nsuper model run gpt-oss:20b \"Explain machine learning\"\n\n# Complex task\nsuper model run gpt-oss:20b \"Design a distributed system architecture\"\n</code></pre>"},{"location":"guides/model-management/#basic-usage-examples","title":"\ud83d\udd27 Basic Usage Examples","text":"<pre><code># Simple calculation\nsuper model run gpt-oss:20b \"Calculate the factorial of 10\"\n\n# Get information\nsuper model run gpt-oss:20b \"What's the latest news about AI?\"\n\n# Complex problem solving\nsuper model run gpt-oss:120b \"Solve: A train leaves station A at 2 PM traveling at 60 mph. Another train leaves station B at 3 PM traveling at 80 mph. When will they meet if the stations are 300 miles apart?\"\n</code></pre>"},{"location":"guides/model-management/#performance-recommendations","title":"\ud83c\udfaf Performance Recommendations","text":"Use Case Recommended Model Hardware Quick responses GPT-OSS-20B 16GB+ RAM Complex tasks GPT-OSS-120B H100 GPU Local development GPT-OSS-20B 16GB+ RAM"},{"location":"guides/model-management/#auto-installation","title":"Auto-Installation","text":"<p>Models are automatically installed when they're not found:</p> <p>open</p> <pre><code># Ollama models\nsuper model run llama3.2:3b \"Hello world\"\n\n# MLX models\nsuper model run mlx-community/Llama-3.2-3B-Instruct-4bit \"Hello world\"\n\n# HuggingFace models\nsuper model run microsoft/phi-1_5 \"Hello world\"\n</code></pre>"},{"location":"guides/model-management/#managing-models","title":"Managing Models","text":""},{"location":"guides/model-management/#listing-models","title":"Listing Models","text":"<pre><code># List all installed models\nsuper model list\n\n# List by backend\nsuper model list --backend ollama\nsuper model list --backend mlx\nsuper model list --backend huggingface\n\n# Filter by size\nsuper model list --size small\n\n# Filter by task\nsuper model list --task code\n</code></pre>"},{"location":"guides/model-management/#removing-models","title":"Removing Models","text":"<pre><code># Remove from specific backend\nsuper model remove llama3.2:3b --backend ollama\n\n# Remove from all backends\nsuper model remove llama3.2:3b --all-backends\n\n# Auto-detect backend\nsuper model remove llama3.2:3b\n</code></pre>"},{"location":"guides/model-management/#getting-model-information","title":"Getting Model Information","text":"<pre><code># Get detailed model info\nsuper model info llama3.2:3b\n\n# Get info for specific backend\nsuper model info microsoft/phi-1_5 --backend huggingface\n</code></pre>"},{"location":"guides/model-management/#refreshing-cache","title":"Refreshing Cache","text":"<pre><code># Refresh model cache\nsuper model refresh\n</code></pre>"},{"location":"guides/model-management/#backend-specific-features","title":"Backend-Specific Features","text":""},{"location":"guides/model-management/#mlx-commands","title":"MLX Commands","text":"<p>MLX provides advanced features for Apple Silicon optimization:</p> <pre><code># Convert HuggingFace models to MLX\nsuper model mlx convert microsoft/phi-2 --output phi-2-mlx\n\n# Quantize MLX models\nsuper model mlx quantize phi-2-mlx --bits 4 --group-size 64\n\n# Finetune MLX models\nsuper model mlx finetune phi-2-mlx training_data --type lora --iters 1000\n\n# Evaluate MLX models\nsuper model mlx evaluate phi-2-mlx --tasks mmlu,arc,hellaswag\n\n# Fuse adapters into base model\nsuper model mlx fuse phi-2-mlx --adapter-path adapters --save-path fused_model\n</code></pre>"},{"location":"guides/model-management/#mlx-evaluation","title":"MLX Evaluation","text":"<p>Evaluate MLX models on standardized benchmarks using LM-Eval integration:</p> <pre><code># Basic evaluation\nsuper model mlx evaluate phi-2-mlx --tasks mmlu,arc,hellaswag\n\n# Advanced evaluation with custom parameters\nsuper model mlx evaluate phi-2-mlx \\\n  --tasks mmlu,arc,hellaswag \\\n  --output-dir results \\\n  --batch-size 32 \\\n  --shots 5 \\\n  --limit 100 \\\n  --seed 42\n\n# Evaluation with chat template\nsuper model mlx evaluate phi-2-mlx \\\n  --tasks mmlu \\\n  --chat-template \\\n  --max-tokens 2048\n</code></pre> <p>Setup Requirements: <pre><code># Install evaluation dependencies\npip install lm_eval\n\n# Verify MLX-LM installation\npython -c \"import mlx_lm; print('MLX-LM ready')\"\n</code></pre></p> <p>Dependencies: - <code>lm_eval</code> - Required for evaluation (install with: <code>pip install lm_eval</code>) - <code>mlx_lm</code> - Required for MLX operations</p> <p>Supported Tasks: - <code>mmlu</code> - Massive Multitask Language Understanding - <code>arc</code> - AI2 Reasoning Challenge - <code>hellaswag</code> - HellaSwag - <code>winogrande</code> - Winogrande - <code>truthfulqa</code> - TruthfulQA - <code>gsm8k</code> - Grade School Math 8K</p> <p>Evaluation Parameters: - <code>--tasks</code>: Comma-separated list of evaluation tasks - <code>--output-dir</code>: Directory to save results - <code>--batch-size</code>: Batch size for evaluation (default: 16) - <code>--shots</code>: Number of few-shot examples - <code>--limit</code>: Limit examples per task - <code>--seed</code>: Random seed for reproducibility - <code>--max-tokens</code>: Maximum tokens for generation - <code>--chat-template</code>: Use chat template for evaluation</p>"},{"location":"guides/model-management/#mlx-fusion","title":"MLX Fusion","text":"<p>Fuse finetuned adapters (LoRA/DoRA) into base models for deployment:</p> <pre><code># Basic fusion\nsuper model mlx fuse phi-2-mlx --adapter-path adapters\n\n# Fusion with dequantization\nsuper model mlx fuse phi-2-mlx --adapter-path adapters --dequantize\n\n# Fusion with GGUF export\nsuper model mlx fuse phi-2-mlx \\\n  --adapter-path adapters \\\n  --export-gguf \\\n  --gguf-path phi-2-fused.gguf\n\n# Fusion with HuggingFace upload\nsuper model mlx fuse phi-2-mlx \\\n  --adapter-path adapters \\\n  --upload username/phi-2-fused\n</code></pre> <p>Fusion Parameters: - <code>--adapter-path</code>: Path to trained adapter weights (default: adapters) - <code>--save-path</code>: Output path for fused model (default: fused_model) - <code>--dequantize</code>: Generate a dequantized model - <code>--export-gguf</code>: Export model in GGUF format - <code>--gguf-path</code>: Path for GGUF export (default: ggml-model-f16.gguf) - <code>--upload</code>: HuggingFace repo to upload fused model to</p> <p>Supported Model Types for GGUF Export: - <code>llama</code> - Llama models - <code>mixtral</code> - Mixtral models - <code>mistral</code> - Mistral models</p>"},{"location":"guides/model-management/#mlx-finetuning","title":"MLX Finetuning","text":"<pre><code># LoRA finetuning\nsuper model mlx finetune microsoft_phi-2 training_data --type lora --iters 1000\n\n# DoRA finetuning\nsuper model mlx finetune microsoft_phi-2 training_data --type dora --iters 1000\n\n# Full finetuning\nsuper model mlx finetune microsoft_phi-2 training_data --type full --iters 1000\n</code></pre>"},{"location":"guides/model-management/#advanced-mlx-configuration","title":"Advanced MLX Configuration","text":"<pre><code># Custom LoRA parameters\nsuper model mlx finetune model_name training_data --type lora --rank 8 --scale 20.0\n\n# DoRA with dropout\nsuper model mlx finetune model_name training_data --type dora --rank 8 --dropout 0.1\n\n# Full finetuning with gradient checkpointing\nsuper model mlx finetune model_name training_data --type full --layers 16\n\n# Advanced training configuration\nsuper model mlx finetune model_name training_data \\\n  --type lora \\\n  --batch-size 4 \\\n  --lr 1e-5 \\\n  --iters 1000 \\\n  --max-length 2048 \\\n  --report-steps 10 \\\n  --eval-steps 200 \\\n  --save-steps 100\n\n# With WandB logging\nsuper model mlx finetune model_name training_data --wandb my_project\n\n# Resume from checkpoint\nsuper model mlx finetune model_name training_data \\\n  --resume checkpoint_path \\\n  --iters 500\n\n# Test after training\nsuper model mlx finetune model_name training_data \\\n  --test \\\n  --grad-checkpoint\n</code></pre>"},{"location":"guides/model-management/#training-data-format","title":"Training Data Format","text":"<p>MLX finetuning requires JSONL format:</p> <pre><code>training_data/\n\u251c\u2500\u2500 train.jsonl    # Training examples\n\u251c\u2500\u2500 valid.jsonl    # Validation examples\n\u2514\u2500\u2500 test.jsonl     # Test examples\n</code></pre> <p>Each JSONL file contains one JSON object per line: <pre><code>{\"text\": \"Your training text here\"}\n{\"text\": \"Another training example\"}\n</code></pre></p>"},{"location":"guides/model-management/#vllm-commands","title":"vLLM Commands","text":"<p>vLLM provides high-performance inference for production environments:</p> <pre><code># Serve vLLM models\nsuper model vllm serve llama-2-7b --host 0.0.0.0 --port 8000\n\n# Generate text with vLLM\nsuper model vllm generate llama-2-7b \"Explain quantum computing\" --max-tokens 200\n\n# Benchmark vLLM models\nsuper model vllm benchmark llama-2-7b --num-requests 100 --request-rate 10\n\n# Quantize vLLM models\nsuper model vllm quantize llama-2-7b --quantization awq --bits 4\n</code></pre>"},{"location":"guides/model-management/#vllm-setup-requirements","title":"vLLM Setup Requirements","text":"<p>System Requirements: - Linux environment (Ubuntu 20.04+ recommended) - NVIDIA GPU with CUDA support - CUDA 11.8+ and cuDNN 8.9+ - Python 3.8+</p> <p>Dependency Structure: - vLLM is an optional dependency - not included in base SuperOptiX installation - Users can install vLLM separately or via SuperOptiX extras - SuperOptiX provides helpful error messages if vLLM is not available</p> <p>Installation Options:</p> <p>Option 1: Install vLLM separately (Recommended) <pre><code># Install vLLM directly\npip install vllm\n\n# Verify installation\npython -c \"import vllm; print('vLLM installed')\"\n\n# Check GPU support\npython -c \"from vllm import LLM; print('GPU support available')\"\n</code></pre></p> <p>Option 2: Install with SuperOptiX vLLM dependency <pre><code># Install SuperOptiX with vLLM support\npip install superoptix[vllm]\n\n# Verify installation\npython -c \"import vllm; print('vLLM installed via SuperOptiX')\"\n</code></pre></p> <p>Note:  - vLLM is an optional dependency that users need to install separately or via the <code>[vllm]</code> extra - SuperOptiX will provide helpful error messages if vLLM is not available when trying to use vLLM commands - vLLM can run on CPU for testing but requires NVIDIA GPU for optimal performance</p>"},{"location":"guides/model-management/#vllm-serving","title":"vLLM Serving","text":"<p>Serve models for high-performance inference:</p> <pre><code># Basic serving\nsuper model vllm serve llama-2-7b --host localhost --port 8000\n\n# Multi-GPU serving\nsuper model vllm serve llama-2-7b \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --tensor-parallel-size 2 \\\n  --gpu-memory-utilization 0.9\n\n# Quantized model serving\nsuper model vllm serve llama-2-7b \\\n  --quantization awq \\\n  --trust-remote-code \\\n  --max-model-len 4096\n</code></pre> <p>Serving Parameters: - <code>--host</code>: Server host (default: localhost) - <code>--port</code>: Server port (default: 8000) - <code>--tensor-parallel-size</code>: Number of GPUs for tensor parallelism - <code>--gpu-memory-utilization</code>: GPU memory utilization (0.0-1.0) - <code>--max-model-len</code>: Maximum model length - <code>--quantization</code>: Quantization method (awq, gptq, squeezellm) - <code>--trust-remote-code</code>: Trust remote code execution - <code>--download-dir</code>: Directory to download models</p>"},{"location":"guides/model-management/#vllm-generation","title":"vLLM Generation","text":"<p>Generate text with high-performance inference:</p> <pre><code># Basic generation\nsuper model vllm generate llama-2-7b \"Write a story about\" --max-tokens 200\n\n# Streaming generation\nsuper model vllm generate llama-2-7b \"Explain AI:\" \\\n  --max-tokens 500 \\\n  --temperature 0.8 \\\n  --stream\n\n# Controlled generation\nsuper model vllm generate llama-2-7b \"Complete this:\" \\\n  --max-tokens 100 \\\n  --temperature 0.1 \\\n  --top-p 0.9 \\\n  --stop \"END,###\"\n</code></pre> <p>Generation Parameters: - <code>--max-tokens</code>: Maximum tokens to generate - <code>--temperature</code>: Sampling temperature (0.0-2.0) - <code>--top-p</code>: Top-p sampling parameter (0.0-1.0) - <code>--top-k</code>: Top-k sampling parameter - <code>--stop</code>: Stop sequences (comma-separated) - <code>--stream</code>: Enable streaming generation - <code>--tensor-parallel-size</code>: Number of GPUs - <code>--quantization</code>: Quantization method</p>"},{"location":"guides/model-management/#vllm-benchmarking","title":"vLLM Benchmarking","text":"<p>Benchmark models for performance analysis:</p> <pre><code># Basic benchmarking\nsuper model vllm benchmark llama-2-7b \\\n  --num-requests 100 \\\n  --request-rate 10\n\n# Performance benchmarking\nsuper model vllm benchmark llama-2-7b \\\n  --num-requests 1000 \\\n  --request-rate 50 \\\n  --prompt-length 512 \\\n  --max-tokens 128 \\\n  --tensor-parallel-size 2\n\n# Quantized model benchmarking\nsuper model vllm benchmark llama-2-7b \\\n  --quantization awq \\\n  --num-requests 500 \\\n  --request-rate 25\n</code></pre> <p>Benchmarking Parameters: - <code>--num-requests</code>: Number of requests to process - <code>--request-rate</code>: Requests per second - <code>--prompt-length</code>: Prompt length in tokens - <code>--max-tokens</code>: Maximum tokens to generate - <code>--tensor-parallel-size</code>: Number of GPUs - <code>--quantization</code>: Quantization method</p>"},{"location":"guides/model-management/#vllm-quantization","title":"vLLM Quantization","text":"<p>Quantize models for reduced memory usage:</p> <pre><code># AWQ quantization\nsuper model vllm quantize llama-2-7b \\\n  --quantization awq \\\n  --bits 4 \\\n  --group-size 128\n\n# GPTQ quantization\nsuper model vllm quantize llama-2-7b \\\n  --quantization gptq \\\n  --bits 4 \\\n  --group-size 128 \\\n  --trust-remote-code\n\n# SqueezeLLM quantization\nsuper model vllm quantize llama-2-7b \\\n  --quantization squeezellm \\\n  --bits 4 \\\n  --group-size 128\n</code></pre> <p>Quantization Parameters: - <code>--output-dir</code>: Output directory (default: quantized_model) - <code>--quantization</code>: Quantization method (awq, gptq, squeezellm) - <code>--bits</code>: Bits for quantization (2, 3, 4, 8) - <code>--group-size</code>: Group size for quantization - <code>--trust-remote-code</code>: Trust remote code execution</p>"},{"location":"guides/model-management/#vllm-troubleshooting","title":"vLLM Troubleshooting","text":"<p>Common Issues: - CUDA out of memory: Reduce <code>--gpu-memory-utilization</code> or use quantization - Model not found: Ensure model is available on HuggingFace Hub - Quantization errors: Check model compatibility with quantization method - Performance issues: Adjust <code>--tensor-parallel-size</code> and <code>--gpu-memory-utilization</code></p> <p>Performance Optimization: - Use appropriate quantization for memory constraints - Adjust tensor parallelism based on GPU count - Monitor GPU memory utilization - Use appropriate model lengths for your use case</p>"},{"location":"guides/model-management/#sglang-commands","title":"SGLang Commands","text":"<p>SGLang provides streaming and optimization for production environments:</p> <pre><code># Serve SGLang models\nsuper model sglang serve llama-2-7b --host 0.0.0.0 --port 8000\n\n# Generate text with SGLang\nsuper model sglang generate llama-2-7b \"Explain streaming generation\" --max-tokens 200\n\n# Optimize SGLang models\nsuper model sglang optimize llama-2-7b --optimization O2\n\n# Benchmark SGLang models\nsuper model sglang benchmark llama-2-7b --num-requests 100 --request-rate 10\n</code></pre>"},{"location":"guides/model-management/#sglang-setup-requirements","title":"SGLang Setup Requirements","text":"<p>System Requirements: - Linux environment (Ubuntu 20.04+ recommended) - NVIDIA GPU with CUDA support - CUDA 11.8+ and cuDNN 8.9+ - Python 3.8+</p> <p>Dependency Structure: - SGLang is an optional dependency - not included in base SuperOptiX installation - Users can install SGLang separately or via SuperOptiX extras - SuperOptiX provides helpful error messages if SGLang is not available</p> <p>Installation Options:</p> <p>Option 1: Install SGLang separately (Recommended) <pre><code># Install SGLang directly\npip install sglang\n\n# Verify installation\npython -c \"import sglang; print('SGLang installed')\"\n\n# Check GPU support\npython -c \"from sglang import SGLang; print('GPU support available')\"\n</code></pre></p> <p>Option 2: Install with SuperOptiX SGLang dependency <pre><code># Install SuperOptiX with SGLang support\npip install superoptix[sglang]\n\n# Verify installation\npython -c \"import sglang; print('SGLang installed via SuperOptiX')\"\n</code></pre></p> <p>Note:  - SGLang is an optional dependency that users need to install separately or via the <code>[sglang]</code> extra - SuperOptiX will provide helpful error messages if SGLang is not available when trying to use SGLang commands - SGLang can run on CPU for testing but requires NVIDIA GPU for optimal performance</p>"},{"location":"guides/model-management/#sglang-serving","title":"SGLang Serving","text":"<p>Serve models for streaming and optimization:</p> <pre><code># Basic serving\nsuper model sglang serve llama-2-7b --host localhost --port 8000\n\n# High-performance serving\nsuper model sglang serve llama-2-7b \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --max-batch-size 64 \\\n  --max-num-batched-tokens 8192 \\\n  --max-num-seqs 512 \\\n  --gpu-memory-utilization 0.95\n\n# Streaming-optimized serving\nsuper model sglang serve llama-2-7b \\\n  --max-batch-size 32 \\\n  --max-num-batched-tokens 4096 \\\n  --trust-remote-code\n</code></pre> <p>Serving Parameters: - <code>--host</code>: Server host (default: localhost) - <code>--port</code>: Server port (default: 8000) - <code>--max-batch-size</code>: Maximum batch size (default: 32) - <code>--max-num-batched-tokens</code>: Maximum number of batched tokens (default: 4096) - <code>--max-num-seqs</code>: Maximum number of sequences (default: 256) - <code>--gpu-memory-utilization</code>: GPU memory utilization (0.0-1.0) - <code>--trust-remote-code</code>: Trust remote code execution</p>"},{"location":"guides/model-management/#sglang-generation","title":"SGLang Generation","text":"<p>Generate text with streaming and optimization:</p> <pre><code># Basic generation\nsuper model sglang generate llama-2-7b \"Write a story about\" --max-tokens 200\n\n# Streaming generation\nsuper model sglang generate llama-2-7b \"Explain AI:\" \\\n  --max-tokens 500 \\\n  --temperature 0.8 \\\n  --stream \\\n  --max-batch-size 16\n\n# Optimized generation\nsuper model sglang generate llama-2-7b \"Complete this:\" \\\n  --max-tokens 100 \\\n  --temperature 0.1 \\\n  --top-p 0.9 \\\n  --stop \"END,###\" \\\n  --max-batch-size 32\n</code></pre> <p>Generation Parameters: - <code>--max-tokens</code>: Maximum tokens to generate - <code>--temperature</code>: Sampling temperature (0.0-2.0) - <code>--top-p</code>: Top-p sampling parameter (0.0-1.0) - <code>--top-k</code>: Top-k sampling parameter - <code>--stop</code>: Stop sequences (comma-separated) - <code>--stream</code>: Enable streaming generation - <code>--max-batch-size</code>: Maximum batch size</p>"},{"location":"guides/model-management/#sglang-optimization","title":"SGLang Optimization","text":"<p>Optimize models for performance:</p> <pre><code># Performance optimization\nsuper model sglang optimize llama-2-7b \\\n  --optimization O2 \\\n  --max-batch-size 64 \\\n  --max-num-batched-tokens 8192 \\\n  --gpu-memory-utilization 0.95\n\n# Memory optimization\nsuper model sglang optimize llama-2-7b \\\n  --optimization O1 \\\n  --max-batch-size 32 \\\n  --max-num-batched-tokens 4096 \\\n  --gpu-memory-utilization 0.8\n</code></pre> <p>Optimization Parameters: - <code>--optimization</code>: Optimization level (O0, O1, O2, O3) - <code>--max-batch-size</code>: Maximum batch size - <code>--max-num-batched-tokens</code>: Maximum number of batched tokens - <code>--gpu-memory-utilization</code>: GPU memory utilization - <code>--trust-remote-code</code>: Trust remote code execution</p>"},{"location":"guides/model-management/#sglang-benchmarking","title":"SGLang Benchmarking","text":"<p>Benchmark models for performance analysis:</p> <pre><code># Basic benchmarking\nsuper model sglang benchmark llama-2-7b \\\n  --num-requests 100 \\\n  --request-rate 10\n\n# Performance benchmarking\nsuper model sglang benchmark llama-2-7b \\\n  --num-requests 1000 \\\n  --request-rate 50 \\\n  --prompt-length 512 \\\n  --max-tokens 128 \\\n  --max-batch-size 64\n\n# Streaming benchmarking\nsuper model sglang benchmark llama-2-7b \\\n  --num-requests 500 \\\n  --request-rate 25 \\\n  --prompt-length 256 \\\n  --max-tokens 64 \\\n  --max-batch-size 32\n</code></pre> <p>Benchmarking Parameters: - <code>--num-requests</code>: Number of requests to process - <code>--request-rate</code>: Requests per second - <code>--prompt-length</code>: Prompt length in tokens - <code>--max-tokens</code>: Maximum tokens to generate - <code>--max-batch-size</code>: Maximum batch size</p>"},{"location":"guides/model-management/#sglang-troubleshooting","title":"SGLang Troubleshooting","text":"<p>Common Issues: - CUDA out of memory: Reduce <code>--gpu-memory-utilization</code> or batch size - Model not found: Ensure model is available on HuggingFace Hub - Streaming errors: Check <code>--max-batch-size</code> and <code>--max-num-batched-tokens</code> - Performance issues: Adjust optimization level and batch parameters</p> <p>Performance Optimization: - Use appropriate optimization levels (O0-O3) for your use case - Adjust batch sizes based on GPU memory - Monitor GPU memory utilization - Use streaming for real-time applications</p>"},{"location":"guides/model-management/#server-management","title":"Server Management","text":""},{"location":"guides/model-management/#starting-local-servers","title":"Starting Local Servers","text":"<pre><code># Start MLX server\nsuper model server mlx mlx-community/phi-2 --port 8000\n\n# Start HuggingFace server\nsuper model server huggingface microsoft/phi-1_5 --port 8001\n</code></pre>"},{"location":"guides/model-management/#using-servers-in-playbooks","title":"Using Servers in Playbooks","text":"<pre><code>language_model:\n  provider: mlx\n  model: mlx-community/phi-2\n  api_base: http://localhost:8000\n</code></pre>"},{"location":"guides/model-management/#model-conversion-and-quantization","title":"Model Conversion and Quantization","text":"<p>Convert and quantize models for MLX backend:</p> <pre><code># Convert HuggingFace model to MLX format\nsuper model convert microsoft/phi-2 --quantize --bits 4\n\n# Quantize existing MLX model\nsuper model quantize my-model --bits 4 --output my-model-q4\n\n# Dequantize a quantized model\nsuper model quantize my-model-q4 --dequantize --output my-model-dequantized\n</code></pre> <p>Note: These commands are experimental and require MLX backend to be available.</p>"},{"location":"guides/model-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/model-management/#common-issues","title":"Common Issues","text":"<p>Model not found: <pre><code># Check available models\nsuper model list --backend &lt;backend&gt;\n\n# Install missing model\nsuper model install &lt;model_name&gt; --backend &lt;backend&gt;\n\n# Refresh cache\nsuper model refresh\n</code></pre></p> <p>Backend not working: <pre><code># Check backend status\nsuper model backends\n\n# Verify installation\nsuper model info &lt;model_name&gt;\n</code></pre></p> <p>LM Studio models: - LM Studio models must be installed via the desktop app - Use <code>super model server lmstudio &lt;model&gt;</code> to start server - Models cannot be removed via CLI (use desktop app)</p> <p>MLX Evaluation Issues: - Missing lm_eval: Install with <code>pip install lm_eval</code> - Model not found: Ensure model is properly installed with <code>super model list --backend mlx</code> - Memory issues: Use <code>--limit</code> to reduce evaluation examples - Slow evaluation: Use <code>--batch-size</code> to optimize performance</p> <p>MLX Fusion Issues: - Adapter not found: Check adapter path with <code>ls adapters/</code> - Model type unsupported: GGUF export supports llama, mixtral, mistral only - Upload failed: Ensure HuggingFace token is configured - Local model not found: Use full path to local model directory - 404 HuggingFace error: Ensure model exists on HuggingFace Hub or use local path</p>"},{"location":"guides/model-management/#auto-installation-workflow","title":"Auto-Installation Workflow","text":"<pre><code># Try to run model (auto-installs if needed)\nsuper model run &lt;model_name&gt; \"test\"\n\n# Install explicitly if needed\nsuper model install &lt;model_name&gt; --backend &lt;backend&gt;\n\n# Run again\nsuper model run &lt;model_name&gt; \"test\" --backend &lt;backend&gt;\n</code></pre>"},{"location":"guides/model-management/#best-practices","title":"Best Practices","text":""},{"location":"guides/model-management/#model-selection","title":"Model Selection","text":"<ul> <li>Small models: Good for testing and development</li> <li>Medium models: Balance of performance and resource usage</li> <li>Large models: Best performance, requires more resources</li> </ul>"},{"location":"guides/model-management/#resources","title":"Resources","text":"<ul> <li>SuperOptiX Documentation - Complete framework documentation</li> <li>DSPy Framework - Foundation framework</li> <li>GPT-OSS-120B Model - HuggingFace repository</li> <li>GPT-OSS-20B Model - HuggingFace repository</li> <li>Ollama Library - Ollama model library</li> </ul>"},{"location":"guides/model-management/#next-steps","title":"Next Steps","text":""},{"location":"guides/model-management/#backend-selection","title":"Backend Selection","text":"<ul> <li>Ollama: Easiest for local development</li> <li>MLX: Best for Apple Silicon performance</li> <li>HuggingFace: Widest model support</li> <li>LM Studio: Good for desktop workflows</li> </ul>"},{"location":"guides/model-management/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Use appropriate model sizes\nsuper model list --size small    # For testing\nsuper model list --size medium   # For development\nsuper model list --size large    # For production\n\n# Optimize generation parameters\nsuper model run model \"prompt\" --max-tokens 100 --temperature 0.7\n</code></pre>"},{"location":"guides/model-management/#examples","title":"Examples","text":""},{"location":"guides/model-management/#complete-workflow","title":"Complete Workflow","text":"<pre><code># Discover models\nsuper model discover\n\n# Install model\nsuper model install llama3.2:3b --backend ollama\n\n# Run model\nsuper model run llama3.2:3b \"Write a Python function to sort a list\"\n\n# Try different model\nsuper model run microsoft/phi-1_5 \"Write a JavaScript function to validate email\"\n\n# Interactive session\nsuper model run llama3.2:3b \"Write a blog post about AI trends\"\n\n# Try MLX model\nsuper model run mlx-community/phi-2 \"Write a short story about a robot\"\n\n# Advanced usage\nsuper model run microsoft/phi-1_5 \"Analyze this text: [your text here]\"\n\n# Educational\nsuper model run llama3.2:3b \"Explain machine learning in simple terms\"\n</code></pre>"},{"location":"guides/model-management/#testing-mlx-features","title":"Testing MLX Features","text":"<p>1. Verify Dependencies: <pre><code># Check MLX-LM installation\npython -c \"import mlx_lm; print('MLX-LM installed')\"\n\n# Check LM-Eval installation (for evaluation)\npython -c \"import lm_eval; print('LM-Eval installed')\"\n</code></pre></p> <p>2. Test Command Structure: <pre><code># Verify all MLX commands are available\nsuper model mlx --help\nsuper model mlx evaluate --help\nsuper model mlx fuse --help\nsuper model mlx finetune --help\nsuper model mlx convert --help\nsuper model mlx quantize --help\n</code></pre></p> <p>3. Test with Real Models: <pre><code># List available MLX models\nsuper model list --backend mlx\n\n# Test evaluation (requires lm_eval)\nsuper model mlx evaluate microsoft_phi-2 --tasks mmlu --limit 10\n\n# Test fusion (requires adapter files)\nsuper model mlx fuse microsoft_phi-2 --adapter-path ./adapters\n</code></pre></p> <p>4. Common Test Scenarios: - Evaluation: Test with small models and limited examples - Fusion: Test with existing adapter files from finetuning - Error Handling: Test with non-existent models/paths - Performance: Test with different batch sizes and limits </p>"},{"location":"guides/model-management/#dependency-overview","title":"Dependency Overview","text":"<p>SuperOptiX uses a modular dependency structure to keep the base installation lightweight:</p>"},{"location":"guides/model-management/#core-dependencies-always-included","title":"Core Dependencies (Always Included)","text":"<ul> <li>Basic CLI functionality</li> <li>Model management</li> <li>DSPy integration</li> <li>Core utilities</li> </ul>"},{"location":"guides/model-management/#optional-dependencies-install-as-needed","title":"Optional Dependencies (Install as needed)","text":"<p>Backend-Specific: <pre><code># MLX (Apple Silicon only)\npip install superoptix[mlx]\n\n# vLLM (Linux with NVIDIA GPU)\npip install superoptix[vllm]\n\n# SGLang (Linux with NVIDIA GPU)\npip install superoptix[sglang]\n\n# HuggingFace (Cross-platform)\npip install superoptix[huggingface]\n</code></pre></p> <p>Vector Databases: <pre><code># Individual databases\npip install superoptix[chromadb]\npip install superoptix[lancedb]\npip install superoptix[weaviate]\npip install superoptix[qdrant]\npip install superoptix[milvus]\n\n# All vector databases\npip install superoptix[vectordb]\n</code></pre></p> <p>Observability: <pre><code>pip install superoptix[observability]\n</code></pre></p> <p>UI Components: <pre><code>pip install superoptix[ui]\n</code></pre></p> <p>Complete Installation: <pre><code># All features (excluding MLX on non-Apple platforms)\npip install superoptix[all]\n\n# All features including MLX (use with caution on non-Apple platforms)\npip install superoptix[all-with-mlx]\n</code></pre></p>"},{"location":"guides/multi-framework/","title":"Multi-Framework Support Guide","text":"<p>SuperOptiX Universal Framework Workflow</p> <p>Build once from SuperSpec YAML, compile to your target framework, and run either: - minimal pipeline (default) - optimized pipeline (<code>--optimize</code>)</p> <p>RLM support is available as an experimental feature. Unified sandbox support is coming soon.</p>"},{"location":"guides/multi-framework/#feature-highlights","title":"Feature Highlights","text":"<ul> <li>\ud83e\uddea RLM (Experimental): Available now across active framework integrations where configured. Unified sandbox support is coming soon.</li> <li>\ud83d\uddc2\ufe0f Connector Integration (StackOne): Compile one connector-enabled SuperSpec into multiple frameworks.</li> <li>\ud83e\uddec GEPA Optimization Path: Keep minimal runtime pipelines by default; enable optimization lifecycle with <code>--optimize</code>.</li> <li>\ud83e\udde9 Framework-Native Templates: Generated pipelines stay close to each framework's native style.</li> <li>\ud83d\udcca Coverage View: See the Framework Feature Matrix for current capability status.</li> </ul>"},{"location":"guides/multi-framework/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"guides/multi-framework/#core-installation","title":"Core Installation","text":"<pre><code>pip install superoptix\n</code></pre> <p>Includes: Core framework with DSPy support</p>"},{"location":"guides/multi-framework/#install-specific-frameworks","title":"Install Specific Frameworks","text":"<pre><code># OpenAI Agents SDK\npip install superoptix[frameworks-openai]\n\n# Claude Agent SDK\npip install superoptix[frameworks-claude-sdk]\n\n# Google ADK\npip install superoptix[frameworks-google]\n\n# Microsoft Agent Framework (legacy support)\npip install superoptix[frameworks-microsoft]\n\n# DeepAgents\npip install superoptix[frameworks-deepagents]\n\n# Pydantic AI\npip install superoptix[frameworks-pydantic-ai]\n\n# CrewAI\npip install superoptix[frameworks-crewai]\n\n# All frameworks at once\npip install superoptix[frameworks]\n</code></pre>"},{"location":"guides/multi-framework/#overview","title":"Overview","text":"<p>SuperOptiX lets you:</p> <ul> <li>Build agents in major frameworks (DSPy, OpenAI SDK, Claude SDK, CrewAI, Google ADK, DeepAgents, Pydantic AI, Microsoft legacy)</li> <li>Optimize with one universal optimizer (GEPA)</li> <li>Use the same compile/run workflow across frameworks</li> <li>Switch frameworks without rewriting code</li> <li>Compare framework behavior side by side</li> </ul>"},{"location":"guides/multi-framework/#generated-artifacts-current-behavior","title":"Generated Artifacts (Current Behavior)","text":"<p>Each compile writes: - framework pipeline file, e.g. <code>my_agent_openai_pipeline.py</code> - sidecar compiled spec file, e.g. <code>my_agent_openai_pipeline_compiled_spec.json</code></p> <p>The pipeline reads the sidecar at runtime. If missing/corrupt, runtime now gives a clear \"recompile agent\" error.</p>"},{"location":"guides/multi-framework/#supported-frameworks","title":"Supported Frameworks","text":""},{"location":"guides/multi-framework/#framework-comparison","title":"Framework Comparison","text":"Framework Optimization Scope Local Models Best For DSPy Signatures, modules, prompts, and evaluation hooks Ollama Complex reasoning, research OpenAI SDK Agent instructions Ollama Simple and fast agents Claude SDK Agent system prompt Cloud only Anthropic-native agent workflows CrewAI Persona and task instructions Ollama Multi-agent teams Google ADK Agent instruction Cloud only Google ecosystem, Gemini Microsoft (Legacy) Agent instructions Ollama Existing Microsoft-framework projects DeepAgents System prompt Cloud only Complex planning, LangGraph Pydantic AI Instructions, output shaping, MCP/tool flow Ollama Type-safe outputs and tool use"},{"location":"guides/multi-framework/#universal-workflow","title":"Universal Workflow","text":"<p>The same workflow works for ALL frameworks:</p> <pre><code># 1) Compile minimal pipeline (default)\nsuper agent compile my_agent --framework &lt;framework&gt;\n\n# 2) Run minimal pipeline\nsuper agent run my_agent --framework &lt;framework&gt; --goal \"your goal\"\n\n# 3) Compile optimized pipeline (optional)\nsuper agent compile my_agent --framework &lt;framework&gt; --optimize\n\n# 4) Run optimization loop (GEPA-backed)\nsuper agent optimize my_agent --framework &lt;framework&gt; --auto light\n</code></pre> <p>Cloud/local routing (common pattern):</p> <pre><code># Cloud Google\nsuper agent compile my_agent --framework &lt;framework&gt; --cloud --provider google-genai --model gemini-2.5-flash\nsuper agent run my_agent --framework &lt;framework&gt; --cloud --provider google-genai --model gemini-2.5-flash --goal \"your goal\"\n\n# Local Ollama\nsuper agent compile my_agent --framework &lt;framework&gt; --local --provider ollama --model llama3.1:8b\nsuper agent run my_agent --framework &lt;framework&gt; --local --provider ollama --model llama3.1:8b --goal \"your goal\"\n</code></pre> <p>Notes: - <code>google-adk</code> and <code>deepagents</code> currently require cloud function-calling models. - <code>claude-sdk</code> requires Anthropic credentials and Claude models. - for non-DSPy frameworks, always pass <code>--framework &lt;framework&gt;</code> on <code>super agent optimize</code> to avoid defaulting to DSPy assets.</p>"},{"location":"guides/multi-framework/#framework-specific-guides","title":"Framework-Specific Guides","text":""},{"location":"guides/multi-framework/#dspy-stanford-research-framework","title":"DSPy (Stanford Research Framework)","text":"<p>Best for: Complex reasoning, research, maximum optimization flexibility</p>"},{"location":"guides/multi-framework/#quick-start","title":"Quick Start","text":"<pre><code># Pull demo agent\nsuper agent pull sentiment_analyzer\n\n# Compile (DSPy is default)\nsuper agent compile sentiment_analyzer\n\n# Evaluate\nsuper agent evaluate sentiment_analyzer\n\n# Optimize\nsuper agent optimize sentiment_analyzer --auto medium\n\n# Run\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"guides/multi-framework/#configuration","title":"Configuration","text":"<pre><code># playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: my_dspy_agent\nspec:\n  target_framework: dspy  # or omit for default\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n  persona:\n    role: Research Assistant\n    goal: Analyze complex topics\n</code></pre>"},{"location":"guides/multi-framework/#what-gepa-optimizes","title":"What GEPA Optimizes","text":"<ul> <li>Signatures (input/output specs)</li> <li>Instructions for each module</li> <li>Chain-of-thought prompts</li> <li>Few-shot examples</li> <li>Module connections</li> </ul> <p>Proven Results: 37.5% \u2192 80% improvement</p>"},{"location":"guides/multi-framework/#openai-agents-sdk-simple-fast","title":"OpenAI Agents SDK (Simple &amp; Fast)","text":"<p>Best for: Simple agents, fast prototyping, 100% local &amp; free with Ollama</p>"},{"location":"guides/multi-framework/#quick-start_1","title":"Quick Start","text":"<pre><code># Pull demo agent (already configured for Ollama!)\nsuper agent pull assistant_openai\n\n# Install Ollama (if not already installed)\nbrew install ollama\nollama pull llama3.1:8b\n\n# Compile &amp; Run (no API keys needed!)\nsuper agent compile assistant_openai --framework openai\nsuper agent run assistant_openai --goal \"Hello!\"\n\n# Evaluate\nsuper agent evaluate assistant_openai\n\n# Optimize\nsuper agent optimize assistant_openai --auto medium --framework openai --reflection-lm ollama:llama3.1:8b\n</code></pre>"},{"location":"guides/multi-framework/#configuration_1","title":"Configuration","text":"<p>Default (FREE Ollama - already configured!): <pre><code># playbook.yaml\nspec:\n  target_framework: openai\n  language_model:\n    location: local\n    provider: ollama\n    model: ollama:llama3.1:8b  # FREE, fast and efficient!\n    api_base: http://localhost:11434\n</code></pre></p> <p>Optional Cloud Models (requires API key): <pre><code># For OpenAI\nspec:\n  target_framework: openai\n  language_model:\n    location: cloud\n    provider: openai\n    model: openai:gpt-4o\n    # Set: export OPENAI_API_KEY=\"sk-...\"\n</code></pre></p>"},{"location":"guides/multi-framework/#what-gepa-optimizes_1","title":"What GEPA Optimizes","text":"<ul> <li>Agent instructions (the main system prompt)</li> </ul> <p>Proven Results: Excellent performance with Ollama (results vary by hardware/model)</p>"},{"location":"guides/multi-framework/#crewai-multi-agent-teams","title":"CrewAI (Multi-Agent Teams)","text":"<p>Best for: Multi-agent collaboration, role-based agents</p>"},{"location":"guides/multi-framework/#quick-start_2","title":"Quick Start","text":"<pre><code># Pull demo agents\nsuper agent pull researcher_crew\nsuper agent pull content_creator_crew\n\n# Compile\nsuper agent compile researcher_crew --framework crewai\n\n# Evaluate\nsuper agent evaluate researcher_crew\n\n# Optimize\nsuper agent optimize content_creator_crew --auto medium --framework crewai --reflection-lm ollama:llama3.1:8b\n\n# Run\nsuper agent run researcher_crew\n</code></pre>"},{"location":"guides/multi-framework/#configuration_2","title":"Configuration","text":"<p>Basic Agent Configuration <pre><code>spec:\n  target_framework: crewai\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n  persona:\n    role: Research Analyst\n    goal: Conduct thorough research on topics\n    backstory: |\n      Experienced researcher with attention to detail.\n  tasks:\n    - name: research\n      description: Research the given topic\n      expected_output: Comprehensive research report\n</code></pre></p> <p>Advanced: Combined Agent + Task Optimization <pre><code>spec:\n  target_framework: crewai\n  persona:\n    role: Content Creator\n    goal: Create engaging content\n    backstory: |\n      Creative writer with audience engagement expertise.\n  tasks:\n    - name: write\n      description: Write compelling content about the topic\n      expected_output: Polished article ready for publication\n</code></pre></p>"},{"location":"guides/multi-framework/#what-gepa-optimizes_2","title":"What GEPA Optimizes","text":"<p>GEPA can optimize: - Agent profile: role, goal, backstory - Task configuration: description, expected_output - Combined optimization: agent profile + task configuration for better results</p> <p>Proven Results: Excellent performance with Ollama (results vary by hardware/model)</p>"},{"location":"guides/multi-framework/#google-adk-gemini-native","title":"Google ADK (Gemini Native)","text":"<p>Best for: Google ecosystem, Gemini integration, free access</p>"},{"location":"guides/multi-framework/#quick-start_3","title":"Quick Start","text":"<pre><code># Set API key\nexport GOOGLE_API_KEY=\"your-key-here\"\n\n# Pull demo agent\nsuper agent pull assistant_adk\n\n# Compile\nsuper agent compile assistant_adk --framework google-adk\n\n# Evaluate\nsuper agent evaluate assistant_adk\n\n# Optimize\nsuper agent optimize assistant_adk --auto medium --framework google-adk --reflection-lm ollama:llama3.1:8b\n\n# Run\nsuper agent run assistant_adk\n</code></pre>"},{"location":"guides/multi-framework/#configuration_3","title":"Configuration","text":"<pre><code>spec:\n  target_framework: google-adk\n  language_model:\n    provider: google\n    model: gemini-2.0-flash  # Free access!\n  persona:\n    instructions: |\n      You are a helpful AI assistant powered by Google's Gemini.\n</code></pre>"},{"location":"guides/multi-framework/#what-gepa-optimizes_3","title":"What GEPA Optimizes","text":"<ul> <li>Agent instruction (system prompt)</li> </ul>"},{"location":"guides/multi-framework/#microsoft-agent-framework-enterprise","title":"Microsoft Agent Framework (Enterprise)","text":"<p>Best for: Azure integration, .NET support, enterprise workflows</p>"},{"location":"guides/multi-framework/#quick-start_4","title":"Quick Start","text":"<pre><code># Pull demo agent\nsuper agent pull assistant_microsoft\n\n# Compile\nsuper agent compile assistant_microsoft --framework microsoft\n\n# Evaluate\nsuper agent evaluate assistant_microsoft\n\n# Optimize\nsuper agent optimize assistant_microsoft --auto medium --framework microsoft --reflection-lm ollama:llama3.1:8b\n\n# Run\nsuper agent run assistant_microsoft\n</code></pre>"},{"location":"guides/multi-framework/#configuration_4","title":"Configuration","text":"<p>With Ollama: <pre><code>spec:\n  target_framework: microsoft\n  language_model:\n    provider: ollama\n    model: gpt-oss:20b\n    api_base: http://localhost:11434\n</code></pre></p> <p>With Azure OpenAI: <pre><code>spec:\n  target_framework: microsoft\n  language_model:\n    provider: azure\n    azure_endpoint: https://your-resource.openai.azure.com\n    azure_deployment_name: gpt-4\n    azure_api_version: 2024-02-15-preview\n</code></pre></p>"},{"location":"guides/multi-framework/#what-gepa-optimizes_4","title":"What GEPA Optimizes","text":"<ul> <li>Agent instructions (system prompt)</li> </ul>"},{"location":"guides/multi-framework/#deepagents-langgraph-planning","title":"DeepAgents (LangGraph Planning)","text":"<p>Best for: Complex planning, multi-step reasoning, advanced workflows</p>"},{"location":"guides/multi-framework/#quick-start_5","title":"Quick Start","text":"<pre><code># Pull demo agent\nsuper agent pull research_agent_deepagents\n\n# Compile\nsuper agent compile research_agent_deepagents --framework deepagents\n\n# Evaluate\nsuper agent evaluate research_agent_deepagents\n\n# Optimize\nsuper agent optimize research_agent_deepagents --auto medium --framework deepagents --reflection-lm ollama:llama3.1:8b\n\n# Run\nsuper agent run research_agent_deepagents\n</code></pre>"},{"location":"guides/multi-framework/#configuration_5","title":"Configuration","text":"<pre><code>spec:\n  target_framework: deepagents\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n  persona:\n    system_prompt: |\n      You are a research agent that plans and executes complex research tasks.\n</code></pre>"},{"location":"guides/multi-framework/#what-gepa-optimizes_5","title":"What GEPA Optimizes","text":"<ul> <li>System prompt (planning instructions)</li> </ul>"},{"location":"guides/multi-framework/#choosing-the-right-framework","title":"Choosing the Right Framework","text":""},{"location":"guides/multi-framework/#decision-matrix","title":"Decision Matrix","text":"<p>Choose DSPy if: - You need maximum optimization flexibility - You want to optimize multiple variables - You're doing research or complex reasoning - You want proven 37.5% \u2192 80% improvements</p> <p>Choose OpenAI SDK if: - You want the simplest API - You need fast prototyping - You're building simple assistants - You want Ollama compatibility</p> <p>Choose CrewAI if: - You need multiple agents working together - You want role-based collaboration - You need task delegation - You want agent + task combined optimization</p> <p>Choose Google ADK if: - You're in the Google ecosystem - You want Gemini 2.0 Flash (free access!) - You need session management - You want Google-native features</p> <p>Choose Microsoft if: - You're using Azure OpenAI - You need .NET integration - You're in enterprise environment - You want built-in observability</p> <p>Choose DeepAgents if: - You need complex planning graphs - You want LangGraph integration - You need multi-step reasoning - You want advanced agentic workflows</p>"},{"location":"guides/multi-framework/#framework-switching","title":"Framework Switching","text":"<p>Switch frameworks without rewriting code!</p> <pre><code># Start with DSPy\nsuper agent compile my_agent --framework dspy\nsuper agent evaluate my_agent\n\n# Try OpenAI SDK\nsuper agent compile my_agent --framework openai\nsuper agent evaluate my_agent\n\n# Compare results!\n</code></pre>"},{"location":"guides/multi-framework/#optimization-comparison","title":"Optimization Comparison","text":""},{"location":"guides/multi-framework/#gepa-results-across-frameworks","title":"GEPA Results Across Frameworks","text":"Framework Demo Agent Baseline After GEPA Improvement DSPy sentiment_analyzer Good Improved Significant improvement (results vary) OpenAI SDK assistant_openai Excellent Excellent Maintained performance (results vary) CrewAI content_creator_crew Good Improved Significant improvement (results vary) Google ADK assistant_adk TBD TBD Ready Microsoft assistant_microsoft TBD TBD Ready DeepAgents research_agent TBD TBD Ready"},{"location":"guides/multi-framework/#advanced-multi-framework-projects","title":"Advanced: Multi-Framework Projects","text":""},{"location":"guides/multi-framework/#example-compare-all-frameworks","title":"Example: Compare All Frameworks","text":"<pre><code># Create project\nsuper init comparison_project\ncd comparison_project\n\n# Pull same agent for all frameworks\nfor fw in dspy openai crewai google-adk microsoft deepagents; do\n  super agent pull assistant_${fw}\n  super agent compile assistant_${fw} --framework ${fw}\n  super agent evaluate assistant_${fw}\n  super agent optimize assistant_${fw} --auto medium\n  super agent evaluate assistant_${fw}  # automatically loads optimized weights\ndone\n\n# Compare results!\n</code></pre>"},{"location":"guides/multi-framework/#cli-quick-reference","title":"CLI Quick Reference","text":"<pre><code># List all demo agents\nsuper agent list --pre-built\n\n# Pull specific framework demo\nsuper agent pull sentiment_analyzer      # DSPy\nsuper agent pull assistant_openai        # OpenAI SDK\nsuper agent pull researcher_crew         # CrewAI\nsuper agent pull assistant_adk           # Google ADK\nsuper agent pull assistant_microsoft     # Microsoft\nsuper agent pull research_agent_deepagents  # DeepAgents\n\n# Compile with framework\nsuper agent compile &lt;agent&gt; --framework &lt;framework&gt;\n\n# Evaluate\nsuper agent evaluate &lt;agent&gt;\n\n# Optimize (same command for all!)\nsuper agent optimize &lt;agent&gt; --auto medium\n\n# Run\nsuper agent run &lt;agent&gt;\n</code></pre>"},{"location":"guides/multi-framework/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/multi-framework/#issue-framework-not-found","title":"Issue: \"Framework not found\"","text":"<p>Solution: Install framework-specific dependencies</p> <pre><code># OpenAI SDK\npip install openai-agents-sdk\n\n# CrewAI\npip install crewai\n\n# Google ADK\npip install google-adk\n\n# Microsoft\npip install agent-framework\n\n# DeepAgents (LangGraph)\npip install langgraph langchain-anthropic\n\n# Pydantic AI\npip install pydantic-ai==1.31.0\n</code></pre>"},{"location":"guides/multi-framework/#issue-ollama-not-supported","title":"Issue: \"Ollama not supported\"","text":"<p>Solution: Some frameworks have limitations</p> <ul> <li>Google ADK: Requires Gemini API key (cloud only)</li> <li>DeepAgents: Check LangChain compatibility</li> </ul>"},{"location":"guides/multi-framework/#issue-optimization-not-working","title":"Issue: \"Optimization not working\"","text":"<p>Solution: Ensure GEPA is configured</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/multi-framework/#next-steps","title":"Next Steps","text":"<ol> <li>Try all frameworks: Pull demo agents and compare</li> <li>Read framework guides: Check individual integration docs</li> <li>Build custom agents: Create your own with SuperSpec</li> <li>Optimize everything: GEPA works on all frameworks!</li> </ol>"},{"location":"guides/multi-framework/#framework-specific-docs","title":"Framework-Specific Docs","text":"<ul> <li>DSPy Integration</li> <li>OpenAI SDK Integration</li> <li>CrewAI Integration &amp; Advanced</li> <li>Google ADK Integration</li> <li>Microsoft Integration</li> <li>DeepAgents Integration</li> <li>Pydantic AI Integration &amp; MCP Demo</li> </ul>"},{"location":"guides/multi-framework/#related-docs","title":"Related Docs","text":"<ul> <li>GEPA Optimization</li> <li>Evaluation &amp; Testing</li> <li>SuperSpec DSL</li> </ul>"},{"location":"guides/multi-framework/#tutorials","title":"Tutorials","text":"<ul> <li>OpenAI SDK + GEPA Optimization Tutorial - Complete step-by-step guide to building custom agents with native OpenAI SDK patterns and optimizing them with GEPA</li> </ul> <p>Ready to build your own optimized agent? Start with the OpenAI SDK + GEPA Tutorial!</p>"},{"location":"guides/observability-comparison/","title":"\ud83d\udd04 Observability Comparison: MLFlow vs LangFuse","text":"<p>This guide helps you choose between MLFlow and LangFuse for SuperOptiX observability based on your specific use case and requirements.</p>"},{"location":"guides/observability-comparison/#overview","title":"\ud83c\udfaf Overview","text":"<p>Both MLFlow and LangFuse provide excellent observability capabilities for SuperOptiX agents, but they are designed for different types of workflows and use cases. This guide will help you make an informed decision.</p>"},{"location":"guides/observability-comparison/#feature-comparison","title":"\ud83d\udcca Feature Comparison","text":"Feature MLFlow LangFuse Primary Focus ML Experiments LLM Observability Token Tracking Manual Automatic Cost Tracking Manual Built-in User Feedback Manual Native A/B Testing Manual Built-in Real-time UI Limited Excellent Artifact Storage Excellent Good Experiment Tracking Excellent Good Model Registry Native Limited Deployment Tracking Excellent Limited Team Collaboration Excellent Good API Integration Python SDK Python SDK Cloud Hosting MLFlow Cloud LangFuse Cloud Self-hosting Yes Yes"},{"location":"guides/observability-comparison/#mlflow-best-for-ml-workflows","title":"\ud83e\uddea MLFlow - Best for ML Workflows","text":""},{"location":"guides/observability-comparison/#strengths","title":"Strengths","text":"<ul> <li>ML Experiment Tracking: Designed specifically for machine learning experiments</li> <li>Artifact Management: Excellent versioning of code, models, and data</li> <li>Reproducibility: Detailed experiment tracking and comparison</li> <li>Model Registry: Built-in model versioning and deployment tracking</li> <li>Team Collaboration: Experiment sharing and model registry</li> <li>Production ML: Model deployment and lifecycle management</li> <li>Traditional ML: Perfect for scikit-learn, TensorFlow, PyTorch workflows</li> </ul>"},{"location":"guides/observability-comparison/#limitations","title":"Limitations","text":"<ul> <li>LLM-specific features: Limited built-in support for LLM observability</li> <li>Token tracking: Requires manual implementation</li> <li>Cost tracking: No automatic cost monitoring</li> <li>Real-time UI: Limited real-time capabilities</li> <li>User feedback: Manual implementation required</li> </ul>"},{"location":"guides/observability-comparison/#ideal-use-cases","title":"\ud83c\udfaf Ideal Use Cases","text":"<ul> <li>Traditional ML pipelines with scikit-learn, TensorFlow, PyTorch</li> <li>Model experimentation and hyperparameter tuning</li> <li>Production model deployment and lifecycle management</li> <li>Team collaboration on ML experiments</li> <li>Artifact versioning and reproducibility</li> <li>ML model registry and deployment tracking</li> </ul>"},{"location":"guides/observability-comparison/#langfuse-best-for-llm-applications","title":"\ud83d\udd0d LangFuse - Best for LLM Applications","text":""},{"location":"guides/observability-comparison/#strengths_1","title":"Strengths","text":"<ul> <li>LLM Observability: Specialized for language model applications</li> <li>Real-time Tracing: Detailed token usage and cost tracking</li> <li>User Feedback: Built-in feedback collection and scoring</li> <li>A/B Testing: Native LLM prompt and model comparison</li> <li>Production LLM: Live monitoring and debugging</li> <li>Cost Optimization: Automatic cost tracking and alerts</li> <li>Prompt Engineering: Specialized tools for prompt optimization</li> </ul>"},{"location":"guides/observability-comparison/#limitations_1","title":"Limitations","text":"<ul> <li>Traditional ML: Limited support for non-LLM workflows</li> <li>Artifact Storage: Basic artifact management</li> <li>Model Registry: Limited model versioning capabilities</li> <li>Team Features: Basic collaboration features</li> <li>Deployment Tracking: Limited deployment monitoring</li> </ul>"},{"location":"guides/observability-comparison/#ideal-use-cases_1","title":"\ud83c\udfaf Ideal Use Cases","text":"<ul> <li>LLM applications and language model workflows</li> <li>Prompt engineering and optimization</li> <li>Real-time cost tracking and optimization</li> <li>User feedback collection for LLM responses</li> <li>A/B testing of different prompts and models</li> <li>Production LLM monitoring and debugging</li> <li>Token usage optimization and cost management</li> </ul>"},{"location":"guides/observability-comparison/#quick-setup-comparison","title":"\ud83d\ude80 Quick Setup Comparison","text":""},{"location":"guides/observability-comparison/#mlflow-setup","title":"MLFlow Setup","text":"<pre><code># Install MLFlow\npip install mlflow\n\n# Start MLFlow server\nmlflow server --host 0.0.0.0 --port 5001 --backend-store-uri sqlite:///mlflow.db\n\n# Configure SuperOptiX agent\nobservability:\n  enabled: true\n  backends:\n    - mlflow\n  mlflow:\n    experiment_name: \"developer_agent\"\n    tracking_uri: \"http://localhost:5001\"\n    log_artifacts: true\n    log_metrics: true\n    log_params: true\n</code></pre>"},{"location":"guides/observability-comparison/#langfuse-setup","title":"LangFuse Setup","text":"<pre><code># Install LangFuse\npip install langfuse\n\n# Start LangFuse with Docker\ndocker compose up -d\n\n# Configure SuperOptiX agent\nobservability:\n  enabled: true\n  backends:\n    - langfuse\n  langfuse:\n    public_key: \"your-public-key\"\n    secret_key: \"your-secret-key\"\n    host: \"http://localhost:3000\"\n    project: \"superoptix-agents\"\n</code></pre>"},{"location":"guides/observability-comparison/#performance-metrics-comparison","title":"\ud83d\udcc8 Performance Metrics Comparison","text":""},{"location":"guides/observability-comparison/#mlflow-metrics","title":"MLFlow Metrics","text":"<ul> <li>Experiment runs: Track experiment iterations</li> <li>Model performance: Accuracy, loss, custom metrics</li> <li>Artifact storage: Code, models, data versioning</li> <li>Reproducibility: Environment, dependencies, parameters</li> <li>Deployment tracking: Model versions in production</li> </ul>"},{"location":"guides/observability-comparison/#langfuse-metrics","title":"LangFuse Metrics","text":"<ul> <li>Token usage: Input/output tokens per request</li> <li>Cost tracking: Real-time cost per request</li> <li>Response time: Latency and throughput</li> <li>Quality scores: User feedback and automated scoring</li> <li>A/B test results: Prompt and model comparison</li> <li>Error rates: Failed requests and debugging</li> </ul>"},{"location":"guides/observability-comparison/#integration-examples","title":"\ud83d\udd27 Integration Examples","text":""},{"location":"guides/observability-comparison/#mlflow-integration","title":"MLFlow Integration","text":"<pre><code>import mlflow\nimport mlflow.sklearn\n\n# Track ML experiment\nwith mlflow.start_run():\n    mlflow.log_param(\"model_type\", \"random_forest\")\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre>"},{"location":"guides/observability-comparison/#langfuse-integration","title":"LangFuse Integration","text":"<pre><code>from langfuse import Langfuse\n\n# Track LLM interaction\nwith langfuse.start_as_current_span(name=\"llm_call\") as span:\n    with langfuse.start_as_current_generation(\n        name=\"response_generation\",\n        model=\"gpt-4\",\n        input={\"prompt\": \"Hello world\"}\n    ) as generation:\n        response = llm.generate(\"Hello world\")\n        generation.update(output=response)\n        langfuse.score_current_span(name=\"quality\", value=0.9)\n</code></pre>"},{"location":"guides/observability-comparison/#decision-framework","title":"\ud83c\udfaf Decision Framework","text":""},{"location":"guides/observability-comparison/#choose-mlflow-if","title":"Choose MLFlow if:","text":"<p>You're doing traditional ML (scikit-learn, TensorFlow, PyTorch) You need model versioning and deployment tracking You want experiment reproducibility and artifact management You're building ML pipelines with multiple steps You need team collaboration on experiments You're deploying models to production</p>"},{"location":"guides/observability-comparison/#choose-langfuse-if","title":"Choose LangFuse if:","text":"<p>You're building LLM applications (GPT, Claude, etc.) You need real-time cost tracking and optimization You want user feedback collection and scoring You're doing prompt engineering and A/B testing You need token usage monitoring and optimization You're debugging LLM responses in production</p>"},{"location":"guides/observability-comparison/#migration-guide","title":"\ud83d\udd04 Migration Guide","text":""},{"location":"guides/observability-comparison/#from-mlflow-to-langfuse","title":"From MLFlow to LangFuse","text":"<p>If you're currently using MLFlow for LLM observability:</p> <ol> <li>Install LangFuse: <code>pip install langfuse</code></li> <li>Update agent configuration: Replace MLFlow config with LangFuse</li> <li>Migrate metrics: Convert MLFlow metrics to LangFuse spans</li> <li>Set up cost tracking: Enable automatic token and cost tracking</li> <li>Configure feedback: Set up user feedback collection</li> </ol>"},{"location":"guides/observability-comparison/#from-langfuse-to-mlflow","title":"From LangFuse to MLFlow","text":"<p>If you need traditional ML capabilities:</p> <ol> <li>Install MLFlow: <code>pip install mlflow</code></li> <li>Update agent configuration: Replace LangFuse config with MLFlow</li> <li>Set up experiment tracking: Configure MLFlow experiments</li> <li>Migrate artifacts: Move artifacts to MLFlow storage</li> <li>Configure model registry: Set up model versioning</li> </ol>"},{"location":"guides/observability-comparison/#related-resources","title":"\ud83d\udcda Related Resources","text":"<ul> <li>MLFlow Integration Guide - Complete MLFlow setup and usage</li> <li>LangFuse Integration Guide - Complete LangFuse setup and usage</li> <li>Observability Guide - General observability overview</li> <li>Agent Development - Build custom agents</li> <li>SuperOptiX CLI Reference - CLI commands reference</li> </ul>"},{"location":"guides/observability-comparison/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Evaluate your use case using the decision framework above</li> <li>Choose the right tool based on your requirements</li> <li>Follow the integration guide for your chosen tool</li> <li>Set up monitoring and start tracking your agents</li> <li>Optimize performance based on the collected metrics</li> </ol> <p>\ud83c\udf89 Both MLFlow and LangFuse provide excellent observability for SuperOptiX agents. Choose the one that best fits your specific use case and requirements! </p>"},{"location":"guides/observability/","title":"\ud83d\udcca SuperOptiX Observability &amp; Tracing Guide","text":"<p>Welcome to the comprehensive guide for SuperOptiX's observability and monitoring system! This guide combines the complete overview, real-world experiment, and quick reference for monitoring, debugging, and analyzing your AI agents using the <code>super observe</code> command.</p>"},{"location":"guides/observability/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX provides a powerful observability system that automatically traces agent execution, tool usage, LLM calls, and performance metrics. The observability system helps you:</p> <ul> <li>Monitor agent performance in real-time</li> <li>Debug issues with detailed trace analysis</li> <li>Analyze patterns and optimize agent behavior</li> <li>Track tool usage and LLM interactions</li> <li>Export trace data for external analysis</li> </ul>"},{"location":"guides/observability/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Let's walk through a complete example of setting up observability for an agent:</p>"},{"location":"guides/observability/#step-1-initialize-a-project-and-create-an-agent","title":"Step 1: Initialize a Project and Create an Agent","text":"<pre><code># Initialize a new SuperOptiX project\nsuper init my_project\ncd my_project\n\n# Pull a pre-built agent (Genies tier for full observability features)\nsuper agent pull developer\n\n# Compile the agent to generate the pipeline\nsuper agent compile developer\n</code></pre>"},{"location":"guides/observability/#step-2-run-the-agent-to-generate-traces","title":"Step 2: Run the Agent to Generate Traces","text":"<pre><code># Run the agent - this automatically enables tracing\nsuper agent run developer --goal \"Write a simple Python function to calculate the factorial of a number\"\n</code></pre> <p>The agent execution automatically generates trace files in <code>.superoptix/traces/</code> with detailed information about: - Model initialization - Tool setup and execution - Pipeline execution flow - Performance metrics - Error handling</p>"},{"location":"guides/observability/#step-3-explore-observability-features","title":"Step 3: Explore Observability Features","text":"<pre><code># List all agents with available traces\nsuper observe list\n\n# View traces for a specific agent\nsuper observe traces developer_20250714_200501\n\n# Launch the interactive dashboard\nsuper observe dashboard --auto-open\n\n# Analyze performance metrics\nsuper observe analyze developer_20250714_200501 --days 1\n</code></pre>"},{"location":"guides/observability/#available-commands","title":"\ud83d\udccb Available Commands","text":""},{"location":"guides/observability/#super-observe-list-list-agents-with-traces","title":"<code>super observe list</code> - List Agents with Traces","text":"<p>Lists all agents that have generated trace files in your project.</p> <pre><code>super observe list\n</code></pre> <p>Output Example: <pre><code>\ud83d\udccb Available Agents with Traces\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Agent ID                  \u2503 Trace Count \u2503 Last Activity       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer                 \u2502 2           \u2502 2025-07-14 20:05:11 \u2502\n\u2502 developer_20250714_200501 \u2502 11          \u2502 2025-07-14 20:05:11 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"guides/observability/#super-observe-traces-view-agent-traces","title":"<code>super observe traces</code> - View Agent Traces","text":"<p>View detailed execution traces for a specific agent.</p> <pre><code># Basic trace view\nsuper observe traces &lt;agent_id&gt;\n\n# Detailed analysis with tool and LLM information\nsuper observe traces &lt;agent_id&gt; --detailed --show-tools --show-llm\n\n# Filter by component or status\nsuper observe traces &lt;agent_id&gt; --component pipeline --status success\n\n# Export traces to JSON or CSV\nsuper observe traces &lt;agent_id&gt; --export json --output traces.json\n</code></pre> <p>Options: - <code>--component</code>: Filter traces by component (pipeline, tool, execution, etc.) - <code>--status</code>: Filter by status (success, error, warning, info) - <code>--limit</code>: Limit number of traces shown (default: 100) - <code>--detailed</code>: Show detailed trace analysis - <code>--show-tools</code>: Show tool execution details - <code>--show-llm</code>: Show LLM call details - <code>--export</code>: Export format (json, csv) - <code>--output</code>: Output file path</p> <p>Output Example: <pre><code>\ud83d\udd0d Loading traces for agent: developer_20250714_200501\nLoaded 11 trace events\n              Traces for Agent: developer_20250714_200501               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Time     \u2503 Component \u2503 Event                   \u2503 Status  \u2503 Duration  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 20:05:01 \u2502 pipeline  \u2502 model_initialized       \u2502 success \u2502 -         \u2502\n\u2502 20:05:01 \u2502 pipeline  \u2502 tools_setup_start       \u2502 success \u2502 -         \u2502\n\u2502 20:05:01 \u2502 pipeline  \u2502 tools_initialized       \u2502 success \u2502 -         \u2502\n\u2502 20:05:01 \u2502 pipeline  \u2502 tools_setup_end         \u2502 success \u2502 0.5ms     \u2502\n\u2502 20:05:01 \u2502 pipeline  \u2502 react_agent_initialized \u2502 success \u2502 -         \u2502\n\u2502 20:05:01 \u2502 execution \u2502 pipeline_forward_start  \u2502 success \u2502 -         \u2502\n\u2502 20:05:05 \u2502 tool      \u2502 calculate_start         \u2502 success \u2502 -         \u2502\n\u2502 20:05:05 \u2502 calculate \u2502 tool_execution_success  \u2502 success \u2502 -         \u2502\n\u2502 20:05:05 \u2502 tool      \u2502 calculate_end           \u2502 success \u2502 0.1ms     \u2502\n\u2502 20:05:11 \u2502 pipeline  \u2502 execution_completed     \u2502 success \u2502 -         \u2502\n\u2502 20:05:11 \u2502 execution \u2502 pipeline_forward_end    \u2502 success \u2502 10270.0ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcca Total: 11 | Errors: 0 | Avg: 3423.5ms\n</code></pre></p>"},{"location":"guides/observability/#super-observe-dashboard-launch-interactive-dashboard","title":"<code>super observe dashboard</code> - Launch Interactive Dashboard","text":"<p>Launch a web-based dashboard for real-time monitoring and analysis.</p> <pre><code># Launch dashboard with default settings\nsuper observe dashboard\n\n# Launch with custom port and auto-open browser\nsuper observe dashboard --port 8502 --host localhost --auto-open\n\n# Monitor a specific agent\nsuper observe dashboard --agent-id developer_20250714_200501\n</code></pre> <p>Options: - <code>--port</code>: Dashboard port (default: 8501) - <code>--host</code>: Dashboard host (default: localhost) - <code>--auto-open</code>: Automatically open browser - <code>--agent-id</code>: Monitor specific agent</p> <p>The dashboard provides: - Real-time trace visualization - Performance metrics and charts - Tool usage analytics - Error tracking and debugging - Memory and context analysis</p>"},{"location":"guides/observability/#super-observe-analyze-performance-analysis","title":"<code>super observe analyze</code> - Performance Analysis","text":"<p>Analyze agent performance over time and generate insights.</p> <pre><code># Analyze last 7 days (default)\nsuper observe analyze &lt;agent_id&gt;\n\n# Analyze specific time period\nsuper observe analyze &lt;agent_id&gt; --days 30\n</code></pre> <p>Output Example: <pre><code>\ud83d\udcca Performance Summary for Agent: developer_20250714_200501\n Total Events              11         \n Successful Events         11         \n Error Events              0          \n Warning Events            0          \n Average Duration          3423.5 ms  \n Median Duration           0.5 ms     \n 95th Percentile Duration  10270.0 ms \n</code></pre></p>"},{"location":"guides/observability/#super-observe-check-trace-configuration-check","title":"<code>super observe check</code> - Trace Configuration Check","text":"<p>Check pipeline tracing configuration and verify trace generation.</p> <pre><code># Check trace configuration\nsuper observe check\n\n# Check with test run\nsuper observe check --agent-id &lt;agent_id&gt; --run-test\n\n# Check DSPy configuration\nsuper observe check --check-dspy\n</code></pre> <p>Options: - <code>--agent-id</code>: Test specific agent - <code>--run-test</code>: Run a test agent execution - <code>--check-dspy</code>: Check DSPy configuration</p>"},{"location":"guides/observability/#super-observe-debug-interactive-debugging","title":"<code>super observe debug</code> - Interactive Debugging","text":"<p>Start an interactive debugging session for an agent.</p> <pre><code># Start debug session\nsuper observe debug agent &lt;agent_id&gt;\n\n# Enable step-by-step debugging\nsuper observe debug agent &lt;agent_id&gt; --enable-step-mode\n\n# Break on errors or memory operations\nsuper observe debug agent &lt;agent_id&gt; --break-on-error --break-on-memory\n</code></pre> <p>Debug Commands: - <code>help</code>: Show available commands - <code>continue</code>: Continue execution - <code>step</code>: Step through execution - <code>breakpoint &lt;component&gt;</code>: Set breakpoint - <code>inspect</code>: Inspect current state - <code>memory</code>: View memory contents - <code>trace</code>: Show execution trace - <code>export</code>: Export debug data</p>"},{"location":"guides/observability/#trace-data-structure","title":"\ud83d\udcca Trace Data Structure","text":"<p>SuperOptiX generates comprehensive trace data in JSONL format. Each trace event contains:</p> <pre><code>{\n  \"event_id\": \"unique-event-identifier\",\n  \"timestamp\": \"2025-07-14T20:05:01.215246\",\n  \"event_type\": \"model_initialized\",\n  \"component\": \"pipeline\",\n  \"data\": {\n    \"model\": \"llama3.1:8b\",\n    \"provider\": \"ollama\",\n    \"tier\": \"genies\",\n    \"adapter\": \"ChatAdapter\"\n  },\n  \"parent_id\": null,\n  \"duration_ms\": null,\n  \"status\": \"success\",\n  \"metadata\": null\n}\n</code></pre> <p>Event Types: - <code>model_initialized</code>: Model setup and configuration - <code>tools_setup_start/end</code>: Tool initialization - <code>tools_initialized</code>: Tool registration - <code>react_agent_initialized</code>: ReAct agent setup - <code>pipeline_forward_start/end</code>: Main execution flow - <code>calculate_start/end</code>: Tool execution - <code>tool_execution_success</code>: Tool results - <code>execution_completed</code>: Pipeline completion</p> <p>Components: - <code>pipeline</code>: Core pipeline operations - <code>execution</code>: Main execution flow - <code>tool</code>: Tool execution - <code>calculate</code>: Calculator tool - <code>text_analyzer</code>: Text analysis tool - <code>file_reader</code>: File reading tool</p>"},{"location":"guides/observability/#real-world-experiment-developer-agent","title":"\ud83d\udd2c Real-World Experiment: Developer Agent","text":"<p>This section documents our comprehensive experiment with the SuperOptiX observability system using a developer agent.</p>"},{"location":"guides/observability/#experiment-overview","title":"\ud83c\udfaf Experiment Overview","text":"<p>Goal: Document and demonstrate the <code>super observe</code> command functionality by creating a test agent, running it, and analyzing the generated traces.</p> <p>Agent Used: Developer Assistant (Genies tier) Task: Write a simple Python function to calculate the factorial of a number</p>"},{"location":"guides/observability/#experiment-steps","title":"\ud83d\ude80 Experiment Steps","text":""},{"location":"guides/observability/#step-1-project-initialization","title":"Step 1: Project Initialization","text":"<pre><code># Initialize a new SuperOptiX project\nsuper init swe\ncd swe\n</code></pre> <p>Result: Created a new SuperOptiX project with the <code>.super</code> file marker.</p>"},{"location":"guides/observability/#step-2-agent-creation","title":"Step 2: Agent Creation","text":"<pre><code># Pull a pre-built developer agent with Genies tier\nsuper agent pull developer\n</code></pre> <p>Result: Successfully added the Developer Assistant agent with: - Name: Developer Assistant - Industry: Software - Tier: Genies - Features: ReAct Agents + Tools + Memory - Location: <code>swe/agents/developer/playbook/developer_playbook.yaml</code></p>"},{"location":"guides/observability/#step-3-agent-compilation","title":"Step 3: Agent Compilation","text":"<pre><code># Compile the agent to generate executable pipeline\nsuper agent compile developer\n</code></pre> <p>Result: Generated a Genies-tier pipeline with: - Framework: DSPy (Mixin template) - Features: ReAct, Tools, RAG Support, Memory - Output: <code>swe/agents/developer/pipelines/developer_pipeline.py</code> - BDD Scenarios: 5 found for testing</p>"},{"location":"guides/observability/#step-4-agent-execution","title":"Step 4: Agent Execution","text":"<pre><code># Run the agent with a specific goal\nsuper agent run developer --goal \"Write a simple Python function to calculate the factorial of a number\"\n</code></pre> <p>Result: Agent executed successfully with: - Agent ID: <code>developer_20250714_200501</code> - Model: <code>llama3.1:8b</code> (Ollama backend) - Tools: 3 tools configured (calculator, text_analyzer, file_reader) - Execution Time: 10.27 seconds - Status: Success </p>"},{"location":"guides/observability/#trace-analysis-results","title":"\ud83d\udcca Trace Analysis Results","text":""},{"location":"guides/observability/#trace-file-generation","title":"Trace File Generation","text":"<p>The agent execution automatically generated trace files in <code>.superoptix/traces/</code>:</p> <pre><code>\ud83d\udcc1 Trace Files Generated:\n\u251c\u2500\u2500 .superoptix/traces/developer_20250714_200501.jsonl (4057 bytes)\n\u2514\u2500\u2500 .superoptix/traces/developer.jsonl (960 bytes)\n</code></pre>"},{"location":"guides/observability/#key-trace-events-observed","title":"Key Trace Events Observed","text":"<ol> <li>Model Initialization: <code>model_initialized</code> - Model setup with Ollama backend</li> <li>Tool Setup: <code>tools_setup_start/end</code> - Tool initialization (0.5ms)</li> <li>Tool Registration: <code>tools_initialized</code> - 3 tools registered</li> <li>ReAct Setup: <code>react_agent_initialized</code> - ReAct agent configuration</li> <li>Execution Start: <code>pipeline_forward_start</code> - Main execution begins</li> <li>Tool Execution: <code>calculate_start/end</code> - Calculator tool used (0.1ms)</li> <li>Execution Complete: <code>pipeline_forward_end</code> - Total execution (10270ms)</li> </ol>"},{"location":"guides/observability/#performance-insights","title":"Performance Insights","text":"<ul> <li>Total execution time: 10.27 seconds</li> <li>Tool execution time: 0.1ms (calculator tool)</li> <li>Setup overhead: 0.5ms for tool initialization</li> <li>LLM processing: Majority of time spent in LLM inference</li> </ul>"},{"location":"guides/observability/#key-findings","title":"\ud83c\udfaf Key Findings","text":""},{"location":"guides/observability/#automatic-tracing","title":"Automatic Tracing","text":"<ul> <li>Tracing is automatic: No manual configuration required</li> <li>Comprehensive coverage: Captures model, tools, execution, and performance data</li> <li>Structured format: JSONL format for easy parsing and analysis</li> </ul>"},{"location":"guides/observability/#rich-trace-data","title":"Rich Trace Data","text":"<ul> <li>Event hierarchy: Parent-child relationships between events</li> <li>Timing information: Precise duration measurements</li> <li>Component separation: Clear separation of pipeline, execution, and tool events</li> <li>Status tracking: Success, error, warning, and info statuses</li> </ul>"},{"location":"guides/observability/#tool-usage-analysis","title":"Tool Usage Analysis","text":"<ul> <li>Tools used: Calculator tool attempted (with syntax error)</li> <li>Tool integration: Seamless integration with ReAct framework</li> <li>Error handling: Graceful handling of tool execution errors</li> </ul>"},{"location":"guides/observability/#quick-reference","title":"\u26a1 Quick Reference","text":""},{"location":"guides/observability/#essential-commands","title":"\ud83d\ude80 Essential Commands","text":""},{"location":"guides/observability/#list-agents-with-traces","title":"List Agents with Traces","text":"<pre><code>super observe list\n</code></pre>"},{"location":"guides/observability/#view-agent-traces","title":"View Agent Traces","text":"<pre><code># Basic trace view\nsuper observe traces &lt;agent_id&gt;\n\n# Detailed analysis\nsuper observe traces &lt;agent_id&gt; --detailed --show-tools --show-llm\n\n# Filter by component\nsuper observe traces &lt;agent_id&gt; --component pipeline\n\n# Export traces\nsuper observe traces &lt;agent_id&gt; --export json --output traces.json\n</code></pre>"},{"location":"guides/observability/#launch-dashboard","title":"Launch Dashboard","text":"<pre><code># Default dashboard\nsuper observe dashboard\n\n# Custom port and auto-open\nsuper observe dashboard --port 8502 --auto-open\n\n# Monitor specific agent\nsuper observe dashboard --agent-id &lt;agent_id&gt;\n</code></pre>"},{"location":"guides/observability/#performance-analysis","title":"Performance Analysis","text":"<pre><code># Analyze last 7 days (default)\nsuper observe analyze &lt;agent_id&gt;\n\n# Analyze specific period\nsuper observe analyze &lt;agent_id&gt; --days 30\n</code></pre>"},{"location":"guides/observability/#debug-agent","title":"Debug Agent","text":"<pre><code># Start debug session\nsuper observe debug agent &lt;agent_id&gt;\n\n# Step-by-step debugging\nsuper observe debug agent &lt;agent_id&gt; --enable-step-mode\n\n# Break on errors\nsuper observe debug agent &lt;agent_id&gt; --break-on-error\n</code></pre>"},{"location":"guides/observability/#check-configuration","title":"Check Configuration","text":"<pre><code># Check trace configuration\nsuper observe check\n\n# Test with specific agent\nsuper observe check --agent-id &lt;agent_id&gt; --run-test\n</code></pre>"},{"location":"guides/observability/#command-options-summary","title":"\ud83d\udccb Command Options Summary","text":""},{"location":"guides/observability/#super-observe-traces-options","title":"<code>super observe traces</code> Options","text":"<ul> <li><code>--component</code>: Filter by component (pipeline, tool, execution)</li> <li><code>--status</code>: Filter by status (success, error, warning, info)</li> <li><code>--limit</code>: Limit number of traces (default: 100)</li> <li><code>--detailed</code>: Show detailed analysis</li> <li><code>--show-tools</code>: Show tool execution details</li> <li><code>--show-llm</code>: Show LLM call details</li> <li><code>--export</code>: Export format (json, csv)</li> <li><code>--output</code>: Output file path</li> </ul>"},{"location":"guides/observability/#super-observe-dashboard-options","title":"<code>super observe dashboard</code> Options","text":"<ul> <li><code>--port</code>: Dashboard port (default: 8501)</li> <li><code>--host</code>: Dashboard host (default: localhost)</li> <li><code>--auto-open</code>: Automatically open browser</li> <li><code>--agent-id</code>: Monitor specific agent</li> </ul>"},{"location":"guides/observability/#super-observe-analyze-options","title":"<code>super observe analyze</code> Options","text":"<ul> <li><code>--days</code>: Number of days to analyze (default: 7)</li> </ul>"},{"location":"guides/observability/#super-observe-debug-options","title":"<code>super observe debug</code> Options","text":"<ul> <li><code>--enable-step-mode</code>: Enable step-by-step debugging</li> <li><code>--break-on-error</code>: Break on error</li> <li><code>--break-on-memory</code>: Break on memory operations</li> </ul>"},{"location":"guides/observability/#debug-commands","title":"\ud83d\udd0d Debug Commands","text":"<p>Once in debug mode: - <code>help</code>: Show available commands - <code>continue</code>: Continue execution - <code>step</code>: Step through execution - <code>breakpoint &lt;component&gt;</code>: Set breakpoint - <code>inspect</code>: Inspect current state - <code>memory</code>: View memory contents - <code>trace</code>: Show execution trace - <code>export</code>: Export debug data</p>"},{"location":"guides/observability/#common-event-types","title":"\ud83c\udfaf Common Event Types","text":"<ul> <li><code>model_initialized</code>: Model setup</li> <li><code>tools_setup_start/end</code>: Tool initialization</li> <li><code>tools_initialized</code>: Tool registration</li> <li><code>react_agent_initialized</code>: ReAct setup</li> <li><code>pipeline_forward_start/end</code>: Main execution</li> <li><code>calculate_start/end</code>: Tool execution</li> <li><code>tool_execution_success</code>: Tool results</li> <li><code>execution_completed</code>: Pipeline completion</li> </ul>"},{"location":"guides/observability/#common-components","title":"\ud83d\udd27 Common Components","text":"<ul> <li><code>pipeline</code>: Core pipeline operations</li> <li><code>execution</code>: Main execution flow</li> <li><code>tool</code>: Tool execution</li> <li><code>calculate</code>: Calculator tool</li> <li><code>text_analyzer</code>: Text analysis tool</li> <li><code>file_reader</code>: File reading tool</li> </ul>"},{"location":"guides/observability/#advanced-features","title":"\ud83d\udd27 Advanced Features","text":""},{"location":"guides/observability/#external-tracing-integrations","title":"External Tracing Integrations","text":"<p>SuperOptiX supports integration with external tracing systems:</p> <ul> <li>MLflow: For experiment tracking and model management</li> <li>Langfuse: For LLM application monitoring</li> <li>Custom tracers: Extensible tracing framework</li> </ul>"},{"location":"guides/observability/#performance-monitoring","title":"Performance Monitoring","text":"<p>The observability system tracks: - Execution duration and timing - Tool usage patterns - LLM call performance - Error rates and types - Memory usage trends - Component-specific metrics</p>"},{"location":"guides/observability/#debugging-capabilities","title":"Debugging Capabilities","text":"<ul> <li>Step-by-step execution: Walk through agent execution</li> <li>Breakpoints: Set breakpoints on specific components</li> <li>State inspection: Examine agent state at any point</li> <li>Memory analysis: View memory contents and operations</li> <li>Error tracking: Detailed error analysis and debugging</li> </ul>"},{"location":"guides/observability/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/observability/#enable-tracing-for-all-agents","title":"Enable Tracing for All Agents","text":"<p>Always run agents with tracing enabled to capture comprehensive execution data:</p> <pre><code># Tracing is automatically enabled for all agent runs\nsuper agent run &lt;agent_id&gt; --goal \"your goal\"\n</code></pre>"},{"location":"guides/observability/#regular-performance-analysis","title":"Regular Performance Analysis","text":"<p>Schedule regular performance analysis to identify optimization opportunities:</p> <pre><code># Weekly performance review\nsuper observe analyze &lt;agent_id&gt; --days 7\n\n# Monthly trend analysis\nsuper observe analyze &lt;agent_id&gt; --days 30\n</code></pre>"},{"location":"guides/observability/#monitor-tool-usage","title":"Monitor Tool Usage","text":"<p>Track tool usage patterns to optimize agent capabilities:</p> <pre><code># View tool execution details\nsuper observe traces &lt;agent_id&gt; --show-tools --detailed\n</code></pre>"},{"location":"guides/observability/#debug-production-issues","title":"Debug Production Issues","text":"<p>Use the debugging system to troubleshoot production problems:</p> <pre><code># Interactive debugging session\nsuper observe debug agent &lt;agent_id&gt; --break-on-error\n</code></pre>"},{"location":"guides/observability/#export-trace-data","title":"Export Trace Data","text":"<p>Export trace data for external analysis and reporting:</p> <pre><code># Export to JSON for analysis\nsuper observe traces &lt;agent_id&gt; --export json --output analysis.json\n\n# Export to CSV for spreadsheet analysis\nsuper observe traces &lt;agent_id&gt; --export csv --output data.csv\n</code></pre>"},{"location":"guides/observability/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"guides/observability/#no-traces-found","title":"No Traces Found","text":"<p>If you don't see any traces:</p> <ol> <li>Verify agent execution: Ensure the agent has been run at least once</li> <li>Check project structure: Make sure you're in a SuperOptiX project directory</li> <li>Verify trace directory: Check <code>.superoptix/traces/</code> for trace files</li> <li>Run trace check: Use <code>super observe check</code> to diagnose issues</li> </ol>"},{"location":"guides/observability/#dashboard-not-starting","title":"Dashboard Not Starting","text":"<p>If the dashboard fails to start:</p> <ol> <li>Check port availability: Ensure port 8501 (or your chosen port) is free</li> <li>Verify Streamlit installation: Install with <code>pip install streamlit</code></li> <li>Check permissions: Ensure write access to <code>/tmp/</code> directory</li> <li>Review logs: Check for error messages in the console output</li> </ol>"},{"location":"guides/observability/#performance-issues","title":"Performance Issues","text":"<p>If you experience performance problems:</p> <ol> <li>Limit trace data: Use <code>--limit</code> to reduce trace output</li> <li>Filter traces: Use <code>--component</code> and <code>--status</code> filters</li> <li>Export selectively: Export only necessary data</li> <li>Monitor disk usage: Clean up old trace files periodically</li> </ol>"},{"location":"guides/observability/#performance-metrics","title":"\ud83d\udcc8 Performance Metrics","text":""},{"location":"guides/observability/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ul> <li>Total Events: Number of trace events</li> <li>Success Rate: Percentage of successful events</li> <li>Error Rate: Percentage of error events</li> <li>Average Duration: Mean execution time</li> <li>95<sup>th</sup> Percentile: Performance threshold</li> <li>Tool Usage: Frequency of tool calls</li> <li>LLM Calls: Number of model interactions</li> </ul>"},{"location":"guides/observability/#typical-values","title":"Typical Values","text":"<ul> <li>Setup Time: 0.1-1.0ms</li> <li>Tool Execution: 0.1-10ms</li> <li>LLM Processing: 1-30 seconds</li> <li>Total Execution: 1-60 seconds</li> </ul>"},{"location":"guides/observability/#quick-start-workflow","title":"\ud83c\udf89 Quick Start Workflow","text":"<ol> <li> <p>Run an agent to generate traces:    <pre><code>super agent run &lt;agent_id&gt; --goal \"your task\"\n</code></pre></p> </li> <li> <p>List available traces:    <pre><code>super observe list\n</code></pre></p> </li> <li> <p>View traces for analysis:    <pre><code>super observe traces &lt;agent_id&gt; --detailed\n</code></pre></p> </li> <li> <p>Launch dashboard for monitoring:    <pre><code>super observe dashboard --auto-open\n</code></pre></p> </li> <li> <p>Analyze performance:    <pre><code>super observe analyze &lt;agent_id&gt; --days 7\n</code></pre></p> </li> <li> <p>Debug issues if needed:    <pre><code>super observe debug agent &lt;agent_id&gt; --break-on-error\n</code></pre></p> </li> </ol>"},{"location":"guides/observability/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Agent Development Guide - Learn how to create and customize agents</li> <li>Tool Development Guide - Build custom tools for your agents</li> <li>Memory System Guide - Understand agent memory and context management</li> <li>Evaluation and Testing Guide - Test and validate your agents</li> <li>CLI Reference - Complete CLI command reference</li> </ul>"},{"location":"guides/observability/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>The SuperOptiX observability system provides comprehensive monitoring, debugging, and analysis capabilities for your AI agents. By following this guide, you can effectively monitor agent performance, debug issues, and optimize your agentic AI workflows.</p> <p>Start exploring your agents today with <code>super observe list</code> and discover the power of comprehensive AI observability! \ud83d\ude80</p>"},{"location":"guides/openai-sdk-integration/","title":"\ud83e\udd16 OpenAI Agents SDK Integration","text":"<p>SuperOptiX now supports OpenAI Agents SDK - a lightweight, provider-agnostic framework that works PERFECTLY with Ollama!</p> <p>Works great with FREE Ollama (No API Keys Needed!)</p> <p>RLM support is experimental. Unified sandbox support is coming soon.</p> <p>Hands-on demo: Clone the MIT-licensed companion repo <code>superoptix-lite-openai</code> to try the OpenAI Agents SDK with SuperOptiX Lite right away. The Code Reviewer example in that project mirrors this guide step by step.</p>"},{"location":"guides/openai-sdk-integration/#what-is-openai-agents-sdk","title":"\ud83c\udfaf What is OpenAI Agents SDK?","text":"<p>OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. Key features:</p> <ul> <li>\ud83c\udf10 Provider-Agnostic: Works with OpenAI, Ollama, and 100+ LLMs</li> <li>\ud83d\udd04 Multi-Agent: Built-in handoffs for agent collaboration</li> <li>\ud83d\udee1\ufe0f Guardrails: Input/output validation</li> <li>\ud83d\udcbe Sessions: Automatic conversation history</li> <li>\ud83d\udcca Tracing: Built-in execution tracking</li> <li>Works with Ollama!: Unlike DeepAgents, no function-calling limitations</li> </ul> <p>Perfect for simple to moderate complexity tasks with local models!</p>"},{"location":"guides/openai-sdk-integration/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install superoptix[frameworks-openai]\n</code></pre> <p>Includes: - openai-agents 0.4.1 - openai SDK (latest) - SuperOptiX core with GEPA 0.0.17</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency)</p>"},{"location":"guides/openai-sdk-integration/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/openai-sdk-integration/#pull-the-demo-agent","title":"Pull the Demo Agent","text":"<pre><code>cd your_project\nsuper agent pull assistant_openai\n</code></pre>"},{"location":"guides/openai-sdk-integration/#configure-model","title":"Configure Model","text":"<p>Uses Ollama by Default! (FREE, no API keys needed!)</p> <p>The <code>assistant_openai</code> agent now defaults to Ollama <code>llama3.1:8b</code>:</p> <pre><code>language_model:\n  location: local\n  provider: ollama\n  model: ollama:llama3.1:8b  # Fast and efficient model\n  temperature: 0.7\n  api_base: http://localhost:11434\n</code></pre> <p>Just install Ollama and run: <pre><code>brew install ollama  # macOS\nollama pull llama3.1:8b\nsuper agent run assistant_openai --framework openai --goal \"Hello!\"\n</code></pre></p> <p>Also Works With Cloud Models (requires API key): <pre><code># OpenAI GPT-4\nlanguage_model:\n  location: cloud\n  provider: openai\n  model: openai:gpt-4o\n  # Set: export OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic Claude\nlanguage_model:\n  location: cloud\n  provider: anthropic\n  model: anthropic:claude-sonnet-4-20250514\n  # Set: export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre></p>"},{"location":"guides/openai-sdk-integration/#run-the-workflow","title":"Run the Workflow","text":"<pre><code># Compile\nsuper agent compile assistant_openai --framework openai\n\n# Evaluate\nsuper agent evaluate assistant_openai\n\n# Optimize with GEPA\nsuper agent optimize assistant_openai --auto medium --framework openai --reflection-lm ollama:llama3.1:8b\n\n# Run\nsuper agent run assistant_openai --framework openai --goal \"What is Python?\"\n</code></pre>"},{"location":"guides/openai-sdk-integration/#simplified-cloudlocal-model-switching","title":"\ud83c\udf29\ufe0f Simplified Cloud/Local Model Switching","text":"<p>The <code>superoptix-lite-openai</code> companion repo includes simplified scripts for easy switching between local and cloud models:</p>"},{"location":"guides/openai-sdk-integration/#local-models-free","title":"Local Models (FREE)","text":"<pre><code>python demo_local.py        # Run demo with Ollama\npython optimize_local.py    # GEPA optimization with Ollama\n</code></pre>"},{"location":"guides/openai-sdk-integration/#cloud-models-openai-anthropic-google","title":"Cloud Models (OpenAI, Anthropic, Google)","text":"<pre><code># Set API key (choose one)\nexport OPENAI_API_KEY=sk-...        # Uses gpt-5\nexport ANTHROPIC_API_KEY=sk-ant-... # Uses claude-sonnet-4.5\nexport GOOGLE_API_KEY=...           # Uses gemini-pro-2.5\n\n# Run cloud scripts (auto-detects provider)\npython demo_cloud.py        # Demo with cloud models\npython optimize_cloud.py    # GEPA optimization with cloud models\n</code></pre> <p>Features: - Auto-detects cloud provider from API key - Uses latest models (gpt-5, claude-sonnet-4.5, gemini-pro-2.5) - Separate scripts for local vs cloud (no complex switching) - Includes cost warnings (optimization uses APIs) - .env file support for API keys</p> <p>See the repo README for complete documentation.</p>"},{"location":"guides/openai-sdk-integration/#creating-your-own-openai-sdk-playbook","title":"\ud83d\udccb Creating Your Own OpenAI SDK Playbook","text":""},{"location":"guides/openai-sdk-integration/#basic-structure","title":"Basic Structure","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: My Assistant\n  id: my_assistant\n  namespace: custom\n  version: 1.0.0\n  level: genies\n\nspec:\n  target_framework: openai\n\n  language_model:\n    location: local\n    provider: ollama\n    model: ollama:gpt-oss:20b\n    api_base: http://localhost:11434\n\n  input_fields:\n    - name: query\n      type: str\n      required: true\n\n  output_fields:\n    - name: response\n      type: str\n      required: true\n\n  persona:\n    role: Helpful AI Assistant\n    goal: Provide clear and helpful responses\n    traits:\n      - helpful\n      - concise\n\n  reasoning:\n    method: direct\n    steps:\n      - Understand the question\n      - Provide clear answer\n\n  # BDD Scenarios\n  feature_specifications:\n    scenarios:\n      - name: Test scenario\n        input:\n          query: \"Hello!\"\n        expected_output:\n          response: \"Greeting\"\n          expected_keywords:\n            - hello\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: response_accuracy\n        auto: medium\n</code></pre>"},{"location":"guides/openai-sdk-integration/#complete-workflow","title":"\ud83d\udd04 Complete Workflow","text":""},{"location":"guides/openai-sdk-integration/#step-1-initialize","title":"Step 1: Initialize","text":"<pre><code>super init my_project\ncd my_project\n</code></pre>"},{"location":"guides/openai-sdk-integration/#step-2-createpull-agent","title":"Step 2: Create/Pull Agent","text":"<pre><code># Pull demo agent\nsuper agent pull assistant_openai\n\n# Or create custom playbook in:\n# agents/my_agent/playbook/my_agent_playbook.yaml\n</code></pre>"},{"location":"guides/openai-sdk-integration/#step-3-compile","title":"Step 3: Compile","text":"<pre><code>super agent compile assistant_openai --framework openai\n</code></pre> <p>What happens: - Reads SuperSpec YAML playbook - Generates Python code using <code>openai_pipeline.py.jinja2</code> - Creates <code>BaseComponent</code> wrapper for GEPA - Initializes OpenAI Agent with Ollama model - Creates evaluation and optimization methods</p> <p>Output: <code>agents/assistant_openai/pipelines/assistant_openai_openai_pipeline.py</code></p>"},{"location":"guides/openai-sdk-integration/#step-4-evaluate","title":"Step 4: Evaluate","text":"<pre><code>super agent evaluate assistant_openai\n</code></pre> <p>Expected Results: <pre><code>\ud83d\udd0d Evaluating assistant_openai...\nTesting 4 BDD scenarios:\n\nOpenAI Agents SDK initialized with Ollama: gpt-oss:20b\nSimple greeting: PASS\nQuestion answering: PASS\nExplanation request: PASS\nMath question: PASS\n\nOverall: 4/4 PASS (100.0%)\n</code></pre></p> <p>Note: Results depend on your model, hardware, and BDD scenario complexity. The agent loads optimized instructions automatically if available.</p>"},{"location":"guides/openai-sdk-integration/#step-5-optimize","title":"Step 5: Optimize","text":"<pre><code>super agent optimize assistant_openai --auto medium --framework openai --reflection-lm ollama:llama3.1:8b\n</code></pre> <p>What GEPA optimizes: - <code>instructions</code>: The agent's system prompt</p> <p>GEPA will test variations to find the best instructions!</p> <p>Note: Requires <code>--reflection-lm</code> parameter for Universal GEPA optimization</p>"},{"location":"guides/openai-sdk-integration/#step-6-re-evaluate","title":"Step 6: Re-evaluate","text":"<pre><code>super agent evaluate assistant_openai\n</code></pre> <p>See if GEPA improved the pass rate!</p>"},{"location":"guides/openai-sdk-integration/#step-7-run","title":"Step 7: Run","text":"<pre><code>super agent run assistant_openai --framework openai --goal \"Explain quantum computing\"\n</code></pre>"},{"location":"guides/openai-sdk-integration/#how-it-works","title":"\ud83d\udd27 How It Works","text":""},{"location":"guides/openai-sdk-integration/#basecomponent-wrapper","title":"BaseComponent Wrapper","text":"<pre><code>class AssistantOpenAiComponent(BaseComponent):\n    def __init__(self, instructions=None, model_config=None, ...):\n        super().__init__(\n            name=\"assistant_openai\",\n            variable=instructions,  # GEPA optimizes this!\n            variable_type=\"instructions\",\n            framework=\"openai\",\n            ...\n        )\n\n    def _initialize_model(self):\n        # Detect Ollama vs OpenAI\n        if model_str.startswith(\"ollama:\"):\n            self._model = OpenAIChatCompletionsModel(\n                model=\"gpt-oss:20b\",\n                openai_client=AsyncOpenAI(\n                    base_url=\"http://localhost:11434/v1\",\n                    api_key=\"ollama\",\n                ),\n            )\n\n    def _initialize_agent(self):\n        self._agent = Agent(\n            name=\"Assistant\",\n            instructions=self.variable,  # GEPA optimizes!\n            tools=self._tools,\n            model=self._model,\n        )\n\n    def forward(self, **inputs):\n        result = Runner.run_sync(self._agent, input=query)\n        return {\"response\": result.final_output}\n\n    def update(self, new_variable):\n        # GEPA calls this during optimization\n        self.variable = new_variable\n        self._agent = None  # Reinitialize with new instructions\n</code></pre>"},{"location":"guides/openai-sdk-integration/#pipeline-class","title":"Pipeline Class","text":"<pre><code>class AssistantOpenAiPipeline:\n    def __init__(self, playbook_path=None):\n        # Load playbook and BDD scenarios\n        self.component = create_assistant_open_ai_agent(model_config=...)\n        self.test_scenarios = self._load_bdd_scenarios()\n\n    def run(self, **inputs):\n        return self.component.forward(**inputs)\n\n    def evaluate(self):\n        # Run all BDD scenarios\n        for scenario in self.test_scenarios:\n            result = self.run(**scenario[\"input\"])\n            # Evaluate against expected output\n\n    def optimize_with_gepa(self, auto=\"medium\"):\n        # Universal GEPA optimization\n        optimizer = UniversalGEPA(...)\n        result = optimizer.compile(\n            component=self.component,\n            trainset=trainset,\n            valset=valset\n        )\n\n    def run_bdd_test_suite(self):\n        # CLI compatibility\n        return self.evaluate()\n</code></pre>"},{"location":"guides/openai-sdk-integration/#openai-sdk-vs-other-frameworks","title":"\ud83d\udcca OpenAI SDK vs Other Frameworks","text":"Feature DSPy DeepAgents OpenAI SDK Ollama Support Full Blocked Perfect Baseline Performance Good N/A Excellent API Complexity Medium High Low Planning Manual Built-in Manual Multi-Agent Manual Subagents Handoffs GEPA Optimization All signatures system_prompt instructions Best For Prompt optimization Complex planning Simple tasks"},{"location":"guides/openai-sdk-integration/#when-to-use-openai-sdk","title":"When to Use OpenAI SDK","text":"<p>Perfect for: - Simple to moderate complexity tasks - Local development with Ollama - Quick prototyping - Clean, minimal agent design - When you want high baseline performance</p> <p>Not ideal for: - Complex multi-step planning (use DeepAgents) - Maximum prompt optimization (use DSPy) - Need filesystem context management (use DeepAgents)</p>"},{"location":"guides/openai-sdk-integration/#example-use-cases","title":"\ud83c\udf93 Example Use Cases","text":""},{"location":"guides/openai-sdk-integration/#question-answering-agent","title":"Question Answering Agent","text":"<pre><code>persona:\n  role: Knowledge Expert\n  goal: Answer questions accurately and concisely\n\nfeature_specifications:\n  scenarios:\n    - name: Python question\n      input:\n        query: \"What is Python?\"\n      expected_output:\n        expected_keywords:\n          - Python\n          - programming\n          - language\n</code></pre>"},{"location":"guides/openai-sdk-integration/#customer-support-agent","title":"Customer Support Agent","text":"<pre><code>persona:\n  role: Customer Support Specialist\n  goal: Help customers with their issues professionally\n\ntools:\n  specific_tools:\n    - check_order_status\n    - process_refund\n    - escalate_to_human\n</code></pre>"},{"location":"guides/openai-sdk-integration/#code-explainer","title":"Code Explainer","text":"<pre><code>persona:\n  role: Senior Software Engineer\n  goal: Explain code concepts clearly to beginners\n\nreasoning:\n  steps:\n    - Analyze the code or concept\n    - Break down complex ideas\n    - Provide clear explanations with examples\n</code></pre>"},{"location":"guides/openai-sdk-integration/#advanced-features","title":"\u2699\ufe0f Advanced Features","text":""},{"location":"guides/openai-sdk-integration/#tools-support","title":"Tools Support","text":"<pre><code>tools:\n  enabled: true\n  specific_tools:\n    - name: search_database\n      description: Search knowledge base\n    - name: send_email\n      description: Send email to user\n</code></pre> <p>Implement in generated pipeline:</p> <pre><code>@function_tool\ndef search_database(query: str) -&gt; str:\n    \"\"\"Search the knowledge base.\"\"\"\n    return f\"Results for: {query}\"\n\ntools = [search_database]\n</code></pre>"},{"location":"guides/openai-sdk-integration/#multi-agent-handoffs","title":"Multi-Agent Handoffs","text":"<pre><code>handoffs:\n  - name: specialist_agent\n    description: Hand off complex technical questions\n    instructions: You are a technical specialist\n</code></pre>"},{"location":"guides/openai-sdk-integration/#guardrails","title":"Guardrails","text":"<pre><code>guardrails:\n  input:\n    - type: content_filter\n      params:\n        block_harmful: true\n  output:\n    - type: pii_detector\n      params:\n        redact: true\n</code></pre>"},{"location":"guides/openai-sdk-integration/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"guides/openai-sdk-integration/#agent-not-responding","title":"Agent Not Responding","text":"<p>Symptom: Agent returns empty response</p> <p>Checklist: 1. Ollama running? (<code>curl http://localhost:11434/api/tags</code>) 2. Model downloaded? (<code>ollama list</code>) 3. Correct model string? (<code>ollama:gpt-oss:20b</code>)</p>"},{"location":"guides/openai-sdk-integration/#low-pass-rate","title":"Low Pass Rate","text":"<p>Symptom: Scenarios failing</p> <p>Solutions: 1. Check BDD scenario keywords are realistic 2. Lower threshold to 0.4 or 0.5 3. Run GEPA optimization to improve instructions 4. Try different model (llama3.1:70b or gpt-oss:120b for more capability)</p>"},{"location":"guides/openai-sdk-integration/#import-error","title":"Import Error","text":"<p>Symptom: <code>ModuleNotFoundError: No module named 'agents'</code></p> <p>Solution: <pre><code>pip install openai-agents\n</code></pre></p>"},{"location":"guides/openai-sdk-integration/#gepa-optimization","title":"\ud83c\udfaf GEPA Optimization","text":""},{"location":"guides/openai-sdk-integration/#what-gets-optimized","title":"What Gets Optimized","text":"<p>OpenAI Agents SDK has one main optimizable variable: - <code>instructions</code>: The agent's system prompt</p>"},{"location":"guides/openai-sdk-integration/#how-gepa-optimizes-openai-sdk-agents","title":"How GEPA Optimizes OpenAI SDK Agents","text":"<p>GEPA optimizes the instructions field by:</p> <ol> <li>Analyzing BDD test scenarios to understand success criteria</li> <li>Generating variations of the instructions prompt</li> <li>Testing each variation against your evaluation scenarios</li> <li>Selecting the best performer based on pass rate</li> </ol> <p>Example transformation:</p> <pre><code># Original (from playbook)\npersona:\n  role: Helpful AI Assistant\n  goal: Provide clear responses\n\n\u2192 instructions = \"Helpful AI Assistant\\nGoal: Provide clear responses\"\n</code></pre> <pre><code># After GEPA optimization\n\u2192 instructions = \"You are a Helpful AI Assistant.\n\nWhen answering questions:\n1. Read the question carefully\n2. Provide accurate, factual information\n3. Use clear, simple language\n4. Be concise but complete\n\nGoal: Provide clear, helpful responses that directly address the user's query.\"\n</code></pre> <p>GEPA typically expands the instructions to be more explicit and structured, which can improve agent behavior consistency.</p>"},{"location":"guides/openai-sdk-integration/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":""},{"location":"guides/openai-sdk-integration/#baseline-performance","title":"Baseline Performance","text":"<p>Task: General question answering Model: Ollama gpt-oss:20b Framework: OpenAI Agents SDK</p> <p>OpenAI SDK typically achieves good baseline performance with local Ollama models. Results will vary based on: - Your hardware capabilities (RAM, CPU/GPU) - Model size and quality (8b vs 20b vs 120b) - BDD scenario complexity - Temperature and other model parameters</p>"},{"location":"guides/openai-sdk-integration/#framework-comparison","title":"Framework Comparison","text":"<p>OpenAI SDK strengths: - Clean, simple API makes agents easier to understand - Works seamlessly with Ollama (no function-calling limitations) - Good baseline performance out of the box</p> <p>DSPy strengths: - More optimization targets (all signatures, not just instructions) - Better for focused, well-defined tasks - Greater improvement potential through optimization</p> <p>DeepAgents limitations: - Requires cloud models (Claude/GPT-4) due to LangChain function-calling requirements - Cannot be tested with Ollama</p>"},{"location":"guides/openai-sdk-integration/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>SuperSpec YAML Playbook\n        \u2193\n    Compiler (AgentCompiler)\n        \u2193\nOpenAI Pipeline Template (openai_pipeline.py.jinja2)\n        \u2193\nGenerated Python Pipeline\n        \u251c\u2500 AssistantOpenAIComponent (BaseComponent wrapper)\n        \u2502   \u251c\u2500 _initialize_model() \u2192 OpenAIChatCompletionsModel (Ollama)\n        \u2502   \u251c\u2500 _initialize_agent() \u2192 Agent(instructions, tools, model)\n        \u2502   \u2514\u2500 forward() \u2192 Runner.run_sync()\n        \u2514\u2500 AssistantOpenAIPipeline\n            \u251c\u2500 run()\n            \u251c\u2500 evaluate()\n            \u251c\u2500 optimize_with_gepa() \u2190 Universal GEPA\n            \u2514\u2500 run_bdd_test_suite()\n</code></pre>"},{"location":"guides/openai-sdk-integration/#model-configuration","title":"\ud83d\udd04 Model Configuration","text":""},{"location":"guides/openai-sdk-integration/#ollama-local-recommended","title":"Ollama (Local) - RECOMMENDED \u2b50","text":"<pre><code>language_model:\n  location: local\n  provider: ollama\n  model: ollama:gpt-oss:20b\n  temperature: 0.7\n  max_tokens: 2000\n  api_base: http://localhost:11434\n</code></pre> <p>Advantages: - Free inference - Privacy (data stays local) - Fast development iteration - Good baseline performance</p> <p>Supported Ollama Models: - <code>ollama:llama3.1:8b</code> (default, fast and efficient) - <code>ollama:gpt-oss:120b</code> (most capable, larger model) - <code>ollama:gpt-oss:20b</code> (faster alternative) - <code>ollama:qwen3:8b</code> (alternative)</p>"},{"location":"guides/openai-sdk-integration/#openai-cloud","title":"OpenAI (Cloud)","text":"<pre><code>language_model:\n  provider: openai\n  model: gpt-4.1\n  temperature: 0.7\n</code></pre> <p>Set API key: <code>export OPENAI_API_KEY=your_key</code></p>"},{"location":"guides/openai-sdk-integration/#comparison-with-other-frameworks","title":"\ud83c\udf93 Comparison with Other Frameworks","text":""},{"location":"guides/openai-sdk-integration/#openai-sdk-advantages","title":"OpenAI SDK Advantages","text":"<ul> <li>Ollama compatibility (unlike DeepAgents)</li> <li>Good baseline performance</li> <li>Simple, clean API</li> <li>Built-in tracing and sessions</li> <li>Fast compilation and execution</li> </ul>"},{"location":"guides/openai-sdk-integration/#dspy-advantages","title":"DSPy Advantages","text":"<ul> <li>Maximum optimization (all signatures)</li> <li>More optimization targets</li> <li>Better for focused tasks</li> </ul>"},{"location":"guides/openai-sdk-integration/#deepagents-advantages","title":"DeepAgents Advantages","text":"<ul> <li>Built-in planning (write_todos)</li> <li>Filesystem context management</li> <li>Subagent spawning</li> <li>Requires Claude/GPT-4 (no Ollama)</li> </ul>"},{"location":"guides/openai-sdk-integration/#use-them-together","title":"Use Them Together!","text":"<pre><code># Simple tasks \u2192 OpenAI SDK (Ollama)\nsuper agent compile assistant --framework openai\n\n# Complex research \u2192 DeepAgents (Claude)\nsuper agent compile researcher --framework deepagents\n\n# Maximum optimization \u2192 DSPy (Ollama)\nsuper agent compile analyzer --framework dspy\n</code></pre>"},{"location":"guides/openai-sdk-integration/#advanced-configuration","title":"\ud83d\udee0\ufe0f Advanced Configuration","text":""},{"location":"guides/openai-sdk-integration/#with-tools","title":"With Tools","text":"<pre><code>spec:\n  tools:\n    enabled: true\n    specific_tools:\n      - name: get_weather\n        description: Get weather for a city\n      - name: send_email\n        description: Send email to user\n</code></pre> <p>Then implement in code or use built-in tools.</p>"},{"location":"guides/openai-sdk-integration/#with-handoffs-multi-agent","title":"With Handoffs (Multi-Agent)","text":"<pre><code>spec:\n  handoffs:\n    - name: technical_specialist\n      description: Handles technical questions\n      instructions: You are a technical expert\n\n    - name: billing_specialist\n      description: Handles billing issues\n      instructions: You handle billing and payments\n</code></pre>"},{"location":"guides/openai-sdk-integration/#with-guardrails","title":"With Guardrails","text":"<pre><code>spec:\n  guardrails:\n    input_guardrails:\n      - type: content_safety\n        params:\n          block_harmful: true\n\n    output_guardrails:\n      - type: pii_filter\n        params:\n          redact_emails: true\n          redact_phones: true\n</code></pre>"},{"location":"guides/openai-sdk-integration/#framework-trade-offs","title":"\ud83d\udcca Framework Trade-offs","text":""},{"location":"guides/openai-sdk-integration/#model-support-comparison","title":"Model Support Comparison","text":"Framework Local Models (Ollama) Cloud Models Optimization Targets OpenAI SDK Full support Yes Instructions only DSPy Full support Yes Multiple signatures DeepAgents Limited* Yes System prompt <p>*DeepAgents has LangChain function-calling limitations with local models</p>"},{"location":"guides/openai-sdk-integration/#cost-development-speed","title":"Cost &amp; Development Speed","text":"Framework Development Complexity Ollama Cost Cloud Cost OpenAI SDK Low (simple API) Free Variable DSPy Medium (more concepts) Free Variable DeepAgents High (planning graphs) N/A Variable <p>Note: Actual performance depends on your specific use case, model choice, and BDD scenarios. Always evaluate with your own data.</p>"},{"location":"guides/openai-sdk-integration/#real-world-examples","title":"\ud83c\udfaf Real-World Examples","text":""},{"location":"guides/openai-sdk-integration/#customer-support-bot","title":"Customer Support Bot","text":"<pre><code>metadata:\n  name: Support Bot\n  id: support_bot\n\nspec:\n  persona:\n    role: Customer Support Specialist\n    goal: Resolve customer issues efficiently\n    traits:\n      - patient\n      - empathetic\n      - solution-focused\n\n  tools:\n    specific_tools:\n      - check_order_status\n      - process_refund\n      - escalate_ticket\n</code></pre>"},{"location":"guides/openai-sdk-integration/#code-helper","title":"Code Helper","text":"<pre><code>metadata:\n  name: Code Helper\n  id: code_helper\n\nspec:\n  persona:\n    role: Senior Developer\n    goal: Help developers with code questions\n    communication_preferences:\n      style: technical\n      verbosity: detailed\n</code></pre>"},{"location":"guides/openai-sdk-integration/#content-writer","title":"Content Writer","text":"<pre><code>metadata:\n  name: Content Writer\n  id: content_writer\n\nspec:\n  persona:\n    role: Professional Content Writer\n    goal: Create engaging, high-quality content\n    traits:\n      - creative\n      - detail-oriented\n</code></pre>"},{"location":"guides/openai-sdk-integration/#under-the-hood","title":"\ud83d\udd2c Under the Hood","text":""},{"location":"guides/openai-sdk-integration/#ollama-integration","title":"Ollama Integration","text":"<p>The template automatically detects Ollama and configures correctly:</p> <pre><code>if model_str.startswith(\"ollama:\"):\n    model_name = model_str.replace(\"ollama:\", \"\")\n    api_base = config.get(\"api_base\", \"http://localhost:11434\")\n\n    self._model = OpenAIChatCompletionsModel(\n        model=model_name,\n        openai_client=AsyncOpenAI(\n            base_url=f\"{api_base}/v1\",\n            api_key=\"ollama\",\n        ),\n    )\n</code></pre> <p>This is based on the official OpenAI Agents SDK example for Ollama!</p>"},{"location":"guides/openai-sdk-integration/#agent-execution-flow","title":"Agent Execution Flow","text":"<ol> <li>User input received</li> <li>Component's <code>forward()</code> called</li> <li>Agent initialized (lazy, cached)</li> <li><code>Runner.run_sync(agent, input)</code> executed</li> <li>OpenAI Agent processes with Ollama</li> <li><code>result.final_output</code> returned</li> <li>Mapped to output fields</li> </ol>"},{"location":"guides/openai-sdk-integration/#the-superoptix-multi-framework-advantage","title":"\ud83c\udfaf The SuperOptiX Multi-Framework Advantage","text":""},{"location":"guides/openai-sdk-integration/#one-playbook-multiple-frameworks","title":"One Playbook, Multiple Frameworks","text":"<p>SuperOptiX allows you to write your agent specification once and compile to any supported framework:</p> <pre><code># Same playbook, different frameworks\nsuper agent compile my_agent --framework openai\nsuper agent compile my_agent --framework crewai\nsuper agent compile my_agent --framework deepagents\n\n# GEPA optimization works across all frameworks\nsuper agent optimize my_agent --framework &lt;framework&gt; --auto medium\n</code></pre>"},{"location":"guides/openai-sdk-integration/#when-to-use-each-framework","title":"When to Use Each Framework","text":"<p>Choose OpenAI SDK when: - You want simple, straightforward agent design - You're using Ollama for local development - You need fast prototyping and iteration - Your use case is simple to moderate complexity</p> <p>Choose DSPy when: - You need maximum optimization flexibility - You want to optimize multiple components (signatures) - You have well-defined, focused tasks - You want proven optimization improvements</p> <p>Choose DeepAgents when: - You need complex planning capabilities - You're using cloud models (Claude/GPT-4) - You need filesystem context management - Your task requires sophisticated multi-step reasoning</p>"},{"location":"guides/openai-sdk-integration/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":""},{"location":"guides/openai-sdk-integration/#start-with-openai-sdk-for-prototyping","title":"Start with OpenAI SDK for Prototyping","text":"<ul> <li>Fast compilation</li> <li>Simple design</li> <li>Works with Ollama</li> <li>High baseline performance</li> </ul>"},{"location":"guides/openai-sdk-integration/#use-clear-instructions","title":"Use Clear Instructions","text":"<pre><code>persona:\n  role: Expert Assistant\n  goal: Specific, measurable goal\n  # Be specific about what the agent should do\n</code></pre>"},{"location":"guides/openai-sdk-integration/#realistic-bdd-scenarios","title":"Realistic BDD Scenarios","text":"<pre><code>scenarios:\n  - name: Specific test case\n    input:\n      query: \"Concrete question\"\n    expected_output:\n      expected_keywords:\n        - keyword1\n        - keyword2\n</code></pre>"},{"location":"guides/openai-sdk-integration/#iterate-and-optimize","title":"Iterate and Optimize","text":"<ol> <li>Get baseline working</li> <li>Add more scenarios</li> <li>Run GEPA optimization</li> <li>Deploy best version</li> </ol>"},{"location":"guides/openai-sdk-integration/#faq","title":"\u2753 FAQ","text":"<p>Q: Why use OpenAI SDK instead of DSPy? A: OpenAI SDK has a simpler, more straightforward API. It works well with Ollama out of the box. Choose DSPy when you need to optimize multiple components (signatures) or want maximum optimization flexibility.</p> <p>Q: Does it work with Ollama? A: Yes! OpenAI SDK has full Ollama support. Unlike DeepAgents (which has LangChain function-calling limitations), OpenAI SDK works seamlessly with local models.</p> <p>Q: Can I use cloud models? A: Yes! Configure your playbook with <code>provider: openai</code> and set the <code>OPENAI_API_KEY</code> environment variable. Supports OpenAI, Anthropic, and other providers.</p> <p>Q: Does GEPA optimize OpenAI SDK agents? A: Yes! Universal GEPA optimizes the <code>instructions</code> field. While OpenAI SDK has fewer optimization targets than DSPy (which optimizes all signatures), GEPA can still improve performance by refining the agent instructions.</p> <p>Q: Can I use tools with OpenAI SDK agents? A: Yes! Define tools in your playbook under <code>tools.specific_tools</code> and implement them using the <code>@function_tool</code> decorator in your pipeline code.</p> <p>Q: What about multi-agent workflows? A: OpenAI SDK supports multi-agent patterns through <code>handoffs</code>, where one agent can delegate to another. This is similar to CrewAI's crew concept but with a simpler API.</p> <p>Q: How does performance compare to other frameworks? A: Performance varies by use case, model, and hardware. OpenAI SDK typically has good baseline performance with Ollama. Run your own evaluations with <code>super agent evaluate</code> to measure performance for your specific use case.</p>"},{"location":"guides/openai-sdk-integration/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>OpenAI Agents SDK Docs: https://openai.github.io/openai-agents-python/</li> <li>Ollama Setup: <code>/docs/llm-setup.md</code></li> <li>Multi-Framework Guide: <code>/docs/guides/multi-framework.md</code></li> <li>Universal GEPA: <code>/plan/MULTI_FRAMEWORK_GEPA_STRATEGY.md</code></li> <li>Hands-on Repo: <code>superoptix-lite-openai</code> \u2014 clone this MIT-licensed companion project to try the OpenAI Agents SDK with SuperOptiX Lite and follow our Code Reviewer tutorial step by step.</li> </ul>"},{"location":"guides/openai-sdk-integration/#multi-framework-summary","title":"\ud83c\udf10 Multi-Framework Summary","text":"<p>SuperOptiX supports 6 agent frameworks: 1. DSPy (maximum optimization, Ollama compatible) 2. OpenAI SDK (simple API, excellent Ollama support) 3. CrewAI (multi-agent teams, role-based collaboration) 4. Google ADK (Gemini integration) 5. Microsoft (Azure OpenAI, enterprise) 6. DeepAgents (complex planning, Claude/GPT-4)</p> <p>All frameworks share: - Same SuperSpec YAML format - Same CLI workflow (<code>compile</code>, <code>evaluate</code>, <code>optimize</code>, <code>run</code>) - Same GEPA optimization engine - Framework-specific strengths preserved</p> <p>Learn more: See the Multi-Framework Guide for comprehensive comparisons and examples.</p>"},{"location":"guides/openai-sdk-integration/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Ready to try OpenAI SDK with SuperOptiX?</p> <pre><code># Pull the demo agent\nsuper agent pull assistant_openai\n\n# Start with Ollama (free, local)\nsuper agent run assistant_openai --framework openai --goal \"Hello!\"\n</code></pre>"},{"location":"guides/openai-sdk-integration/#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<p>Want to build your own custom agent with native OpenAI SDK patterns and optimize it with GEPA?</p>"},{"location":"guides/openai-sdk-integration/#openai-sdk-gepa-optimization-tutorial","title":"\ud83d\udd27 OpenAI SDK + GEPA Optimization Tutorial","text":"<p>This comprehensive step-by-step tutorial teaches you how to:</p> <p>Write agents using official OpenAI Agents SDK patterns (Agent, Runner, OpenAIChatCompletionsModel) Integrate your native SDK code with SuperOptiX for GEPA compatibility Define BDD test scenarios for measurable evaluation metrics Run GEPA optimization to automatically improve agent prompts Implement automatic optimization loading for deployment deployment</p> <p>Example project: Code Reviewer Agent that detects security vulnerabilities Hands-on repo: <code>superoptix-lite-openai</code> \u2014 clone it to follow the tutorial with a fully wired SuperOptiX Lite playground.</p> <p>Time: 30-45 minutes | Difficulty: Intermediate | Prerequisites: Python, Ollama, Git access to the repo above</p> <p>\ud83d\udc49 Start the tutorial now</p>"},{"location":"guides/optimas-integration/","title":"\u26a1 Optimas Integration Guide","text":"<p>SuperOptiX integrates seamlessly with the Optimas framework, enabling you to use advanced prompt optimization techniques (OPRO, MIPRO, COPRO) with multiple LLM frameworks including OpenAI SDK, CrewAI, AutoGen, and DSPy.</p> <p>About Optimas: Optimas optimizes compound AI systems using globally aligned Local Reward Functions (LRFs) so that local improvements translate to higher end\u2011to\u2011end performance. Learn more on the official site and paper:</p> <ul> <li>Optimas website: optimas.stanford.edu</li> <li>Optimas paper (Wu et al., 2025): arXiv: 2507.03041</li> </ul> <p>Related frameworks supported by this guide:</p> <ul> <li>DSPy: dspy.ai</li> <li>CrewAI: docs.crewai.com</li> <li>AutoGen: microsoft.github.io/autogen</li> <li>OpenAI Agent SDK: platform.openai.com/docs/agents</li> <li>LiteLLM (used by some targets): github.com/BerriAI/litellm</li> </ul>"},{"location":"guides/optimas-integration/#what-optimas-is-and-why-it-matters","title":"What Optimas is (and why it matters)","text":"<p>Optimas is a unified optimization framework for compound AI systems:</p> <ul> <li>Learns a Local Reward Function (LRF) per component that remains globally aligned, so local updates are safe and beneficial to the whole system. This enables efficient optimization without always running the entire pipeline. See: arXiv: 2507.03041.</li> <li>Supports heterogeneous configuration types:</li> <li>Prompts and textual instructions via metric\u2011guided search</li> <li>Hyperparameters and discrete choices (e.g., top\u2011k, tool/model selection, routing)</li> <li>Model parameters where supported (e.g., RL with PPO)</li> <li>Works across frameworks via target adapters: OpenAI Agent SDK, CrewAI, AutoGen, and DSPy</li> <li>Compound\u2011system optimization: operates across multiple components and tools, not just single prompts</li> <li>Multiple optimizers available: OPRO (single\u2011iteration), MIPRO (multi\u2011iteration), COPRO (cooperative)</li> </ul>"},{"location":"guides/optimas-integration/#what-this-unlocks","title":"What this unlocks","text":"<ul> <li>Optimize prompts, hyperparameters, model parameters, and model routers across compound AI systems</li> <li>Run OPRO, MIPRO, and COPRO optimization loops through a single CLI workflow</li> <li>Keep your preferred agent stack (DSPy, CrewAI, AutoGen, OpenAI SDK) and get consistent optimization behavior</li> </ul>"},{"location":"guides/optimas-integration/#why-this-is-impactful","title":"Why this is impactful","text":"<ul> <li>Globally aligned local rewards: maximizing a component\u2019s local reward increases overall system quality, improving data efficiency by reducing full system runs</li> <li>Heterogeneous updates across prompts, hyperparameters, routing/model selection, and (where applicable) model parameters via RL</li> <li>Reported average relative improvement of 11.92% across five compound systems with theoretical guarantees and strong empirical results:</li> <li>Optimas site: optimas.stanford.edu</li> <li>Paper: arXiv: 2507.03041</li> </ul>"},{"location":"guides/optimas-integration/#where-optimas-fits-in-superoptix","title":"Where Optimas fits in SuperOptiX","text":"<p>Optimas integrates into the standard SuperOptiX lifecycle:</p> <ol> <li>Compile your agent for a specific target</li> <li>Evaluate to establish a baseline</li> <li>Optimize with Optimas (OPRO/MIPRO/COPRO) using the same CLI across targets</li> <li>Run the optimized agent</li> </ol> <p>This extends optimization beyond prompts to hyperparameters, model selection/routing, and parameters where supported.</p> <ul> <li>Focus\u2011aligned: SuperOptiX is built for optimization; Optimas operationalizes it across agents and tools</li> <li>Beyond prompts: optimize prompts, hyperparameters, parameters, and routers for production workflows</li> <li>One CLI to rule them all: compile \u2192 evaluate \u2192 optimize \u2192 run across all targets</li> </ul>"},{"location":"guides/optimas-integration/#optimas-vs-dspy-complementary","title":"Optimas vs. DSPy (complementary)","text":"<ul> <li>DSPy is a framework for composing LLM pipelines and programmatic teleprompting</li> <li>Optimas is an optimization engine that runs globally aligned local updates across multi\u2011component systems, regardless of the underlying framework (including DSPy)</li> <li>In practice: build in your preferred stack; use Optimas to optimize end\u2011to\u2011end. If using DSPy, try <code>--optimizer mipro</code> for deeper prompt refinement (OPRO and COPRO also supported)</li> </ul>"},{"location":"guides/optimas-integration/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/optimas-integration/#install-superoptix-with-optimas-support","title":"Install SuperOptiX with Optimas Support","text":"<pre><code># Install with Optimas support\npip install \"superoptix[optimas]\"\n\n# For OpenAI SDK target (recommended - most reliable)\npip install \"superoptix[optimas,optimas-openai]\"\n\n# For CrewAI target\npip install \"superoptix[optimas,optimas-crewai]\"\n\n# For AutoGen target  \npip install \"superoptix[optimas,optimas-autogen]\"\n\n# For DSPy target\npip install \"superoptix[optimas,optimas-dspy]\"\n</code></pre>"},{"location":"guides/optimas-integration/#install-additional-dependencies","title":"Install Additional Dependencies","text":"<pre><code># Required for DSPy 3.0.0 streaming support\npip install litellm\n\n# For CrewAI (install manually to avoid conflicts)\npip install crewai\npip install json-repair&gt;=0.30.0\n</code></pre>"},{"location":"guides/optimas-integration/#quick-demo","title":"Quick Demo","text":"<pre><code># Initialize project\nsuper init test_optimas\ncd test_optimas\n\n# Pull demo agents\nsuper agent pull optimas_openai      # OpenAI SDK (recommended)\nsuper agent pull optimas_crewai      # CrewAI\nsuper agent pull optimas_autogen     # AutoGen\nsuper agent pull optimas_dspy        # DSPy\n\n# Test compilation\nsuper agent compile optimas_openai --target optimas-openai\nsuper agent compile optimas_crewai --target optimas-crewai\nsuper agent compile optimas_autogen --target optimas-autogen\nsuper agent compile optimas_dspy --target optimas-dspy\n</code></pre>"},{"location":"guides/optimas-integration/#target-compatibility-matrix","title":"\ud83d\udcca Target Compatibility Matrix","text":"Target Compile Evaluate Optimize Run Status Notes OpenAI SDK Fully Working Most reliable, no threading issues CrewAI Fully Working Requires manual dependency installation AutoGen \u26a0\ufe0f Mostly Working Optimization works but can be slow DSPy Fully Working All optimizers now working properly"},{"location":"guides/optimas-integration/#environment-variables","title":"\ud83d\udd27 Environment Variables","text":""},{"location":"guides/optimas-integration/#opro-optimization-variables","title":"OPRO Optimization Variables","text":"<pre><code># Core OPRO settings\nSUPEROPTIX_OPRO_MAX_TOKENS=256          # Max tokens per prompt\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3        # Number of prompt candidates\nSUPEROPTIX_OPRO_MAX_WORKERS=3           # Max concurrent workers\nSUPEROPTIX_OPRO_TEMPERATURE=0.8         # Creativity level (0.0-1.0)\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=120     # Timeout in seconds\n</code></pre>"},{"location":"guides/optimas-integration/#mipro-optimization-variables","title":"MIPRO Optimization Variables","text":"<pre><code># MIPRO settings (for DSPy targets)\nSUPEROPTIX_MIPRO_NUM_CANDIDATES=3       # Number of candidates\nSUPEROPTIX_MIPRO_NUM_THREADS=3          # Number of threads\n</code></pre>"},{"location":"guides/optimas-integration/#copro-optimization-variables","title":"COPRO Optimization Variables","text":"<pre><code># COPRO settings (for DSPy targets)\nSUPEROPTIX_COPRO_BREADTH=3              # Search breadth\nSUPEROPTIX_COPRO_DEPTH=3                # Search depth\n</code></pre>"},{"location":"guides/optimas-integration/#litellm-configuration-variables","title":"LiteLLM Configuration Variables","text":"<pre><code># LiteLLM settings (affects DSPy and CrewAI)\nLITELLM_TIMEOUT=60                      # Request timeout\nLITELLM_MAX_RETRIES=3                   # Max retry attempts\nLITELLM_MAX_RESPONSE=4000               # Max response tokens\nLITELLM_CACHE_ENABLED=false             # Disable caching\nLITELLM_LOG_LEVEL=ERROR                 # Log level\n</code></pre>"},{"location":"guides/optimas-integration/#optimizer-options","title":"\ud83c\udfaf Optimizer Options","text":"<p>The <code>--optimizer</code> flag allows you to specify which optimization method to use:</p>"},{"location":"guides/optimas-integration/#available-optimizers","title":"Available Optimizers","text":"<ul> <li><code>--optimizer opro</code>: OPRO (Optimization by PROmpting) - Single-iteration optimization</li> <li><code>--optimizer mipro</code>: MIPRO (Multi-Iteration PROmpting) - Multi-iteration optimization  </li> <li><code>--optimizer copro</code>: COPRO (Cooperative PROmpting) - Cooperative optimization</li> </ul>"},{"location":"guides/optimas-integration/#optimizer-specific-environment-variables","title":"Optimizer-Specific Environment Variables","text":"<pre><code># OPRO settings (default)\nSUPEROPTIX_OPRO_MAX_TOKENS=256\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3\nSUPEROPTIX_OPRO_MAX_WORKERS=3\nSUPEROPTIX_OPRO_TEMPERATURE=0.8\n\n# MIPRO settings (for DSPy targets)\nSUPEROPTIX_MIPRO_NUM_CANDIDATES=3\nSUPEROPTIX_MIPRO_NUM_THREADS=3\n\n# COPRO settings (for DSPy targets)\nSUPEROPTIX_COPRO_BREADTH=3\nSUPEROPTIX_COPRO_DEPTH=3\n</code></pre>"},{"location":"guides/optimas-integration/#target-specific-configuration","title":"\ud83c\udfaf Target-Specific Configuration","text":"<p>Note: The playbook structure uses <code>tasks</code> instead of <code>components</code>. Each task defines the agent's capabilities and behavior.</p>"},{"location":"guides/optimas-integration/#openai-sdk-target-recommended","title":"OpenAI SDK Target (Recommended)","text":"<pre><code># optimas_openai_playbook.yaml\nname: optimas_openai\ndescription: OpenAI SDK integration with Optimas\nlanguage_model:\n  provider: ollama\n  model: llama3.2:1b\n  base_url: http://localhost:11434\n  api_key: \"\"\n\ntasks:\n  - name: implement_feature\n    instruction: &gt;-\n      You are a Software Developer. Your goal is to write clean, efficient, and\n      maintainable code. Implement the feature based on the provided requirement.\n    inputs:\n      - name: feature_requirement\n        type: str\n        required: true\n    outputs:\n      - name: implementation\n        type: str\n</code></pre> <p>Why OpenAI SDK is recommended: - Most reliable and stable - No threading issues - Fast optimization and execution - Works perfectly with all optimizers</p>"},{"location":"guides/optimas-integration/#crewai-target","title":"CrewAI Target","text":"<pre><code># optimas_crewai_playbook.yaml\nname: optimas_crewai\ndescription: CrewAI integration with Optimas\nlanguage_model:\n  provider: ollama\n  model: llama3.2:1b\n  base_url: http://localhost:11434\n  api_key: \"\"\n\ntasks:\n  - name: implement_feature\n    instruction: &gt;-\n      You are a Software Developer. Your goal is to write clean, efficient, and\n      maintainable code. Implement the feature based on the provided requirement.\n    inputs:\n      - name: feature_requirement\n        type: str\n        required: true\n    outputs:\n      - name: implementation\n        type: str\n</code></pre> <p>\u26a0\ufe0f CrewAI Dependencies: <pre><code># Install manually to avoid conflicts\npip install crewai\npip install json-repair&gt;=0.30.0\n</code></pre></p>"},{"location":"guides/optimas-integration/#autogen-target","title":"AutoGen Target","text":"<pre><code># optimas_autogen_playbook.yaml\nname: optimas_autogen\ndescription: AutoGen integration with Optimas\nlanguage_model:\n  provider: ollama\n  model: llama3.2:1b\n  base_url: http://localhost:11434\n  api_key: \"\"\n  model_info:\n    model_name: \"llama3.2:1b\"\n    max_tokens: 4096\n    temperature: 0.7\n    top_p: 0.9\n\ntasks:\n  - name: implement_feature\n    instruction: &gt;-\n      You are a Software Developer. Your goal is to write clean, efficient, and\n      maintainable code. Implement the feature based on the provided requirement.\n    inputs:\n      - name: feature_requirement\n        type: str\n        required: true\n    outputs:\n      - name: implementation\n        type: str\n</code></pre> <p>\u26a0\ufe0f AutoGen Notes: - Requires detailed <code>model_info</code> for non-OpenAI models - Optimization can be slow but works reliably - Best for complex multi-agent workflows</p>"},{"location":"guides/optimas-integration/#dspy-target","title":"DSPy Target","text":"<pre><code># optimas_dspy_playbook.yaml\nname: optimas_dspy\ndescription: DSPy integration with Optimas\nlanguage_model:\n  provider: ollama\n  model: llama3.2:1b\n  base_url: http://localhost:11434\n  api_key: \"\"\n\ntasks:\n  - name: implement_feature\n    instruction: &gt;-\n      You are a Software Developer. Your goal is to write clean, efficient, and\n      maintainable code. Implement the feature based on the provided requirement.\n    inputs:\n      - name: feature_requirement\n        type: str\n        required: true\n    outputs:\n      - name: implementation\n        type: str\n</code></pre> <p>DSPy Features: - All optimizers (OPRO, MIPRO, COPRO) now working properly - Excellent for research and production optimization - Fast optimization and execution - Great for prompt engineering workflows</p>"},{"location":"guides/optimas-integration/#complete-workflow-examples","title":"\ud83d\ude80 Complete Workflow Examples","text":""},{"location":"guides/optimas-integration/#openai-sdk-workflow-recommended","title":"OpenAI SDK Workflow (Recommended)","text":"<pre><code># Compile\nsuper agent compile optimas_openai --target optimas-openai\n\n# Evaluate\nsuper agent evaluate optimas_openai --engine optimas --target optimas-openai\n\n# Optimize with environment variables\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.8 \\\nsuper agent optimize optimas_openai --engine optimas --target optimas-openai --optimizer opro\n\n# Run\nsuper agent run optimas_openai --engine optimas --target optimas-openai --goal \"Write a Python function to add two numbers\"\n</code></pre>"},{"location":"guides/optimas-integration/#crewai-workflow","title":"CrewAI Workflow","text":"<pre><code># Compile\nsuper agent compile optimas_crewai --target optimas-crewai\n\n# Evaluate\nsuper agent evaluate optimas_crewai --engine optimas --target optimas-crewai\n\n# Optimize\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nsuper agent optimize optimas_crewai --engine optimas --target optimas-crewai --optimizer opro\n\n# Run\nsuper agent run optimas_crewai --engine optimas --target optimas-crewai --goal \"Write a Python function to calculate factorial\"\n</code></pre>"},{"location":"guides/optimas-integration/#autogen-workflow","title":"AutoGen Workflow","text":"<pre><code># Compile\nsuper agent compile optimas_autogen --target optimas-autogen\n\n# Evaluate\nsuper agent evaluate optimas_autogen --engine optimas --target optimas-autogen\n\n# Optimize (can be slow)\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=180 \\\nsuper agent optimize optimas_autogen --engine optimas --target optimas-autogen --optimizer opro\n\n# Run\nsuper agent run optimas_autogen --engine optimas --target optimas-autogen --goal \"Write a Python function to reverse a string\"\n</code></pre>"},{"location":"guides/optimas-integration/#dspy-workflow","title":"DSPy Workflow","text":"<pre><code># Compile\nsuper agent compile optimas_dspy --target optimas-dspy\n\n# Evaluate\nsuper agent evaluate optimas_dspy --engine optimas --target optimas-dspy\n\n# Optimize\nSUPEROPTIX_OPRO_MAX_TOKENS=256 \\\nSUPEROPTIX_OPRO_NUM_CANDIDATES=3 \\\nSUPEROPTIX_OPRO_MAX_WORKERS=3 \\\nSUPEROPTIX_OPRO_TEMPERATURE=0.8 \\\nsuper agent optimize optimas_dspy --engine optimas --target optimas-dspy --optimizer opro\n\n# Run\nsuper agent run optimas_dspy --engine optimas --target optimas-dspy --goal \"Write a Python function to calculate fibonacci numbers\"\n</code></pre>"},{"location":"guides/optimas-integration/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"guides/optimas-integration/#common-issues","title":"Common Issues","text":""},{"location":"guides/optimas-integration/#dspy-optimization-performance","title":"DSPy Optimization Performance","text":"<p>For Best Results: - Use appropriate optimization parameters for your use case - Monitor optimization progress and adjust parameters as needed - Consider using MIPRO or COPRO optimizers for specific DSPy workflows</p>"},{"location":"guides/optimas-integration/#crewai-dependency-conflicts","title":"CrewAI Dependency Conflicts","text":"<p>Symptoms: <pre><code>json-repair version conflicts\n</code></pre></p> <p>Solutions: <pre><code>pip install crewai --no-deps\npip install json-repair&gt;=0.30.0\n</code></pre></p>"},{"location":"guides/optimas-integration/#autogen-model-info-errors","title":"AutoGen Model Info Errors","text":"<p>Symptoms: <pre><code>model_info is required when model name is not a valid OpenAI model\n</code></pre></p> <p>Solutions: - Add detailed <code>model_info</code> section in playbook - Use OpenAI-compatible model names</p>"},{"location":"guides/optimas-integration/#optimization-timeouts","title":"Optimization Timeouts","text":"<p>Symptoms: <pre><code>OPRO timed out after 120s on component\n</code></pre></p> <p>Solutions: - Increase timeout: <code>SUPEROPTIX_OPRO_COMPILE_TIMEOUT=300</code> - Reduce model size or token limits - Use smaller optimization parameters</p>"},{"location":"guides/optimas-integration/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/optimas-integration/#for-fast-optimization","title":"For Fast Optimization:","text":"<pre><code>SUPEROPTIX_OPRO_MAX_TOKENS=128\nSUPEROPTIX_OPRO_NUM_CANDIDATES=2\nSUPEROPTIX_OPRO_MAX_WORKERS=2\nSUPEROPTIX_OPRO_TEMPERATURE=0.7\n</code></pre>"},{"location":"guides/optimas-integration/#for-high-quality-optimization","title":"For High-Quality Optimization:","text":"<pre><code>SUPEROPTIX_OPRO_MAX_TOKENS=512\nSUPEROPTIX_OPRO_NUM_CANDIDATES=5\nSUPEROPTIX_OPRO_MAX_WORKERS=4\nSUPEROPTIX_OPRO_TEMPERATURE=0.9\nSUPEROPTIX_OPRO_COMPILE_TIMEOUT=300\n</code></pre>"},{"location":"guides/optimas-integration/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"guides/optimas-integration/#target-selection","title":"Target Selection","text":"<ul> <li>Production: Use OpenAI SDK target (most reliable)</li> <li>Multi-agent: Use CrewAI or AutoGen targets</li> <li>Research &amp; Optimization: Use DSPy target (fully supported)</li> </ul>"},{"location":"guides/optimas-integration/#environment-variables_1","title":"Environment Variables","text":"<ul> <li>Set all relevant variables before running commands</li> <li>Use inline variable setting for reproducibility</li> <li>Monitor timeout values for large models</li> </ul>"},{"location":"guides/optimas-integration/#model-configuration","title":"Model Configuration","text":"<ul> <li>Use local models (Ollama) for development</li> <li>Ensure proper <code>model_info</code> for non-OpenAI models</li> <li>Test with smaller models first</li> </ul>"},{"location":"guides/optimas-integration/#optimization-strategy","title":"Optimization Strategy","text":"<ul> <li>Start with OPRO (most reliable)</li> <li>Use MIPRO/COPRO only with DSPy targets</li> <li>Monitor optimization progress and adjust parameters</li> </ul>"},{"location":"guides/optimas-integration/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<ul> <li>Optimas Examples - Working examples for all targets</li> <li>CLI Reference - Complete command reference</li> <li>Agent Development - Building custom agents</li> </ul>"},{"location":"guides/optimas-integration/#external-references","title":"\ud83d\udcd6 External References","text":"<ul> <li>Optimas website: optimas.stanford.edu</li> <li>Optimas paper (Wu et al., 2025): arXiv: 2507.03041</li> <li>DSPy: dspy.ai</li> <li>CrewAI: docs.crewai.com</li> <li>AutoGen: microsoft.github.io/autogen</li> <li>OpenAI Agent SDK: platform.openai.com/docs/agents</li> <li>LiteLLM: github.com/BerriAI/litellm</li> </ul>"},{"location":"guides/optimization/","title":"\ud83d\ude80 Agent Optimization Strategy","text":"<p>SuperOptiX provides universal optimization across all 6 major agent frameworks using GEPA (Genetic-Pareto) as the primary optimizer.</p> <p>\ud83c\udf1f Key Achievement: The world's first framework-agnostic optimizer that delivers proven results across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/optimization/#overview","title":"Overview","text":"<p>Agent optimization in SuperOptiX follows a GEPA-first strategy that works seamlessly across all supported frameworks:</p> <ul> <li>\ud83d\udd27 Universal Optimizer: Same GEPA optimizer works for all frameworks</li> <li>\ud83d\udcca Proven Results: DSPy 37.5% \u2192 80%, OpenAI/CrewAI 100% pass rates</li> <li>\u26a1 Sample Efficient: Achieves improvements with just 3-10 scenarios</li> <li>\ud83c\udfaf Framework Agnostic: Build with any framework, optimize with one tool</li> <li>\ud83d\udd04 Consistent Workflow: Same commands work regardless of framework</li> </ul>"},{"location":"guides/optimization/#gepa-the-universal-optimization-strategy","title":"\ud83c\udfaf GEPA: The Universal Optimization Strategy","text":""},{"location":"guides/optimization/#why-gepa-first","title":"Why GEPA-First?","text":"<p>GEPA (Genetic-Pareto) is SuperOptiX's universal optimizer that delivers consistent results across all frameworks:</p> Framework Variables Optimized Proven Results Status \ud83d\udd2c DSPy 10+ variables 37.5% \u2192 80% (+42.5 pts) Proven \ud83e\udd16 OpenAI SDK 1 variable (instructions) 100% pass rate Proven \ud83d\udc65 CrewAI 5 variables (role+goal+backstory+task) 100% pass rate Proven \ud83d\udd2e Google ADK 1 variable (instruction) Ready for optimization Ready \ud83c\udfe2 Microsoft 1 variable (instructions) Ready for optimization Ready \ud83c\udf0a DeepAgents 1 variable (system_prompt) Ready for optimization Ready"},{"location":"guides/optimization/#gepas-advantages","title":"GEPA's Advantages","text":"<p>\ud83d\udd27 Framework Agnostic: The ONLY optimizer that works across all major frameworks \ud83d\udcca Sample Efficiency: Achieves significant improvements with just 3-10 scenarios \ud83c\udfaf Domain Adaptable: Incorporates domain-specific feedback effectively \ud83d\udca1 Interpretable: Generates human-readable prompt improvements \ud83d\udd04 Multi-Objective: Optimizes for multiple criteria simultaneously</p>"},{"location":"guides/optimization/#universal-optimization-workflow","title":"\ud83d\ude80 Universal Optimization Workflow","text":""},{"location":"guides/optimization/#step-1-choose-your-framework-pull-agent","title":"Step 1: Choose Your Framework &amp; Pull Agent","text":"\ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>super agent pull sentiment_analyzer\nsuper agent compile sentiment_analyzer\nsuper agent evaluate sentiment_analyzer\n</code></pre> <pre><code>super agent pull assistant_openai\nsuper agent compile assistant_openai\nsuper agent evaluate assistant_openai\n</code></pre> <pre><code>super agent pull researcher_crew\nsuper agent compile researcher_crew\nsuper agent evaluate researcher_crew\n</code></pre> <pre><code>super agent pull assistant_adk\nsuper agent compile assistant_adk\nsuper agent evaluate assistant_adk\n</code></pre> <pre><code>super agent pull assistant_microsoft\nsuper agent compile assistant_microsoft\nsuper agent evaluate assistant_microsoft\n</code></pre> <pre><code>super agent pull research_agent_deepagents\nsuper agent compile research_agent_deepagents\nsuper agent evaluate research_agent_deepagents\n</code></pre>"},{"location":"guides/optimization/#step-2-optimize-with-gepa-same-command-for-all","title":"Step 2: Optimize with GEPA (Same Command for ALL!)","text":"<pre><code># Universal GEPA command - works on ANY framework!\nsuper agent optimize &lt;agent_name&gt; --auto medium\n\n# Examples for each framework:\nsuper agent optimize sentiment_analyzer --auto medium        # DSPy\nsuper agent optimize assistant_openai --auto medium          # OpenAI SDK\nsuper agent optimize researcher_crew --auto medium           # CrewAI\nsuper agent optimize assistant_adk --auto medium             # Google ADK\nsuper agent optimize assistant_microsoft --auto medium       # Microsoft\nsuper agent optimize research_agent_deepagents --auto medium # DeepAgents\n</code></pre>"},{"location":"guides/optimization/#step-3-evaluate-deploy","title":"Step 3: Evaluate &amp; Deploy","text":"<pre><code># Evaluate optimized version\nsuper agent evaluate &lt;agent_name&gt;  # automatically loads optimized weights\n\n# Run in production\nsuper agent run &lt;agent_name&gt;\n</code></pre>"},{"location":"guides/optimization/#gepa-configuration-options","title":"\u2699\ufe0f GEPA Configuration Options","text":""},{"location":"guides/optimization/#automatic-mode-recommended","title":"Automatic Mode (Recommended) \u2b50","text":"<pre><code># Works for ANY framework!\nsuper agent optimize &lt;agent_name&gt; --auto [light|medium|intensive]\n</code></pre> <p>Optimization Levels: - <code>light</code>: Quick optimization (2-3 iterations, ~5 minutes) - <code>medium</code>: Balanced optimization (5 iterations, ~10-15 minutes) \u2b50 Recommended - <code>intensive</code>: Thorough optimization (10+ iterations, ~30+ minutes)</p>"},{"location":"guides/optimization/#manual-configuration","title":"Manual Configuration","text":"<pre><code>super agent optimize &lt;agent_name&gt; --optimizer gepa --max-iterations 5\n</code></pre>"},{"location":"guides/optimization/#advanced-configuration-via-playbook","title":"Advanced Configuration (via playbook)","text":"<p>Works for all frameworks! Edit your agent playbook:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match    # Evaluation metric\n        auto: medium                  # Budget: light, medium, intensive\n        reflection_lm: qwen3:8b       # Model for reflection\n        reflection_minibatch_size: 3   # Examples per reflection\n        skip_perfect_score: true      # Skip if already perfect\n</code></pre>"},{"location":"guides/optimization/#optimization-results-by-framework","title":"\ud83d\udcca Optimization Results by Framework","text":""},{"location":"guides/optimization/#dspy-sentiment-analysis-agent","title":"DSPy: Sentiment Analysis Agent","text":"<p>Framework: DSPy (Stanford Research Framework) Variables Optimized: 10+ (signature instructions, field descriptions, reasoning steps, etc.)</p> <p>Before GEPA: <pre><code>Pass Rate: 37.5% (3/8 scenarios)\n</code></pre></p> <p>After GEPA Optimization (5 iterations, medium mode): <pre><code>Pass Rate: 80.0% (6.5/8 scenarios)\nImprovement: +42.5 percentage points \ud83c\udfc6\n</code></pre></p> <p>What GEPA Improved: - Better nuanced sentiment identification - Improved sarcasm and context handling - More accurate confidence scores - Clearer reasoning chains</p>"},{"location":"guides/optimization/#openai-sdk-ai-assistant","title":"OpenAI SDK: AI Assistant","text":"<p>Framework: OpenAI Agents SDK Variables Optimized: 1 (instructions)</p> <p>Before GEPA: <pre><code>Pass Rate: 100% (4/4 scenarios)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Pass Rate: 100% (4/4 scenarios)\nImprovement: Maintained excellence ```\n\n**What GEPA Improved**:\n- Enhanced response quality and clarity\n- Better instruction following\n- More consistent behavior patterns\n- Improved instruction structure\n\n---\n\n### **CrewAI: Research Crew (Phase 2)**\n\n**Framework**: CrewAI (Multi-Agent Collaboration)  \n**Variables Optimized**: 5 (role, goal, backstory, task description, expected output)\n\n**Before GEPA**:\n</code></pre> Pass Rate: 75% (\u00be scenarios) <pre><code>**After GEPA Optimization** (combined agent+task optimization):\n</code></pre> Pass Rate: 100% (4/4 scenarios) Improvement: +25 percentage points \u2b50 <pre><code>**What GEPA Improved**:\n- Clearer role definitions\n- Better goal alignment with tasks\n- Improved agent-task coordination\n- Enhanced task output quality\n- Better multi-agent collaboration patterns\n\n## \ud83c\udfaf When to Use Alternative Optimizers\n\nWhile GEPA is recommended for most cases, here are alternatives for specific scenarios:\n\n### **SIMBA** (DSPy Only)\n**Use When**: Large datasets, performance-critical applications\n```bash\nsuper agent optimize &lt;agent&gt; --optimizer simba\n</code></pre></p>"},{"location":"guides/optimization/#miprov2-dspy-only","title":"MIPROv2 (DSPy Only)","text":"<p>Use When: Instruction-following tasks, multi-step workflows <pre><code>super agent optimize &lt;agent&gt; --optimizer miprov2\n</code></pre></p>"},{"location":"guides/optimization/#bootstrapfewshot-dspy-only","title":"BootstrapFewShot (DSPy Only)","text":"<p>Use When: Getting started, limited training data <pre><code>super agent optimize &lt;agent&gt; --optimizer bootstrapfewshot\n</code></pre></p>"},{"location":"guides/optimization/#best-practices","title":"\ud83d\udcc8 Best Practices","text":""},{"location":"guides/optimization/#1-always-establish-baseline","title":"1. Always Establish Baseline","text":"<pre><code># CRITICAL: Always evaluate before optimizing\nsuper agent evaluate &lt;agent_name&gt;\n</code></pre>"},{"location":"guides/optimization/#2-start-with-medium-budget","title":"2. Start with Medium Budget","text":"<pre><code># Start with balanced optimization\nsuper agent optimize &lt;agent_name&gt; --auto medium\n\n# Increase if results justify cost\nsuper agent optimize &lt;agent_name&gt; --auto intensive\n</code></pre>"},{"location":"guides/optimization/#3-validate-improvements","title":"3. Validate Improvements","text":"<pre><code># Always re-evaluate after optimization\nsuper agent evaluate &lt;agent_name&gt;  # automatically loads optimized weights\n</code></pre>"},{"location":"guides/optimization/#4-use-quality-training-scenarios","title":"4. Use Quality Training Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: comprehensive_test\n      description: Cover main patterns and edge cases\n      input:\n        problem: \"Well-defined, realistic problem\"\n      expected_output:\n        answer: \"Complete expected response with reasoning\"\n</code></pre>"},{"location":"guides/optimization/#5-monitor-resource-usage","title":"5. Monitor Resource Usage","text":"<p>Memory Requirements: - GEPA: ~25GB peak (main model + reflection model) - Local Models: Ollama works great for free optimization - Cloud Models: Monitor API usage for cost control</p>"},{"location":"guides/optimization/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/optimization/#common-issues","title":"Common Issues","text":"<p>Issue: \"Optimization failed\" Solution: Check BDD scenarios are well-defined and evaluable</p> <p>Issue: \"Memory error\" Solution: Use <code>--auto light</code> or switch to smaller models</p> <p>Issue: \"No improvement after optimization\" Solution: Check evaluation metrics and scenario quality</p>"},{"location":"guides/optimization/#performance-tips","title":"Performance Tips","text":"<ul> <li>Local Models: Use Ollama for free, unlimited optimization</li> <li>Cloud Models: Start with <code>light</code> budget to test effectiveness</li> <li>Batch Processing: Run multiple agents in parallel for efficiency</li> </ul>"},{"location":"guides/optimization/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>After optimization:</p> <ol> <li>Deploy: Use <code>super agent run</code> with optimized weights</li> <li>Monitor: Track performance in production</li> <li>Iterate: Re-optimize when adding new scenarios</li> <li>Scale: Apply same workflow to other frameworks</li> </ol> <p>Learn More: - GEPA Optimization Guide - Detailed GEPA documentation - Multi-Framework Support - Framework comparisons - Evaluation &amp; Testing - Testing strategies</p>"},{"location":"guides/orchestra-development/","title":"\ud83c\udfbc Orchestra Development Guide","text":"<p>Master multi-agent orchestration with SuperOptiX orchestras</p>"},{"location":"guides/orchestra-development/#what-are-superoptix-orchestras","title":"\ud83c\udfaf What Are SuperOptiX Orchestras?","text":"<p>SuperOptiX orchestras coordinate multiple agents to accomplish complex goals through sequential execution. Unlike individual agents that work in isolation, orchestras enable agents to:</p> <ul> <li>Collaborate sequentially - Each agent builds upon the previous agent's output</li> <li>Share context - Results from one agent become input for the next</li> <li>Maintain state - Workspace directories store intermediate results</li> <li>Execute workflows - Complex multi-step processes with dependencies</li> </ul>"},{"location":"guides/orchestra-development/#execution-strategy-support","title":"\ud83d\udd04 Execution Strategy Support","text":"Tier Execution Strategy Parallel Tasks Advanced Techniques Oracles Sequential only 1 Basic orchestration Genies Sequential only 1 Basic orchestration Higher Tiers Parallel, Mixed, Hierarchical Multiple Kubernetes orchestration <p>Note: The current version (Oracles and Genies) supports sequential execution only. Advanced orchestration features are available in higher tiers.</p>"},{"location":"guides/orchestra-development/#orchestra-architecture","title":"\ud83c\udfd7\ufe0f Orchestra Architecture","text":""},{"location":"guides/orchestra-development/#core-components","title":"Core Components","text":"<pre><code>graph TD\n    A[Orchestra YAML] --&gt; B[Orchestra Runner]\n    B --&gt; C[Task Scheduler]\n    C --&gt; D[Agent 1]\n    D --&gt; E[Agent 2]\n    E --&gt; F[Agent 3]\n\n    G[Workspace] --&gt; B\n    H[Goal] --&gt; B\n\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style G fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style H fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/orchestra-development/#workspace-management","title":"Workspace Management","text":"<p>Orchestras use shared workspaces to maintain state between agents:</p> <pre><code>your-project/\n\u251c\u2500\u2500 orchestra_workspaces/\n\u2502   \u2514\u2500\u2500 your_orchestra/\n\u2502       \u251c\u2500\u2500 task_outputs/\n\u2502       \u2502   \u251c\u2500\u2500 agent1_output.txt\n\u2502       \u2502   \u251c\u2500\u2500 agent2_output.txt\n\u2502       \u2502   \u2514\u2500\u2500 agent3_output.txt\n\u2502       \u251c\u2500\u2500 intermediate/\n\u2502       \u2502   \u251c\u2500\u2500 data.json\n\u2502       \u2502   \u2514\u2500\u2500 artifacts/\n\u2502       \u2514\u2500\u2500 final_results/\n\u2502           \u2514\u2500\u2500 orchestra_output.txt\n\u2514\u2500\u2500 orchestras/\n    \u2514\u2500\u2500 your_orchestra.yaml\n</code></pre>"},{"location":"guides/orchestra-development/#creating-your-first-orchestra","title":"\ud83d\ude80 Creating Your First Orchestra","text":""},{"location":"guides/orchestra-development/#create-an-orchestra","title":"Create an Orchestra","text":"<pre><code># Create an orchestra with existing agents\nsuper orchestra create sdlc\n\n# This automatically:\n# - Detects existing agents in your project\n# - Loads tasks from agent playbooks\n# - Generates orchestra configuration\n# - Creates workspace directory\n</code></pre>"},{"location":"guides/orchestra-development/#orchestra-configuration","title":"Orchestra Configuration","text":"<p>The generated orchestra file (<code>sdlc_orchestra.yaml</code>):</p> <pre><code>metadata:\n  name: \"Sdlc Orchestra\"\n  id: \"sdlc\"\n  version: \"1.0.0\"\n  kind: \"basic\"\n  description: \"An orchestra to accomplish a specific goal with flexible execution strategies.\"\n\norchestra:\n  id: \"sdlc\"\n  name: \"Sdlc Orchestra\"\n  description: \"An orchestra to accomplish a specific goal with flexible execution strategies.\"\n\n  # Shared workspace for stateful collaboration\n  workspace:\n    type: local_fs \n    path: \"./orchestra_workspaces/sdlc\"\n\n  # Execution configuration (sequential only for current version)\n  execution:\n    strategy: \"sequential\"\n    max_parallel_tasks: 1\n    task_timeout_seconds: 300\n    retry_strategy: \"exponential_backoff\"\n    enable_metrics: true\n    enable_trace: true\n\n# Agents participating in the orchestra\nagents:\n  - developer\n  - devops_engineer\n  - qa_engineer\n\n# Task definitions with dependencies\ntasks:\n  - name: implement_feature\n    agent: developer\n    description: &gt;\n      Apply developer expertise to '{goal}'. Implement the feature based on the provided requirement\n    priority: \"medium\"\n    timeout_seconds: 600\n\n  - name: configure_ci_pipeline\n    agent: devops_engineer\n    description: &gt;\n      Apply devops engineer expertise to '{goal}'. Configure a basic CI/CD pipeline based on the project requirements\n    context: [\"implement_feature\"]  # Depends on previous task\n    priority: \"medium\"\n    timeout_seconds: 600\n\n  - name: create_test_plan\n    agent: qa_engineer\n    description: &gt;\n      Apply qa engineer expertise to '{goal}'. Create a high-level test plan including test cases for the given feature\n    context: [\"configure_ci_pipeline\"]  # Depends on previous task\n    priority: \"medium\"\n    timeout_seconds: 600\n</code></pre>"},{"location":"guides/orchestra-development/#run-the-orchestra","title":"Run the Orchestra","text":"<pre><code># Run with a specific goal\nsuper orchestra run sdlc --goal \"Build a simple login feature for a web application\"\n</code></pre>"},{"location":"guides/orchestra-development/#how-orchestras-work","title":"\ud83c\udfad How Orchestras Work","text":""},{"location":"guides/orchestra-development/#sequential-execution-flow","title":"Sequential Execution Flow","text":"<p>The orchestra runner executes tasks in sequence, passing results between agents:</p> <ol> <li>Task 1 executes with the initial goal</li> <li>Task 1 output becomes input for Task 2</li> <li>Task 2 output becomes input for Task 3</li> <li>This continues until all tasks are completed</li> <li>Final result is the output from the last task</li> </ol>"},{"location":"guides/orchestra-development/#task-execution-process","title":"Task Execution Process","text":"<ol> <li>Task Loading: Orchestra runner loads task definitions from YAML</li> <li>Dependency Resolution: Determines execution order based on <code>context</code> dependencies</li> <li>Agent Execution: Each task runs its assigned agent with current input</li> <li>Result Passing: Output from one task becomes input for the next</li> <li>Workspace Storage: Intermediate results are saved to workspace directory</li> </ol>"},{"location":"guides/orchestra-development/#workspace-integration","title":"Workspace Integration","text":"<p>The orchestra runner automatically manages workspace directories:</p> <ul> <li>Creates workspace when orchestra starts</li> <li>Saves task outputs to individual files</li> <li>Maintains state between task executions</li> <li>Provides access to intermediate results</li> </ul>"},{"location":"guides/orchestra-development/#orchestra-examples","title":"\ud83d\udccb Orchestra Examples","text":""},{"location":"guides/orchestra-development/#example-1-software-development-lifecycle","title":"Example 1: Software Development Lifecycle","text":"<pre><code># sdlc_orchestra.yaml\nmetadata:\n  name: \"Software Development Lifecycle\"\n  id: \"sdlc\"\n\norchestra:\n  workspace:\n    type: local_fs \n    path: \"./orchestra_workspaces/sdlc\"\n  execution:\n    strategy: \"sequential\"\n    max_parallel_tasks: 1\n\nagents:\n  - developer\n  - devops_engineer\n  - qa_engineer\n\ntasks:\n  - name: implement_feature\n    agent: developer\n    description: \"Implement the feature based on requirements\"\n\n  - name: configure_ci_pipeline\n    agent: devops_engineer\n    description: \"Set up CI/CD pipeline for the implemented feature\"\n    context: [\"implement_feature\"]\n\n  - name: create_test_plan\n    agent: qa_engineer\n    description: \"Create comprehensive test plan for the feature\"\n    context: [\"configure_ci_pipeline\"]\n</code></pre> <p>Usage: <pre><code>super orchestra run sdlc --goal \"Build a user authentication system\"\n</code></pre></p>"},{"location":"guides/orchestra-development/#example-2-content-creation-pipeline","title":"Example 2: Content Creation Pipeline","text":"<pre><code># content_creation_orchestra.yaml\nmetadata:\n  name: \"Content Creation Pipeline\"\n  id: \"content_creation\"\n\norchestra:\n  workspace:\n    type: local_fs \n    path: \"./orchestra_workspaces/content_creation\"\n  execution:\n    strategy: \"sequential\"\n    max_parallel_tasks: 1\n\nagents:\n  - researcher\n  - writer\n  - editor\n\ntasks:\n  - name: research_topic\n    agent: researcher\n    description: \"Research the given topic thoroughly\"\n\n  - name: write_content\n    agent: writer\n    description: \"Write comprehensive content based on research\"\n    context: [\"research_topic\"]\n\n  - name: edit_and_polish\n    agent: editor\n    description: \"Edit and polish the written content\"\n    context: [\"write_content\"]\n</code></pre> <p>Usage: <pre><code>super orchestra run content_creation --goal \"Create a comprehensive guide on machine learning\"\n</code></pre></p>"},{"location":"guides/orchestra-development/#example-3-data-analysis-workflow","title":"Example 3: Data Analysis Workflow","text":"<pre><code># data_analysis_orchestra.yaml\nmetadata:\n  name: \"Data Analysis Workflow\"\n  id: \"data_analysis\"\n\norchestra:\n  workspace:\n    type: local_fs \n    path: \"./orchestra_workspaces/data_analysis\"\n  execution:\n    strategy: \"sequential\"\n    max_parallel_tasks: 1\n\nagents:\n  - data_collector\n  - data_analyzer\n  - report_generator\n\ntasks:\n  - name: collect_data\n    agent: data_collector\n    description: \"Collect and prepare data for analysis\"\n\n  - name: analyze_data\n    agent: data_analyzer\n    description: \"Perform statistical analysis on the collected data\"\n    context: [\"collect_data\"]\n\n  - name: generate_report\n    agent: report_generator\n    description: \"Generate comprehensive report with insights\"\n    context: [\"analyze_data\"]\n</code></pre> <p>Usage: <pre><code>super orchestra run data_analysis --goal \"Analyze customer satisfaction survey data\"\n</code></pre></p>"},{"location":"guides/orchestra-development/#customizing-orchestras","title":"\ud83d\udd27 Customizing Orchestras","text":""},{"location":"guides/orchestra-development/#modify-task-descriptions","title":"Modify Task Descriptions","text":"<p>Make task descriptions more specific to your goal:</p> <pre><code>tasks:\n  - name: implement_feature\n    agent: developer\n    description: &gt;\n      Implement a secure user authentication system with the following requirements:\n      - User registration with email verification\n      - Secure password hashing\n      - JWT token-based authentication\n      - Password reset functionality\n      Use modern security best practices and provide clear documentation.\n</code></pre>"},{"location":"guides/orchestra-development/#adjust-task-priorities","title":"Adjust Task Priorities","text":"<p>Set task priorities to control execution order:</p> <pre><code>tasks:\n  - name: critical_feature\n    agent: developer\n    priority: \"high\"\n    description: \"Implement critical security feature\"\n\n  - name: nice_to_have\n    agent: developer\n    priority: \"low\"\n    description: \"Add optional UI enhancements\"\n</code></pre>"},{"location":"guides/orchestra-development/#configure-timeouts","title":"Configure Timeouts","text":"<p>Set appropriate timeouts for different task types:</p> <pre><code>tasks:\n  - name: quick_validation\n    agent: validator\n    timeout_seconds: 60\n    description: \"Quick validation check\"\n\n  - name: complex_analysis\n    agent: analyzer\n    timeout_seconds: 1800  # 30 minutes\n    description: \"Complex data analysis\"\n</code></pre>"},{"location":"guides/orchestra-development/#workspace-management_1","title":"\ud83d\udcc1 Workspace Management","text":""},{"location":"guides/orchestra-development/#workspace-structure","title":"Workspace Structure","text":"<p>Orchestra workspaces are automatically created and managed:</p> <pre><code>orchestra_workspaces/\n\u2514\u2500\u2500 your_orchestra/\n    \u251c\u2500\u2500 task_outputs/           # Individual task results\n    \u2502   \u251c\u2500\u2500 task1_output.txt\n    \u2502   \u251c\u2500\u2500 task2_output.txt\n    \u2502   \u2514\u2500\u2500 task3_output.txt\n    \u251c\u2500\u2500 intermediate/           # Shared data between tasks\n    \u2502   \u251c\u2500\u2500 shared_data.json\n    \u2502   \u2514\u2500\u2500 artifacts/\n    \u2514\u2500\u2500 final_results/          # Final orchestra output\n        \u2514\u2500\u2500 orchestra_output.txt\n</code></pre>"},{"location":"guides/orchestra-development/#accessing-workspace-data","title":"Accessing Workspace Data","text":"<pre><code># In your custom tools or agents\nimport json\nfrom pathlib import Path\n\ndef read_workspace_data(orchestra_name: str, filename: str):\n    \"\"\"Read data from orchestra workspace.\"\"\"\n    workspace_path = Path(f\"./orchestra_workspaces/{orchestra_name}\")\n    file_path = workspace_path / \"intermediate\" / filename\n\n    if file_path.exists():\n        with open(file_path, 'r') as f:\n            return json.load(f)\n    return None\n\ndef write_workspace_data(orchestra_name: str, filename: str, data: dict):\n    \"\"\"Write data to orchestra workspace.\"\"\"\n    workspace_path = Path(f\"./orchestra_workspaces/{orchestra_name}\")\n    intermediate_path = workspace_path / \"intermediate\"\n    intermediate_path.mkdir(parents=True, exist_ok=True)\n\n    file_path = intermediate_path / filename\n    with open(file_path, 'w') as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"guides/orchestra-development/#workspace-cleanup","title":"Workspace Cleanup","text":"<pre><code># Clean up workspace after orchestra completion\nrm -rf orchestra_workspaces/your_orchestra/\n\n# Or keep for analysis\nls -la orchestra_workspaces/your_orchestra/\n</code></pre>"},{"location":"guides/orchestra-development/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/orchestra-development/#agent-preparation","title":"Agent Preparation","text":"<p>Before running orchestras, ensure your agents are optimized:</p> <pre><code># Compile all agents\nsuper agent compile --all\n\n# Evaluate baseline performance\nsuper agent evaluate developer\nsuper agent evaluate devops_engineer\nsuper agent evaluate qa_engineer\n\n# Optimize agents\nsuper agent optimize developer\nsuper agent optimize devops_engineer\nsuper agent optimize qa_engineer\n\n# Re-evaluate after optimization\nsuper agent evaluate developer\nsuper agent evaluate devops_engineer\nsuper agent evaluate qa_engineer\n</code></pre>"},{"location":"guides/orchestra-development/#goal-definition","title":"Goal Definition","text":"<p>Write clear, specific goals for better orchestration results:</p> <pre><code># Good: Specific and actionable\nsuper orchestra run sdlc --goal \"Build a secure user authentication system with JWT tokens, password hashing, and email verification\"\n\n# Avoid: Too vague\nsuper orchestra run sdlc --goal \"Make a login system\"\n</code></pre>"},{"location":"guides/orchestra-development/#task-dependencies","title":"Task Dependencies","text":"<p>Design logical task dependencies:</p> <pre><code>tasks:\n  - name: gather_requirements\n    agent: analyst\n    description: \"Gather and document requirements\"\n\n  - name: design_architecture\n    agent: architect\n    description: \"Design system architecture\"\n    context: [\"gather_requirements\"]  # Depends on requirements\n\n  - name: implement_system\n    agent: developer\n    description: \"Implement the system\"\n    context: [\"design_architecture\"]  # Depends on architecture\n</code></pre>"},{"location":"guides/orchestra-development/#error-handling","title":"Error Handling","text":"<p>Monitor orchestra execution and handle failures:</p> <pre><code># Run with verbose output for debugging\nsuper orchestra run sdlc --goal \"your goal\" --verbose\n\n# Check workspace for partial results\nls -la orchestra_workspaces/sdlc/task_outputs/\n</code></pre>"},{"location":"guides/orchestra-development/#monitoring-and-debugging","title":"\ud83d\udd0d Monitoring and Debugging","text":""},{"location":"guides/orchestra-development/#execution-monitoring","title":"Execution Monitoring","text":"<p>The orchestra runner provides detailed execution information:</p> <pre><code>super orchestra run sdlc --goal \"your goal\" --verbose\n</code></pre> <p>Output includes: - Task execution order - Execution times - Success/failure status - Workspace location - Tier validation results</p>"},{"location":"guides/orchestra-development/#workspace-analysis","title":"Workspace Analysis","text":"<p>After orchestra completion, analyze the workspace:</p> <pre><code># View task outputs\ncat orchestra_workspaces/sdlc/task_outputs/implement_feature_output.txt\ncat orchestra_workspaces/sdlc/task_outputs/configure_ci_pipeline_output.txt\ncat orchestra_workspaces/sdlc/task_outputs/create_test_plan_output.txt\n\n# View final results\ncat orchestra_workspaces/sdlc/final_results/orchestra_output.txt\n</code></pre>"},{"location":"guides/orchestra-development/#debugging-failed-orchestras","title":"Debugging Failed Orchestras","text":"<pre><code># Check which task failed\nls -la orchestra_workspaces/sdlc/task_outputs/\n\n# View error logs\ncat orchestra_workspaces/sdlc/task_outputs/failed_task_output.txt\n\n# Re-run with dry-run to validate configuration\nsuper orchestra run sdlc --goal \"your goal\" --dry-run\n</code></pre>"},{"location":"guides/orchestra-development/#advanced-techniques-higher-tiers","title":"\ud83d\ude80 Advanced Techniques (Higher Tiers)","text":"<p>Note: These features are available in higher tiers beyond Oracles and Genies.</p>"},{"location":"guides/orchestra-development/#parallel-execution","title":"Parallel Execution","text":"<pre><code>execution:\n  strategy: \"parallel\"\n  max_parallel_tasks: 3\n</code></pre>"},{"location":"guides/orchestra-development/#mixed-strategy","title":"Mixed Strategy","text":"<pre><code>execution:\n  strategy: \"mixed\"\n  task_groups:\n    - name: \"research_phase\"\n      tasks: [\"gather_data\", \"analyze_requirements\"]\n      execution_strategy: \"parallel\"\n    - name: \"development_phase\"\n      tasks: [\"implement\", \"test\"]\n      execution_strategy: \"sequential\"\n</code></pre>"},{"location":"guides/orchestra-development/#kubernetes-orchestration","title":"Kubernetes Orchestration","text":"<pre><code>orchestration_type: \"kubernetes\"\nreplicas: 3\nresources:\n  cpu: \"1000m\"\n  memory: \"2Gi\"\n</code></pre>"},{"location":"guides/orchestra-development/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Create your first orchestra: <code>super orchestra create my_orchestra</code></li> <li>Customize the configuration: Edit the generated YAML file</li> <li>Run with a specific goal: <code>super orchestra run my_orchestra --goal \"your goal\"</code></li> <li>Analyze results: Check the workspace directory</li> <li>Optimize agents: Improve individual agent performance</li> <li>Iterate and improve: Refine orchestra configuration based on results</li> </ol>"},{"location":"guides/orchestra-development/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Agent Development Guide - Learn how to create agents for orchestration</li> <li>Tool Development - Create tools that work with orchestras</li> <li>SuperSpec DSL Guide - Configure agents for orchestration</li> <li>CLI Reference - Orchestra command reference</li> <li>Quick Start Guide - Complete orchestra example</li> </ul> <p>Ready to orchestrate multiple agents for complex goals? Start with sequential execution and build powerful multi-agent workflows! \ud83c\udfbc </p>"},{"location":"guides/protocol-first-agents/","title":"Protocol-First Agents in SuperOptiX","text":""},{"location":"guides/protocol-first-agents/#introduction","title":"\ud83d\ude80 Introduction","text":"<p>SuperOptiX supports two approaches for building agents:</p> <ol> <li>Tool-First (Traditional): Manually load and configure tools</li> <li>Protocol-First (New!): Automatic tool discovery via protocols</li> </ol> <p>This guide covers the protocol-first approach, powered by vendored Agenspy components.</p>"},{"location":"guides/protocol-first-agents/#what-is-protocol-first","title":"\ud83c\udfaf What is Protocol-First?","text":"<p>Protocol-first agents treat communication protocols (like MCP) as first-class primitives. Instead of manually loading tools, agents connect to protocol servers and automatically discover available tools.</p>"},{"location":"guides/protocol-first-agents/#traditional-tool-first","title":"Traditional Tool-First","text":"<pre><code># Manual tool loading\ntools = [calculator, search, github_api]\nagent = MyAgent(tools=tools)\n</code></pre>"},{"location":"guides/protocol-first-agents/#protocol-first","title":"Protocol-First","text":"<pre><code># Automatic tool discovery\nagent = MyAgent(mcp_servers=['mcp://localhost:8080'])\n# Tools discovered automatically!\n</code></pre>"},{"location":"guides/protocol-first-agents/#key-benefits","title":"\u2728 Key Benefits","text":""},{"location":"guides/protocol-first-agents/#zero-configuration","title":"Zero Configuration","text":"<ul> <li>No manual tool loading</li> <li>No tool registry management</li> <li>Tools appear automatically when servers add them</li> </ul>"},{"location":"guides/protocol-first-agents/#dynamic-discovery","title":"Dynamic Discovery","text":"<ul> <li>New tools available instantly</li> <li>No agent recompilation needed</li> <li>Protocol handles versioning</li> </ul>"},{"location":"guides/protocol-first-agents/#protocol-level-optimization","title":"Protocol-Level Optimization","text":"<ul> <li>GEPA can optimize at protocol level</li> <li>Session management built-in</li> <li>Future-ready for Agent2Agent protocol</li> </ul>"},{"location":"guides/protocol-first-agents/#better-maintainability","title":"Better Maintainability","text":"<ul> <li>Less code to maintain</li> <li>No tool version conflicts</li> <li>Protocol handles compatibility</li> </ul>"},{"location":"guides/protocol-first-agents/#quick-start","title":"\ud83d\udccb Quick Start","text":""},{"location":"guides/protocol-first-agents/#step-1-create-a-protocol-first-playbook","title":"Step 1: Create a Protocol-First Playbook","text":"<p>Add two fields to your playbook's <code>spec</code> section:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\n\nmetadata:\n  name: \"My Protocol Agent\"\n  id: \"my_protocol_agent\"\n  version: \"1.0.0\"\n  level: \"genies\"  # Protocol-first requires genies tier\n\nspec:\n  # NEW: Enable protocol-first mode\n  tool_backend: \"agenspy\"\n\n  # NEW: List MCP server URIs\n  mcp_servers:\n    - \"mcp://localhost:8080/math\"\n    - \"mcp://localhost:8080/github\"\n\n  # Rest of your spec...\n  language_model:\n    provider: \"openai\"\n    model: \"gpt-4\"\n    temperature: 0.7\n\n  persona:\n    role: \"Research Assistant\"\n    goal: \"Help with research and analysis\"\n</code></pre>"},{"location":"guides/protocol-first-agents/#step-2-compile-the-agent","title":"Step 2: Compile the Agent","text":"<pre><code>super agent compile my_protocol_agent\n</code></pre> <p>Output: <pre><code>\ud83e\udd16 Generating Protocol-First Genies-Tier pipeline (Agenspy - Automatic tool discovery via MCP)...\n\ud83d\udd0c Protocol-First Approach: Automatic tool discovery from MCP servers\n\ud83e\udd16 Agenspy Integration: Vendored protocol-first components\n\ud83d\udee0\ufe0f  Auto Tool Discovery: No manual tool loading or registration\n\ud83c\udfaf Key Differentiator: Protocol-level optimization + session management\n\ud83d\udce1 MCP Servers: 2 configured (mcp://localhost:8080/math, mcp://localhost:8080/github)\nSuccessfully generated Genies-tier pipeline (protocol-first/agenspy)\n</code></pre></p>"},{"location":"guides/protocol-first-agents/#step-3-run-the-agent","title":"Step 3: Run the Agent","text":"<pre><code>super agent run my_protocol_agent --goal \"Calculate 25 * 17 and search GitHub for SuperOptiX\"\n</code></pre> <p>Behind the Scenes: 1. Agent connects to MCP servers 2. Tools discovered automatically (calculator, github_search, etc.) 3. Agent uses ReAct with protocol-discovered tools 4. Results include protocol metadata</p>"},{"location":"guides/protocol-first-agents/#configuration-options","title":"\ud83d\udd27 Configuration Options","text":""},{"location":"guides/protocol-first-agents/#playbook-schema","title":"Playbook Schema","text":"<pre><code>spec:\n  # Required: Choose backend\n  tool_backend: \"agenspy\"  # or \"dspy\" for tool-first\n\n  # Required for protocol-first: MCP servers\n  mcp_servers:\n    - \"mcp://localhost:8080/math\"\n    - \"mcp://localhost:8080/github\"\n    - \"mcp://localhost:8080/code\"\n\n  # Optional: Protocol settings in config section\nconfig:\n  protocol_timeout: 30\n  auto_reconnect: true\n  max_tool_retries: 3\n  cache_protocol_capabilities: true\n</code></pre>"},{"location":"guides/protocol-first-agents/#auto-detection","title":"Auto-Detection","text":"<p>If you provide <code>mcp_servers</code> but omit <code>tool_backend</code>, SuperOptiX automatically uses protocol-first:</p> <pre><code>spec:\n  # tool_backend auto-detected as \"agenspy\"\n  mcp_servers:\n    - \"mcp://localhost:8080/math\"\n</code></pre>"},{"location":"guides/protocol-first-agents/#complete-example","title":"\ud83c\udf93 Complete Example","text":"<p>See <code>examples/protocol_agent_basic.yaml</code> for a full example:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\n\nmetadata:\n  name: \"GitHub Research Assistant\"\n  id: \"github_research_assistant\"\n  level: \"genies\"\n\nspec:\n  tool_backend: \"agenspy\"\n\n  mcp_servers:\n    - \"mcp://localhost:8080/github\"\n    - \"mcp://localhost:8080/code\"\n\n  language_model:\n    provider: \"openai\"\n    model: \"gpt-4\"\n    temperature: 0.7\n\n  persona:\n    role: \"GitHub analyst\"\n    goal: \"Analyze repositories and code\"\n\n  reasoning:\n    method: \"react\"\n    max_iterations: 5\n\n  input_fields:\n    - name: \"query\"\n      description: \"Research query\"\n\n  output_fields:\n    - name: \"analysis\"\n      description: \"Analysis results\"\n</code></pre>"},{"location":"guides/protocol-first-agents/#how-it-works","title":"\ud83d\udd0d How It Works","text":""},{"location":"guides/protocol-first-agents/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Protocol-First Agent (ProtocolAgent)   \u2502\n\u2502                                         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 DSPy ReAct Module                  \u2502 \u2502\n\u2502 \u2502 (with protocol-discovered tools)   \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                         \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Protocol Registry                  \u2502 \u2502\n\u2502 \u2502  \u2022 MCP Client 1 (math)             \u2502 \u2502\n\u2502 \u2502  \u2022 MCP Client 2 (github)           \u2502 \u2502\n\u2502 \u2502  \u2022 MCP Client 3 (code)             \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 MCP Servers                             \u2502\n\u2502  \u2022 mcp://localhost:8080/math            \u2502\n\u2502  \u2022 mcp://localhost:8080/github          \u2502\n\u2502  \u2022 mcp://localhost:8080/code            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/protocol-first-agents/#execution-flow","title":"Execution Flow","text":"<ol> <li>Initialization</li> <li>Agent connects to MCP servers</li> <li>Tools discovered automatically</li> <li> <p>Protocol registry tracks connections</p> </li> <li> <p>Query Processing</p> </li> <li>User provides query</li> <li>ReAct decides which tools to use</li> <li> <p>Tools execute via protocol sessions</p> </li> <li> <p>Tool Execution</p> </li> <li>Tool call routed to appropriate MCP server</li> <li>Server executes and returns result</li> <li> <p>Result formatted for agent</p> </li> <li> <p>Response Generation</p> </li> <li>Agent synthesizes tool outputs</li> <li>Final response returned</li> <li>Protocol metadata included</li> </ol>"},{"location":"guides/protocol-first-agents/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/protocol-first-agents/#mcp-server-uris","title":"MCP Server URIs","text":"<pre><code># Good: Descriptive server names\nmcp_servers:\n  - \"mcp://localhost:8080/math\"\n  - \"mcp://localhost:8080/github\"\n\n# Avoid: Generic names\nmcp_servers:\n  - \"mcp://localhost:8080\"\n</code></pre>"},{"location":"guides/protocol-first-agents/#tier-selection","title":"Tier Selection","text":"<pre><code># Protocol-first requires genies tier\nmetadata:\n  level: \"genies\"\n\nspec:\n  tool_backend: \"agenspy\"\n</code></pre>"},{"location":"guides/protocol-first-agents/#reasoning-method","title":"Reasoning Method","text":"<pre><code># Use ReAct for tool-using agents\nreasoning:\n  method: \"react\"\n  max_iterations: 5\n</code></pre>"},{"location":"guides/protocol-first-agents/#error-handling","title":"Error Handling","text":"<pre><code># Configure retries and timeouts\nconfig:\n  protocol_timeout: 30\n  max_tool_retries: 3\n  auto_reconnect: true\n</code></pre>"},{"location":"guides/protocol-first-agents/#migration-guide","title":"\ud83d\udd04 Migration Guide","text":""},{"location":"guides/protocol-first-agents/#from-tool-first-to-protocol-first","title":"From Tool-First to Protocol-First","text":"<p>Before (Tool-First): <pre><code>spec:\n  tools:\n    enabled: true\n    categories:\n      - \"core\"\n      - \"finance\"\n    specific_tools:\n      - \"calculator\"\n      - \"web_search\"\n</code></pre></p> <p>After (Protocol-First): <pre><code>spec:\n  tool_backend: \"agenspy\"\n  mcp_servers:\n    - \"mcp://localhost:8080/math\"\n    - \"mcp://localhost:8080/web\"\n</code></pre></p> <p>Key Changes: 1. Replace <code>tools</code> section with <code>mcp_servers</code> 2. Add <code>tool_backend: \"agenspy\"</code> 3. Ensure <code>level: \"genies\"</code> 4. Recompile agent</p>"},{"location":"guides/protocol-first-agents/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"guides/protocol-first-agents/#agent-not-using-protocol-first","title":"Agent Not Using Protocol-First","text":"<p>Problem: Agent compiled as tool-first despite configuration</p> <p>Solution: Check that: - <code>tool_backend: \"agenspy\"</code> is set OR - <code>mcp_servers</code> list is not empty - Agent is genies tier (<code>level: \"genies\"</code>)</p>"},{"location":"guides/protocol-first-agents/#no-tools-discovered","title":"No Tools Discovered","text":"<p>Problem: Agent reports 0 tools discovered</p> <p>Solution: - Verify MCP servers are running - Check server URIs are correct - Check network connectivity - Review logs for connection errors</p>"},{"location":"guides/protocol-first-agents/#protocol-connection-failed","title":"Protocol Connection Failed","text":"<p>Problem: \"Failed to connect to MCP server\"</p> <p>Solution: - Verify server URI format: <code>mcp://host:port/path</code> - Check server is running and accessible - Increase timeout: <code>config.protocol_timeout: 60</code> - Enable auto-reconnect: <code>config.auto_reconnect: true</code></p>"},{"location":"guides/protocol-first-agents/#api-reference","title":"\ud83d\udcda API Reference","text":""},{"location":"guides/protocol-first-agents/#protocolagent-class","title":"ProtocolAgent Class","text":"<pre><code>from superoptix.agent_bases import ProtocolAgent\n\nclass MyAgent(ProtocolAgent):\n    def __init__(self, mcp_servers: List[str]):\n        super().__init__(agent_id=\"my_agent\")\n\n        # Protocols added automatically\n        # Tools discovered automatically\n\n    def forward(self, query: str):\n        # Execute with protocol tools\n        return self.react(--goal query)\n</code></pre>"},{"location":"guides/protocol-first-agents/#protocol-registry","title":"Protocol Registry","text":"<pre><code>from superoptix.protocols import registry, ProtocolType\n\n# Create protocol\nprotocol = registry.create_protocol(\n    ProtocolType.MCP,\n    server_url=\"mcp://localhost:8080/math\"\n)\n\n# Connect\nprotocol.connect()\n\n# Get capabilities\ncaps = protocol.get_capabilities()\nprint(f\"Tools: {caps['tools']}\")\n</code></pre>"},{"location":"guides/protocol-first-agents/#mcp-client","title":"MCP Client","text":"<pre><code>from superoptix.protocols.mcp import MCPClient\n\n# Create client\nclient = MCPClient(server_url=\"mcp://localhost:8080/math\")\n\n# Connect and discover tools\nif client.connect():\n    print(f\"Connected! Tools: {list(client.available_tools.keys())}\")\n</code></pre>"},{"location":"guides/protocol-first-agents/#advanced-usage","title":"\ud83d\ude80 Advanced Usage","text":""},{"location":"guides/protocol-first-agents/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<pre><code>spec:\n  mcp_servers:\n    - \"mcp://localhost:8080/math\"      # Math tools\n    - \"mcp://localhost:8080/github\"    # GitHub API\n    - \"mcp://localhost:8080/code\"      # Code analysis\n    - \"mcp://localhost:8080/web\"       # Web search\n</code></pre>"},{"location":"guides/protocol-first-agents/#protocol-rag","title":"Protocol + RAG","text":"<pre><code>spec:\n  tool_backend: \"agenspy\"\n  mcp_servers:\n    - \"mcp://localhost:8080/github\"\n\n  rag:\n    enabled: true\n    collection: \"github_docs\"\n    knowledge_base:\n      - \"docs/github_api.md\"\n    top_k: 5\n</code></pre>"},{"location":"guides/protocol-first-agents/#gepa-optimization","title":"GEPA Optimization","text":"<p>Protocol-first agents are fully compatible with GEPA:</p> <pre><code># Compile\nsuper agent compile my_protocol_agent\n\n# Evaluate\nsuper agent evaluate my_protocol_agent\n\n# Optimize with GEPA\nsuper agent optimize my_protocol_agent\n\n# Re-evaluate\nsuper agent evaluate my_protocol_agent\n</code></pre>"},{"location":"guides/protocol-first-agents/#faq","title":"\ud83c\udf93 FAQ","text":""},{"location":"guides/protocol-first-agents/#q-do-i-need-real-mcp-servers","title":"Q: Do I need real MCP servers?","text":"<p>A: No! SuperOptiX includes mock MCP servers for development and testing.</p>"},{"location":"guides/protocol-first-agents/#q-can-i-mix-tool-first-and-protocol-first","title":"Q: Can I mix tool-first and protocol-first?","text":"<p>A: Not in the same agent. Choose one approach per agent. (Hybrid mode planned for v2.0)</p>"},{"location":"guides/protocol-first-agents/#q-is-protocol-first-slower-than-tool-first","title":"Q: Is protocol-first slower than tool-first?","text":"<p>A: No! Mock MCP has similar performance. Real MCP depends on server latency.</p>"},{"location":"guides/protocol-first-agents/#q-does-gepa-work-with-protocol-first","title":"Q: Does GEPA work with protocol-first?","text":"<p>A: Yes! Protocol-first agents are fully compatible with GEPA optimization.</p>"},{"location":"guides/protocol-first-agents/#q-can-i-use-custom-protocols","title":"Q: Can I use custom protocols?","text":"<p>A: Currently only MCP is supported. Custom protocols planned for v2.0.</p>"},{"location":"guides/protocol-first-agents/#future-enhancements","title":"\ud83d\udd2e Future Enhancements","text":""},{"location":"guides/protocol-first-agents/#coming-soon","title":"Coming Soon","text":"<ul> <li>RealMCPClient (background MCP servers)</li> <li>Additional mock tools</li> <li>Protocol-level GEPA optimization</li> </ul>"},{"location":"guides/protocol-first-agents/#roadmap","title":"Roadmap","text":"<ul> <li>Agent2Agent protocol support</li> <li>Hybrid mode (tool-first + protocol-first)</li> <li>Custom protocol implementations</li> <li>Protocol marketplace</li> </ul>"},{"location":"guides/protocol-first-agents/#key-takeaways","title":"\ud83c\udf1f Key Takeaways","text":"<ol> <li>Protocol-first = Automatic tool discovery</li> <li>No manual tool loading</li> <li> <p>Tools update dynamically</p> </li> <li> <p>Two new fields in playbook</p> </li> <li><code>tool_backend: \"agenspy\"</code></li> <li> <p><code>mcp_servers: [...]</code></p> </li> <li> <p>Fully backward compatible</p> </li> <li>Tool-first still works</li> <li> <p>Choose per agent</p> </li> <li> <p>GEPA compatible</p> </li> <li>Optimize protocol-first agents</li> <li> <p>Protocol-level optimization coming</p> </li> <li> <p>Key differentiator</p> </li> <li>SuperOptiX is the only DSPy framework with protocol-first support</li> </ol>"},{"location":"guides/protocol-first-agents/#related-documentation","title":"\ud83d\udcd6 Related Documentation","text":"<ul> <li>Examples: protocol_agent_basic.yaml</li> <li>API Reference: Protocols</li> <li>SuperSpec DSL</li> <li>GEPA Optimization</li> </ul> <p>Questions or feedback? Open an issue on GitHub or join our Discord!</p> <p>Want to contribute? Check out our Contributing Guide</p> <p>Last Updated: 2025-10-20 Version: 1.0.0</p>"},{"location":"guides/pydantic-ai-integration/","title":"\ud83d\udc0d Pydantic AI Integration","text":"<p>SuperOptiX supports Pydantic AI with framework-native minimal pipelines and optional optimization workflows.</p> <p>Works with local and cloud models</p> <p>Native MCP (Model Context Protocol) Support - Built-in tool integration</p> <p>Plain Text Output Mode - Natural responses without JSON formatting issues</p> <p>Model Settings - Full control over generation parameters</p> <p>RLM (Experimental) - Available for testing; unified sandbox support is coming soon.</p>"},{"location":"guides/pydantic-ai-integration/#what-is-pydantic-ai","title":"\ud83c\udfaf What is Pydantic AI?","text":"<p>Pydantic AI is a modern, type-safe framework for building AI agents with:</p> <ul> <li>\ud83c\udfaf Type Safety: Structured outputs using Pydantic models</li> <li>\ud83d\udd27 Tool Integration: Native MCP (Model Context Protocol) support for tools</li> <li>\ud83c\udf10 Provider Agnostic: Works with OpenAI, Ollama, Anthropic, and 100+ LLMs</li> <li>\u26a1 Async/Await: Built-in async support for high-performance applications</li> <li>\ud83d\udcca Model Settings: Fine-grained control (max_tokens, top_p, etc.)</li> <li>\ud83d\udd0c MCP Native: Direct integration with MCP servers for tool discovery</li> </ul> <p>Perfect for deployment applications requiring type safety and reliable tool integration!</p>"},{"location":"guides/pydantic-ai-integration/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install superoptix[frameworks-pydantic-ai]\n</code></pre> <p>Includes: - pydantic-ai 1.31.0 (exact version pinned) - SuperOptiX core with GEPA 0.0.17</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency)</p>"},{"location":"guides/pydantic-ai-integration/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/pydantic-ai-integration/#initialize-project","title":"Initialize Project","text":"<pre><code>super init my_project\ncd my_project\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#pull-demo-agent","title":"Pull Demo Agent","text":"<pre><code>super agent pull developer\n</code></pre> <p>This pulls the <code>developer</code> agent playbook into your project.</p>"},{"location":"guides/pydantic-ai-integration/#configure-model","title":"Configure Model","text":""},{"location":"guides/pydantic-ai-integration/#runtime-modes-important","title":"Runtime Modes (Important)","text":"<p>Pydantic AI in SuperOptiX can run in: - <code>direct</code> mode: provider API directly (recommended for most users) - <code>gateway</code> mode: requires gateway configuration and <code>PYDANTIC_AI_GATEWAY_API_KEY</code></p> <p>If your playbook/runtime is gateway-mode and the key is missing, run will fail early by design.</p>"},{"location":"guides/pydantic-ai-integration/#example-config-local-ollama","title":"Example Config (Local Ollama)","text":"<pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b  # Pydantic AI auto-adds 'ollama:' prefix if needed\n    api_base: http://localhost:11434\n</code></pre> <p>Run: <pre><code>brew install ollama  # macOS\nollama pull llama3.1:8b\nsuper agent compile developer --framework pydantic-ai\nsuper agent run developer --framework pydantic-ai --direct --goal \"Implement a user registration API endpoint with email validation\"\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#example-config-cloud-google-requires-key","title":"Example Config (Cloud Google, requires key)","text":"<pre><code># Google Gemini\nspec:\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n    # Set: export GOOGLE_API_KEY=\"...\"\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#run-the-workflow","title":"Run the Workflow","text":"<pre><code>super agent compile developer --framework pydantic-ai --cloud --provider google-genai --model gemini-2.5-flash\n\n# Run (direct mode)\nsuper agent run developer --framework pydantic-ai --direct --cloud --provider google-genai --model gemini-2.5-flash --goal \"Your task here\"\n\n# Optional optimize loop\nsuper agent compile developer --framework pydantic-ai --optimize\nsuper agent optimize developer --framework pydantic-ai --auto light\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#creating-your-own-pydantic-ai-playbook","title":"\ud83d\udccb Creating Your Own Pydantic AI Playbook","text":""},{"location":"guides/pydantic-ai-integration/#basic-structure","title":"Basic Structure","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: My Assistant\n  id: my_assistant\n  namespace: custom\n  version: 1.0.0\n  level: genies\n\nspec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n\n  input_fields:\n    - name: query\n      type: str\n      description: User query or question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Generated response\n\n  persona:\n    role: Helpful AI Assistant\n    goal: Provide clear and helpful responses\n    backstory: I am an AI assistant trained to help users with their questions.\n\n  # BDD Scenarios\n  feature_specifications:\n    scenarios:\n      - name: Test scenario\n        input:\n          query: \"Hello!\"\n        expected_output:\n          response: \"Greeting\"\n          expected_keywords:\n            - hello\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        reflection_lm: llama3.1:8b\n        auto: medium\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#output-configuration","title":"Output Configuration","text":"<p>Define what your agent should return:</p> <pre><code>spec:\n  output_fields:\n    - name: implementation\n      type: str\n      description: Code implementation\n    - name: explanation\n      type: str\n      description: Brief explanation of the code\n</code></pre> <p>Plain Text Mode (Default): The agent returns natural text responses, which works reliably with smaller models like <code>llama3.1:8b</code>. The output is mapped to your defined fields.</p> <p>Example Output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Implementation \u2502 def add_numbers(a, b):                  \u2502\n\u2502                \u2502     \"\"\"Add two numbers together.\"\"\"     \u2502\n\u2502                \u2502     return a + b                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>This approach avoids JSON formatting issues that can occur with smaller models.</p>"},{"location":"guides/pydantic-ai-integration/#model-settings","title":"Model Settings","text":"<p>Configure generation parameters (excluding <code>temperature</code> as it's deprecated by OpenAI):</p> <pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    max_tokens: 4000  # Default: 4000 (supports detailed responses)\n    top_p: 0.9        # Optional: Nucleus sampling (0.0-1.0)\n    frequency_penalty: 0.0  # Optional: Reduce repetition (-2.0 to 2.0)\n    presence_penalty: 0.0   # Optional: Encourage new topics (-2.0 to 2.0)\n</code></pre> <p>Configuration Options:</p> <ul> <li><code>max_tokens</code> (default: <code>4000</code>): Maximum number of tokens in the response. </li> <li>Increase for longer responses (test plans, detailed code, comprehensive explanations)</li> <li>Decrease for shorter responses (faster, cheaper)</li> <li> <p>Recommended values:</p> <ul> <li>Quick responses: <code>1000-2000</code></li> <li>Standard responses: <code>2000-4000</code> (default)</li> <li>Detailed/comprehensive: <code>4000-8000</code></li> <li>Very detailed: <code>8000-16000</code> (may require larger models)</li> </ul> </li> <li> <p><code>top_p</code> (optional): Nucleus sampling threshold (0.0-1.0). Controls diversity of output.</p> </li> <li><code>0.9-1.0</code>: More creative, diverse responses</li> <li><code>0.5-0.9</code>: Balanced</li> <li> <p><code>0.0-0.5</code>: More focused, deterministic</p> </li> <li> <p><code>frequency_penalty</code> (optional): Reduces repetition (-2.0 to 2.0)</p> </li> <li>Positive values: Reduce repetition</li> <li> <p>Negative values: Allow more repetition</p> </li> <li> <p><code>presence_penalty</code> (optional): Encourages new topics (-2.0 to 2.0)</p> </li> <li>Positive values: Encourage new topics</li> <li>Negative values: Stay on topic</li> </ul> <p>These settings are passed to Pydantic AI's <code>ModelSettings</code> class and control the model's generation behavior.</p>"},{"location":"guides/pydantic-ai-integration/#mcp-model-context-protocol-integration","title":"\ud83d\udd0c MCP (Model Context Protocol) Integration","text":"<p>Pydantic AI has native MCP support! You can connect to MCP servers directly in your playbook.</p>"},{"location":"guides/pydantic-ai-integration/#basic-mcp-configuration","title":"Basic MCP Configuration","text":"<pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: \"fs_\"  # Optional: prefix to avoid naming conflicts\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#supported-mcp-server-types","title":"Supported MCP Server Types","text":""},{"location":"guides/pydantic-ai-integration/#local-stdio-server","title":"Local stdio Server","text":"<p>Runs MCP server as a subprocess:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n          env:  # Optional environment variables\n            API_KEY: \"${MY_API_KEY}\"\n          timeout: 30  # Optional timeout in seconds\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#remote-streamable-http-server","title":"Remote Streamable HTTP Server","text":"<p>Connects to a remote MCP server over HTTP:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: weather_api\n        type: streamable_http\n        config:\n          url: \"https://mcp-server.com/mcp\"\n        tool_prefix: \"weather_\"\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#remote-sse-server-deprecated","title":"Remote SSE Server (Deprecated)","text":"<p>Connects to a remote MCP server using Server-Sent Events:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: legacy_server\n        type: sse\n        config:\n          url: \"http://localhost:3001/sse\"\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<p>You can connect multiple MCP servers:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: \"fs_\"\n\n      - name: weather_api\n        type: streamable_http\n        config:\n          url: \"https://mcp-server.com/mcp\"\n        tool_prefix: \"weather_\"\n</code></pre> <p>Each server's tools will be available with their respective prefixes.</p>"},{"location":"guides/pydantic-ai-integration/#mcp-tool-optimization","title":"MCP Tool Optimization","text":"<p>\u26a0\ufe0f Resource Warning: MCP tool optimization runs two sequential phases, effectively doubling the resource usage. Only run this if you have adequate GPU/compute resources and understand the cost implications.</p> <p>SuperOptiX can optimize both MCP tool descriptions AND agent instructions in a two-phase optimization process. This ensures your agent uses tools effectively AND understands its role clearly.</p>"},{"location":"guides/pydantic-ai-integration/#enable-mcp-tool-optimization","title":"Enable MCP Tool Optimization","text":"<p>Add the <code>optimization</code> section under <code>mcp</code> in your playbook:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: \"fs_\"  # Tools will be prefixed with fs_ at runtime\n\n    # Enable MCP tool description optimization\n    optimization:\n      optimize_tool_descriptions: true\n      # IMPORTANT: Use actual MCP server tool names (WITHOUT prefix)\n      # The optimizer queries the server directly to find tools\n      tool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n</code></pre> <p>Important Notes: - Use actual MCP server tool names WITHOUT prefix - The optimizer queries the MCP server directly, which returns the original tool names (e.g., <code>read_file</code>, not <code>fs_read_file</code>) - The <code>tool_prefix</code> only affects how tools appear at runtime in the agent, not how the optimizer finds them - You can optimize multiple tools from the same server - Tool optimization uses the same training data (BDD scenarios) as instruction optimization</p> <p>Common MCP Filesystem Server Tools: | Server Tool Name | Description | |-----------------|-------------| | <code>read_file</code> | Read file contents | | <code>write_file</code> | Write/create files | | <code>list_directory</code> | List directory contents | | <code>create_directory</code> | Create directories | | <code>move_file</code> | Move/rename files | | <code>search_files</code> | Search for files |</p>"},{"location":"guides/pydantic-ai-integration/#two-phase-optimization-process","title":"Two-Phase Optimization Process","text":"<p>When you run <code>super agent optimize</code>, SuperOptiX performs two sequential optimizations:</p>"},{"location":"guides/pydantic-ai-integration/#phase-1-mcp-tool-description-optimization","title":"Phase 1: MCP Tool Description Optimization","text":"<p>What Gets Optimized: - Tool descriptions for each tool in <code>tool_names</code> - GEPA learns better descriptions that help the model understand:   - When to use each tool   - What each tool does   - How to use each tool effectively</p> <p>Example Transformation:</p> <p>Before Optimization: <pre><code>{\n  \"tool_description_fs_read_file\": \"Tool: fs_read_file\",\n  \"tool_description_fs_write_file\": \"Tool: fs_write_file\",\n  \"tool_description_fs_list_files\": \"Tool: fs_list_files\"\n}\n</code></pre></p> <p>After GEPA Optimization: <pre><code>{\n  \"tool_description_fs_read_file\": \"Read file contents from the filesystem. Use when user asks to view, show, display, or read file contents. Returns the full text content of the specified file path. Requires a valid file path parameter.\",\n  \"tool_description_fs_write_file\": \"Write content to files on the filesystem. Use when user asks to create, save, update, or write file contents. Requires file path and content parameters. Overwrites existing files.\",\n  \"tool_description_fs_list_files\": \"List files and directories in a given path. Use when user asks to see what files are available, browse directories, find files, or explore the filesystem. Returns a list of files and folders in the specified directory.\"\n}\n</code></pre></p> <p>Output File: <pre><code>{project_name}/agents/{agent_name}/optimized/{agent_name}_mcp_tool_descriptions.json\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#phase-2-agent-instruction-optimization","title":"Phase 2: Agent Instruction Optimization","text":"<p>What Gets Optimized: - Agent's system prompt/instructions (built from <code>persona.role</code>, <code>persona.goal</code>, <code>persona.backstory</code>, etc.) - GEPA learns better instructions that help the model:   - Understand its role more clearly   - Use tools more effectively   - Generate better responses</p> <p>Output File: <pre><code>{project_name}/agents/{agent_name}/optimized/{agent_name}_pydantic_ai_optimized.json\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#running-optimization","title":"Running Optimization","text":"<p>\u26a0\ufe0f Before Running: - Ensure you have high-end GPU or cloud GPU access - Understand that optimization makes many LLM API calls - Use local Ollama models (e.g., <code>ollama/llama3.1:8b</code>) to minimize costs - Cloud models (GPT-4, Claude) will incur significant API charges</p> <pre><code># Quick test (super light - ~1-2 minutes, ~20 API calls)\n# RECOMMENDED: Use local Ollama to avoid API costs\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --max-metric-calls 20 \\\n  --reflection-lm ollama/llama3.1:8b\n\n# Light mode for better results (~5-10 minutes, ~50-100 API calls)\n# Use local Ollama: --reflection-lm ollama/llama3.1:8b\n# Cloud models (costly): --reflection-lm openai/gpt-4o\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --auto light \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre> <p>What You'll See:</p> <pre><code>\ud83d\udd27 Phase 1: Optimizing MCP Tool Descriptions\n   Optimizing 3 tool(s): fs_read_file, fs_write_file, fs_list_files\n   MCP tool optimization complete!\n   Best score: 0.850\n   Saved to: .../developer_mcp_tool_descriptions.json\n\n\u26a1 Phase 2: Running GEPA optimization for instructions...\n   Budget: light\n   Training examples: 5\n   Validation examples: 0\n   Optimization complete!\n   Best score: 0.920\n   Saved to: .../developer_pydantic_ai_optimized.json\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#using-optimized-results","title":"Using Optimized Results","text":"<p>The generated pipeline automatically loads optimized values:</p> <ol> <li>Tool Descriptions: Applied when MCP servers are initialized</li> <li>Instructions: Loaded when the agent is created</li> </ol> <p>You don't need to manually apply the optimizations - the pipeline handles it automatically!</p>"},{"location":"guides/pydantic-ai-integration/#complete-example","title":"Complete Example","text":"<pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b  # Works great with plain text output mode\n\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: \"fs_\"  # Runtime prefix (tools become fs__read_file, etc.)\n\n    # Enable tool optimization\n    optimization:\n      optimize_tool_descriptions: true\n      # Use actual MCP server tool names (WITHOUT prefix)\n      tool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n\n  # Agent instruction optimization (always runs)\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: light\n        reflection_lm: ollama/llama3.1:8b  # Use forward slash for LiteLLM\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#best-practices","title":"Best Practices","text":"<p>\u26a0\ufe0f When to Skip Optimization: - Your agent already performs well (pass rate &gt; 80%) - You don't have high-end GPU or cloud GPU access - You want to avoid API costs - You're in early development/testing phase</p> <p>Optimization is optional - many agents work great without it!</p> <ol> <li> <p>Use Tool Prefixes: Prevents naming conflicts when using multiple MCP servers    <pre><code>tool_prefix: \"fs_\"  # Tools become fs_read_file, fs_write_file, etc.\n</code></pre></p> </li> <li> <p>Optimize Key Tools: Focus on tools that are used frequently or are critical to your agent's functionality</p> </li> <li> <p>Reflection Model: Use a smaller, faster model for reflection (GEPA runs it many times):    <pre><code>--reflection-lm ollama/llama3.1:8b  # Fast, good enough for reflection, FREE!\n</code></pre>    \u26a0\ufe0f Avoid cloud models (GPT-4, Claude) unless you understand the costs - they can cost $5-100+ per optimization run!</p> </li> <li> <p>Task Model: Use a larger model for the actual agent (better quality):    <pre><code>model: llama3.1:8b  # Works well with plain text mode (local, free)\n# or gpt-oss:120b for better quality (requires GPU)\n</code></pre></p> </li> <li> <p>Training Data: Ensure your BDD scenarios include examples that use MCP tools:    <pre><code>feature_specifications:\n  scenarios:\n    - name: read_config_file\n      input:\n        feature_requirement: \"Read /private/tmp/config.json and tell me what database host is configured\"  # Use /private/tmp on macOS\n      expected_output:\n        implementation: \"localhost\"  # Should use fs_read_file tool\n</code></pre></p> </li> </ol>"},{"location":"guides/pydantic-ai-integration/#benefits-of-two-phase-optimization","title":"Benefits of Two-Phase Optimization","text":"<p>Better Tool Usage: Optimized descriptions help the model choose the right tool at the right time Better Instructions: Optimized prompts improve overall agent behavior Compound Effect: Both optimizations work together for maximum performance Automatic: Pipeline automatically applies both optimizations when available</p>"},{"location":"guides/pydantic-ai-integration/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Tool optimization fails with \"None of the specified tools found\"</p> <p>Solution: Use actual MCP server tool names without prefix: <pre><code># Wrong - uses prefixed names\ntool_names: [\"fs_read_file\", \"fs_write_file\"]\n\n# Correct - uses actual server tool names\ntool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n</code></pre></p> <p>The optimizer queries the MCP server directly, which returns unprefixed tool names. The <code>tool_prefix</code> only affects runtime tool naming in the agent.</p> <p>Issue: Tool optimization fails but instruction optimization succeeds</p> <p>Solution: Check that: - Tool names match actual MCP server tool names (without prefix) - MCP server is accessible and tools are available - Training scenarios include tool usage examples</p> <p>Issue: Optimization takes too long</p> <p>Quick Solutions: - Super Light: Use <code>--max-metric-calls 20</code> for fastest test (~1-2 minutes) - Light Mode: Use <code>--auto light</code> for balanced speed/quality (~5-10 minutes) - Use smaller reflection model: <code>--reflection-lm ollama/llama3.1:8b</code> - Reduce number of tools to optimize</p>"},{"location":"guides/pydantic-ai-integration/#mcp-demo-tutorial","title":"\ud83c\udfac MCP Demo Tutorial","text":"<p>For a complete step-by-step demo of MCP with Pydantic AI, including: - Quick start with <code>pydantic-mcp</code> demo agent - Setting up filesystem MCP server - Testing file operations (read, write, list) - Verified working examples with <code>llama3.1:8b</code> - Troubleshooting common issues</p> <p>See: Pydantic AI MCP Demo Guide</p> <p>Quick Start: <pre><code>super init swe &amp;&amp; cd swe\nsuper agent pull pydantic-mcp\nsuper agent compile pydantic-mcp --framework pydantic-ai\nsuper agent run pydantic-mcp --goal \"List all files in /private/tmp\"  # Use /private/tmp on macOS\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#gepa-optimization","title":"\ud83c\udfaf GEPA Optimization","text":"<p>\u26a0\ufe0f IMPORTANT: Resource Requirements</p> <p>GEPA optimization is resource-intensive and should only be run when: - You have a high-end GPU (or cloud GPU access) - You understand the cost implications (many LLM API calls) - You have adequate time budget (5-60 minutes depending on settings)</p> <p>Resource Usage: - Makes many LLM API calls (reflection + evaluation) - Can consume significant GPU memory and compute resources - Cloud API costs can add up quickly (especially with GPT-4, Claude, etc.)</p> <p>For local testing: Use <code>--max-metric-calls 20</code> with <code>ollama/llama3.1:8b</code> to minimize resource usage.</p>"},{"location":"guides/pydantic-ai-integration/#what-gets-optimized","title":"What Gets Optimized","text":"<p>Pydantic AI has one main optimizable variable: - <code>instructions</code>: The agent's system prompt (built from <code>persona.role</code>, <code>persona.goal</code>, <code>persona.backstory</code>, etc.)</p>"},{"location":"guides/pydantic-ai-integration/#how-gepa-optimizes-pydantic-ai-agents","title":"How GEPA Optimizes Pydantic AI Agents","text":"<p>GEPA optimizes the instructions field by:</p> <ol> <li>Analyzing BDD test scenarios to understand success criteria</li> <li>Generating variations of the instructions prompt</li> <li>Testing each variation against your evaluation scenarios</li> <li>Selecting the best performer based on pass rate</li> </ol> <p>Example transformation:</p> <pre><code># Original (from playbook)\npersona:\n  role: Software Developer\n  goal: Write clean, efficient code\n  backstory: I am an experienced developer\n\n\u2192 instructions = \"Software Developer\\nGoal: Write clean, efficient code\\nBackstory: I am an experienced developer\"\n</code></pre> <pre><code># After GEPA optimization\n\u2192 instructions = \"You are a Software Developer.\n\nWhen writing code:\n1. Ensure it is clean and maintainable\n2. Follow best practices and conventions\n3. Include proper error handling\n4. Write comprehensive tests\n\nGoal: Write clean, efficient code that meets requirements and is deployment-ready.\n\nBackstory: I am an experienced developer with expertise in multiple programming languages and frameworks.\"\n</code></pre> <p>GEPA typically expands the instructions to be more explicit and structured, which improves agent behavior consistency.</p>"},{"location":"guides/pydantic-ai-integration/#optimization-command","title":"Optimization Command","text":"<p>\u26a0\ufe0f Resource &amp; Cost Warning:</p> <p>GEPA optimization is resource-intensive and makes many LLM API calls: - Super Light: ~20 API calls (~$0.10-2.00 with cloud models) - Light: ~50-100 API calls (~$0.50-10.00 with cloud models) - Medium: ~150-300 API calls (~$5-50 with cloud models) - Heavy: ~300-600 API calls (~$20-100+ with cloud models)</p> <p>Recommendations: - Use local Ollama models (<code>ollama/llama3.1:8b</code>) to avoid API costs - Only optimize when you have high-end GPU or cloud GPU access - Start with <code>--max-metric-calls 20</code> to test - Avoid cloud models (GPT-4, Claude) unless you understand the costs</p>"},{"location":"guides/pydantic-ai-integration/#quick-test-super-light","title":"Quick Test (Super Light) \u26a1","text":"<p>\u26a0\ufe0f Resource Warning: Even \"super light\" optimization makes LLM API calls and can take time.</p> <p>\ud83d\udca1 Tip: If your playbook has <code>auto: light</code> or <code>max_full_evals</code> set, the CLI <code>--max-metric-calls</code> argument will override it. CLI arguments always take precedence.</p> <p>For fastest optimization to test if it works:</p> <pre><code># Very fast: only 3 metric calls (~30 seconds - 1 minute)\nsuper agent optimize my_agent \\\n  --framework pydantic-ai \\\n  --max-metric-calls 3 \\\n  --reflection-lm ollama/llama3.1:8b\n\n# Quick test: 10 metric calls (~1-2 minutes)\nsuper agent optimize my_agent \\\n  --framework pydantic-ai \\\n  --max-metric-calls 10 \\\n  --reflection-lm ollama/llama3.1:8b\n\n# Light optimization: 20 metric calls (~2-3 minutes)\nsuper agent optimize my_agent \\\n  --framework pydantic-ai \\\n  --max-metric-calls 20 \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre> <p>Use this when: - Testing if optimization works - Limited resources/time - Quick iteration during development - Just need to verify the process - Using local Ollama models (recommended to avoid API costs)</p> <p>Note: <code>--max-metric-calls 20</code> limits total evaluations more precisely than <code>--max-full-evals 1</code>, ensuring faster completion.</p> <p>Cost Tip: Use local <code>ollama/llama3.1:8b</code> for reflection_lm to avoid API costs. Cloud models (GPT-4, Claude) will incur charges.</p>"},{"location":"guides/pydantic-ai-integration/#recommended-balanced","title":"Recommended (Balanced)","text":"<pre><code>super agent optimize my_agent \\\n  --framework pydantic-ai \\\n  --auto light \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre> <p>Budget Options: - <code>--auto light</code>: Fast optimization (~2-5 iterations, ~5-10 minutes) \u2b50 Recommended - <code>--auto medium</code>: Balanced optimization (~5-10 iterations, ~15-30 minutes) - <code>--auto heavy</code>: Thorough optimization (~10-20 iterations, ~30-60 minutes) - <code>--max-full-evals N</code>: Specify exact number of iterations (use <code>1</code> for super quick test) - <code>--max-metric-calls N</code>: Limit total metric evaluations</p>"},{"location":"guides/pydantic-ai-integration/#optimization-results","title":"Optimization Results","text":"<p>Optimized instructions are saved to: <pre><code>{project_name}/agents/{agent_name}/optimized/{agent_name}_pydantic_ai_optimized.json\n</code></pre></p> <p>The generated pipeline automatically loads optimized instructions if available.</p>"},{"location":"guides/pydantic-ai-integration/#field-description-optimization","title":"\ud83d\udd0d Field Description Optimization","text":"<p>\u26a0\ufe0f IMPORTANT: Resource Requirements</p> <p>Field description optimization is resource-intensive and should only be run when: - You have a high-end GPU (or cloud GPU access) - You understand the cost implications (additional LLM API calls) - You plan to use structured output mode (required for optimized descriptions to take effect)</p>"},{"location":"guides/pydantic-ai-integration/#what-gets-optimized_1","title":"What Gets Optimized","text":"<p>Field Description Optimization uses GEPA to optimize Pydantic model field descriptions (<code>Field(description=...)</code>) for structured output. This improves the model's understanding of what each output field should contain.</p> <p>Requires: - <code>output_fields</code> defined in your playbook - <code>optimize_field_descriptions: true</code> in optimization config - Structured output mode enabled (<code>output_mode: structured</code>) to use optimized descriptions</p>"},{"location":"guides/pydantic-ai-integration/#how-it-works","title":"How It Works","text":"<p>GEPA optimizes field descriptions by:</p> <ol> <li>Extracting field descriptions from <code>output_fields</code> in your playbook</li> <li>Creating evaluation scenarios based on your BDD test cases</li> <li>Generating variations of each field description</li> <li>Testing each variation to see which descriptions lead to better structured outputs</li> <li>Selecting the best descriptions that improve structured data extraction accuracy</li> </ol>"},{"location":"guides/pydantic-ai-integration/#example-transformation","title":"Example Transformation","text":"<p>Before Optimization: <pre><code>spec:\n  output_fields:\n    - name: implementation\n      type: string\n      description: The code implementation of the feature\n</code></pre></p> <p>After GEPA Optimization: <pre><code>{\n  \"original_descriptions\": {\n    \"implementation\": \"The code implementation of the feature\"\n  },\n  \"optimized_descriptions\": {\n    \"implementation\": \"Complete, deployment-ready code implementation with proper imports, error handling, and documentation. Include full function/class definitions, not pseudocode or descriptions.\"\n  },\n  \"score\": 0.95,\n  \"iterations\": 3\n}\n</code></pre></p> <p>The optimized description is more explicit about what the model should produce, leading to better structured output quality.</p>"},{"location":"guides/pydantic-ai-integration/#enable-field-description-optimization","title":"Enable Field Description Optimization","text":"<p>Add to your playbook's <code>optimization</code> section:</p> <pre><code>spec:\n  output_fields:\n    - name: implementation\n      type: string\n      description: The code implementation of the feature\n      required: true\n\n  optimization:\n    optimize_field_descriptions: true  # Enable field description optimization\n    optimizer:\n      name: GEPA\n      params:\n        auto: light\n        reflection_lm: ollama/llama3.1:8b\n</code></pre> <p>Important Notes: - Field description optimization runs as Phase 1.5 (between MCP tool optimization and instruction optimization) - It only runs if <code>output_fields</code> are defined in your playbook - Optimized descriptions are saved but only used when structured output mode is enabled</p>"},{"location":"guides/pydantic-ai-integration/#running-field-description-optimization","title":"Running Field Description Optimization","text":"<pre><code># Quick test (super light - ~1-2 minutes)\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --max-metric-calls 20 \\\n  --reflection-lm ollama/llama3.1:8b\n\n# Light mode (~5-10 minutes)\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --auto light \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre> <p>What You'll See:</p> <pre><code>\ud83d\udd27 Phase 1: Optimizing MCP Tool Descriptions (if enabled)\n   ...\n\n\ud83d\udccb Phase 1.5: Optimizing Field Descriptions\n   Optimizing 1 field descriptions:\n     - implementation: The code implementation of the feature\n\n   Field description optimization complete!\n   Best score: 0.95\n   Saved to: .../developer_field_descriptions_optimized.json\n\n\u26a1 Phase 2: Running GEPA optimization for instructions...\n   ...\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#output-file","title":"Output File","text":"<p>Optimized field descriptions are saved to: <pre><code>{project_name}/agents/{agent_name}/optimized/{agent_name}_field_descriptions_optimized.json\n</code></pre></p> <p>File Format: <pre><code>{\n  \"original_descriptions\": {\n    \"implementation\": \"The code implementation of the feature\"\n  },\n  \"optimized_descriptions\": {\n    \"implementation\": \"Complete, deployment-ready code implementation...\"\n  },\n  \"score\": 0.95,\n  \"iterations\": 3\n}\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#using-optimized-field-descriptions","title":"Using Optimized Field Descriptions","text":"<p>Optimized descriptions are automatically used when: 1. Structured output mode is enabled (<code>output_mode: structured</code>) 2. The optimization file exists in the <code>optimized/</code> directory 3. You've run <code>super agent optimize</code> with <code>optimize_field_descriptions: true</code></p> <p>The generated pipeline automatically loads and applies optimized descriptions when creating the BaseModel for structured output.</p>"},{"location":"guides/pydantic-ai-integration/#benefits","title":"Benefits","text":"<p>Better Structured Output: More explicit field descriptions improve the model's understanding Improved Accuracy: Optimized descriptions lead to better structured data extraction Type Safety: Works seamlessly with Pydantic's BaseModel validation Automatic: Pipeline automatically applies optimized descriptions when available</p>"},{"location":"guides/pydantic-ai-integration/#when-to-use","title":"When to Use","text":"<p>Use field description optimization when: - You're using structured output mode - Your structured outputs aren't accurate enough - You have well-defined BDD test scenarios - You have adequate GPU/compute resources</p> <p>Skip field description optimization when: - You're using plain text output mode (descriptions won't be used) - Your structured outputs already work well - You don't have resources for additional optimization - <code>output_fields</code> aren't defined in your playbook</p>"},{"location":"guides/pydantic-ai-integration/#structured-output-mode","title":"\ud83d\udcca Structured Output Mode","text":"<p>Pydantic AI supports structured output using Pydantic BaseModel for type-safe, validated responses. SuperOptiX provides an opt-in structured output mode that uses optimized field descriptions when available.</p>"},{"location":"guides/pydantic-ai-integration/#what-is-structured-output","title":"What is Structured Output?","text":"<p>Structured Output uses Pydantic BaseModel to enforce type-safe responses: - Type Validation: Responses are validated against the BaseModel schema - Field Descriptions: Each field has a description that guides the model - Type Safety: Python type hints ensure correct data types - Automatic Parsing: Responses are automatically parsed into BaseModel instances</p> <p>Default Mode (Plain Text): - Agent returns plain text strings - No JSON structure enforcement - Works great for code generation, explanations, etc. - Better compatibility with smaller models (8b)</p> <p>Structured Output Mode (Opt-in): - Agent returns validated BaseModel instances - Type-safe, structured data - Uses optimized field descriptions when available - Requires larger models (70b+) for reliable results</p>"},{"location":"guides/pydantic-ai-integration/#enable-structured-output","title":"Enable Structured Output","text":"<p>Add <code>output_mode: structured</code> to your playbook:</p> <pre><code>spec:\n  output_mode: structured  # Enable structured output (opt-in, defaults to plain)\n  output_fields:\n    - name: implementation\n      type: string\n      description: The code implementation of the feature\n      required: true\n</code></pre> <p>Requirements: - <code>output_fields</code> must be defined - Requires larger models (70b+) for reliable structured output - Works best with optimized field descriptions</p>"},{"location":"guides/pydantic-ai-integration/#example-playbook","title":"Example Playbook","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Developer Assistant\n  id: developer\nspec:\n  # Enable structured output\n  output_mode: structured\n\n  language_model:\n    provider: ollama\n    model: llama3.1:70b  # Larger model recommended for structured output\n    api_base: http://localhost:11434\n\n  output_fields:\n    - name: implementation\n      type: string\n      description: The code implementation of the feature\n      required: true\n\n  optimization:\n    optimize_field_descriptions: true  # Optimize field descriptions\n    optimizer:\n      name: GEPA\n      params:\n        auto: light\n        reflection_lm: ollama/llama3.1:8b\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#how-it-works_1","title":"How It Works","text":"<p>When structured output is enabled:</p> <ol> <li>BaseModel Creation: A Pydantic BaseModel is created from <code>output_fields</code></li> <li>Optimized Descriptions: If available, optimized field descriptions are used</li> <li>Agent Configuration: Agent is configured with <code>output_type=BaseModel</code></li> <li>Response Validation: Model responses are validated against the BaseModel</li> <li>Type-Safe Output: Responses are returned as BaseModel instances</li> </ol> <p>Generated Code: <pre><code># BaseModel created from output_fields\nclass DeveloperOutput(BaseModel):\n    implementation: str = Field(\n        description=\"Complete, deployment-ready code implementation...\"  # Optimized description if available\n    )\n\n# Agent configured with structured output\nagent = Agent(\n    model=model,\n    instructions=instructions,\n    output_type=DeveloperOutput  # Structured output enabled\n)\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#verification","title":"Verification","text":"<p>When running an agent with structured output, you'll see:</p> <pre><code>Using structured output mode (BaseModel)\n   Output Model: DeveloperOutput\n   Using optimized field descriptions\n</code></pre> <p>Response Output: <pre><code>Structured Output Received!\n   Type: DeveloperOutput\n   Model: DeveloperOutput\n   \ud83d\udcca Pydantic v2 model validated successfully\n   \ud83d\udccb Structured Data (JSON):\n   {\n     \"implementation\": \"...actual code here...\"\n   }\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#benefits_1","title":"Benefits","text":"<p>Type Safety: Responses are validated against Pydantic models Better Structure: Enforces consistent output format Optimized Descriptions: Uses GEPA-optimized field descriptions Validation: Automatic validation ensures correct data types Integration: Works seamlessly with Pydantic AI's native structured output</p>"},{"location":"guides/pydantic-ai-integration/#when-to-use-structured-output","title":"When to Use Structured Output","text":"<p>Use structured output when: - You need type-safe, validated responses - You're using larger models (70b+) - You have well-defined output schemas - You've optimized field descriptions - You need consistent data structure</p> <p>Use plain text output when: - You're using smaller models (8b) - You want maximum compatibility - Output format is flexible - You don't need structured validation - Default mode - works great for most use cases</p>"},{"location":"guides/pydantic-ai-integration/#switching-between-modes","title":"Switching Between Modes","text":"<p>Enable structured output: <pre><code>spec:\n  output_mode: structured\n</code></pre></p> <p>Disable (use plain text - default): <pre><code>spec:\n  # output_mode: plain  # Default, can omit\n  # or remove output_mode entirely\n</code></pre></p> <p>Important: Always recompile after changing <code>output_mode</code>: <pre><code>super agent compile developer --framework pydantic-ai\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":""},{"location":"guides/pydantic-ai-integration/#baseline-performance","title":"Baseline Performance","text":"<p>Task: Code generation and explanation Model: Ollama llama3.1:8b Framework: Pydantic AI</p> <p>Pydantic AI achieves good baseline performance with local Ollama models. Results vary based on: - Hardware capabilities (RAM, CPU/GPU) - Model size and quality (8b vs 70b) - BDD scenario complexity - Model settings (max_tokens, top_p, etc.)</p>"},{"location":"guides/pydantic-ai-integration/#framework-comparison","title":"Framework Comparison","text":"<p>Pydantic AI strengths: - Type-safe structured outputs (validated by Pydantic) - Native MCP support (no extra configuration) - Modern async/await API - Clean, simple architecture - Works seamlessly with Ollama</p> <p>DSPy strengths: - More optimization targets (all signatures) - Better for focused, well-defined tasks - Greater improvement potential through optimization</p> <p>OpenAI SDK strengths: - Built-in multi-agent handoffs - Session management - Guardrails support</p>"},{"location":"guides/pydantic-ai-integration/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>SuperSpec YAML Playbook\n        \u2193\n    Compiler (AgentCompiler)\n        \u2193\nPydantic AI Pipeline Template (pydantic_ai_pipeline.py.jinja2)\n        \u2193\nGenerated Python Pipeline\n        \u251c\u2500 MyAssistantComponent (BaseComponent wrapper)\n        \u2502   \u251c\u2500 _initialize_model() \u2192 infer_model() (Pydantic AI)\n        \u2502   \u251c\u2500 _initialize_mcp_servers() \u2192 [MCPServerStdio, ...] (if enabled)\n        \u2502   \u251c\u2500 _get_model_settings() \u2192 ModelSettings\n        \u2502   \u251c\u2500 _initialize_agent() \u2192 Agent(\n        \u2502   \u2502                         model,\n        \u2502   \u2502                         instructions,  \u2190 Optimized by GEPA!\n        \u2502   \u2502                         model_settings,\n        \u2502   \u2502                         toolsets=[...]  \u2190 MCP servers\n        \u2502   \u2502                       )\n        \u2502   \u2514\u2500 forward() \u2192 agent.run() (async, plain text output)\n        \u2514\u2500 MyAssistantPipeline\n            \u251c\u2500 run()\n            \u251c\u2500 evaluate()\n            \u251c\u2500 optimize_with_gepa() \u2190 Universal GEPA\n            \u2514\u2500 run_bdd_test_suite()\n</code></pre> <p>Note: The template uses plain text output mode (no <code>output_type</code> parameter) for reliable responses with smaller models.</p>"},{"location":"guides/pydantic-ai-integration/#model-configuration","title":"\ud83d\udd04 Model Configuration","text":""},{"location":"guides/pydantic-ai-integration/#ollama-local-recommended","title":"Ollama (Local) - RECOMMENDED \u2b50","text":"<pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b  # or llama3.1:70b for better quality\n    api_base: http://localhost:11434\n    max_tokens: 4000  # Adjust based on response length needs (default: 4000)\n    top_p: 0.9  # Optional: Control output diversity\n</code></pre> <p>Setup: <pre><code># Install Ollama\nbrew install ollama  # macOS\n# or download from https://ollama.com\n\n# Pull model\nollama pull llama3.1:8b\n\n# Set environment (optional, auto-configured by pipeline)\n# Note: The pipeline automatically sets these if api_base is provided in playbook\nexport OLLAMA_BASE_URL=http://localhost:11434/v1\nexport OLLAMA_API_KEY=ollama  # Placeholder key (Ollama doesn't require real key)\n</code></pre></p> <p>The pipeline automatically: - Adds <code>ollama:</code> prefix if missing - Sets <code>OLLAMA_BASE_URL</code> with <code>/v1</code> suffix - Uses Pydantic AI's <code>infer_model()</code> for automatic model creation</p>"},{"location":"guides/pydantic-ai-integration/#openai-cloud","title":"OpenAI (Cloud)","text":"<pre><code>spec:\n  language_model:\n    provider: openai\n    model: gpt-4o\n    max_tokens: 4000  # Adjust as needed (default: 4000)\n    top_p: 0.9  # Optional\n</code></pre> <p>Setup: <pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre></p> <p>Pydantic AI automatically detects OpenAI from the model string or provider field.</p>"},{"location":"guides/pydantic-ai-integration/#anthropic-cloud","title":"Anthropic (Cloud)","text":"<pre><code>spec:\n  language_model:\n    provider: anthropic\n    model: claude-3-5-sonnet\n    max_tokens: 4000  # Adjust as needed (default: 4000)\n    top_p: 0.9  # Optional (Anthropic may ignore this)\n</code></pre> <p>Setup: <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#other-providers","title":"Other Providers","text":"<p>Pydantic AI supports 100+ providers via LiteLLM. Just specify the provider and model:</p> <pre><code>spec:\n  language_model:\n    provider: google  # or groq, together, bedrock, etc.\n    model: gemini-pro\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#troubleshooting_1","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"guides/pydantic-ai-integration/#model-not-found","title":"Model Not Found","text":"<p>Symptom: <code>Unknown provider: llama3.1</code> or <code>ModelHTTPError: 404</code></p> <p>Solutions: 1. Ensure model string has provider prefix: <code>ollama:llama3.1:8b</code> 2. Check <code>OLLAMA_BASE_URL</code> includes <code>/v1</code>: <code>http://localhost:11434/v1</code> 3. Verify Ollama is running: <code>curl http://localhost:11434/api/tags</code> 4. Check model is downloaded: <code>ollama list</code></p> <p>The pipeline auto-detects Ollama models, but explicit prefix is safer.</p>"},{"location":"guides/pydantic-ai-integration/#low-pass-rate-in-evaluation","title":"Low Pass Rate in Evaluation","text":"<p>Symptom: Evaluation scenarios failing</p> <p>Solutions: 1. Check BDD scenario keywords are realistic 2. Lower threshold in evaluate() method (default is 0.6) 3. Run GEPA optimization to improve instructions 4. Try different model (llama3.1:70b for more capability) 5. Adjust model settings in playbook:    <pre><code>spec:\n  language_model:\n    max_tokens: 8000  # Increase for longer responses\n    top_p: 0.9\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#mcp-server-connection-issues","title":"MCP Server Connection Issues","text":"<p>Symptom: <code>Failed to initialize MCP server</code> or tools not available</p> <p>Solutions: 1. For stdio servers:    - Verify command exists: <code>which npx</code>    - Check args are correct    - Ensure MCP server package is installed</p> <ol> <li>For remote servers:</li> <li>Verify URL is accessible: <code>curl https://mcp-server.com/mcp</code></li> <li>Check network connectivity</li> <li> <p>Verify server is running</p> </li> <li> <p>General:</p> </li> <li>Check server logs for errors</li> <li>Verify <code>mcp</code> package is installed: <code>pip install mcp</code></li> <li>Test server independently first</li> </ol>"},{"location":"guides/pydantic-ai-integration/#import-error","title":"Import Error","text":"<p>Symptom: <code>ModuleNotFoundError: No module named 'pydantic_ai'</code></p> <p>Solution: <pre><code>pip install superoptix[frameworks-pydantic-ai]\n# or\npip install pydantic-ai==1.31.0\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#optimization-takes-too-long","title":"Optimization Takes Too Long","text":"<p>Symptom: Optimization never completes or takes too long</p> <p>\u26a0\ufe0f Note: Optimization is inherently resource-intensive. If it's taking too long, consider if optimization is necessary for your use case. Many agents work well without optimization.</p> <p>Quick Solutions:</p> <ol> <li> <p>Ultra Fast (~30s-1m, ~3 API calls): Minimal metric calls for quick verification:    <pre><code># Use local Ollama to avoid API costs\nsuper agent optimize developer --framework pydantic-ai --max-metric-calls 3 --reflection-lm ollama/llama3.1:8b\n</code></pre></p> </li> <li> <p>Super Light (~1-2 minutes, ~10 API calls): Limit total metric calls:    <pre><code># Use local Ollama to avoid API costs\nsuper agent optimize developer --framework pydantic-ai --max-metric-calls 10 --reflection-lm ollama/llama3.1:8b\n</code></pre></p> </li> <li> <p>Light Mode (Recommended - ~5-10 minutes, ~50-100 API calls):    <pre><code># Use local Ollama for cost-free optimization\nsuper agent optimize developer --framework pydantic-ai --auto light --reflection-lm ollama/llama3.1:8b\n</code></pre></p> </li> </ol> <p>Cloud models are costly: <pre><code># NOT RECOMMENDED - Expensive!\nsuper agent optimize developer --framework pydantic-ai --auto light --reflection-lm openai/gpt-4o\n# This can cost $5-20+ per optimization run!\n</code></pre></p> <ol> <li> <p>Reduce iterations in playbook:    <pre><code>optimization:\n  optimizer:\n    params:\n      max_metric_calls: 20  # Limit total evaluations\n      reflection_lm: ollama/llama3.1:8b\n</code></pre></p> </li> <li> <p>Use smaller reflection model: <code>--reflection-lm ollama/llama3.1:8b</code></p> </li> <li>Reduce training dataset size (fewer BDD scenarios)</li> </ol>"},{"location":"guides/pydantic-ai-integration/#json-metadata-instead-of-content","title":"JSON Metadata Instead of Content","text":"<p>Symptom: Agent returns JSON like <code>{\"action\": \"do_something\", \"params\": {...}}</code> instead of actual content</p> <p>Solution: This was fixed in SuperOptiX 0.2.1. The template now uses plain text output mode: <pre><code>pip install --upgrade superoptix\nsuper agent compile your_agent --framework pydantic-ai  # Recompile\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#mcp-server-not-initializing","title":"MCP Server Not Initializing","text":"<p>Symptom: No \"\ud83d\udee0\ufe0f Initialized MCP stdio server\" message during run</p> <p>Solutions: 1. Check playbook filename uses underscores: <code>my_agent_playbook.yaml</code> (not hyphens) 2. Verify <code>mcp.enabled: true</code> in playbook 3. Check MCP server command is correct:    <pre><code>mcp:\n  enabled: true\n  servers:\n    - name: filesystem\n      type: stdio\n      config:\n        command: npx\n        args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS (or /tmp on Linux)\n</code></pre></p>"},{"location":"guides/pydantic-ai-integration/#under-the-hood","title":"\ud83d\udd2c Under the Hood","text":""},{"location":"guides/pydantic-ai-integration/#model-initialization","title":"Model Initialization","text":"<p>The pipeline uses Pydantic AI's <code>infer_model()</code> for automatic model creation:</p> <pre><code>from pydantic_ai.models import infer_model\n\n# Auto-detects provider from model string\nmodel = infer_model(\"ollama:llama3.1:8b\")\n# or\nmodel = infer_model(\"openai:gpt-4o\")\n</code></pre> <p>For Ollama, it automatically: - Adds <code>ollama:</code> prefix if missing - Sets <code>OLLAMA_BASE_URL</code> environment variable - Configures OpenAI-compatible API endpoint</p>"},{"location":"guides/pydantic-ai-integration/#plain-text-output-mode","title":"Plain Text Output Mode","text":"<p>The template uses plain text output for reliable responses with all model sizes:</p> <pre><code># Agent configured for plain text output\nagent = Agent(\n    model=model,\n    instructions=instructions,\n    # No output_type - uses plain text mode\n    toolsets=[server] if server else None,  # \u2190 MCP servers as toolsets\n)\n\n# Result is plain text mapped to output fields\nresult = await agent.run(input_text)\nresponse_text = str(result.output)\n\n# Mapped to first output field\nreturn {\"implementation\": response_text}\n</code></pre> <p>Why Plain Text Mode? - Works reliably with 8b models - No JSON formatting issues - Natural, readable responses - Better for code generation and documentation tasks</p>"},{"location":"guides/pydantic-ai-integration/#mcp-server-integration","title":"MCP Server Integration","text":"<p>MCP servers are registered as <code>toolsets</code> on the Agent:</p> <pre><code>from pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    command=\"npx\",\n    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"],  # Use /private/tmp on macOS (or /tmp on Linux)\n)\n\nagent = Agent(\n    model=model,\n    instructions=instructions,\n    toolsets=[server],  # \u2190 MCP tools automatically available!\n)\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#agent-execution-flow","title":"Agent Execution Flow","text":"<ol> <li>User input received</li> <li>Component's <code>forward()</code> called (async method)</li> <li>Agent initialized (lazy, cached)</li> <li><code>await agent.run(input)</code> executed (async execution)</li> <li>Agent processes with model + tools</li> <li>Returns <code>result.output</code> (validated BaseModel or str)</li> <li>Mapped to output fields dict</li> </ol>"},{"location":"guides/pydantic-ai-integration/#the-superoptix-multi-framework-advantage","title":"\ud83c\udfaf The SuperOptiX Multi-Framework Advantage","text":""},{"location":"guides/pydantic-ai-integration/#one-playbook-multiple-frameworks","title":"One Playbook, Multiple Frameworks","text":"<p>SuperOptiX allows you to write your agent specification once and compile to any supported framework:</p> <pre><code># Same playbook, different frameworks\nsuper agent compile my_agent --framework pydantic-ai\nsuper agent compile my_agent --framework openai\nsuper agent compile my_agent --framework deepagents\n\n# GEPA optimization works across all frameworks\nsuper agent optimize my_agent --framework pydantic-ai --max-metric-calls 20  # Super light test\n# or\nsuper agent optimize my_agent --framework pydantic-ai --auto light  # Recommended\n</code></pre>"},{"location":"guides/pydantic-ai-integration/#when-to-use-pydantic-ai","title":"When to Use Pydantic AI","text":"<p>Choose Pydantic AI when: - You need type-safe structured outputs - You want native MCP tool integration - You prefer modern async/await APIs - You're building deployment applications - You need validated, reliable responses</p> <p>Choose DSPy when: - You need maximum optimization flexibility - You want to optimize multiple components - You have well-defined, focused tasks - You want proven optimization improvements</p> <p>Choose OpenAI SDK when: - You need multi-agent handoffs - You want built-in session management - You need guardrails support - You prefer simple, straightforward API</p>"},{"location":"guides/pydantic-ai-integration/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Pydantic AI Documentation</li> <li>Pydantic AI GitHub</li> <li>MCP Protocol Specification</li> <li>SuperOptiX Multi-Framework Guide</li> <li>GEPA Optimization Guide</li> <li>MCP Optimization Tutorial</li> </ul>"},{"location":"guides/pydantic-ai-integration/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<ol> <li>Try the demo: <code>super agent pull developer &amp;&amp; super agent compile developer --framework pydantic-ai</code></li> <li>Add MCP tools: Configure MCP servers in your playbook</li> <li>Optimize (Optional): Run GEPA optimization to improve performance</li> <li>\u26a0\ufe0f Only run if: You have high-end GPU AND understand the costs</li> <li>\u26a0\ufe0f Resource Warning: Makes many LLM API calls (20-600+ depending on settings)</li> <li>\u26a0\ufe0f Cost Warning: Use local <code>ollama/llama3.1:8b</code> to avoid API charges ($0 vs $5-100+)</li> <li>Ultra fast: <code>--max-metric-calls 3</code> (~30s-1m, ~3 API calls)</li> <li>Quick test: <code>--max-metric-calls 10</code> (~1-2 minutes, ~10 API calls)</li> <li>Recommended: <code>--auto light</code> (~5-10 minutes, ~50-100 API calls)</li> <li>Skip optimization if your agent already works well - it's optional!</li> <li>Deploy: Use the generated pipeline in your application</li> </ol>"},{"location":"guides/pydantic-ai-integration/#optimization-time-guide","title":"\ud83d\udcca Optimization Time Guide","text":"Option Command Time API Calls Use Case Ultra Fast <code>--max-metric-calls 3</code> ~30s-1m ~3 calls Verify optimization works Super Light <code>--max-metric-calls 10</code> ~1-2 min ~10 calls Quick test Light <code>--max-metric-calls 20</code> ~2-3 min ~20 calls Quick test, verify it works Light <code>--auto light</code> ~5-10 min ~50-100 calls \u2b50 Recommended - Balanced speed/quality Medium <code>--auto medium</code> ~15-30 min ~150-300 calls Better results, more iterations Heavy <code>--auto heavy</code> ~30-60 min ~300-600 calls Maximum quality, deployment ready <p>\u26a0\ufe0f Cost Estimates (with cloud models like GPT-4o): - Super Light: ~$0.10-2.00 - Light: ~$0.50-10.00 - Medium: ~$5-50.00 - Heavy: ~$20-100+</p> <p>\ud83d\udca1 Save money: Use <code>--reflection-lm ollama/llama3.1:8b</code> for free local optimization!</p>"},{"location":"guides/pydantic-ai-integration/#observability-with-logfire","title":"\ud83d\udcca Observability with LogFire","text":"<p>SuperOptiX includes native LogFire integration for Pydantic AI agents, providing comprehensive observability for your agents. See the LogFire Integration Guide for:</p> <ul> <li>Tracing agent executions</li> <li>Monitoring LLM calls and tool usage</li> <li>Tracking token usage and costs</li> <li>Viewing traces in LogFire dashboard or local backends (Jaeger)</li> </ul> <p>Happy building! \ud83d\ude80</p>"},{"location":"guides/pydantic-ai-mcp-demo/","title":"\ud83d\udc0d Pydantic AI MCP Demo","text":"<p>Step-by-step guide to using MCP (Model Context Protocol) with Pydantic AI in SuperOptiX.</p> <p>This demo shows how to: 1. Set up a simple MCP server 2. Configure MCP in your playbook 3. Compile and run a Pydantic AI agent with MCP tools 4. Optimize MCP tool descriptions with GEPA</p> <p>Tested and Working with <code>llama3.1:8b</code> on local Ollama! \ud83c\udf89</p>"},{"location":"guides/pydantic-ai-mcp-demo/#prerequisites","title":"Prerequisites","text":"<pre><code># Install SuperOptiX with Pydantic AI\npip install superoptix[frameworks-pydantic-ai]\n\n# Install MCP SDK (included with pydantic-ai, but ensure it's there)\npip install mcp\n\n# Node.js (for filesystem MCP server)\nnode --version  # Should be 18+\nnpx --version   # Required for running MCP servers\n\n# Ollama (for local model inference)\nollama pull llama3.1:8b\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#quick-start-pull-the-mcp-demo-agent","title":"Quick Start: Pull the MCP Demo Agent","text":"<p>The fastest way to try MCP with Pydantic AI:</p> <pre><code># Initialize project\nsuper init swe\ncd swe\n\n# Pull the pre-configured MCP demo agent\nsuper agent pull pydantic-mcp\n\n# Compile with Pydantic AI framework\nsuper agent compile pydantic-mcp --framework pydantic-ai\n\n# Test it!\nsuper agent run pydantic-mcp --goal \"List all files in /private/tmp\"\n</code></pre> <p>Expected Output: <pre><code>Using model: llama3.1:8b\n\ud83d\udee0\ufe0f  Initialized MCP stdio server: filesystem\nInitialized 1 MCP server(s)\n\ud83d\ude80 Running agent with input: List all files in /tmp\nModel response received\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect   \u2503 Value                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Response \u2502 The files in the /private/tmp directory are: \u2502\n\u2502          \u2502 * node-compile-cache (directory)             \u2502\n\u2502          \u2502 * test.txt (file)                            \u2502\n\u2502          \u2502 * ... more files ...                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#demo-filesystem-mcp-server-step-by-step","title":"Demo: Filesystem MCP Server (Step by Step)","text":""},{"location":"guides/pydantic-ai-mcp-demo/#step-1-initialize-project","title":"Step 1: Initialize Project","text":"<pre><code># Create a new project\nsuper init swe\ncd swe\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#step-2-pull-the-mcp-demo-agent","title":"Step 2: Pull the MCP Demo Agent","text":"<pre><code>super agent pull pydantic-mcp\n</code></pre> <p>This creates: <pre><code>swe/agents/pydantic-mcp/\n\u251c\u2500\u2500 playbook/\n\u2502   \u2514\u2500\u2500 pydantic_mcp_playbook.yaml  # MCP configuration included!\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 (generated after compile)\n</code></pre></p> <p>Important: The playbook filename should use underscores (<code>pydantic_mcp_playbook.yaml</code>), not hyphens. The generated pipeline expects this naming convention.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#step-3-review-the-mcp-configuration","title":"Step 3: Review the MCP Configuration","text":"<p>The <code>pydantic_mcp_playbook.yaml</code> includes:</p> <pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b  # Works great with 8b models!\n    api_base: http://localhost:11434\n\n  # MCP Integration\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: npx\n          args:\n            - \"-y\"\n            - \"@modelcontextprotocol/server-filesystem\"\n            - \"/private/tmp\"  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: fs_  # Tools become: fs_read_file, fs_write_file, etc.\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#step-4-compile-with-pydantic-ai","title":"Step 4: Compile with Pydantic AI","text":"<pre><code>super agent compile pydantic-mcp --framework pydantic-ai\n</code></pre> <p>Expected output: <pre><code>\ud83d\udd28 Compiling agent 'pydantic-mcp'...\n\ud83d\ude80 Compiling with PYDANTIC-AI framework...\nSuccessfully compiled with PYDANTIC-AI framework\n\ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#step-5-test-mcp-operations","title":"Step 5: Test MCP Operations","text":"<p>Create a test file first: <pre><code>echo \"Hello from MCP test file!\" &gt; /private/tmp/test.txt\n</code></pre></p> <p>Test 1: List Files <pre><code>super agent run pydantic-mcp --goal \"List all files in /private/tmp\"\n</code></pre></p> <p>Test 2: Read a File <pre><code>super agent run pydantic-mcp --goal \"Read the file at /private/tmp/test.txt\"\n</code></pre></p> <p>Expected output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect   \u2503 Value                                                             \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Response \u2502 The contents of the file at /private/tmp/test.txt have been read          \u2502\n\u2502          \u2502 successfully. The output is:                                      \u2502\n\u2502          \u2502                                                                   \u2502\n\u2502          \u2502 \"Hello from MCP test file!\"                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Test 3: Write a File <pre><code>super agent run pydantic-mcp --goal \"Create a file /private/tmp/mcp_demo.txt with content: MCP demo successful!\"\n</code></pre></p> <p>Verify: <pre><code>cat /private/tmp/mcp_demo.txt\n# Output: MCP demo successful!\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#mcp-tools-available","title":"MCP Tools Available","text":"<p>With the filesystem MCP server, your agent has access to these tools:</p> Tool Description <code>fs__list_directory</code> List files and directories <code>fs__read_text_file</code> Read text file contents <code>fs__write_text_file</code> Write/create text files <code>fs__create_directory</code> Create new directories <code>fs__move_file</code> Move/rename files <p>Note: Tool names have double underscore (<code>__</code>) between prefix and name due to how Pydantic AI processes the MCP server tools.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#custom-agent-with-mcp-manual-setup","title":"Custom Agent with MCP (Manual Setup)","text":"<p>If you want to add MCP to your own agent:</p>"},{"location":"guides/pydantic-ai-mcp-demo/#step-1-create-or-edit-your-playbook","title":"Step 1: Create or Edit Your Playbook","text":"<p>Add the <code>mcp</code> section to any playbook:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: My MCP Agent\n  id: my_mcp_agent\n\nspec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n\n  input_fields:\n    - name: query\n      type: string\n      description: User's request\n\n  output_fields:\n    - name: response\n      type: string\n      description: Agent's response\n\n  persona:\n    role: File Operations Assistant\n    goal: Help users with file system operations using MCP tools\n    backstory: You are an assistant with filesystem access through MCP.\n\n  # Add MCP Configuration\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: npx\n          args:\n            - \"-y\"\n            - \"@modelcontextprotocol/server-filesystem\"\n            - \"/private/tmp\"  # Use /private/tmp on macOS (or /tmp on Linux)\n        tool_prefix: fs_\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#step-2-save-with-correct-filename","title":"Step 2: Save with Correct Filename","text":"<p>Save as <code>my_mcp_agent_playbook.yaml</code> (using underscores).</p> <p>Important: The playbook filename should match the pattern <code>{agent_id}_playbook.yaml</code> using underscores.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#demo-python-based-mcp-server-no-nodejs-required","title":"Demo: Python-Based MCP Server (No Node.js Required!)","text":"<p>If you don't have Node.js, you can create a simple Python MCP server:</p>"},{"location":"guides/pydantic-ai-mcp-demo/#create-simple-calculator-mcp-server","title":"Create Simple Calculator MCP Server","text":"<p>Create <code>simple_calculator_server.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Simple calculator MCP server for demo.\"\"\"\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator\")\n\n@mcp.tool()\ndef add(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: float, b: float) -&gt; float:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n@mcp.tool()\ndef subtract(a: float, b: float) -&gt; float:\n    \"\"\"Subtract b from a.\"\"\"\n    return a - b\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre> <p>Make it executable: <pre><code>chmod +x simple_calculator_server.py\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#configure-in-playbook","title":"Configure in Playbook","text":"<pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: calculator\n        type: stdio\n        config:\n          command: \"python3\"\n          args: [\"simple_calculator_server.py\"]\n        tool_prefix: \"calc_\"\n</code></pre> <p>Now the agent has access to <code>calc_add</code>, <code>calc_multiply</code>, <code>calc_subtract</code> tools!</p>"},{"location":"guides/pydantic-ai-mcp-demo/#demo-mcp-tool-optimization","title":"Demo: MCP Tool Optimization","text":""},{"location":"guides/pydantic-ai-mcp-demo/#step-1-enable-tool-optimization","title":"Step 1: Enable Tool Optimization","text":"<p>Edit your playbook to enable MCP tool description optimization:</p> <pre><code>spec:\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/tmp\"]\n        tool_prefix: \"fs_\"  # Runtime prefix (optional)\n\n    # Enable tool optimization\n    optimization:\n      optimize_tool_descriptions: true\n      # IMPORTANT: Use actual MCP server tool names (WITHOUT prefix)\n      # The optimizer queries the server directly\n      tool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n</code></pre> <p>Important: The <code>tool_names</code> must match the actual MCP server tool names, not the prefixed names. The optimizer queries the MCP server directly, which returns unprefixed tool names like <code>read_file</code>, <code>write_file</code>, <code>list_directory</code>.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#step-2-add-bdd-scenarios-for-tool-usage","title":"Step 2: Add BDD Scenarios for Tool Usage","text":"<p>Add scenarios that use MCP tools:</p> <pre><code>feature_specifications:\n  scenarios:\n    - name: read_file_scenario\n      input:\n        feature_requirement: Read the file /tmp/config.json and tell me what database host it specifies\n      expected_output:\n        implementation: read_file fs_read_file /tmp/config.json database host config\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#step-3-optimize","title":"Step 3: Optimize","text":"<p>\u26a0\ufe0f IMPORTANT: Resource &amp; Cost Warning</p> <p>Optimization is resource-intensive and makes many LLM API calls: - Requires high-end GPU or cloud GPU access - Makes many LLM API calls (20-100+ depending on settings) - Can incur significant costs with cloud models (GPT-4, Claude) - RECOMMENDED: Use local <code>ollama/llama3.1:8b</code> to avoid API charges</p> <p>MCP optimization runs TWO phases, effectively doubling resource usage.</p> <p>Quick Test (Super Light - ~1-2 minutes, ~20 API calls): <pre><code># Use local Ollama for free optimization\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --max-metric-calls 20 \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre></p> <p>Note: Use <code>--max-metric-calls 20</code> instead of <code>--max-full-evals 1</code> for more precise control over total evaluations.</p> <p>Recommended (Light Mode - ~5-10 minutes, ~50-100 API calls): <pre><code># Use local Ollama (free) instead of cloud models (costly)\nsuper agent optimize developer \\\n  --framework pydantic-ai \\\n  --auto light \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre></p> <p>What happens: 1. Phase 1: GEPA optimizes MCP tool descriptions (e.g., how <code>read_file</code> is described) 2. Phase 2: GEPA optimizes agent instructions</p> <p>Results saved to: - <code>swe/agents/developer/optimized/developer_mcp_tool_descriptions.json</code> - <code>swe/agents/developer/optimized/developer_pydantic_ai_optimized.json</code></p>"},{"location":"guides/pydantic-ai-mcp-demo/#complete-example-playbook","title":"Complete Example Playbook","text":"<p>Here's a complete playbook with MCP enabled:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Developer Assistant with MCP\n  id: developer_mcp\n  namespace: software\n  version: 1.0.0\n  level: genies\n\nspec:\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n\n  input_fields:\n    - name: feature_requirement\n      type: str\n      description: Feature to implement\n\n  output_fields:\n    - name: implementation\n      type: str\n      description: Code implementation\n\n  persona:\n    role: Software Developer\n    goal: Write clean, efficient code with file system access\n\n  # MCP Configuration\n  mcp:\n    enabled: true\n    servers:\n      - name: filesystem\n        type: stdio\n        config:\n          command: \"npx\"\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/private/tmp\"]  # Use /private/tmp on macOS\n        tool_prefix: \"fs_\"  # Runtime prefix (tools become fs__read_file at runtime)\n\n    # Tool optimization - use actual MCP server tool names (WITHOUT prefix)\n    optimization:\n      optimize_tool_descriptions: true\n      tool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n\n  feature_specifications:\n    scenarios:\n      - name: read_config_file\n        input:\n          feature_requirement: Read /private/tmp/config.json and extract the database URL\n        expected_output:\n          implementation: read_file config.json database URL extract\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        reflection_lm: ollama/llama3.1:8b  # Use forward slash for LiteLLM\n        auto: light\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#testing-the-demo","title":"Testing the Demo","text":""},{"location":"guides/pydantic-ai-mcp-demo/#compile","title":"Compile","text":"<pre><code>super agent compile developer_mcp --framework pydantic-ai\n</code></pre> <p>Verify MCP initialization: - Look for: <code>\ud83d\udee0\ufe0f  Initialized MCP stdio server: filesystem</code> - Look for: <code>Initialized 1 MCP server(s)</code></p>"},{"location":"guides/pydantic-ai-mcp-demo/#run-with-file-operations","title":"Run with File Operations","text":"<pre><code># Create a test file first\necho '{\"db_host\": \"localhost\", \"db_port\": 5432}' &gt; /private/tmp/config.json\n\n# Run agent\nsuper agent run developer_mcp \\\n  --goal \"Read /private/tmp/config.json and tell me what database port is configured\"\n</code></pre> <p>Expected behavior: - Agent uses <code>fs_read_file</code> tool - Reads the file - Returns the database port</p>"},{"location":"guides/pydantic-ai-mcp-demo/#evaluate","title":"Evaluate","text":"<pre><code>super agent evaluate developer_mcp\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#optimize-if-enabled","title":"Optimize (if enabled)","text":"<p>\u26a0\ufe0f Resource Warning: MCP tool optimization runs two phases (tools + instructions), effectively doubling resource usage. Only run if you have adequate GPU/compute resources.</p> <p>Quick Test (Super Light - ~1-2 minutes, ~20 API calls): <pre><code># RECOMMENDED: Use local Ollama to avoid API costs\nsuper agent optimize developer_mcp \\\n  --framework pydantic-ai \\\n  --max-metric-calls 20 \\\n  --reflection-lm ollama/llama3.1:8b\n</code></pre></p> <p>Note: <code>--max-metric-calls 20</code> provides more precise control than <code>--max-full-evals 1</code>.</p> <p>Cost Tip: Local Ollama models are free. Cloud models (GPT-4, Claude) will incur charges (~$0.50-10+ per optimization run).</p> <p>Recommended (Light Mode - ~5-10 minutes, ~50-100 API calls): <pre><code># Use local Ollama (free) instead of cloud models (costly)\nsuper agent optimize developer_mcp \\\n  --framework pydantic-ai \\\n  --auto light \\\n  --reflection-lm ollama/llama3.1:8b  # Use forward slash for LiteLLM\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/pydantic-ai-mcp-demo/#mcp-server-not-starting","title":"MCP Server Not Starting","text":"<p>Error: <code>Failed to initialize MCP server</code> or no \"\ud83d\udee0\ufe0f Initialized MCP stdio server\" message</p> <p>Solutions:</p> <ol> <li> <p>Check Node.js/npx: <pre><code>which npx\nnpx --version\n# Should be available\n</code></pre></p> </li> <li> <p>Test the MCP server manually: <pre><code>npx -y @modelcontextprotocol/server-filesystem /private/tmp\n# Should output: \"Secure MCP Filesystem Server running on stdio\"\n# Note: On macOS, use /private/tmp (or /tmp on Linux)\n</code></pre></p> </li> <li> <p>Check playbook filename:</p> </li> <li>Use underscores: <code>pydantic_mcp_playbook.yaml</code>    - Not hyphens: <code>pydantic-mcp_playbook.yaml</code> </li> <li>Verify MCP config is correct: <pre><code>mcp:\n  enabled: true  # Must be true!\n  servers:\n    - name: filesystem\n      type: stdio\n      config:\n        command: npx  # Not \"npx\" with quotes\n        args:\n          - \"-y\"\n          - \"@modelcontextprotocol/server-filesystem\"\n          - \"/private/tmp\"  # Use /private/tmp on macOS (or /tmp on Linux)\n</code></pre></li> </ol>"},{"location":"guides/pydantic-ai-mcp-demo/#tools-not-being-called","title":"Tools Not Being Called","text":"<p>Symptom: Agent explains how to do something instead of using tools</p> <p>Solutions:</p> <ol> <li> <p>Verify MCP server initialized:    Look for these messages during run:    <pre><code>\ud83d\udee0\ufe0f  Initialized MCP stdio server: filesystem\nInitialized 1 MCP server(s)\n</code></pre></p> </li> <li> <p>Use explicit paths: <pre><code># Instead of: \"read test.txt\"\n# Use: \"Read the file at /private/tmp/test.txt\" (on macOS)\nsuper agent run pydantic-mcp --goal \"Read the file at /private/tmp/test.txt\"\n</code></pre></p> </li> <li> <p>Check allowed directories:    The filesystem server only allows access to specified directories:    <pre><code>Client does not support MCP Roots, using allowed directories: [ '/private/tmp' ]\n</code></pre>    Note: On macOS, <code>/tmp</code> is actually <code>/private/tmp</code>.</p> </li> </ol>"},{"location":"guides/pydantic-ai-mcp-demo/#path-access-denied","title":"Path Access Denied","text":"<p>Error: <code>Access denied - path outside allowed directories</code></p> <p>Cause: The model tried to access a file outside the allowed directory.</p> <p>Solution:  - macOS: Use <code>/private/tmp</code> in both MCP server config and file paths - Linux: Use <code>/tmp</code> as normal - Or modify the MCP server config to allow more directories:   <pre><code>args:\n  - \"-y\"\n  - \"@modelcontextprotocol/server-filesystem\"\n  - \"/private/tmp\"  # macOS\n  # - \"/tmp\"        # Linux\n  - \"/home/user/projects\"  # Add more allowed directories\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#model-returns-json-instead-of-content","title":"Model Returns JSON Instead of Content","text":"<p>Symptom: Response looks like <code>{\"field\": \"value\"}</code> instead of actual text</p> <p>Solution: This was fixed in SuperOptiX 0.2.1. Update: <pre><code>pip install --upgrade superoptix\n</code></pre></p> <p>The Pydantic AI template now uses plain text output mode, which works better with 8b models.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#mcp-tool-optimization-fails","title":"MCP Tool Optimization Fails","text":"<p>Error: <code>None of the specified tools found. Available: ['read_file', 'write_file', ...]</code></p> <p>Cause: The <code>tool_names</code> in your playbook don't match the actual MCP server tool names.</p> <p>Solution: Use actual MCP server tool names without the prefix: <pre><code># Wrong - using prefixed names\ntool_names: [\"fs_read_file\", \"fs_write_file\", \"fs_list_files\"]\n\n# Correct - using actual server tool names\ntool_names: [\"read_file\", \"write_file\", \"list_directory\"]\n</code></pre></p> <p>Why? The GEPA optimizer queries the MCP server directly, which returns unprefixed tool names. The <code>tool_prefix</code> only affects runtime naming in the agent.</p>"},{"location":"guides/pydantic-ai-mcp-demo/#litellm-provider-error","title":"LiteLLM Provider Error","text":"<p>Error: <code>LLM Provider NOT provided. You passed model=ollama:llama3.1:8b</code></p> <p>Solution: Use forward slash instead of colon for the reflection LM: <pre><code># Wrong\n--reflection-lm ollama:llama3.1:8b\n\n# Correct\n--reflection-lm ollama/llama3.1:8b\n</code></pre></p>"},{"location":"guides/pydantic-ai-mcp-demo/#verified-working-examples","title":"Verified Working Examples","text":"<p>These commands were tested and work with <code>llama3.1:8b</code>:</p> <pre><code># List directory contents (use /private/tmp on macOS, /tmp on Linux)\nsuper agent run pydantic-mcp --goal \"List all files in /private/tmp\"\n# Returns actual file list from filesystem\n\n# Read a file\necho \"Test content\" &gt; /private/tmp/test.txt\nsuper agent run pydantic-mcp --goal \"Read the file at /private/tmp/test.txt\"\n# Returns: \"Test content\"\n\n# Write a file\nsuper agent run pydantic-mcp --goal \"Create a file /private/tmp/hello.txt with content: Hello World\"\n# Creates the file\ncat /private/tmp/hello.txt  # Verify: \"Hello World\"\n</code></pre>"},{"location":"guides/pydantic-ai-mcp-demo/#whats-next","title":"What's Next?","text":"<ul> <li>Multiple MCP Servers: Connect multiple servers for diverse tools</li> <li>Remote Servers: Use HTTP/SSE servers for remote tools</li> <li>Custom Servers: Build your own MCP servers in Python</li> <li>Optimization: Optimize tool descriptions with GEPA</li> </ul>"},{"location":"guides/pydantic-ai-mcp-demo/#summary","title":"Summary","text":"Feature Status Notes MCP with Pydantic AI Working Full integration Local 8b models Working <code>llama3.1:8b</code> tested Filesystem operations Working Read, write, list Plain text output Working No JSON metadata Tool optimization Working GEPA support (use unprefixed tool names) Instruction optimization Working Two-phase optimization"},{"location":"guides/pydantic-ai-mcp-demo/#related-documentation","title":"Related Documentation","text":"<ul> <li>Pydantic AI Integration Guide</li> <li>MCP Protocol Guide</li> <li>GEPA Optimization Guide</li> <li>Tool Optimization Guide</li> </ul>"},{"location":"guides/rag/","title":"\ud83d\udd0d RAG (Retrieval-Augmented Generation) Guide","text":"<p>SuperOptiX provides universal RAG support across all 6 major agent frameworks, with powerful MCP (Model Context Protocol) integration for advanced knowledge retrieval.</p> <p>\ud83c\udf1f Key Achievement: RAG works seamlessly across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/rag/#overview","title":"Overview","text":"<p>RAG enhances AI agents by providing them with access to external knowledge sources. Instead of relying solely on pre-trained knowledge, agents can retrieve relevant information from documents, databases, or other sources to provide more accurate and up-to-date responses.</p>"},{"location":"guides/rag/#multi-framework-rag","title":"Multi-Framework RAG","text":"<p>RAG in SuperOptiX works consistently across all frameworks: - \ud83d\udd27 Framework Agnostic: Same RAG configuration works for all frameworks - \ud83d\udcca MCP Integration: Advanced Model Context Protocol support - \u26a1 Multiple Vector DBs: ChromaDB, LanceDB, Weaviate, Qdrant, Milvus - \ud83c\udfaf Universal Optimization: GEPA can optimize RAG-enhanced agents - \ud83d\udd04 Consistent API: Same configuration format across frameworks</p>"},{"location":"guides/rag/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/rag/#universal-rag-configuration","title":"Universal RAG Configuration","text":"<p>RAG works the same way across all frameworks! Just add the <code>rag</code> section to your playbook:</p> \ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>spec:\n  target_framework: dspy\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre> <pre><code>spec:\n  target_framework: openai\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre> <pre><code>spec:\n  target_framework: crewai\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre> <pre><code>spec:\n  target_framework: google-adk\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre> <pre><code>spec:\n  target_framework: microsoft\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre> <pre><code>spec:\n  target_framework: deepagents\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre>"},{"location":"guides/rag/#universal-workflow","title":"Universal Workflow","text":"<pre><code># Same workflow for ALL frameworks!\nsuper agent compile &lt;agent_name&gt;  # RAG automatically configured\nsuper agent evaluate &lt;agent_name&gt;  # Test with knowledge retrieval\nsuper agent optimize &lt;agent_name&gt; --auto medium  # GEPA optimizes RAG-enhanced agents\nsuper agent run &lt;agent_name&gt;  # Use with RAG-enhanced responses\n</code></pre>"},{"location":"guides/rag/#rag-configuration","title":"RAG Configuration","text":"<p>RAG is configured through the playbook YAML file. Here's the structure:</p> <pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma  # or vector_database: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: knowledge_base\n</code></pre>"},{"location":"guides/rag/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>enabled</code> bool true Enable/disable RAG <code>retriever_type</code> string - Vector database type <code>top_k</code> int 5 Number of documents to retrieve <code>chunk_size</code> int 512 Document chunk size <code>chunk_overlap</code> int 50 Overlap between chunks <code>embedding_model</code> string all-MiniLM-L6-v2 Sentence transformer model"},{"location":"guides/rag/#mcp-model-context-protocol-integration","title":"\ud83d\udd0c MCP (Model Context Protocol) Integration","text":"<p>SuperOptiX supports MCP (Model Context Protocol) for advanced RAG capabilities and tool integration.</p>"},{"location":"guides/rag/#what-is-mcp","title":"What is MCP?","text":"<p>MCP (Model Context Protocol) is a universal protocol for connecting AI agents to external data sources, tools, and knowledge bases. It provides: - Standardized Connections: Connect to any MCP server - Advanced RAG: Enhanced knowledge retrieval - Tool Integration: Seamless tool discovery and execution - Multi-Framework Support: Works across all frameworks</p>"},{"location":"guides/rag/#mcp-configuration","title":"MCP Configuration","text":"<pre><code>spec:\n  target_framework: dspy  # Works with any framework\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/repo\"]\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre>"},{"location":"guides/rag/#mcp-benefits","title":"MCP Benefits","text":"<ul> <li>Universal Protocol: Works across all frameworks</li> <li>Rich Integrations: Connect to filesystems, databases, APIs, Git repos</li> <li>Tool Discovery: Automatic tool detection and execution</li> <li>Enhanced RAG: Better context retrieval with MCP servers</li> <li>GEPA Optimization: Optimize MCP-enhanced agents with GEPA</li> </ul>"},{"location":"guides/rag/#example-mcp-rag-multi-framework","title":"Example: MCP + RAG + Multi-Framework","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: mcp_enhanced_agent\nspec:\n  target_framework: openai  # Works with ANY framework!\n  language_model:\n    provider: ollama\n    model: gpt-oss:20b\n  rag:\n    enabled: true\n    retriever_type: chroma\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./docs\"]\n  optimization:\n    optimizer:\n      name: GEPA  # Optimize MCP-enhanced agents!\n      params:\n        auto: medium\n</code></pre> <p>Learn More: See our MCP Protocol Guide for detailed MCP integration examples.</p>"},{"location":"guides/rag/#supported-vector-databases","title":"\ud83d\udce6 Supported Vector Databases","text":""},{"location":"guides/rag/#chromadb-recommended-for-local-development","title":"ChromaDB (Recommended for Local Development)","text":"<p>ChromaDB is the default and recommended option for local development. It's lightweight, requires no external dependencies, and automatically downloads embedding models.</p>"},{"location":"guides/rag/#configuration","title":"Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: my_knowledge_base\n</code></pre>"},{"location":"guides/rag/#example-usage","title":"Example Usage","text":"<pre><code>from agile.agents.qa.pipelines.qa_pipeline import QaPipeline\n\n# Initialize with ChromaDB\npipeline = QaPipeline()\n\n# Add documents\ndocuments = [\n    {\n        'content': 'ChromaDB is a lightweight vector database for AI applications.',\n        'metadata': {'source': 'chroma_docs', 'version': '1.0'}\n    }\n]\n\npipeline.add_documents(documents)\n\n# Query\nresult = await pipeline.forward(\"What is ChromaDB?\")\nprint(result['response'])\n</code></pre>"},{"location":"guides/rag/#lancedb","title":"LanceDB","text":"<p>LanceDB is a modern vector database built on Apache Arrow, offering high performance and easy integration.</p>"},{"location":"guides/rag/#configuration_1","title":"Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: lancedb\n    config:\n      top_k: 5\n      chunk_size: 512\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      table_name: knowledge_table\n      database_path: ./data/lancedb\n</code></pre>"},{"location":"guides/rag/#example-usage_1","title":"Example Usage","text":"<pre><code>from agile.agents.qa.pipelines.qa_pipeline import QaPipeline\n\n# Initialize with LanceDB\npipeline = QaPipeline()\n\n# Add documents\ndocuments = [\n    {\n        'content': 'LanceDB provides fast vector search with Apache Arrow.',\n        'metadata': {'source': 'lancedb_docs', 'category': 'database'}\n    }\n]\n\npipeline.add_documents(documents)\n\n# Query\nresult = await pipeline.forward(\"What is LanceDB?\")\nprint(result['response'])\n</code></pre>"},{"location":"guides/rag/#weaviate","title":"Weaviate","text":"<p>Weaviate is a vector database with a rich ecosystem and cloud offerings.</p>"},{"location":"guides/rag/#configuration_2","title":"Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: weaviate\n    config:\n      top_k: 5\n      chunk_size: 512\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: knowledge_collection\n      weaviate_url: http://localhost:8080\n      api_key: your_api_key  # Optional\n</code></pre>"},{"location":"guides/rag/#example-usage_2","title":"Example Usage","text":"<pre><code>from agile.agents.qa.pipelines.qa_pipeline import QaPipeline\n\n# Initialize with Weaviate\npipeline = QaPipeline()\n\n# Add documents\ndocuments = [\n    {\n        'content': 'Weaviate is a vector database with rich ecosystem.',\n        'metadata': {'source': 'weaviate_docs', 'category': 'database'}\n    }\n]\n\npipeline.add_documents(documents)\n\n# Query\nresult = await pipeline.forward(\"What is Weaviate?\")\nprint(result['response'])\n</code></pre>"},{"location":"guides/rag/#qdrant","title":"Qdrant","text":"<p>Qdrant is a high-performance vector database with advanced filtering capabilities.</p>"},{"location":"guides/rag/#configuration_3","title":"Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: qdrant\n    config:\n      top_k: 5\n      chunk_size: 512\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: knowledge_collection\n      qdrant_url: http://localhost:6333\n      api_key: your_api_key  # Optional\n</code></pre>"},{"location":"guides/rag/#milvus","title":"Milvus","text":"<p>Milvus is a scalable vector database designed for production use.</p>"},{"location":"guides/rag/#configuration_4","title":"Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: milvus\n    config:\n      top_k: 5\n      chunk_size: 512\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: knowledge_collection\n      milvus_host: localhost\n      milvus_port: 19530\n</code></pre>"},{"location":"guides/rag/#document-ingestion","title":"Document Ingestion","text":""},{"location":"guides/rag/#adding-documents","title":"Adding Documents","text":"<pre><code># Single document\npipeline.add_document({\n    'content': 'Your document content here.',\n    'metadata': {\n        'source': 'manual',\n        'category': 'general',\n        'date': '2024-01-01'\n    }\n})\n\n# Multiple documents\ndocuments = [\n    {\n        'content': 'First document content.',\n        'metadata': {'source': 'file1', 'category': 'technical'}\n    },\n    {\n        'content': 'Second document content.',\n        'metadata': {'source': 'file2', 'category': 'business'}\n    }\n]\npipeline.add_documents(documents)\n</code></pre>"},{"location":"guides/rag/#document-chunking","title":"Document Chunking","text":"<p>Documents are automatically chunked for better retrieval:</p> <pre><code>spec:\n  rag:\n    config:\n      chunk_size: 512      # Characters per chunk\n      chunk_overlap: 50    # Overlap between chunks\n      chunk_strategy: \"recursive\"  # recursive, fixed, semantic\n</code></pre>"},{"location":"guides/rag/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"guides/rag/#custom-embedding-models","title":"Custom Embedding Models","text":"<pre><code>spec:\n  rag:\n    vector_store:\n      embedding_model: sentence-transformers/all-mpnet-base-v2  # Better quality\n      # or\n      embedding_model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  # Multilingual\n</code></pre>"},{"location":"guides/rag/#retrieval-strategies","title":"Retrieval Strategies","text":"<pre><code>spec:\n  rag:\n    config:\n      top_k: 5\n      similarity_threshold: 0.7\n      rerank: true  # Enable re-ranking for better results\n      hybrid_search: true  # Combine dense and sparse retrieval\n</code></pre>"},{"location":"guides/rag/#filtering-and-metadata","title":"Filtering and Metadata","text":"<pre><code># Query with metadata filters\nresult = await pipeline.forward(\n    \"What is DSPy?\",\n    filters={\n        'category': 'framework',\n        'source': 'docs'\n    }\n)\n</code></pre>"},{"location":"guides/rag/#rag-with-tools","title":"RAG with Tools","text":"<p>RAG can be combined with tools for enhanced capabilities:</p> <pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma\n  tool_calling:\n    enabled: true\n    available_tools: [\"web_search\", \"file_reader\"]\n</code></pre>"},{"location":"guides/rag/#best-practices","title":"Best Practices","text":""},{"location":"guides/rag/#document-quality","title":"Document Quality","text":"<ul> <li>Use high-quality, relevant documents</li> <li>Ensure proper formatting and structure</li> <li>Include comprehensive metadata</li> <li>Regular updates and maintenance</li> </ul>"},{"location":"guides/rag/#chunking-strategy","title":"Chunking Strategy","text":"<ul> <li>Choose appropriate chunk sizes (512-1024 characters)</li> <li>Use overlap to maintain context</li> <li>Consider semantic boundaries</li> <li>Test different strategies for your use case</li> </ul>"},{"location":"guides/rag/#embedding-models","title":"Embedding Models","text":"<ul> <li>Use appropriate models for your domain</li> <li>Consider multilingual requirements</li> <li>Balance quality vs. performance</li> <li>Regular model updates</li> </ul>"},{"location":"guides/rag/#vector-database-selection","title":"Vector Database Selection","text":"<ul> <li>ChromaDB: Development and prototyping</li> <li>LanceDB: High-performance local applications</li> <li>Weaviate: Rich ecosystem and cloud features</li> <li>Qdrant: Advanced filtering requirements</li> <li>Milvus: Large-scale production deployments</li> </ul>"},{"location":"guides/rag/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor retrieval latency</li> <li>Optimize chunk sizes</li> <li>Use appropriate top_k values</li> <li>Implement caching strategies</li> </ul>"},{"location":"guides/rag/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/rag/#common-issues","title":"Common Issues","text":""},{"location":"guides/rag/#rag-not-working","title":"RAG Not Working","text":"<pre><code># Check RAG status\nstatus = pipeline.get_rag_status()\nprint(f\"RAG enabled: {status['enabled']}\")\nprint(f\"Documents loaded: {status['document_count']}\")\n</code></pre>"},{"location":"guides/rag/#poor-retrieval-quality","title":"Poor Retrieval Quality","text":"<pre><code># Adjust configuration\nspec:\n  rag:\n    config:\n      top_k: 10  # Increase for more candidates\n      similarity_threshold: 0.5  # Lower threshold\n    vector_store:\n      embedding_model: sentence-transformers/all-mpnet-base-v2  # Better model\n</code></pre>"},{"location":"guides/rag/#performance-issues","title":"Performance Issues","text":"<pre><code># Optimize for performance\nspec:\n  rag:\n    config:\n      top_k: 3  # Reduce for faster retrieval\n      chunk_size: 256  # Smaller chunks\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2  # Faster model\n</code></pre>"},{"location":"guides/rag/#integration-examples","title":"Integration Examples","text":""},{"location":"guides/rag/#rag-memory-integration","title":"RAG + Memory Integration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma\n  memory:\n    enabled: true\n    long_term:\n      enable_embeddings: true\n</code></pre>"},{"location":"guides/rag/#rag-optimization-integration","title":"RAG + Optimization Integration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma\n  optimization:\n    enabled: true\n    strategy: bootstrap_few_shot\n</code></pre>"},{"location":"guides/rag/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Agent Development Guide - Complete agent development workflow</li> <li>Memory Guide - Memory systems integration</li> <li>Quick Start Guide - Getting started with SuperOptiX </li> </ul>"},{"location":"guides/rails-analogy/","title":"For Rails Developers: SuperOptiX Explained","text":""},{"location":"guides/rails-analogy/#overview","title":"Overview","text":"<p>If you're a web developer familiar with Ruby on Rails, you already understand the core philosophy behind SuperOptiX. This guide explains SuperOptiX through the lens of Rails concepts you know and love.</p> <p>TL;DR: SuperOptiX brings Rails-style convention, scaffolding, and productivity to AI agent development.</p>"},{"location":"guides/rails-analogy/#the-rails-revolution","title":"The Rails Revolution","text":""},{"location":"guides/rails-analogy/#what-rails-did-for-web-development","title":"What Rails Did for Web Development","text":"<p>In 2004, Rails revolutionized web development by providing:</p> <ol> <li>Convention Over Configuration</li> <li>Predictable project structure</li> <li>Standard patterns everyone follows</li> <li> <p>Less boilerplate, more productivity</p> </li> <li> <p>Scaffold Generators</p> </li> <li><code>rails generate scaffold Post title:string body:text</code></li> <li>Instant CRUD operations</li> <li> <p>Focus on business logic, not plumbing</p> </li> <li> <p>Migration System</p> </li> <li><code>rails db:migrate</code></li> <li>Evolve schema over time</li> <li> <p>Version-controlled changes</p> </li> <li> <p>Testing Framework (RSpec)</p> </li> <li>Spec-driven development</li> <li><code>describe</code> and <code>it</code> blocks</li> <li> <p>Clear, readable tests</p> </li> <li> <p>You Focus on Business Logic</p> </li> <li>Rails handles infrastructure</li> <li>You build features</li> <li>Framework provides rails (pun intended)</li> </ol>"},{"location":"guides/rails-analogy/#the-superoptix-parallel","title":"The SuperOptiX Parallel","text":""},{"location":"guides/rails-analogy/#what-superoptix-does-for-ai-agents","title":"What SuperOptiX Does for AI Agents","text":"<p>SuperOptiX brings the same philosophy to AI agents:</p> \ud83d\ude82 Ruby on Rails <p>Web Application Framework</p> <ul> <li>Convention over configuration</li> <li>Scaffold generators</li> <li>Database migrations</li> <li>RSpec testing</li> <li>MVC architecture</li> <li>ActiveRecord ORM</li> </ul> \ud83e\udd16 SuperOptiX <p>AI Agent Framework</p> <ul> <li>Convention over configuration</li> <li>Agent spec generators</li> <li>GEPA optimization (like migrations)</li> <li>Spec-driven testing</li> <li>Multi-layer architecture</li> <li>Framework-agnostic adapters</li> </ul>"},{"location":"guides/rails-analogy/#convention-over-configuration","title":"Convention Over Configuration","text":""},{"location":"guides/rails-analogy/#rails-conventions","title":"Rails Conventions","text":"<pre><code># Rails knows where everything goes\napp/\n\u251c\u2500\u2500 models/          # Rails looks here for models\n\u251c\u2500\u2500 controllers/     # Rails looks here for controllers  \n\u251c\u2500\u2500 views/           # Rails looks here for views\n\u2514\u2500\u2500 db/migrations/   # Rails looks here for migrations\n\n# No configuration needed! Just follow conventions.\n</code></pre>"},{"location":"guides/rails-analogy/#superoptix-conventions","title":"SuperOptiX Conventions","text":"<pre><code># SuperOptiX knows where everything goes\n&lt;project&gt;/\n\u251c\u2500\u2500 agents/          # SuperOptiX looks here for agents\n\u251c\u2500\u2500 tools/           # SuperOptiX looks here for tools\n\u251c\u2500\u2500 knowledge/       # SuperOptiX looks here for knowledge\n\u251c\u2500\u2500 memory/          # SuperOptiX looks here for memory\n\u2514\u2500\u2500 protocols/       # SuperOptiX looks here for protocols\n\n# No configuration needed! Just follow conventions.\n</code></pre> <p>Benefit: Everyone's SuperOptiX project has the same structure. Onboarding is instant.</p>"},{"location":"guides/rails-analogy/#scaffold-generators","title":"Scaffold Generators","text":""},{"location":"guides/rails-analogy/#rails-scaffolding","title":"Rails Scaffolding","text":"<pre><code># Generate a complete resource\nrails generate scaffold Post title:string body:text author:string\n\n# Rails creates:\n# - Model (Post)\n# - Controller (PostsController)\n# - Views (index, show, edit, new)\n# - Migration (create_posts)\n# - Routes\n# - Tests\n</code></pre> <p>You get a working CRUD app. You customize from there.</p>"},{"location":"guides/rails-analogy/#superoptix-scaffolding","title":"SuperOptiX Scaffolding","text":"<pre><code># Generate a complete agent\nsuper spec generate blog_writer --template genie\n\n# SuperOptiX creates:\n# - Playbook (blog_writer_playbook.yaml)\n# - Spec scenarios (Given-When-Then)\n# - Pipeline structure\n# - Evaluation framework\n# - Optimization config\n</code></pre> <p>You get a working agent scaffold. You customize from there.</p> <p>Parallel: Both give you a working skeleton. You add the intelligence/business logic.</p>"},{"location":"guides/rails-analogy/#migration-system-vs-optimization-system","title":"Migration System vs Optimization System","text":""},{"location":"guides/rails-analogy/#rails-migrations","title":"Rails Migrations","text":"<pre><code># Create migration\nrails generate migration AddPublishedToPosts published:boolean\n\n# Apply migration\nrails db:migrate\n\n# Rollback if needed\nrails db:rollback\n</code></pre> <p>Migrations evolve your database schema over time. Version-controlled. Reversible.</p>"},{"location":"guides/rails-analogy/#superoptix-optimization","title":"SuperOptiX Optimization","text":"<pre><code># Create agent\nsuper spec generate customer_agent\n\n# Run optimization (like migration)\nsuper agent optimize customer_agent\n\n# Evaluate results\nsuper agent evaluate customer_agent\n\n# Rollback to previous version if needed\nsuper agent load customer_agent --checkpoint previous\n</code></pre> <p>GEPA evolves your agent prompts over time. Version-controlled (checkpoints). Reversible.</p> <p>Parallel: Both systems evolve your application over time with version control.</p>"},{"location":"guides/rails-analogy/#rspec-vs-superspec","title":"RSpec vs SuperSpec","text":""},{"location":"guides/rails-analogy/#rspec-testing-ruby","title":"RSpec Testing (Ruby)","text":"<pre><code># spec/models/post_spec.rb\ndescribe Post do\n  describe '#publish' do\n    it 'sets published to true' do\n      post = Post.new(published: false)\n      post.publish\n      expect(post.published).to be true\n    end\n\n    it 'sends notification email' do\n      post = Post.new\n      expect { post.publish }.to change { ActionMailer::Base.deliveries.count }.by(1)\n    end\n  end\nend\n</code></pre> <p>Clear, readable, testable specifications.</p>"},{"location":"guides/rails-analogy/#superspec-testing-ai-agents","title":"SuperSpec Testing (AI Agents)","text":"<pre><code># agent_playbook.yaml\nspec:\n  evaluation:\n  feature_specifications:\n    scenarios:\n      - name: blog_post_publishing\n        description: Agent should format and publish blog post correctly\n        input:\n          blog_draft: Draft blog post about AI trends\n          publish_request: true\n        expected_output:\n          formatted_post: Post with proper formatting and SEO metadata\n\n      - name: notification_handling\n        description: Agent should send notifications after publishing\n        input:\n          published_post: Blog post data\n          notification_type: email_and_social\n        expected_output:\n          notifications_sent: Confirmation of email and social media updates\n</code></pre> <p>Clear, readable, testable specifications. Same philosophy, different domain.</p>"},{"location":"guides/rails-analogy/#focus-on-business-logic","title":"Focus on Business Logic","text":""},{"location":"guides/rails-analogy/#rails-philosophy","title":"Rails Philosophy","text":"<p>Rails handles: - Database connections - Request routing - Session management - Asset compilation - Background jobs</p> <p>You focus on: - Business rules - User experience - Domain logic</p>"},{"location":"guides/rails-analogy/#superoptix-philosophy","title":"SuperOptiX Philosophy","text":"<p>SuperOptiX handles: - Framework integration - Prompt optimization - Memory management - Tool orchestration - Context optimization</p> <p>You focus on: - Agent intelligence - Task specifications - Domain expertise</p> <p>Same division of labor!</p>"},{"location":"guides/rails-analogy/#dont-repeat-yourself-dry","title":"\"Don't Repeat Yourself\" (DRY)","text":""},{"location":"guides/rails-analogy/#rails-dry","title":"Rails DRY","text":"<pre><code># Define once\nclass User &lt; ApplicationRecord\n  validates :email, presence: true\nend\n\n# Rails generates:\n# - Database validations\n# - Form validations  \n# - API validations\n# - Error messages\n</code></pre> <p>One definition, many uses.</p>"},{"location":"guides/rails-analogy/#superoptix-dry","title":"SuperOptiX DRY","text":"<pre><code># Define once\nspec:\n  persona:\n    role: Customer Support Agent\n    goal: Help customers efficiently\n\n  target_framework: dspy\n</code></pre> <pre><code># SuperOptiX generates:\n# - Framework-specific code (DSPy/CrewAI/OpenAI/etc)\n# - Evaluation pipelines\n# - Optimization workflows\n# - Deployment configs\n</code></pre> <p>One specification, many frameworks.</p> <p>Same principle: Write once, use everywhere.</p>"},{"location":"guides/rails-analogy/#the-rails-way-vs-superoptix-way","title":"The \"Rails Way\" vs \"SuperOptiX Way\"","text":""},{"location":"guides/rails-analogy/#rails-way","title":"Rails Way","text":"<p>There's a \"Rails way\" to build web apps: - RESTful routes - Skinny controllers, fat models - Service objects for complex logic - Convention over configuration</p> <p>If you follow the Rails way, you get: - Maintainable code - Predictable structure - Easy collaboration - Fast development</p>"},{"location":"guides/rails-analogy/#superoptix-way","title":"SuperOptiX Way","text":"<p>There's a \"SuperOptiX way\" to build AI agents: - Spec-driven development - Context-first design - GEPA optimization for quality - Convention over configuration</p> <p>If you follow the SuperOptiX way, you get: - Optimized agents - Predictable structure - Easy collaboration - Fast development</p> <p>Same benefits, same philosophy.</p>"},{"location":"guides/rails-analogy/#productivity-gains","title":"Productivity Gains","text":""},{"location":"guides/rails-analogy/#rails-beforeafter","title":"Rails Before/After","text":"<p>Before Rails: - Write models manually - Write SQL manually - Write routing manually - Write CRUD manually - Lots of boilerplate</p> <p>With Rails: - <code>rails generate scaffold</code> - Convention handles the rest - Focus on business logic</p>"},{"location":"guides/rails-analogy/#superoptix-beforeafter","title":"SuperOptiX Before/After","text":"<p>Before SuperOptiX: - Write prompts manually - Test manually - Optimize by trial and error - Rewrite for each framework - Lots of experimentation</p> <p>With SuperOptiX: - <code>super spec generate</code> - Write spec scenarios - <code>super agent optimize</code> - Works on any framework - GEPA handles optimization</p>"},{"location":"guides/rails-analogy/#commands-side-by-side","title":"Commands Side-by-Side","text":""},{"location":"guides/rails-analogy/#rails-workflow","title":"Rails Workflow","text":"<pre><code># Generate scaffold\nrails generate scaffold Post title:string body:text\n\n# Run migration\nrails db:migrate\n\n# Run tests\nrails test\n\n# Start server\nrails server\n\n# Deploy\ngit push heroku main\n</code></pre>"},{"location":"guides/rails-analogy/#superoptix-workflow","title":"SuperOptiX Workflow","text":"<pre><code># Generate scaffold\nsuper spec generate blog_writer --template genie\n\n# Run optimization (like migration)\nsuper agent optimize blog_writer\n\n# Run tests\nsuper agent evaluate blog_writer\n\n# Run agent\nsuper agent run blog_writer\n\n# Deploy (orchestrate)\nsuper orchestra run blog_orchestra\n</code></pre> <p>Same cadence, same flow, same developer experience.</p>"},{"location":"guides/rails-analogy/#when-to-use-each","title":"When to Use Each","text":""},{"location":"guides/rails-analogy/#use-rails-when","title":"Use Rails When:","text":"<ul> <li>Building web applications</li> <li>Need CRUD operations</li> <li>HTTP requests/responses</li> <li>Database-backed apps</li> </ul>"},{"location":"guides/rails-analogy/#use-superoptix-when","title":"Use SuperOptiX When:","text":"<ul> <li>Building AI agents</li> <li>Need intelligent behavior</li> <li>LLM-powered reasoning</li> <li>Context-aware systems</li> </ul>"},{"location":"guides/rails-analogy/#use-both-together","title":"Use Both Together:","text":"<ul> <li>Rails app with AI agents</li> <li>SuperOptiX agents as Rails background jobs</li> <li>Rails API + SuperOptiX agents</li> <li>Best of both worlds!</li> </ul>"},{"location":"guides/rails-analogy/#learning-curve","title":"Learning Curve","text":""},{"location":"guides/rails-analogy/#if-you-know-rails","title":"If You Know Rails","text":"<p>You already understand: - Convention over configuration - Generator commands - Migration workflows - Spec-driven testing (RSpec) - Project structure patterns</p> <p>You'll learn: - Agent specifications (like model specs) - GEPA optimization (like schema migrations) - Multi-framework support (like multi-database) - Context optimization (new concept)</p>"},{"location":"guides/rails-analogy/#community-ecosystem","title":"Community &amp; Ecosystem","text":""},{"location":"guides/rails-analogy/#rails-ecosystem","title":"Rails Ecosystem","text":"<ul> <li>RubyGems: Shared libraries</li> <li>Rails Guides: Comprehensive docs</li> <li>Conventions: Everyone follows them</li> <li>Generators: Community gems add generators</li> <li>Plugins: Extend Rails easily</li> </ul>"},{"location":"guides/rails-analogy/#superoptix-ecosystem","title":"SuperOptiX Ecosystem","text":"<ul> <li>Marketplace: Shared agents (like RubyGems)</li> <li>SuperOptiX Guides: Comprehensive docs (like Rails Guides)</li> <li>Conventions: Standard project structure</li> <li>Spec Generator: Community can contribute templates</li> <li>Framework Adapters: Extend to new frameworks easily</li> </ul> <p>Same community-driven growth model.</p>"},{"location":"guides/rails-analogy/#philosophy","title":"Philosophy","text":""},{"location":"guides/rails-analogy/#rails-philosophy-david-heinemeier-hansson","title":"Rails Philosophy (David Heinemeier Hansson)","text":"<p>\"Convention over Configuration\"</p> <p>\"Optimize for programmer happiness\"</p> <p>\"The menu is omakase\" (chef's choice - opinionated defaults)</p>"},{"location":"guides/rails-analogy/#superoptix-philosophy_1","title":"SuperOptiX Philosophy","text":"<p>\"Convention over Configuration\"</p> <p>\"Optimize for agent performance\"</p> <p>\"Spec-driven by default\" (opinionated defaults)</p> <p>Aligned philosophies.</p>"},{"location":"guides/rails-analogy/#code-comparison","title":"Code Comparison","text":""},{"location":"guides/rails-analogy/#rails-model","title":"Rails Model","text":"<pre><code># app/models/user.rb\nclass User &lt; ApplicationRecord\n  # Convention: table name is 'users'\n  # Convention: primary key is 'id'\n\n  validates :email, presence: true, uniqueness: true\n  has_many :posts\n\n  def full_name\n    \"#{first_name} #{last_name}\"\n  end\nend\n</code></pre> <p>Convention handles most of it. You add business logic.</p>"},{"location":"guides/rails-analogy/#superoptix-agent","title":"SuperOptiX Agent","text":"<pre><code># agents/customer_agent/playbook.yaml\nspec:\n  # Convention: persona defines role/goal\n  # Convention: evaluation uses feature specifications\n\n  persona:\n    role: Customer Support Agent\n    goal: Help customers efficiently\n\n  feature_specifications:\n    scenarios:\n      - name: refund_handling\n        description: Agent should process refund requests correctly\n        input:\n          customer_request: Refund request for recent order\n          order_id: ORD-12345\n        expected_output:\n          response: Refund policy explained and request processed\n</code></pre> <p>Convention handles most of it. You add agent intelligence.</p>"},{"location":"guides/rails-analogy/#activerecord-vs-framework-adapters","title":"ActiveRecord vs Framework Adapters","text":""},{"location":"guides/rails-analogy/#rails-activerecord","title":"Rails ActiveRecord","text":"<pre><code># One model definition works with multiple databases\nclass User &lt; ApplicationRecord\nend\n\n# Works with:\n# - PostgreSQL\n# - MySQL\n# - SQLite\n# - Oracle\n# Just change database.yml!\n</code></pre>"},{"location":"guides/rails-analogy/#superoptix-framework-adapters","title":"SuperOptiX Framework Adapters","text":"<pre><code># One agent spec works with multiple frameworks\nspec:\n  target_framework: dspy  # or crewai, openai, google, microsoft, deepagents\n\n# SuperOptiX generates code for:\n# - DSPy\n# - CrewAI  \n# - OpenAI SDK\n# - Google ADK\n# - Microsoft\n# - DeepAgents\n# Just change target_framework!\n</code></pre> <p>Same abstraction pattern!</p>"},{"location":"guides/rails-analogy/#restful-routes-vs-agent-workflows","title":"RESTful Routes vs Agent Workflows","text":""},{"location":"guides/rails-analogy/#rails-routes","title":"Rails Routes","text":"<pre><code># config/routes.rb\nresources :posts\n# Generates: index, show, create, update, destroy\n</code></pre> <p>Convention provides standard operations.</p>"},{"location":"guides/rails-analogy/#superoptix-workflows","title":"SuperOptiX Workflows","text":"<pre><code># SuperOptiX conventions\nsuper agent compile    # Like: rails routes\nsuper agent evaluate   # Like: rails test\nsuper agent optimize   # Like: rails db:migrate\nsuper agent run        # Like: rails server\n</code></pre> <p>Convention provides standard operations.</p>"},{"location":"guides/rails-analogy/#rails-engines-vs-superoptix-protocols","title":"Rails Engines vs SuperOptiX Protocols","text":""},{"location":"guides/rails-analogy/#rails-engines","title":"Rails Engines","text":"<p>Mountable mini-applications within Rails:</p> <pre><code># Engines provide reusable functionality\nmount SomeEngine::Engine, at: \"/some_path\"\n</code></pre>"},{"location":"guides/rails-analogy/#superoptix-protocols","title":"SuperOptiX Protocols","text":"<p>Pluggable communication protocols:</p> <pre><code>spec:\n  protocol: mcp  # or a2a, or custom\n</code></pre> <p>Both provide modular, reusable components.</p>"},{"location":"guides/rails-analogy/#when-superoptix-differs-from-rails","title":"When SuperOptiX Differs from Rails","text":""},{"location":"guides/rails-analogy/#rails-is-opinionated-about-implementation","title":"Rails is Opinionated About Implementation","text":"<p>Rails says: \"Use our MVC pattern, our ORM, our routing\"</p>"},{"location":"guides/rails-analogy/#superoptix-is-opinionated-about-structure-not-framework","title":"SuperOptiX is Opinionated About Structure, Not Framework","text":"<p>SuperOptiX says: \"Use our spec structure, our optimization approach, but choose ANY agent framework (DSPy, CrewAI, OpenAI, etc.)\"</p> <p>SuperOptiX is MORE flexible than Rails in this way!</p>"},{"location":"guides/rails-analogy/#getting-started-rails-developer-perspective","title":"Getting Started (Rails Developer Perspective)","text":""},{"location":"guides/rails-analogy/#rails-learning-path","title":"Rails Learning Path","text":"<ol> <li>Install Rails</li> <li><code>rails new myapp</code></li> <li>Generate scaffold</li> <li>Customize models/controllers</li> <li>Deploy</li> </ol>"},{"location":"guides/rails-analogy/#superoptix-learning-path","title":"SuperOptiX Learning Path","text":"<ol> <li>Install SuperOptiX</li> <li><code>super init myproject</code></li> <li>Generate agent spec</li> <li>Customize persona/specs</li> <li>Deploy (orchestrate)</li> </ol> <p>Same 5-step journey.</p>"},{"location":"guides/rails-analogy/#cli-command-comparison","title":"CLI Command Comparison","text":"Rails Command SuperOptiX Equivalent <code>rails new myapp</code> <code>super init myproject</code> <code>rails generate scaffold Post</code> <code>super spec generate blog_writer</code> <code>rails db:migrate</code> <code>super agent optimize blog_writer</code> <code>rails test</code> <code>super agent evaluate blog_writer</code> <code>rails server</code> <code>super agent run blog_writer</code> <code>rails console</code> <code>super agent run --interactive</code> <code>bundle install</code> <code>pip install superoptix[frameworks-dspy]</code> <p>If you know Rails commands, you already know SuperOptiX patterns!</p>"},{"location":"guides/rails-analogy/#rspec-style-specifications","title":"RSpec Style Specifications","text":""},{"location":"guides/rails-analogy/#rspec-ruby-testing","title":"RSpec (Ruby Testing)","text":"<pre><code>describe CustomerSupportAgent do\n  describe '#handle_refund' do\n    context 'when order is within 30 days' do\n      it 'approves refund immediately' do\n        agent = CustomerSupportAgent.new\n        result = agent.handle_refund(order: recent_order)\n\n        expect(result.approved).to be true\n        expect(result.processing_time).to be &lt; 24.hours\n      end\n    end\n  end\nend\n</code></pre>"},{"location":"guides/rails-analogy/#superspec-ai-agent-testing","title":"SuperSpec (AI Agent Testing)","text":"<pre><code>spec:\n  feature_specifications:\n    scenarios:\n      - name: refund_within_policy\n        description: Agent should approve refunds within policy window\n        input:\n          customer_request: Refund for laptop purchased 15 days ago\n          purchase_date: 2024-10-09\n        expected_output:\n          approval_status: approved\n          confirmation: Email sent to customer\n</code></pre> <p>Same structured approach. Same clarity. Different domain.</p>"},{"location":"guides/rails-analogy/#opinionated-defaults","title":"Opinionated Defaults","text":""},{"location":"guides/rails-analogy/#rails-defaults","title":"Rails Defaults","text":"<ul> <li>SQLite for development</li> <li>PostgreSQL for production (recommended)</li> <li>MiniTest or RSpec for testing</li> <li>Webpacker for assets</li> </ul> <p>You can change them, but defaults work great.</p>"},{"location":"guides/rails-analogy/#superoptix-defaults","title":"SuperOptiX Defaults","text":"<ul> <li>DSPy for development (recommended)</li> <li>Any framework for production</li> <li>GEPA for optimization</li> <li>RSpec-style BDD for evaluation</li> </ul> <p>You can change them, but defaults work great.</p> <p>Both frameworks: Strong opinions, loosely held.</p>"},{"location":"guides/rails-analogy/#for-rails-devs-why-youll-love-superoptix","title":"For Rails Devs: Why You'll Love SuperOptiX","text":""},{"location":"guides/rails-analogy/#familiar-mental-model","title":"Familiar Mental Model","text":"<p>You already understand: - Convention over configuration - Generator workflows - Spec-driven development - Migration/evolution systems</p>"},{"location":"guides/rails-analogy/#same-developer-experience","title":"Same Developer Experience","text":"<ul> <li>Clean CLI commands</li> <li>Predictable project structure</li> <li>Fast iteration cycles</li> <li>Test-first development</li> </ul>"},{"location":"guides/rails-analogy/#productivity-benefits","title":"Productivity Benefits","text":"<p>Rails reduced boilerplate and let you focus on business logic.</p> <p>SuperOptiX does the same for AI agents: less manual prompt engineering, more focus on agent intelligence.</p>"},{"location":"guides/rails-analogy/#same-philosophy","title":"Same Philosophy","text":"<ul> <li>Optimize for developer happiness</li> <li>Convention over configuration</li> <li>Don't repeat yourself</li> <li>Framework provides rails, you provide business logic</li> </ul>"},{"location":"guides/rails-analogy/#integrating-with-rails-apps","title":"Integrating with Rails Apps","text":"<p>SuperOptiX agents work great alongside Rails:</p>"},{"location":"guides/rails-analogy/#option-1-background-jobs","title":"Option 1: Background Jobs","text":"<pre><code># app/jobs/agent_job.rb\nclass AgentJob &lt; ApplicationJob\n  def perform(user_query)\n    # Call SuperOptiX agent\n    result = `super agent run support_agent --goal \"#{user_query}\"`\n\n    # Process result\n    Notification.create(content: result)\n  end\nend\n</code></pre>"},{"location":"guides/rails-analogy/#option-2-api-endpoints","title":"Option 2: API Endpoints","text":"<pre><code># app/controllers/agents_controller.rb\nclass AgentsController &lt; ApplicationController\n  def query\n    result = SuperOptiX::Agent.run(\n      agent_name: params[:agent],\n      goal: params[:query]\n    )\n\n    render json: { response: result }\n  end\nend\n</code></pre>"},{"location":"guides/rails-analogy/#option-3-rails-superoptix-orchestra","title":"Option 3: Rails + SuperOptiX Orchestra","text":"<p>Use Rails for web app, SuperOptiX for AI workflows:</p> <pre><code># Rails handles HTTP, DB, authentication\nrails server\n\n# SuperOptiX handles AI agents, orchestration\nsuper orchestra run support_team\n</code></pre>"},{"location":"guides/rails-analogy/#learning-resources","title":"Learning Resources","text":""},{"location":"guides/rails-analogy/#for-rails-developers","title":"For Rails Developers","text":"<p>Start here: 1. Quick Start - Familiar workflow 2. SuperSpec DSL - Like Rails DSL 3. Multi-Framework Guide - Framework strategy and tradeoffs 4. GEPA Optimization - Like migrations</p> <p>Then explore: - Multi-Framework Support - Framework flexibility - Orchestra Development - Multi-agent systems - Memory Systems - Context management</p> <p>Advanced: - Protocol-First Agents - Like Rails engines - MCP Optimization - Tool optimization</p>"},{"location":"guides/rails-analogy/#common-questions","title":"Common Questions","text":""},{"location":"guides/rails-analogy/#is-superoptix-as-opinionated-as-rails","title":"\"Is SuperOptiX as opinionated as Rails?\"","text":"<p>Yes and no.</p> <p>Opinionated about: - Project structure (convention over configuration) - Spec-driven development (like RSpec) - Optimization approach (GEPA as default) - Testing framework (Given-When-Then)</p> <p>Flexible about: - Agent framework (6 options vs Rails' \"one way\") - Optimization strategy (GEPA, DSPy optimizers, custom) - Deployment (any platform, any protocol)</p> <p>More flexible than Rails where it matters (framework choice).</p>"},{"location":"guides/rails-analogy/#do-i-need-to-know-rails-to-use-superoptix","title":"\"Do I need to know Rails to use SuperOptiX?\"","text":"<p>No! The analogy just helps Rails developers understand faster.</p> <p>If you don't know Rails, SuperOptiX stands on its own as a full-stack AI agent optimization framework.</p>"},{"location":"guides/rails-analogy/#can-i-use-superoptix-without-the-conventions","title":"\"Can I use SuperOptiX without the conventions?\"","text":"<p>Yes, but you lose productivity (like using Rails without conventions).</p> <p>The conventions are there to help you move fast. Follow them, and you'll be productive immediately.</p>"},{"location":"guides/rails-analogy/#summary","title":"Summary","text":""},{"location":"guides/rails-analogy/#what-rails-did-for-web-development_1","title":"What Rails Did for Web Development","text":"<ul> <li>Brought convention, structure, and productivity</li> <li>Made web development accessible</li> <li>Established patterns everyone follows</li> <li>Reduced boilerplate significantly</li> </ul>"},{"location":"guides/rails-analogy/#what-superoptix-does-for-ai-agents_1","title":"What SuperOptiX Does for AI Agents","text":"<ul> <li>Brings convention, structure, and productivity</li> <li>Makes AI agent development accessible</li> <li>Establishes patterns for optimization</li> <li>Automates prompt optimization</li> </ul>"},{"location":"guides/rails-analogy/#if-you-loved-rails","title":"If You Loved Rails...","text":"<p>You'll love SuperOptiX for the same reasons: - Convention over configuration - Scaffold generators - Spec-driven development - Migration/evolution system - Focus on business logic - Community and ecosystem</p>"},{"location":"guides/rails-analogy/#next-steps","title":"Next Steps","text":"<ol> <li>Try the Quick Start: Quick Start Guide</li> <li>Explore SuperSpec DSL: SuperSpec Guide</li> <li>Learn GEPA Optimization: GEPA Optimizer</li> <li>Build Your First Agent: Follow the same flow as <code>rails generate scaffold</code></li> </ol> <p>Welcome to SuperOptiX - where Rails philosophy meets AI agents! \ud83d\ude82\ud83e\udd16</p> <p>Related Guides: - Quick Start - SuperSpec DSL Reference - GEPA Optimization - Multi-Framework Support</p>"},{"location":"guides/responsible-ai/","title":"\ud83e\udd16 Responsible AI Practices Guide","text":"<p>\ud83c\udfaf Use AI responsibly with SuperOptiX</p> <p>This guide covers ethical AI practices, cost management, environmental considerations, and responsible development when using SuperOptiX with cloud LLMs.</p> <p>\u26a0\ufe0f Important Notice: The code samples and configuration examples in this guide are for reference purposes only and may not represent actual SuperOptiX supported configurations. Always refer to the official SuperOptiX documentation for current supported features and syntax.</p>"},{"location":"guides/responsible-ai/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Ethical AI Principles</li> <li>Cost Responsibility</li> <li>Environmental Impact</li> <li>Data Privacy</li> <li>Bias and Fairness</li> <li>Transparency</li> <li>Best Practices</li> <li>Monitoring and Auditing</li> </ul>"},{"location":"guides/responsible-ai/#ethical-ai-principles","title":"\ud83c\udfaf Ethical AI Principles","text":""},{"location":"guides/responsible-ai/#core-principles","title":"Core Principles","text":"<p>When using SuperOptiX, commit to these ethical AI principles:</p> <ol> <li>Beneficence: Ensure AI systems benefit humanity</li> <li>Non-maleficence: Avoid causing harm</li> <li>Autonomy: Respect human agency and decision-making</li> <li>Justice: Ensure fairness and avoid discrimination</li> <li>Transparency: Be open about AI capabilities and limitations</li> <li>Accountability: Take responsibility for AI system outcomes</li> </ol>"},{"location":"guides/responsible-ai/#implementation-guidelines","title":"Implementation Guidelines","text":"<pre><code># Ethical AI configuration in your playbook\nspec:\n  ethical_guidelines:\n    - \"Always prioritize human safety and well-being\"\n    - \"Avoid generating harmful, misleading, or biased content\"\n    - \"Respect user privacy and data protection\"\n    - \"Provide clear limitations and disclaimers\"\n    - \"Enable human oversight and intervention\"\n    - \"Ensure transparency in AI decision-making\"\n\n  safety_constraints:\n    - \"Do not generate content that could cause harm\"\n    - \"Do not provide medical, legal, or financial advice without disclaimers\"\n    - \"Do not impersonate humans or organizations\"\n    - \"Do not generate content that violates laws or regulations\"\n</code></pre>"},{"location":"guides/responsible-ai/#cost-responsibility","title":"\ud83d\udcb0 Cost Responsibility","text":""},{"location":"guides/responsible-ai/#critical-cost-warnings","title":"\u26a0\ufe0f Critical Cost Warnings","text":"<p>\ud83d\udea8 AI costs can escalate quickly!</p> <ul> <li>Monitor usage continuously</li> <li>Set strict budget limits</li> <li>Use local models for development</li> <li>Optimize only when necessary</li> </ul>"},{"location":"guides/responsible-ai/#cost-management-strategies","title":"Cost Management Strategies","text":""},{"location":"guides/responsible-ai/#set-usage-limits","title":"Set Usage Limits","text":"<pre><code># Environment variables for cost control\nexport OPENAI_USAGE_LIMIT=50      # Stop at $50\nexport ANTHROPIC_USAGE_LIMIT=100   # Stop at $100\nexport GOOGLE_USAGE_LIMIT=30       # Stop at $30\n\n# Monitor usage in real-time\nsuper observability dashboard --live\n</code></pre>"},{"location":"guides/responsible-ai/#use-local-models-for-development","title":"Use Local Models for Development","text":"<pre><code># Development configuration (FREE)\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b  # Free local model\n    temperature: 0.7\n    max_tokens: 2000\n</code></pre>"},{"location":"guides/responsible-ai/#cost-effective-production","title":"Cost-Effective Production","text":"<pre><code># Production configuration (cost-controlled)\nspec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o-mini  # Cheaper alternative\n    temperature: 0.7\n    max_tokens: 1000    # Limit output\n    cache: true         # Enable caching\n    num_retries: 2      # Limit retries\n</code></pre>"},{"location":"guides/responsible-ai/#budget-monitoring","title":"Budget Monitoring","text":"<pre><code># Set up comprehensive monitoring\nsuper observability dashboard --auto-refresh\n\n# Check usage regularly\nsuper agent inspect your_agent --show-usage\n\n# Set up alerts\nexport OPENAI_USAGE_ALERT=10   # Alert at $10\nexport ANTHROPIC_USAGE_ALERT=20 # Alert at $20\n</code></pre>"},{"location":"guides/responsible-ai/#environmental-impact","title":"\ud83c\udf0d Environmental Impact","text":""},{"location":"guides/responsible-ai/#carbon-footprint-awareness","title":"Carbon Footprint Awareness","text":"<p>\ud83c\udf31 Large language models have significant environmental impact</p>"},{"location":"guides/responsible-ai/#environmental-considerations","title":"Environmental Considerations","text":"<ul> <li>Energy consumption: Cloud LLMs require substantial computing power</li> <li>Carbon emissions: Data centers contribute to climate change</li> <li>Resource usage: Water and electricity consumption</li> <li>E-waste: Hardware lifecycle and disposal</li> </ul>"},{"location":"guides/responsible-ai/#sustainable-ai-practices","title":"Sustainable AI Practices","text":""},{"location":"guides/responsible-ai/#use-local-models-when-possible","title":"Use Local Models When Possible","text":"<pre><code># Sustainable development workflow\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b  # Lower environmental impact\n    temperature: 0.7\n    max_tokens: 2000\n</code></pre>"},{"location":"guides/responsible-ai/#optimize-model-usage","title":"Optimize Model Usage","text":"<pre><code># Efficient configuration\nspec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: gpt-4o-mini  # Smaller, more efficient model\n    temperature: 0.7\n    max_tokens: 1000    # Limit output to reduce computation\n    cache: true         # Cache responses to avoid redundant calls\n</code></pre>"},{"location":"guides/responsible-ai/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple requests together\nsuper agent run your_agent --batch-mode --input-file requests.txt\n\n# Use efficient scheduling\nsuper agent run your_agent --schedule --off-peak-hours\n</code></pre>"},{"location":"guides/responsible-ai/#green-ai-checklist","title":"Green AI Checklist","text":"<ul> <li>Use local models for development and testing</li> <li>Choose efficient models (smaller, faster)</li> <li>Cache responses to avoid redundant computation</li> <li>Batch requests when possible</li> <li>Monitor usage and optimize patterns</li> <li>Consider carbon offsets for large deployments</li> </ul>"},{"location":"guides/responsible-ai/#data-privacy","title":"\ud83d\udd12 Data Privacy","text":""},{"location":"guides/responsible-ai/#privacy-first-design","title":"Privacy-First Design","text":""},{"location":"guides/responsible-ai/#data-minimization","title":"Data Minimization","text":"<pre><code># Privacy-conscious configuration\nspec:\n  data_handling:\n    retention_policy: \"7_days\"      # Short retention\n    anonymization: true             # Anonymize data\n    encryption: true                # Encrypt sensitive data\n    local_processing: true          # Process locally when possible\n\n  privacy_constraints:\n    - \"Do not store personal information\"\n    - \"Do not share user data with third parties\"\n    - \"Implement data deletion on request\"\n    - \"Use encryption for data transmission\"\n</code></pre>"},{"location":"guides/responsible-ai/#secure-configuration","title":"Secure Configuration","text":"<pre><code># Secure environment setup\n# Never commit API keys or sensitive data\necho \".env\" &gt;&gt; .gitignore\necho \"*.key\" &gt;&gt; .gitignore\necho \"secrets/\" &gt;&gt; .gitignore\n\n# Use environment variables\nexport OPENAI_API_KEY=your-key-here\nexport ANTHROPIC_API_KEY=your-key-here\n</code></pre>"},{"location":"guides/responsible-ai/#data-protection","title":"Data Protection","text":"<pre><code># Data protection in playbook\nspec:\n  security:\n    api_key_rotation: \"30_days\"     # Rotate keys regularly\n    access_logging: true            # Log access attempts\n    audit_trail: true               # Maintain audit trail\n    data_encryption: true           # Encrypt data at rest\n</code></pre>"},{"location":"guides/responsible-ai/#gdpr-and-privacy-compliance","title":"GDPR and Privacy Compliance","text":"<pre><code># GDPR-compliant configuration\nspec:\n  privacy:\n    gdpr_compliance: true\n    data_retention: \"30_days\"       # Limited retention\n    user_consent: required          # Require explicit consent\n    data_portability: true          # Enable data export\n    right_to_deletion: true         # Enable data deletion\n</code></pre>"},{"location":"guides/responsible-ai/#bias-and-fairness","title":"\u2696\ufe0f Bias and Fairness","text":""},{"location":"guides/responsible-ai/#bias-detection-and-mitigation","title":"Bias Detection and Mitigation","text":""},{"location":"guides/responsible-ai/#bias-testing","title":"Bias Testing","text":"<pre><code># Bias testing configuration\nspec:\n  fairness_testing:\n    enabled: true\n    test_cases:\n      - \"gender_bias_tests\"\n      - \"racial_bias_tests\"\n      - \"age_bias_tests\"\n      - \"cultural_bias_tests\"\n    mitigation_strategies:\n      - \"prompt_engineering\"\n      - \"model_selection\"\n      - \"output_filtering\"\n</code></pre>"},{"location":"guides/responsible-ai/#fairness-constraints","title":"Fairness Constraints","text":"<pre><code># Fairness constraints in playbook\nspec:\n  fairness_guidelines:\n    - \"Treat all users equally regardless of background\"\n    - \"Avoid stereotypes and discriminatory language\"\n    - \"Provide balanced and diverse perspectives\"\n    - \"Acknowledge limitations and biases\"\n    - \"Enable user feedback for bias reporting\"\n</code></pre>"},{"location":"guides/responsible-ai/#diverse-testing","title":"Diverse Testing","text":"<pre><code># Test with diverse datasets\nsuper agent evaluate your_agent --diversity-check\n\n# Validate fairness\nsuper agent validate your_agent --fairness-metrics\n</code></pre>"},{"location":"guides/responsible-ai/#bias-mitigation-strategies","title":"Bias Mitigation Strategies","text":"<pre><code># Bias mitigation in configuration\nspec:\n  bias_mitigation:\n    diverse_training_data: true     # Use diverse training data\n    balanced_prompts: true          # Ensure balanced prompts\n    output_filtering: true          # Filter biased outputs\n    user_feedback: true             # Collect user feedback\n    continuous_monitoring: true     # Monitor for bias\n</code></pre>"},{"location":"guides/responsible-ai/#transparency","title":"\ud83d\udd0d Transparency","text":""},{"location":"guides/responsible-ai/#transparent-ai-systems","title":"Transparent AI Systems","text":""},{"location":"guides/responsible-ai/#clear-disclaimers","title":"Clear Disclaimers","text":"<pre><code># Transparency configuration\nspec:\n  transparency:\n    ai_disclaimer: true             # Always include AI disclaimer\n    capability_limits: true         # Clearly state limitations\n    confidence_scores: true         # Show confidence levels\n    reasoning_traces: true          # Show reasoning process\n    source_attribution: true        # Attribute sources when possible\n</code></pre>"},{"location":"guides/responsible-ai/#explainable-ai","title":"Explainable AI","text":"<pre><code># Explainability features\nspec:\n  explainability:\n    reasoning_steps: true           # Show reasoning steps\n    confidence_indicators: true     # Show confidence levels\n    uncertainty_quantification: true # Quantify uncertainty\n    alternative_suggestions: true   # Provide alternatives\n</code></pre>"},{"location":"guides/responsible-ai/#user-communication","title":"User Communication","text":"<pre><code># Clear user communication\nspec:\n  user_communication:\n    ai_identity: \"I am an AI assistant\"\n    capability_disclosure: \"I can help with [specific tasks]\"\n    limitation_disclosure: \"I cannot [specific limitations]\"\n    error_handling: \"I apologize, but I cannot [reason]\"\n    human_escalation: \"For [specific cases], please contact a human\"\n</code></pre>"},{"location":"guides/responsible-ai/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"guides/responsible-ai/#development-best-practices","title":"Development Best Practices","text":""},{"location":"guides/responsible-ai/#start-local-scale-cloud","title":"Start Local, Scale Cloud","text":"<pre><code># Development workflow\n# Step 1: Local development (FREE, PRIVATE)\nsuper agent compile your_agent\n\n# Step 2: Local testing\nsuper agent evaluate your_agent\n\n# Step 3: Local optimization (FREE)\nsuper agent optimize your_agent\n\n# Step 4: Cloud validation (minimal cost)\n# Edit playbook for cloud provider\nsuper agent compile your_agent\nsuper agent evaluate your_agent --max-examples 3\n</code></pre>"},{"location":"guides/responsible-ai/#responsible-optimization","title":"Responsible Optimization","text":"<pre><code># Safe optimization practices\n# Use local models for optimization\nsuper agent optimize your_agent --local-only\n\n# Limit iterations and examples\nsuper agent optimize your_agent --max-iterations 5 --max-examples 3\n\n# Monitor costs closely\nsuper observability dashboard --live\n</code></pre>"},{"location":"guides/responsible-ai/#security-first","title":"Security First","text":"<pre><code># Security best practices\n# Never commit secrets\ngit add .gitignore\ngit commit -m \"Add security ignores\"\n\n# Use environment variables\nexport API_KEYS_SECURE=true\n\n# Regular security audits\nsuper agent audit your_agent --security-check\n</code></pre>"},{"location":"guides/responsible-ai/#production-best-practices","title":"Production Best Practices","text":""},{"location":"guides/responsible-ai/#gradual-deployment","title":"Gradual Deployment","text":"<pre><code># Gradual deployment strategy\n# Phase 1: Limited beta testing\nsuper agent deploy your_agent --beta --max-users 10\n\n# Phase 2: Expanded testing\nsuper agent deploy your_agent --beta --max-users 100\n\n# Phase 3: Full deployment\nsuper agent deploy your_agent --production\n</code></pre>"},{"location":"guides/responsible-ai/#continuous-monitoring","title":"Continuous Monitoring","text":"<pre><code># Comprehensive monitoring\nsuper observability dashboard --production-mode\n\n# Set up alerts\nsuper agent monitor your_agent --alerts\n\n# Regular audits\nsuper agent audit your_agent --comprehensive\n</code></pre>"},{"location":"guides/responsible-ai/#user-feedback-integration","title":"User Feedback Integration","text":"<pre><code># User feedback system\nspec:\n  feedback_system:\n    enabled: true\n    feedback_channels:\n      - \"in-app_feedback\"\n      - \"email_feedback\"\n      - \"bias_reporting\"\n    feedback_processing: \"automated\"\n    feedback_response: \"within_24_hours\"\n</code></pre>"},{"location":"guides/responsible-ai/#monitoring-and-auditing","title":"\ud83d\udcca Monitoring and Auditing","text":""},{"location":"guides/responsible-ai/#continuous-monitoring_1","title":"Continuous Monitoring","text":""},{"location":"guides/responsible-ai/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code># Performance monitoring\nsuper observability dashboard --performance-metrics\n\n# Monitor key metrics\nsuper agent monitor your_agent --metrics accuracy,latency,cost\n\n# Set up alerts\nsuper agent monitor your_agent --alerts --threshold 0.8\n</code></pre>"},{"location":"guides/responsible-ai/#ethical-monitoring","title":"Ethical Monitoring","text":"<pre><code># Ethical monitoring\nsuper agent monitor your_agent --ethical-metrics\n\n# Bias detection\nsuper agent monitor your_agent --bias-detection\n\n# Fairness monitoring\nsuper agent monitor your_agent --fairness-metrics\n</code></pre>"},{"location":"guides/responsible-ai/#cost-monitoring","title":"Cost Monitoring","text":"<pre><code># Cost monitoring\nsuper observability dashboard --cost-tracking\n\n# Budget alerts\nsuper agent monitor your_agent --budget-alerts\n\n# Usage optimization\nsuper agent optimize your_agent --cost-aware\n</code></pre>"},{"location":"guides/responsible-ai/#regular-auditing","title":"Regular Auditing","text":""},{"location":"guides/responsible-ai/#comprehensive-audits","title":"Comprehensive Audits","text":"<pre><code># Regular comprehensive audits\nsuper agent audit your_agent --comprehensive --monthly\n\n# Security audits\nsuper agent audit your_agent --security --quarterly\n\n# Ethical audits\nsuper agent audit your_agent --ethical --quarterly\n</code></pre>"},{"location":"guides/responsible-ai/#compliance-audits","title":"Compliance Audits","text":"<pre><code># Compliance checking\nsuper agent audit your_agent --gdpr-compliance\n\n# Privacy audits\nsuper agent audit your_agent --privacy-audit\n\n# Fairness audits\nsuper agent audit your_agent --fairness-audit\n</code></pre>"},{"location":"guides/responsible-ai/#responsible-ai-checklist","title":"\ud83c\udfaf Responsible AI Checklist","text":""},{"location":"guides/responsible-ai/#development-phase","title":"Development Phase","text":"<ul> <li>Use local models for development and testing</li> <li>Implement privacy controls from the start</li> <li>Test for bias with diverse datasets</li> <li>Set up monitoring before deployment</li> <li>Document limitations and capabilities</li> <li>Plan for user feedback collection</li> </ul>"},{"location":"guides/responsible-ai/#deployment-phase","title":"Deployment Phase","text":"<ul> <li>Gradual rollout with limited users</li> <li>Continuous monitoring of performance and ethics</li> <li>User feedback collection and processing</li> <li>Regular audits for compliance and fairness</li> <li>Cost monitoring and budget controls</li> <li>Transparent communication with users</li> </ul>"},{"location":"guides/responsible-ai/#maintenance-phase","title":"Maintenance Phase","text":"<ul> <li>Regular updates based on feedback</li> <li>Bias mitigation improvements</li> <li>Performance optimization with cost awareness</li> <li>Security updates and vulnerability patches</li> <li>Compliance monitoring and updates</li> <li>Environmental impact assessment and optimization</li> </ul>"},{"location":"guides/responsible-ai/#quick-reference","title":"\ud83d\ude80 Quick Reference","text":""},{"location":"guides/responsible-ai/#responsible-development-commands","title":"Responsible Development Commands","text":"<pre><code># Start with local development\nsuper agent compile your_agent\n\n# Test locally\nsuper agent evaluate your_agent\n\n# Optimize locally (FREE)\nsuper agent optimize your_agent\n\n# Validate ethically\nsuper agent validate your_agent --ethical-check\n\n# Deploy responsibly\nsuper agent deploy your_agent --beta --monitoring\n</code></pre>"},{"location":"guides/responsible-ai/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># Monitor everything\nsuper observability dashboard --comprehensive\n\n# Check ethics\nsuper agent monitor your_agent --ethical-metrics\n\n# Track costs\nsuper observability dashboard --cost-tracking\n\n# Audit regularly\nsuper agent audit your_agent --comprehensive\n</code></pre>"},{"location":"guides/responsible-ai/#final-thoughts","title":"\ud83c\udfaf Final Thoughts","text":"<p>\ud83e\udd16 AI is a powerful tool - use it responsibly!</p> <ul> <li>Prioritize human well-being</li> <li>Monitor costs and environmental impact</li> <li>Ensure fairness and transparency</li> <li>Protect user privacy</li> <li>Continuously improve and learn</li> </ul> <p>Remember: The goal is to create AI systems that benefit humanity while minimizing harm and maximizing positive impact! \ud83d\ude80 </p>"},{"location":"guides/rlm-experimental/","title":"RLM Support (Experimental)","text":"<p>SuperOptiX supports RLM (Recursive Language Model) workflows as an experimental capability.</p>"},{"location":"guides/rlm-experimental/#status","title":"Status","text":"<ul> <li>RLM support is available in active framework integrations where configured.</li> <li>The API and behavior may evolve while we stabilize cross-framework ergonomics.</li> <li>Unified sandbox support for RLM is coming soon.</li> </ul>"},{"location":"guides/rlm-experimental/#what-this-means-for-users","title":"What This Means for Users","text":"<ul> <li>You can start testing RLM flows today in supported pipelines.</li> <li>Expect ongoing improvements in defaults, tracing, and guardrails.</li> <li>For stable production workloads, keep a non-RLM fallback path in your playbook.</li> </ul>"},{"location":"guides/rlm-experimental/#recommended-usage","title":"Recommended Usage","text":"<ul> <li>Start with small prompts and short iterations.</li> <li>Keep model settings conservative while tuning.</li> <li>Validate output with scenarios before enabling broader usage.</li> </ul>"},{"location":"guides/rlm-experimental/#related-guides","title":"Related Guides","text":"<ul> <li>Multi-Framework Support</li> <li>DSPy Optimizers</li> <li>GEPA Optimization</li> </ul>"},{"location":"guides/sglang-inference/","title":"\u26a1 SGLang Production Inference","text":"SGLang High-Performance Inference <p>Structured generation and fast inference with SGLang</p> \ud83e\udde0 Model Management \ud83d\ude80 vLLM \u2601\ufe0f Cloud Inference"},{"location":"guides/sglang-inference/#what-is-sglang","title":"\ud83c\udfaf What is SGLang?","text":"<p>SGLang (Structured Generation Language) is a fast serving framework for large language models and vision language models with:</p> <ul> <li>Structured Generation: Native support for constrained output formats (JSON, regex, etc.)</li> <li>RadixAttention: Advanced KV cache reuse across requests</li> <li>High Performance: Competitive with or faster than vLLM on many workloads</li> <li>OpenAI Compatible: Drop-in replacement for OpenAI API</li> </ul> Unique Features <ul> <li>Structured output generation (JSON, regex)</li> <li>RadixAttention for cache reuse</li> <li>Faster than vLLM on structured tasks</li> <li>Multi-modal support (Vision + Language)</li> <li>OpenAI API compatible</li> </ul> \ud83c\udfaf Best For <ul> <li>Structured data extraction</li> <li>JSON output requirements</li> <li>Agentic workflows with tools</li> <li>Multi-modal applications</li> <li>High cache-hit workloads</li> </ul>"},{"location":"guides/sglang-inference/#installation","title":"\ud83d\ude80 Installation","text":""},{"location":"guides/sglang-inference/#option-1-pip","title":"Option 1: pip","text":"<pre><code># Install SGLang\npip install \"sglang[all]\"\n\n# Or minimal install\npip install sglang\n</code></pre>"},{"location":"guides/sglang-inference/#option-2-docker","title":"Option 2: Docker","text":"<pre><code># Pull SGLang Docker image\ndocker pull lmsysorg/sglang:latest\n\n# Run SGLang server\ndocker run --gpus all \\\n    -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 30000\n</code></pre>"},{"location":"guides/sglang-inference/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"guides/sglang-inference/#start-sglang-server","title":"Start SGLang Server","text":"<pre><code># Basic SGLang server\npython -m sglang.launch_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 30000\n\n# With advanced settings\npython -m sglang.launch_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 30000 \\\n    --tp 2 \\\n    --mem-fraction-static 0.9 \\\n    --max-running-requests 256\n</code></pre>"},{"location":"guides/sglang-inference/#superoptix-configuration","title":"SuperOptiX Configuration","text":"<pre><code>spec:\n  language_model:\n    provider: openai  # SGLang is OpenAI API compatible\n    model: meta-llama/Llama-3-8B-Instruct\n    api_base: http://localhost:30000/v1\n    api_key: \"dummy\"  # SGLang doesn't require real API key\n    temperature: 0.7\n    max_tokens: 1000\n</code></pre>"},{"location":"guides/sglang-inference/#structured-generation","title":"\ud83c\udfaf Structured Generation","text":"<p>SGLang's killer feature is structured output:</p>"},{"location":"guides/sglang-inference/#json-schema-enforcement","title":"JSON Schema Enforcement","text":"<pre><code>from sglang import function, gen\n\n@function\ndef extract_info(s, text):\n    s += f\"Extract structured info from: {text}\\n\"\n    s += gen(\"output\", max_tokens=200, \n             regex=r'\\{\"name\": \"[^\"]+\", \"age\": \\d+, \"email\": \"[^\"]+\"}\\')\n\n# Guaranteed JSON output!\n</code></pre>"},{"location":"guides/sglang-inference/#with-superoptix","title":"With SuperOptiX","text":"<pre><code>spec:\n  language_model:\n    provider: openai\n    model: meta-llama/Llama-3-8B-Instruct\n    api_base: http://localhost:30000/v1\n    response_format: json_object  # Structured output\n\n  output_fields:\n    - name: response\n      type: str\n      format: json  # Enforce JSON\n</code></pre>"},{"location":"guides/sglang-inference/#vllm-vs-sglang-comparison","title":"\ud83d\udcca vLLM vs SGLang Comparison","text":"Feature vLLM SGLang Throughput Excellent Excellent  Structured Output Basic Advanced \ud83c\udfc6 Cache Reuse Good RadixAttention \ud83d\ude80 Multi-Modal Limited Vision + Language  Maturity Production  Emerging \u26a1"},{"location":"guides/sglang-inference/#superoptix-integration-example","title":"\ud83d\udcdd SuperOptiX Integration Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: sglang_structured_agent\n  id: sglang_structured_agent\n  namespace: production\n  version: 1.0.0\n  level: genies\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: openai  # SGLang is OpenAI compatible\n    model: meta-llama/Llama-3-8B-Instruct\n    api_base: http://localhost:30000/v1\n    api_key: \"dummy\"\n    temperature: 0.7\n    max_tokens: 2000\n    response_format: json_object  # Structured output!\n\n  persona:\n    role: Data Extraction Specialist\n    goal: Extract structured information accurately\n\n  input_fields:\n    - name: text\n      type: str\n\n  output_fields:\n    - name: response\n      type: str\n      format: json\n\n  feature_specifications:\n    scenarios:\n      - name: JSON extraction\n        input:\n          text: \"John Doe is 30 years old, email: john@example.com\"\n        expected_output:\n          response: '{\"name\": \"John Doe\", \"age\": 30}'\n          expected_keywords:\n            - John Doe\n            - \"30\"\n</code></pre>"},{"location":"guides/sglang-inference/#usage-with-superoptix-cli","title":"\ud83d\udd04 Usage with SuperOptiX CLI","text":"<pre><code># Start SGLang server\npython -m sglang.launch_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 30000 &amp;\n\n# Create agent with SGLang\nsuper agent pull assistant_openai\n\n# Update playbook to use SGLang endpoint\n# Edit agents/demo/assistant_openai_playbook.yaml:\n#   language_model:\n#     provider: openai\n#     api_base: http://localhost:30000/v1\n\n# Compile and run\nsuper agent compile assistant_openai\nsuper agent run assistant_openai --goal \"Extract info: Alice, age 25, alice@email.com\"\n</code></pre>"},{"location":"guides/sglang-inference/#next-steps","title":"\ud83d\ude80 Next Steps","text":"\ud83d\ude80 Try vLLM \ud83e\udde0 Model Management \u2601\ufe0f Cloud Inference"},{"location":"guides/sglang-inference/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>Official Documentation: https://sglang.readthedocs.io</li> <li>GitHub: https://github.com/sgl-project/sglang</li> <li>Paper: \"SGLang: Efficient Execution of Structured Language Model Programs\"</li> </ul>"},{"location":"guides/stackone-claude-sdk/","title":"StackOne + Claude Agent SDK Guide","text":"<p>This guide explains how to run StackOne tools through Claude Agent SDK using SuperOptiX's <code>StackOneBridge</code>.</p>"},{"location":"guides/stackone-claude-sdk/#what-this-integration-does","title":"What This Integration Does","text":"<ul> <li>Converts StackOne tools into Claude SDK <code>SdkMcpTool</code> objects.</li> <li>Bundles them into one in-process MCP server using <code>create_sdk_mcp_server(...)</code>.</li> <li>Returns Claude-compatible allowed tool names (<code>mcp__stackone__&lt;tool_name&gt;</code>).</li> <li>Supports discovery mode (<code>tool_search</code>, <code>tool_execute</code>) for large tool inventories.</li> </ul>"},{"location":"guides/stackone-claude-sdk/#install","title":"Install","text":"<pre><code>pip install superoptix stackone-ai claude-agent-sdk\n</code></pre> <p>If you installed SuperOptiX in editable mode:</p> <pre><code>pip install -e \".[frameworks-claude-sdk]\"\n</code></pre>"},{"location":"guides/stackone-claude-sdk/#authentication","title":"Authentication","text":"<p>Claude Agent SDK needs Anthropic auth at runtime:</p> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>Optional advanced endpoint override:</p> <pre><code>export ANTHROPIC_BASE_URL=\"https://your-anthropic-compatible-endpoint\"\n</code></pre>"},{"location":"guides/stackone-claude-sdk/#recommended-claude-models","title":"Recommended Claude Models","text":"<p>Use current model aliases from Anthropic's model overview:</p> <ul> <li><code>claude-opus-4-5</code></li> <li><code>claude-sonnet-4-5</code></li> <li><code>claude-haiku-4-5</code></li> </ul> <p>Latest snapshot IDs:</p> <ul> <li><code>claude-opus-4-5-20251101</code></li> <li><code>claude-sonnet-4-5-20250929</code></li> <li><code>claude-haiku-4-5-20251001</code></li> </ul>"},{"location":"guides/stackone-claude-sdk/#end-to-end-example","title":"End-to-End Example","text":"<pre><code>import asyncio\nfrom stackone_ai import StackOneToolSet\nfrom claude_agent_sdk import ClaudeAgentOptions, query\nfrom superoptix.adapters import StackOneBridge\n\n\nasync def main():\n    toolset = StackOneToolSet()\n    tools = toolset.fetch_tools(\n        include_tools=[\"hris_list_employees\", \"hris_get_employee\"],\n        account_ids=[\"your_stackone_account_id\"],\n    )\n\n    bridge = StackOneBridge(tools)\n    mcp_server, tool_names = bridge.to_claude_sdk()\n\n    options = ClaudeAgentOptions(\n        system_prompt=\"You are an HR assistant. Use tools for factual answers.\",\n        mcp_servers={\"stackone\": mcp_server},\n        allowed_tools=tool_names,\n        model=\"claude-sonnet-4-5\",\n    )\n\n    async for message in query(\n        prompt=\"List all employees in engineering\", options=options\n    ):\n        print(message)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/stackone-claude-sdk/#discovery-mode-large-tool-catalogs","title":"Discovery Mode (Large Tool Catalogs)","text":"<p>If you have many tools, inject only two meta-tools and let the agent discover dynamically:</p> <pre><code>mcp_server, tool_names = StackOneBridge(tools).to_discovery_tools(framework=\"claude_sdk\")\n</code></pre> <p>This gives the model:</p> <ul> <li><code>tool_search</code>: find the best tool</li> <li><code>tool_execute</code>: execute by name with args</li> </ul>"},{"location":"guides/stackone-claude-sdk/#how-superoptix-playbook-should-look","title":"How SuperOptiX Playbook Should Look","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: anthropic\n    model: claude-sonnet-4-5\n    temperature: 0.2\n</code></pre> <p>SuperOptiX compile/run guidance intentionally asks users to edit this playbook directly and recompile, rather than mutating pulled templates automatically.</p>"},{"location":"guides/stackone-claude-sdk/#troubleshooting","title":"Troubleshooting","text":"<ul> <li><code>Claude SDK authentication is not configured</code>:   Set <code>ANTHROPIC_API_KEY</code> in the shell where you run <code>super</code>.</li> <li><code>provider='anthropic' expects a Claude model</code>:   Replace model with one of the IDs above and recompile.</li> <li><code>Task exception ... cancel scope</code>:   Use latest generated pipeline; SuperOptiX now avoids early stream termination.</li> </ul>"},{"location":"guides/stackone-claude-sdk/#references","title":"References","text":"<ul> <li>Anthropic model overview: https://platform.claude.com/docs/en/about-claude/models/overview</li> <li>Claude SDK auth: https://github.com/anthropics/claude-code-sdk-python?tab=readme-ov-file#authentication</li> </ul>"},{"location":"guides/stackone-integration/","title":"StackOne Integration Guide","text":"<p>SuperOptiX serves as the Universal Bridge for StackOne, allowing you to use their unified SaaS API tools with major frameworks (DSPy, Pydantic AI, CrewAI, Google ADK, OpenAI SDK, DeepAgents) while adding optimization and evaluation workflows.</p>"},{"location":"guides/stackone-integration/#key-features","title":"\ud83d\ude80 Key Features","text":"Feature Description Universal Bridge Use StackOne tools in DSPy, Pydantic AI, CrewAI, Google ADK, OpenAI SDK, and DeepAgents. GEPA Optimization Automatically rewrite tool descriptions to fix LLM errors. Vertical Benchmarks Pre-built evaluation suites for HRIS, ATS, and CRM tasks. Type Safety Full Pydantic model generation for strict validation."},{"location":"guides/stackone-integration/#installation","title":"\ud83d\udce6 Installation","text":"<p>Install both SuperOptiX and the StackOne SDK:</p> <pre><code>pip install superoptix stackone-ai\n</code></pre> <p>For Claude Agent SDK integration:</p> <pre><code>pip install superoptix stackone-ai claude-agent-sdk\n</code></pre>"},{"location":"guides/stackone-integration/#current-cli-workflow-recommended","title":"Current CLI Workflow (Recommended)","text":"<p>Use one StackOne playbook and compile it into any framework:</p> <pre><code>export STACKONE_API_KEY=\"...\"\nexport STACKONE_ACCOUNT_IDS=\"acc_123\"\nsuper agent pull stackone-calendly\n</code></pre> <pre><code># DSPy\nsuper agent compile stackone-calendly --framework dspy\nsuper agent run stackone-calendly --framework dspy --goal \"What is my Calendly username?\"\n\n# Pydantic AI\nsuper agent compile stackone-calendly --framework pydantic-ai --cloud --provider google-genai --model gemini-2.5-flash\nsuper agent run stackone-calendly --framework pydantic-ai --direct --cloud --provider google-genai --model gemini-2.5-flash --goal \"What is my Calendly username?\"\n\n# OpenAI SDK\nsuper agent compile stackone-calendly --framework openai --cloud --provider google-genai --model gemini-2.5-flash\nsuper agent run stackone-calendly --framework openai --cloud --provider google-genai --model gemini-2.5-flash --goal \"What is my Calendly username?\"\n\n# Claude Agent SDK\nsuper agent compile stackone-calendly --framework claude-sdk\nsuper agent run stackone-calendly --framework claude-sdk --goal \"What is my Calendly username?\"\n\n# CrewAI\nsuper agent compile stackone-calendly --framework crewai --cloud --provider google-genai --model gemini-2.5-flash\nsuper agent run stackone-calendly --framework crewai --cloud --provider google-genai --model gemini-2.5-flash --goal \"What is my Calendly username?\"\n</code></pre> <p>For Claude-specific setup details, see StackOne + Claude Agent SDK.</p>"},{"location":"guides/stackone-integration/#the-universal-bridge","title":"\ud83c\udf09 The Universal Bridge","text":"<p>The <code>StackOneBridge</code> adapter allows you to convert StackOne tools into the native format of your chosen framework.</p>"},{"location":"guides/stackone-integration/#dspy-integration","title":"DSPy Integration","text":"<p>Perfect for programmable agents and optimization.</p> <pre><code>from stackone_ai import StackOneToolSet\nfrom superoptix.adapters import StackOneBridge\nimport dspy\n\n# Fetch Tools\ntools = StackOneToolSet().fetch_tools(actions=[\"hris_*\"])\n\n# Bridge to DSPy\ndspy_tools = StackOneBridge(tools).to_dspy()\n\n# Use in Agent\nagent = dspy.ReAct(\"query -&gt; answer\", tools=dspy_tools)\n</code></pre>"},{"location":"guides/stackone-integration/#dspy-via-superspec-no-dspy-coding","title":"DSPy via SuperSpec (No DSPy Coding)","text":"<p>Use the connector through SuperSpec and let SuperOptiX wire DSPy tools automatically:</p> <pre><code>spec:\n  target_framework: dspy\n  dspy:\n    module: react\n    tools:\n      mode: stackone_discovery\n      trace:\n        enabled: true   # optional: transient live tool logs\n      stackone:\n        enabled: true\n        api_key_env: STACKONE_API_KEY\n        account_ids_env: STACKONE_ACCOUNT_IDS\n        providers: [\"bamboohr\"]\n        actions: [\"hris_*\"]\n</code></pre> <p>Then run:</p> <pre><code>export STACKONE_API_KEY=\"...\"\nexport STACKONE_ACCOUNT_IDS=\"acc_123\"\nsuper agent pull stackone-calendly\nsuper agent compile stackone-calendly --framework dspy\nsuper agent run stackone-calendly --framework dspy --goal \"List meetings and highlight conflicts\"\n</code></pre> <p>Calendly-focused demo:</p> <pre><code>export STACKONE_API_KEY=\"...\"\nexport STACKONE_ACCOUNT_IDS=\"acc_123\"\nsuper agent pull stackone-calendly\nsuper agent compile stackone-calendly --framework dspy\nsuper agent run stackone-calendly --framework dspy --goal \"Show my meetings for next week and any conflicts\"\n</code></pre> <p>You can also enable live transient thinking logs from the shell:</p> <pre><code>export SUPEROPTIX_DSPY_THINKING_LOGS=1\n</code></pre>"},{"location":"guides/stackone-integration/#pydantic-ai-integration-type-safe","title":"Pydantic AI Integration (Type-Safe)","text":"<p>Generates strictly typed Pydantic models for every tool, ensuring the agent follows the schema exactly.</p> <pre><code>from pydantic_ai import Agent\n\n# Bridge to Pydantic AI\npai_tools = StackOneBridge(tools).to_pydantic_ai()\n\n# Use in Agent (Type-safe!)\nagent = Agent('openai:gpt-4o', tools=pai_tools)\n</code></pre>"},{"location":"guides/stackone-integration/#crewai-integration","title":"CrewAI Integration","text":"<p>Perfect for multi-agent workflows with role-based agents. Supports both sync and async tools.</p> <pre><code>from crewai import Agent, Task, Crew, Process\nfrom crewai.llm import LLM\n\n# Bridge to CrewAI (Sync)\ncrewai_tools = StackOneBridge(tools).to_crewai()\n\n# Or for async workflows:\n# crewai_async_tools = StackOneBridge(tools).to_crewai_async()\n\n# Create Agent with StackOne tools\nhr_agent = Agent(\n    role=\"HR Assistant\",\n    goal=\"Help with HR queries using HRIS tools\",\n    backstory=\"You are an HR specialist with access to employee management systems.\",\n    llm=LLM(model=\"gpt-4o-mini\"),\n    tools=crewai_tools,\n)\n\n# Create Task and Crew\ntask = Task(\n    description=\"List all employees in engineering\",\n    expected_output=\"Employee list with names and roles\",\n    agent=hr_agent,\n)\n\ncrew = Crew(agents=[hr_agent], tasks=[task], process=Process.sequential)\n\n# Run\nresult = crew.kickoff()\n</code></pre>"},{"location":"guides/stackone-integration/#google-adk-gemini","title":"Google ADK / Gemini","text":"<p>Converts tools to Google's specific <code>FunctionDeclaration</code> format.</p> <pre><code>import google.generativeai as genai\n\n# Bridge to Google ADK\ngoogle_tools = StackOneBridge(tools).to_google_adk()\n\n# Initialize Gemini\nmodel = genai.GenerativeModel('gemini-1.5-pro', tools=[google_tools])\n</code></pre>"},{"location":"guides/stackone-integration/#microsoft-semantic-kernel","title":"Microsoft Semantic Kernel","text":"<p>Converts tools into Semantic Kernel Plugins/Functions.</p> <pre><code>import semantic_kernel as sk\n\n# Bridge to Semantic Kernel\nsk_functions = StackOneBridge(tools).to_semantic_kernel()\n\n# Register as Plugin\nkernel = sk.Kernel()\nfor func in sk_functions:\n    kernel.add_function(plugin_name=\"StackOne\", function=func)\n</code></pre>"},{"location":"guides/stackone-integration/#claude-agent-sdk-in-process-mcp","title":"Claude Agent SDK (In-Process MCP)","text":"<p>Uses StackOne tools as Claude SDK MCP tools with no subprocess server required.</p> <pre><code>from stackone_ai import StackOneToolSet\nfrom claude_agent_sdk import ClaudeAgentOptions, query\nfrom superoptix.adapters import StackOneBridge\n\ntoolset = StackOneToolSet()\ntools = toolset.fetch_tools(\n    include_tools=[\"hris_list_employees\", \"hris_get_employee\"],\n    account_ids=[\"your_stackone_account_id\"],\n)\n\nbridge = StackOneBridge(tools)\nmcp_server, tool_names = bridge.to_claude_sdk()\n\noptions = ClaudeAgentOptions(\n    system_prompt=\"You are an HR assistant. Use StackOne tools for factual answers.\",\n    mcp_servers={\"stackone\": mcp_server},\n    allowed_tools=tool_names,\n    model=\"claude-sonnet-4-5\",\n)\n\nasync for message in query(prompt=\"List all employees in engineering\", options=options):\n    pass\n</code></pre> <p>Model examples (Anthropic docs): - <code>claude-opus-4-5</code> - <code>claude-sonnet-4-5</code> - <code>claude-haiku-4-5</code></p> <p>Runtime setup:</p> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>For large toolsets with discovery mode:</p> <pre><code>mcp_server, tool_names = StackOneBridge(tools).to_discovery_tools(framework=\"claude_sdk\")\n</code></pre> <p>See full guide: <code>docs/guides/stackone-claude-sdk.md</code></p>"},{"location":"guides/stackone-integration/#dynamic-tool-discovery-discovery-mode","title":"\ud83d\udd0d Dynamic Tool Discovery (Discovery Mode)","text":"<p>StackOne provides 100+ tools. Loading them all into an LLM's context window is expensive and confusing. Discovery Mode provides the agent with just two \"meta-tools\" to navigate the entire ecosystem at runtime.</p>"},{"location":"guides/stackone-integration/#how-it-works","title":"How it Works","text":"<ol> <li>The agent receives <code>tool_search</code> and <code>tool_execute</code>.</li> <li>The agent searches for a capability (e.g., \"how to find employees\").</li> <li>The agent receives the specific tool name from the index and executes it.</li> </ol>"},{"location":"guides/stackone-integration/#usage","title":"Usage","text":"<pre><code># Fetch a large set of tools (e.g., everything)\nall_tools = toolset.fetch_tools(account_ids=[\"acc_123\"])\n\n# Get Discovery Tools for your framework\n# Supported: 'dspy', 'pydantic_ai', 'crewai', 'google', 'semantic_kernel'\ndiscovery_tools = StackOneBridge(all_tools).to_discovery_tools(framework=\"dspy\")\n\n# For CrewAI:\n# discovery_tools = StackOneBridge(all_tools).to_discovery_tools(framework=\"crewai\")\n\n# Equip the agent (Only 2 tools injected!)\nagent = dspy.ReAct(\"question -&gt; answer\", tools=discovery_tools)\n</code></pre>"},{"location":"guides/stackone-integration/#gepa-optimization","title":"\ud83e\uddec GEPA Optimization","text":"<p>Is the LLM struggling to use a specific HRIS tool? Use GEPA to automatically rewrite the tool's description based on real failure data.</p> <pre><code>from superoptix.benchmarks.stackone import HRISBenchmark\n\n# Load Benchmark Data\ndataset = HRISBenchmark().get_dataset()\n\n# Run Optimization\n# This uses the 'StackOneOptimizableComponent' to mutate tool descriptions\noptimized_tools = bridge.optimize(\n    dataset=dataset,\n    metric=my_accuracy_metric,\n    max_iterations=5\n)\n\n# Result: Tools now have \"LLM-optimized\" descriptions\nprint(optimized_tools[0].description)\n</code></pre>"},{"location":"guides/stackone-integration/#evaluation-benchmarks","title":"\ud83d\udcca Evaluation Benchmarks","text":"<p>SuperOptiX includes pre-built benchmarks for StackOne's core verticals. Use these to prove your agent works before deploying.</p>"},{"location":"guides/stackone-integration/#available-benchmarks","title":"Available Benchmarks","text":"<ul> <li><code>HRISBenchmark</code>: Employee retrieval, employment details, team structure.</li> <li><code>ATSBenchmark</code>: Job search, candidate profiles, application tracking.</li> <li><code>CRMBenchmark</code>: Account management, opportunity lists, contact lookup.</li> </ul>"},{"location":"guides/stackone-integration/#usage_1","title":"Usage","text":"<pre><code>from superoptix.benchmarks.stackone import ATSBenchmark\n\nbenchmark = ATSBenchmark()\ndataset = benchmark.get_dataset()\n\nfor case in dataset:\n    print(f\"Testing: {case['input']}\")\n    # ... run your agent ...\n    score = benchmark.evaluate_tool_call(tool_name, tool_args, expected=case)\n</code></pre>"},{"location":"guides/stackone-integration/#deployment-blueprints","title":"\ud83d\udcc4 Deployment Blueprints","text":"<p>Don't want to code? Use our pre-built YAML blueprints to deploy optimized agents instantly.</p> Agent ID Description <code>stackone-calendly</code> Calendly scheduling and identity queries via StackOne <p>Run directly from CLI:</p> <pre><code># Pull the blueprint\nsuper agent pull stackone-calendly\n\n# Compile for a framework (examples)\nsuper agent compile stackone-calendly --framework dspy\nsuper agent compile stackone-calendly --framework pydantic-ai --cloud --provider google-genai --model gemini-2.5-flash\n\n# Run the agent\nsuper agent run stackone-calendly --framework dspy --goal \"What is my Calendly username?\"\nsuper agent run stackone-calendly --framework pydantic-ai --direct --cloud --provider google-genai --model gemini-2.5-flash --goal \"What is my Calendly username?\"\n</code></pre>"},{"location":"guides/super-cli/","title":"\ud83d\udcac Super CLI - Conversational Interface (Beta)","text":"<p>AI-Powered Command-Line Interface for SuperOptiX</p> <p>Experience a new way to interact with SuperOptiX through natural language and intelligent automation.</p>"},{"location":"guides/super-cli/#overview","title":"\ud83c\udfaf Overview","text":"<p>Super CLI is SuperOptiX's conversational interface that combines:</p> <ul> <li>Natural Language Understanding - Talk to SuperOptiX naturally</li> <li>Intelligent Command Generation - AI translates your intent to commands</li> <li>MCP Client Integration - Built-in Model Context Protocol support</li> <li>Model Flexibility - Switch between Ollama, OpenAI, Anthropic models</li> <li>Beautiful UI - Engaging animations and visual feedback</li> </ul> <p>Beta Feature</p> <p>Super CLI is currently in beta. We're actively improving the experience based on user feedback!</p>"},{"location":"guides/super-cli/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"guides/super-cli/#launch-super-cli","title":"Launch Super CLI","text":"<pre><code># Just type 'super' with no arguments\nsuper\n</code></pre> <p>You'll see: <pre><code>\u2728 Welcome to Super CLI [BETA] \u2728\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Super CLI                          \u2502\n\u2502  The Official SuperOptiX CLI        \u2502\n\u2502  Using: ollama (gpt-oss:120b)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udca1 Quick Start \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Essential Commands:                 \u2502\n\u2502   /help     Full command reference  \u2502\n\u2502   /ask      Ask questions           \u2502\n\u2502   /model    List models             \u2502\n\u2502   /mcp      MCP server status       \u2502\n\u2502   /exit     Exit CLI                \u2502\n\u2502                                     \u2502\n\u2502 Natural Language:                   \u2502\n\u2502   \"Build a developer agent\"         \u2502\n\u2502   \"Evaluate my agent\"               \u2502\n\u2502   \"Optimize with GEPA\"              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500 \ud83d\udcac Message \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u25b6 _\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"guides/super-cli/#natural-language-mode","title":"\ud83d\udcac Natural Language Mode","text":""},{"location":"guides/super-cli/#just-type-what-you-want","title":"Just Type What You Want","text":"<p>Super CLI understands natural language and translates it to the right commands.</p> <p>Examples:</p> <pre><code># Build an agent\nSuperOptiX \u203a build a developer agent\n\n\u2728 Let me cook...\n  \u2713 Understood: build (developer)\n\ud83d\udd27 Preparing the perfect command...\n  \u2713 Generated 1 command(s)\n\ud83d\ude80 Launching your request...\nAgent created successfully!\n</code></pre> <pre><code># Evaluate an agent\nSuperOptiX \u203a evaluate my customer support agent\n\n\ud83e\udd14 Analyzing your request...\n  \u2713 Understood: evaluate (customer_support)\n\u2699\ufe0f Tuning parameters...\n  \u2713 Generated 1 command(s)\n\u26a1 Running the command...\nEvaluation complete!\n</code></pre> <pre><code># Optimize with GEPA\nSuperOptiX \u203a optimize the code review agent with GEPA\n\n\ud83d\udca1 Got an idea...\n  \u2713 Understood: optimize (code_review)\n\ud83d\uddfa\ufe0f Charting the course...\n  \u2713 Generated 1 command(s)\n\ud83e\udde0 AI at work...\nOptimization complete!\n</code></pre>"},{"location":"guides/super-cli/#supported-intents","title":"Supported Intents","text":"What You Say What It Does \"build/create a  agent\" Generates agent with SuperSpec \"compile \" Compiles agent playbook to pipeline \"evaluate/test \" Runs BDD evaluation \"optimize \" Optimizes agent with GEPA \"run  with \" Executes agent with goal \"list agents\" Shows available agents \"show config\" Displays configuration"},{"location":"guides/super-cli/#slash-commands","title":"\ud83d\udd27 Slash Commands","text":""},{"location":"guides/super-cli/#core-commands","title":"Core Commands","text":"<pre><code># Get help\nSuperOptiX \u203a /help\n\n# Ask questions about SuperOptiX\nSuperOptiX \u203a /ask how does GEPA optimization work?\n\n# List available models\nSuperOptiX \u203a /model list\n\n# Switch models\nSuperOptiX \u203a /model set gpt-4o-mini\n\n# Show configuration\nSuperOptiX \u203a /config\n\n# Exit\nSuperOptiX \u203a /exit\n</code></pre>"},{"location":"guides/super-cli/#agent-management","title":"Agent Management","text":"<pre><code># List marketplace agents\nSuperOptiX \u203a /agents\n\n# List compiled playbooks in current project\nSuperOptiX \u203a /playbooks\n</code></pre>"},{"location":"guides/super-cli/#mcp-commands","title":"MCP Commands","text":"<pre><code># Check MCP server status\nSuperOptiX \u203a /mcp status\n\n# List MCP servers\nSuperOptiX \u203a /mcp list\n\n# Show available MCP tools\nSuperOptiX \u203a /mcp tools\n\n# Add MCP server\nSuperOptiX \u203a /mcp add\n\n# Enable/disable server\nSuperOptiX \u203a /mcp enable &lt;server_name&gt;\nSuperOptiX \u203a /mcp disable &lt;server_name&gt;\n</code></pre>"},{"location":"guides/super-cli/#utility-commands","title":"Utility Commands","text":"<pre><code># Show conversation history\nSuperOptiX \u203a /history\n\n# Clear screen\nSuperOptiX \u203a /clear\n</code></pre>"},{"location":"guides/super-cli/#model-management","title":"\ud83e\udd16 Model Management","text":""},{"location":"guides/super-cli/#built-in-support-for-multiple-providers","title":"Built-in Support for Multiple Providers","text":"<p>Local Models (Ollama): - <code>llama3.2:1b</code>, <code>llama3.2:3b</code> - <code>gpt-oss:20b</code>, <code>gpt-oss:120b</code> - <code>qwen2.5-coder:7b</code> - Any Ollama model</p> <p>Cloud Models (OpenAI): - <code>gpt-4o</code>, <code>gpt-4o-mini</code> - <code>gpt-3.5-turbo</code></p> <p>Cloud Models (Anthropic): - <code>claude-3-5-sonnet-20241022</code> - <code>claude-3-opus-20240229</code></p>"},{"location":"guides/super-cli/#list-available-models","title":"List Available Models","text":"<pre><code>SuperOptiX \u203a /model list\n</code></pre> <p>Output: <pre><code>\ud83e\udd16 Available Models\n\nLocal Models (Ollama):\n  \u2022 llama3.2:1b           Fast, lightweight\n  \u2022 gpt-oss:120b         Powerful reasoning\n  \u2022 qwen2.5-coder:7b     Code specialist\n\nCloud Models (OpenAI):\n  \u2022 gpt-4o-mini          Cost-effective, fast\n  \u2022 gpt-4o               Most capable\n\nCloud Models (Anthropic):\n  \u2022 claude-3-5-sonnet    Best for complex tasks\n</code></pre></p>"},{"location":"guides/super-cli/#switch-models","title":"Switch Models","text":"<pre><code># Switch to local model\nSuperOptiX \u203a /model set gpt-oss:120b\n\n# Switch to cloud model\nSuperOptiX \u203a /model set gpt-4o-mini\n</code></pre> <p>Notes: - Model used for CLI conversation only - Agents use their playbook-specified models - No interference between CLI and agent models</p>"},{"location":"guides/super-cli/#mcp-client-features","title":"\ud83d\udd27 MCP Client Features","text":""},{"location":"guides/super-cli/#what-is-mcp","title":"What is MCP?","text":"<p>Model Context Protocol - Industry standard for connecting AI models to external tools and data sources.</p> <p>SuperOptiX Super CLI acts as an MCP client, allowing you to: - Connect to MCP servers (filesystem, databases, APIs) - Use MCP tools for knowledge access - Manage server configurations - Monitor server status</p>"},{"location":"guides/super-cli/#check-mcp-status","title":"Check MCP Status","text":"<pre><code>SuperOptiX \u203a /mcp status\n</code></pre> <p>Output: <pre><code>\ud83d\udd27 MCP Client Status\n\nStatus: Ready \nActive Servers:\n  \u2022 filesystem - Local file access\n    Status: Connected     Tools: 3 available\n\nAvailable Tools:\n  \u2022 read_file(path) - Read file contents\n  \u2022 write_file(path, content) - Write to file\n  \u2022 list_directory(path) - List files in directory\n</code></pre></p>"},{"location":"guides/super-cli/#manage-mcp-servers","title":"Manage MCP Servers","text":"<pre><code># List configured servers\nSuperOptiX \u203a /mcp list\n\n# Add a server\nSuperOptiX \u203a /mcp add\n# Interactive wizard will guide you\n\n# Enable/disable server\nSuperOptiX \u203a /mcp enable filesystem\nSuperOptiX \u203a /mcp disable database\n</code></pre>"},{"location":"guides/super-cli/#mcp-tools","title":"MCP Tools","text":"<pre><code># List available tools\nSuperOptiX \u203a /mcp tools\n\n# Tools are automatically used by Super CLI when relevant\n# For example, when you ask about code, it can read files via MCP\n</code></pre>"},{"location":"guides/super-cli/#visual-features","title":"\ud83c\udfa8 Visual Features","text":""},{"location":"guides/super-cli/#engaging-animations","title":"Engaging Animations","text":"<p>58+ Rotating Messages keep the experience fresh:</p> <p>Thinking Phase: - \"\u2728 Let me cook...\" - \"\ud83e\udde0 Neurons firing...\" - \"\ud83d\udcab Channeling the AI spirits...\" - \"\ud83d\udd2e Figuring this out...\" - And 26+ more!</p> <p>Preparing Phase: - \"\ud83d\udd27 Preparing the perfect command...\" - \"\ud83d\uddfa\ufe0f Charting the course...\" - \"\ud83c\udfaf Aiming for perfection...\" - And 5+ more!</p> <p>Executing Phase: - \"\ud83d\ude80 Launching your request...\" - \"\ud83d\udd25 Executing with style...\" - \"\u26a1 Zapping it into existence...\" - And 17+ more!</p>"},{"location":"guides/super-cli/#progressive-status","title":"Progressive Status","text":"<pre><code>\u2728 Let me cook...\n\n  \u2713 Understood: build (developer)\n\n\ud83d\udd27 Preparing the perfect command...\n\n  \u2713 Generated 1 command(s)\n\n\ud83d\ude80 Launching your request...\n\nSuccess!\n</code></pre> <p>Smooth, continuous animations until each step completes!</p>"},{"location":"guides/super-cli/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"guides/super-cli/#first-time-setup","title":"First-Time Setup","text":"<p>Super CLI runs a setup wizard on first launch:</p> <pre><code>super\n</code></pre> <p>You'll configure: 1. Model Provider - Ollama (local) or OpenAI/Anthropic (cloud) 2. Default Model - Which model to use for conversations 3. API Keys - For cloud providers (optional)</p> <p>Wizard saves to: <code>~/.superoptix_config.json</code></p>"},{"location":"guides/super-cli/#view-configuration","title":"View Configuration","text":"<pre><code>SuperOptiX \u203a /config\n</code></pre> <p>Shows: - Current model provider - Active model - API key status (masked) - MCP server status</p>"},{"location":"guides/super-cli/#change-model","title":"Change Model","text":"<pre><code># Switch model anytime\nSuperOptiX \u203a /model set gpt-4o-mini\n\nSwitched to: gpt-4o-mini\n   Provider: openai\n</code></pre>"},{"location":"guides/super-cli/#knowledge-base","title":"\ud83e\udde0 Knowledge Base","text":""},{"location":"guides/super-cli/#ask-about-superoptix","title":"Ask About SuperOptiX","text":"<p>Super CLI has built-in knowledge about SuperOptiX features:</p> <pre><code>SuperOptiX \u203a /ask how does GEPA optimization work?\n</code></pre> <p>Gets answers from: 1. Curated Knowledge - Pre-built Q&amp;A (12 topics) 2. Documentation - Direct access to docs/ files 3. MCP Tools - Filesystem access for examples and code</p> <p>Topics covered: - Memory optimization - RAG setup - Tool integration - GEPA algorithm - SuperSpec DSL - Agent tiers - Compilation process - Evaluation methods - Optimization levels - CLI commands - Fresh flag usage - Multi-agent orchestration</p>"},{"location":"guides/super-cli/#use-cases","title":"\ud83c\udfaf Use Cases","text":""},{"location":"guides/super-cli/#quick-prototyping","title":"Quick Prototyping","text":"<pre><code>SuperOptiX \u203a build a customer support agent for e-commerce\n\n\u2728 Working on it...\nAgent created successfully!\n\nSuperOptiX \u203a compile it\n\n\ud83d\udd27 Assembling the pieces...\nCompiled!\n\nSuperOptiX \u203a evaluate the agent\n\n\ud83d\ude80 Launching evaluation...\nEvaluation complete!\nScore: 85%\n</code></pre>"},{"location":"guides/super-cli/#iterative-optimization","title":"Iterative Optimization","text":"<pre><code>SuperOptiX \u203a optimize customer_support with GEPA at high level\n\n\ud83e\udde0 AI at work...\nOptimization complete!\nBest score: 92%\n\nSuperOptiX \u203a evaluate it again\n\n\u26a1 Running the command...\nEvaluation complete!\nScore: 92% (improved 7%!)\n</code></pre>"},{"location":"guides/super-cli/#multi-agent-workflows","title":"Multi-Agent Workflows","text":"<pre><code>SuperOptiX \u203a create a software development team\n\n\ud83d\udca1 Got an idea...\nCreated developer, qa_engineer, devops_engineer\n\nSuperOptiX \u203a compile all of them\n\n\ud83d\udd28 Building it now...\nAll agents compiled!\n\nSuperOptiX \u203a create an orchestra for them\n\n\ud83c\udfb5 Orchestrating the plan...\nOrchestra created!\n</code></pre>"},{"location":"guides/super-cli/#command-reference","title":"\ud83d\udcd6 Command Reference","text":""},{"location":"guides/super-cli/#natural-language","title":"Natural Language","text":"Command Pattern Example What It Does <code>build/create &lt;name&gt;</code> \"build a developer agent\" Generate agent with SuperSpec <code>compile &lt;name&gt;</code> \"compile developer\" Compile playbook to pipeline <code>evaluate &lt;name&gt;</code> \"evaluate my agent\" Run BDD tests <code>optimize &lt;name&gt;</code> \"optimize with GEPA\" GEPA optimization <code>run &lt;name&gt; with &lt;goal&gt;</code> \"run developer with goal X\" Execute agent"},{"location":"guides/super-cli/#slash-commands_1","title":"Slash Commands","text":"Command Description <code>/help</code> Full command reference with examples <code>/ask &lt;question&gt;</code> Ask about SuperOptiX features <code>/model list</code> List available models <code>/model set &lt;name&gt;</code> Switch to different model <code>/config</code> Show current configuration <code>/agents</code> List marketplace agent templates <code>/playbooks</code> List compiled playbooks (requires project) <code>/mcp status</code> Check MCP server status <code>/mcp list</code> List MCP servers <code>/mcp tools</code> Show available MCP tools <code>/history</code> Show conversation history <code>/clear</code> Clear screen <code>/exit</code> or <code>/quit</code> Exit Super CLI"},{"location":"guides/super-cli/#security-privacy","title":"\ud83d\udd12 Security &amp; Privacy","text":""},{"location":"guides/super-cli/#local-first-by-default","title":"Local-First by Default","text":"<p>Super CLI prioritizes local models: - Uses Ollama by default (runs on your machine) - No data sent to cloud unless you choose cloud models - MCP servers run locally - All data stays on your machine</p>"},{"location":"guides/super-cli/#cloud-models-optional","title":"Cloud Models (Optional)","text":"<p>If you choose OpenAI or Anthropic: - Your conversations go to their APIs - You control the API keys - Keys stored locally in <code>~/.superoptix_config.json</code> - You can delete anytime</p>"},{"location":"guides/super-cli/#mcp-security","title":"MCP Security","text":"<p>MCP servers have controlled access: - Filesystem server: Read/write permissions you grant - Each server can be enabled/disabled - Full audit trail of tool usage - You control what data is accessible</p>"},{"location":"guides/super-cli/#visual-features_1","title":"\ud83c\udfa8 Visual Features","text":""},{"location":"guides/super-cli/#animations","title":"Animations","text":"<p>Continuous feedback during processing: - Thinking animations while parsing intent - Preparing animations while generating commands - Executing animations while running commands - Smooth transitions between steps</p> <p>58+ rotating messages: - \"\u2728 Let me cook...\" - \"\ud83e\udde0 Neurons firing...\" - \"\ud83d\udd25 Warming up the engines...\" - Different every time for engagement!</p>"},{"location":"guides/super-cli/#clean-output","title":"Clean Output","text":"<p>Focus on results, not technical details: - Success indicators - Clear error messages - \ud83d\udcca Relevant information only - No command clutter</p>"},{"location":"guides/super-cli/#advanced-features","title":"\u2699\ufe0f Advanced Features","text":""},{"location":"guides/super-cli/#conversation-context","title":"Conversation Context","text":"<p>Super CLI remembers your session: - Recent interactions - Current agent you're working on - Project state</p> <p>Example: <pre><code>SuperOptiX \u203a build a developer agent\nCreated!\n\nSuperOptiX \u203a compile it\n# Knows \"it\" refers to developer agent\nCompiled!\n\nSuperOptiX \u203a evaluate\n# Knows which agent to evaluate\nEvaluated!\n</code></pre></p>"},{"location":"guides/super-cli/#hybrid-knowledge-access","title":"Hybrid Knowledge Access","text":"<p>Super CLI uses multiple knowledge sources:</p> <ol> <li>Curated Q&amp;A - Pre-built answers for common questions</li> <li>Documentation Access - Reads from docs/ directory</li> <li>MCP Tools - Filesystem access for examples and code</li> </ol> <p>Combines all three for comprehensive answers!</p>"},{"location":"guides/super-cli/#model-isolation","title":"Model Isolation","text":"<p>Important: Super CLI's model is separate from agent models!</p> <ul> <li>CLI model: Used for conversation and intent parsing</li> <li>Agent models: Specified in playbooks, used during execution</li> </ul> <p>No interference: Changing CLI model doesn't affect your agents!</p>"},{"location":"guides/super-cli/#best-practices","title":"\ud83d\ude80 Best Practices","text":""},{"location":"guides/super-cli/#choose-the-right-model","title":"Choose the Right Model","text":"<p>For CLI Conversation: - Local (Ollama): <code>gpt-oss:120b</code> - Great balance of speed and capability - Cloud (OpenAI): <code>gpt-4o-mini</code> - Fast and cost-effective - Cloud (Anthropic): <code>claude-3-5-sonnet</code> - Best for complex reasoning</p> <p>For Agents: - Configure in playbook YAML - Can use different model than CLI - Independent of CLI model choice</p>"},{"location":"guides/super-cli/#use-natural-language","title":"Use Natural Language","text":"<p>Good: - \"build a developer agent\" - \"evaluate my agent\" - \"optimize with GEPA at high level\"</p> <p>Also works (slash commands): - \"/agents\" - Quick reference - \"/help\" - Full documentation - \"/ask \" - Specific questions"},{"location":"guides/super-cli/#leverage-mcp","title":"Leverage MCP","text":"<p>Enable filesystem server: <pre><code>SuperOptiX \u203a /mcp status\nSuperOptiX \u203a /mcp enable filesystem\n</code></pre></p> <p>Now Super CLI can: - Read your code files - Access examples - Provide context-aware answers</p>"},{"location":"guides/super-cli/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/super-cli/#cli-wont-start","title":"CLI Won't Start","text":"<p>Check installation: <pre><code>uv pip show superoptix | grep Version\n# Should show: Version: 0.1.2 or higher\n</code></pre></p> <p>Reinstall if needed: <pre><code>uv pip install --upgrade superoptix\n</code></pre></p>"},{"location":"guides/super-cli/#model-not-found","title":"Model Not Found","text":"<p>For Ollama models: <pre><code># List available\nollama list\n\n# Pull model if needed\nollama pull gpt-oss:120b\n</code></pre></p> <p>For cloud models: <pre><code># Set API key\nexport OPENAI_API_KEY=your-key\n# Or configure in setup wizard\n</code></pre></p>"},{"location":"guides/super-cli/#mcp-server-not-working","title":"MCP Server Not Working","text":"<p>Check server status: <pre><code>SuperOptiX \u203a /mcp status\n</code></pre></p> <p>Restart server: <pre><code>SuperOptiX \u203a /mcp disable filesystem\nSuperOptiX \u203a /mcp enable filesystem\n</code></pre></p>"},{"location":"guides/super-cli/#comparison-traditional-cli-vs-super-cli","title":"\ud83d\udcca Comparison: Traditional CLI vs Super CLI","text":"Feature Traditional CLI Super CLI Syntax Exact commands required Natural language Examples <code>super spec generate genies dev --namespace software</code> \"build a developer agent\" Learning Curve Medium (learn commands) Low (just talk) Flexibility Fixed syntax Understands variations Feedback Text output Animated, visual Model Support Agent models only CLI + Agent models MCP Manual integration Built-in client Knowledge External docs Built-in Q&amp;A + docs <p>Both available - use what fits your workflow!</p>"},{"location":"guides/super-cli/#common-workflows","title":"\ud83c\udfaf Common Workflows","text":""},{"location":"guides/super-cli/#build-compile-evaluate-optimize","title":"Build \u2192 Compile \u2192 Evaluate \u2192 Optimize","text":"<pre><code>SuperOptiX \u203a build a developer agent\nCreated!\n\nSuperOptiX \u203a compile it\nCompiled!\n\nSuperOptiX \u203a evaluate the agent\nEvaluated! Score: 78%\n\nSuperOptiX \u203a optimize with GEPA\nOptimized! Score: 91%\n\nSuperOptiX \u203a evaluate again\nEvaluated! Score: 91%\n</code></pre>"},{"location":"guides/super-cli/#agent-discovery","title":"Agent Discovery","text":"<pre><code>SuperOptiX \u203a /agents\n\ud83d\udce6 Available Agents\n\n\ud83d\udd39 Software Development\n  \u2022 developer, qa_engineer, devops_engineer\n\n\ud83d\udd39 Customer Support\n  \u2022 customer_support, sales_agent\n\nSuperOptiX \u203a /ask tell me about the developer agent\n\ud83d\udcda The developer agent...\n</code></pre>"},{"location":"guides/super-cli/#mcp-integration","title":"MCP Integration","text":"<pre><code>SuperOptiX \u203a /mcp status\n\ud83d\udd27 MCP Client Ready \nSuperOptiX \u203a /ask show me an example of RAG optimization\n\ud83d\udcda Reading from docs/guides/rag.md...\n(Uses MCP filesystem access)\n</code></pre>"},{"location":"guides/super-cli/#future-enhancements","title":"\ud83d\udd2e Future Enhancements","text":""},{"location":"guides/super-cli/#coming-soon","title":"Coming Soon","text":"<ul> <li>\ud83c\udfaf Multi-turn conversations (context across sessions)</li> <li>\ud83e\udde0 Learning from your patterns</li> <li>\ud83d\udd0c More MCP server integrations</li> <li>\ud83c\udf10 Remote MCP server support</li> <li>\ud83d\udcca Visual dashboards in terminal</li> <li>\ud83c\udfa8 Customizable themes</li> <li>\ud83d\udde3\ufe0f Voice input support</li> </ul>"},{"location":"guides/super-cli/#feedback-welcome","title":"Feedback Welcome!","text":"<p>Super CLI is in beta - we want your input!</p> <ul> <li>\ud83d\udcac Join our Discord: SuperagenticAI</li> <li>\ud83d\udc1b Report issues: GitHub Issues</li> <li>\ud83d\udca1 Feature requests: Discussions</li> </ul>"},{"location":"guides/super-cli/#related-documentation","title":"\ud83d\udcd6 Related Documentation","text":"<ul> <li>CLI Complete Guide - Traditional CLI reference</li> <li>MCP Tools Guide - MCP integration details</li> <li>GEPA Optimization - Optimization strategies</li> <li>Agent Development - Building agents</li> </ul>"},{"location":"guides/super-cli/#summary","title":"\u2728 Summary","text":"<p>Super CLI brings conversational AI to your terminal:</p> <ul> <li>\ud83d\udcac Natural language - just talk</li> <li>\ud83e\udd16 Model flexibility - local or cloud</li> <li>\ud83d\udd27 MCP client - built-in tool access</li> <li>\ud83c\udfa8 Beautiful UI - engaging animations</li> <li>\ud83d\udcda Knowledge access - built-in docs</li> <li>\ud83d\ude80 Production ready - stable and tested</li> </ul> <p>Experience the future of AI development tools!</p> <pre><code>super  # Launch and explore!\n</code></pre>"},{"location":"guides/superspec-agent-building/","title":"\ud83c\udfd7\ufe0f SuperSpec Agent Building - Multi-Framework","text":"\ud83c\udfd7\ufe0f SuperSpec Agent Building - Multi-Framework <p> Build powerful AI agents across 6 major frameworks with SuperSpec DSL         One specification format, six frameworks: DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, DeepAgents     </p>"},{"location":"guides/superspec-agent-building/#overview","title":"\ud83d\udccb Overview","text":"<p>Agent building in SuperSpec is the process of creating intelligent, autonomous systems using the SuperSpec Domain-Specific Language (DSL). This approach combines framework flexibility with a structured, YAML-based specification format that makes agent development accessible, maintainable, and scalable.</p> <p>\ud83c\udf1f Key Achievement: Write once, deploy to any of 6 major frameworks!</p>"},{"location":"guides/superspec-agent-building/#agent-building-philosophy","title":"\ud83c\udfaf Agent Building Philosophy","text":""},{"location":"guides/superspec-agent-building/#1-framework-freedom","title":"1. Framework Freedom","text":"<p>SuperSpec enables you to: - Choose Your Framework: Select from 6 major frameworks based on your needs - Keep Your Specification: Same YAML format works across all frameworks - Universal Optimization: GEPA works on all frameworks - Consistent Workflow: compile \u2192 evaluate \u2192 optimize \u2192 run</p> <pre><code>graph TD\n    A[SuperSpec YAML] --&gt; B{Choose Framework}\n    B --&gt;|DSPy| C1[Pure DSPy Agent]\n    B --&gt;|OpenAI SDK| C2[OpenAI Agent]\n    B --&gt;|CrewAI| C3[CrewAI Agent]\n    B --&gt;|Google ADK| C4[Gemini Agent]\n    B --&gt;|Microsoft| C5[Azure Agent]\n    B --&gt;|DeepAgents| C6[LangGraph Agent]\n    C1 &amp; C2 &amp; C3 &amp; C4 &amp; C5 &amp; C6 --&gt; D[GEPA Optimization]\n    D --&gt; E[Production Deployment]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/superspec-agent-building/#2-declarative-design","title":"2. Declarative Design","text":"<p>SuperSpec follows a declarative approach where you specify what the agent should do rather than how to do it. This separation of concerns allows the framework to handle the complex orchestration while you focus on defining the agent's capabilities and behavior.</p>"},{"location":"guides/superspec-agent-building/#3-framework-selection-guide","title":"3. Framework Selection Guide","text":"Framework Variables Local Models Best For \ud83d\udd2c DSPy 10+ Ollama Complex reasoning, research \ud83e\udd16 OpenAI SDK 1 Ollama Simple &amp; fast \ud83d\udc65 CrewAI 5 Ollama Multi-agent teams \ud83d\udd2e Google ADK 1 Cloud only Gemini native, free tier \ud83c\udfe2 Microsoft 1 Ollama Enterprise Azure \ud83c\udf0a DeepAgents 1 Ollama Complex planning"},{"location":"guides/superspec-agent-building/#agent-building-components","title":"\ud83c\udfd7\ufe0f Agent Building Components","text":""},{"location":"guides/superspec-agent-building/#1-core-building-blocks-universal","title":"1. Core Building Blocks (Universal)","text":"<p>Every SuperSpec agent consists of these fundamental components, regardless of framework:</p> <pre><code>metadata:\n  name: \"Agent Name\"\n  id: \"agent_id\"\n  version: \"1.0.0\"\n\nspec:\n  target_framework: dspy|openai|crewai|google-adk|microsoft|deepagents  # Choose your framework\n  language_model: {...}\n  persona: {...}\n  tasks: [...]  # Framework-specific\n  feature_specifications: {...}  # Universal BDD scenarios\n  optimization: {...}  # Universal GEPA optimization\n</code></pre>"},{"location":"guides/superspec-agent-building/#2-language-model-configuration","title":"2. Language Model Configuration","text":"<p>The foundation of any agent is its language model:</p> <pre><code>spec:\n  language_model:\n    location: local|self-hosted|cloud\n    provider: ollama|openai|anthropic|...\n    model: llama3.2:1b|gpt-4|claude-3-sonnet\n    temperature: 0.0\n    max_tokens: 2048\n    cache: true\n</code></pre>"},{"location":"guides/superspec-agent-building/#model-selection-guidelines","title":"Model Selection Guidelines","text":"Use Case Recommended Model Reasoning Development/Testing <code>llama3.2:1b</code> Fast, local, cost-effective Production (Oracles) <code>llama3.1:8b</code> Good balance of speed/quality Production (Genies) <code>llama3.1:70b</code> High-quality reasoning Cloud Production <code>gpt-4</code> or <code>claude-3-sonnet</code> Best performance"},{"location":"guides/superspec-agent-building/#3-persona-definition","title":"3. Persona Definition","text":"<p>Define the agent's personality and expertise:</p> <pre><code>spec:\n  persona:\n    name: \"DataBot\"\n    role: \"Data Analyst\"\n    goal: \"Perform comprehensive data analysis and provide insights\"\n    traits:\n    - analytical\n    - detail-oriented\n    - precise\n    - helpful\n    expertise_areas:\n    - data analysis\n    - statistical modeling\n    - financial analysis\n    - visualization\n    communication_preferences:\n      style: technical\n      tone: professional\n      verbosity: detailed\n</code></pre>"},{"location":"guides/superspec-agent-building/#4-task-specification","title":"4. Task Specification","text":"<p>Define what the agent can do:</p> <pre><code>spec:\n  tasks:\n  - name: analyze_data\n    description: \"Perform comprehensive data analysis\"\n    instruction: |\n      You are a Data Analyst. Analyze the provided data and generate insights, \n      visualizations, and recommendations. Use appropriate statistical methods \n      and create clear, actionable insights.\n    inputs:\n    - name: data_source\n      type: str\n      description: \"Source of data to analyze\"\n      required: true\n    - name: analysis_type\n      type: str\n      description: \"Type of analysis to perform\"\n      required: true\n    outputs:\n    - name: analysis_report\n      type: str\n      description: \"Comprehensive analysis report\"\n    - name: visualizations\n      type: list[str]\n      description: \"Generated data visualizations\"\n    - name: recommendations\n      type: str\n      description: \"Actionable recommendations\"\n</code></pre>"},{"location":"guides/superspec-agent-building/#5-agentflow-design","title":"5. AgentFlow Design","text":"<p>Define how the agent executes tasks:</p> <pre><code>spec:\n  agentflow:\n  - name: load_data\n    type: ActWithTools\n    task: analyze_data\n  - name: perform_analysis\n    type: Think\n    task: analyze_data\n    depends_on: [\"load_data\"]\n  - name: generate_visualizations\n    type: Generate\n    task: create_visualization\n    depends_on: [\"perform_analysis\"]\n  - name: synthesize_results\n    type: Generate\n    task: analyze_data\n    depends_on: [\"generate_visualizations\"]\n</code></pre>"},{"location":"guides/superspec-agent-building/#available-agentflow-types","title":"Available AgentFlow Types","text":"Tier Allowed Types Description Oracles <code>Generate</code>, <code>Think</code>, <code>Compare</code>, <code>Route</code> Basic reasoning and generation Genies <code>Generate</code>, <code>Think</code>, <code>ActWithTools</code>, <code>Search</code>, <code>Compare</code>, <code>Route</code> Advanced reasoning with tools"},{"location":"guides/superspec-agent-building/#agent-building-workflow","title":"\ud83d\ude80 Agent Building Workflow","text":""},{"location":"guides/superspec-agent-building/#step-1-define-requirements","title":"Step 1: Define Requirements","text":"<pre><code># Start with a template\nsuper spec generate my_agent basic --rag\n\n# Or create from scratch\ntouch my_agent_playbook.yaml\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-2-configure-basic-structure","title":"Step 2: Configure Basic Structure","text":"<pre><code>metadata:\n  name: \"My Custom Agent\"\n  id: \"my_custom_agent\"\n  version: \"1.0.0\"\n  level: genies\n  description: \"A custom agent for specific tasks\"\n  tags: [\"custom\", \"specialized\"]\n\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.7\n    max_tokens: 2048\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-3-define-persona","title":"Step 3: Define Persona","text":"<pre><code>spec:\n  persona:\n    name: \"CustomBot\"\n    role: \"Specialized Assistant\"\n    goal: \"Provide expert assistance in my domain\"\n    traits:\n    - knowledgeable\n    - helpful\n    - precise\n    expertise_areas:\n    - my_domain\n    - problem_solving\n    communication_preferences:\n      style: professional\n      tone: helpful\n      verbosity: detailed\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-4-specify-tasks","title":"Step 4: Specify Tasks","text":"<pre><code>spec:\n  tasks:\n  - name: solve_problem\n    description: \"Solve problems in my domain\"\n    instruction: |\n      You are a specialized assistant. Analyze the problem and provide \n      comprehensive solutions using your expertise and available tools.\n    inputs:\n    - name: problem\n      type: str\n      description: \"The problem to solve\"\n      required: true\n    - name: context\n      type: str\n      description: \"Additional context\"\n      required: false\n    outputs:\n    - name: solution\n      type: str\n      description: \"Comprehensive solution\"\n    - name: explanation\n      type: str\n      description: \"Detailed explanation\"\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-5-design-agentflow","title":"Step 5: Design AgentFlow","text":"<pre><code>spec:\n  agentflow:\n  - name: analyze_problem\n    type: Think\n    task: solve_problem\n  - name: gather_information\n    type: ActWithTools\n    task: solve_problem\n    depends_on: [\"analyze_problem\"]\n  - name: generate_solution\n    type: Generate\n    task: solve_problem\n    depends_on: [\"gather_information\"]\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-6-add-advanced-features-genies-only","title":"Step 6: Add Advanced Features (Genies Only)","text":"<pre><code>spec:\n  # Memory for context retention\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n\n  # Tools for external capabilities\n  tools:\n    enabled: true\n    categories:\n    - core\n    - utilities\n    specific_tools:\n    - calculator\n    - text_analyzer\n    - web_search\n\n  # RAG for knowledge retrieval\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-7-configure-evaluation","title":"Step 7: Configure Evaluation","text":"<pre><code>spec:\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.8\n    - name: solution_quality\n      threshold: 0.7\n</code></pre>"},{"location":"guides/superspec-agent-building/#step-8-add-bdd-scenarios","title":"Step 8: Add BDD Scenarios","text":"<pre><code>spec:\n  feature_specifications:\n    scenarios:\n    - name: basic_functionality\n      description: \"The agent should solve basic problems\"\n      input:\n        problem: \"What is 2+2?\"\n      expected_output:\n        solution: \"Should provide correct answer\"\n    - name: complex_functionality\n      description: \"The agent should handle complex scenarios\"\n      input:\n        problem: \"Analyze this complex dataset\"\n      expected_output:\n        solution: \"Should provide comprehensive analysis\"\n</code></pre>"},{"location":"guides/superspec-agent-building/#agent-building-examples","title":"\ud83c\udfaf Agent Building Examples","text":""},{"location":"guides/superspec-agent-building/#example-1-simple-oracle-agent","title":"Example 1: Simple Oracle Agent","text":"<pre><code>metadata:\n  name: \"Math Tutor\"\n  id: \"math_tutor\"\n  version: \"1.0.0\"\n  level: oracles\n  description: \"A simple math tutoring agent\"\n  tags: [\"education\", \"math\", \"oracles\"]\n\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n    temperature: 0.0\n    max_tokens: 1024\n\n  persona:\n    name: \"MathTutor\"\n    role: \"Math Tutor\"\n    goal: \"Help students understand mathematical concepts\"\n    traits:\n    - patient\n    - encouraging\n    - knowledgeable\n    expertise_areas:\n    - mathematics\n    - teaching\n    - problem_solving\n\n  tasks:\n  - name: explain_concept\n    description: \"Explain mathematical concepts clearly\"\n    instruction: |\n      You are a Math Tutor. Explain mathematical concepts in a clear, \n      step-by-step manner that students can easily understand.\n    inputs:\n    - name: concept\n      type: str\n      description: \"Mathematical concept to explain\"\n      required: true\n    - name: student_level\n      type: str\n      description: \"Student's level (beginner, intermediate, advanced)\"\n      required: false\n    outputs:\n    - name: explanation\n      type: str\n      description: \"Clear explanation of the concept\"\n    - name: examples\n      type: list[str]\n      description: \"Example problems and solutions\"\n\n  agentflow:\n  - name: explain\n    type: Generate\n    task: explain_concept\n\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.9\n    - name: explanation_clarity\n      threshold: 0.8\n</code></pre>"},{"location":"guides/superspec-agent-building/#example-2-advanced-genie-agent","title":"Example 2: Advanced Genie Agent","text":"<pre><code>metadata:\n  name: \"Financial Analyst\"\n  id: \"financial_analyst\"\n  version: \"1.0.0\"\n  level: genies\n  description: \"Advanced financial analysis agent\"\n  tags: [\"finance\", \"analysis\", \"genies\"]\n\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.7\n    max_tokens: 2048\n\n  persona:\n    name: \"FinanceBot\"\n    role: \"Financial Analyst\"\n    goal: \"Provide comprehensive financial analysis and insights\"\n    traits:\n    - analytical\n    - detail-oriented\n    - conservative\n    - trustworthy\n    expertise_areas:\n    - financial_analysis\n    - investment_planning\n    - risk_assessment\n    - market_research\n    communication_preferences:\n      style: professional\n      tone: authoritative\n      verbosity: detailed\n\n  tasks:\n  - name: analyze_investment\n    description: \"Analyze investment opportunities\"\n    instruction: |\n      You are a Financial Analyst. Analyze investment opportunities \n      comprehensively, considering risk, return, market conditions, \n      and regulatory factors.\n    inputs:\n    - name: investment_data\n      type: str\n      description: \"Investment data and parameters\"\n      required: true\n    - name: risk_tolerance\n      type: str\n      description: \"Investor's risk tolerance level\"\n      required: true\n    outputs:\n    - name: analysis_report\n      type: str\n      description: \"Comprehensive investment analysis\"\n    - name: risk_assessment\n      type: str\n      description: \"Detailed risk assessment\"\n    - name: recommendations\n      type: str\n      description: \"Investment recommendations\"\n\n  agentflow:\n  - name: gather_market_data\n    type: ActWithTools\n    task: analyze_investment\n  - name: perform_analysis\n    type: Think\n    task: analyze_investment\n    depends_on: [\"gather_market_data\"]\n  - name: calculate_metrics\n    type: ActWithTools\n    task: analyze_investment\n    depends_on: [\"perform_analysis\"]\n  - name: generate_report\n    type: Generate\n    task: analyze_investment\n    depends_on: [\"calculate_metrics\"]\n\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n    episodic:\n      enabled: true\n      max_episodes: 50\n\n  tools:\n    enabled: true\n    categories:\n    - core\n    - finance\n    - utilities\n    specific_tools:\n    - calculator\n    - financial_calculator\n    - text_analyzer\n    - web_search\n\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n    vector_store:\n      collection_name: financial_knowledge\n\n  evaluation:\n    builtin_metrics:\n    - name: analysis_accuracy\n      threshold: 0.8\n    - name: risk_assessment_quality\n      threshold: 0.7\n    - name: recommendation_safety\n      threshold: 0.9\n\n  feature_specifications:\n    scenarios:\n    - name: basic_analysis\n      description: \"Perform basic investment analysis\"\n      input:\n        investment_data: \"Stock price data for AAPL\"\n        risk_tolerance: \"moderate\"\n      expected_output:\n        analysis_report: \"Should provide comprehensive analysis\"\n        risk_assessment: \"Should assess risks appropriately\"\n        recommendations: \"Should provide safe recommendations\"\n</code></pre>"},{"location":"guides/superspec-agent-building/#agent-building-best-practices","title":"\ud83e\uddea Agent Building Best Practices","text":""},{"location":"guides/superspec-agent-building/#1-start-simple-iterate","title":"1. Start Simple, Iterate","text":"<pre><code># Start with basic configuration\nmetadata:\n  level: oracles  # Start with Oracles tier\n\n# Add complexity gradually\nspec:\n  # Basic LLM\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n\n  # Simple persona\n  persona:\n    role: \"Assistant\"\n    goal: \"Help with tasks\"\n\n  # Single task\n  tasks:\n  - name: help\n    instruction: \"Help with user requests\"\n    inputs:\n    - name: request\n      type: str\n      required: true\n    outputs:\n    - name: response\n      type: str\n</code></pre>"},{"location":"guides/superspec-agent-building/#2-use-descriptive-names-and-descriptions","title":"2. Use Descriptive Names and Descriptions","text":"<pre><code>metadata:\n  name: \"Financial Data Analyst\"  # Clear, descriptive name\n  description: \"Specialized agent for financial data analysis and reporting\"\n  tags: [\"finance\", \"data-analysis\", \"genies\"]\n\npersona:\n  role: \"Financial Data Analyst\"\n  goal: \"Provide accurate financial analysis and insights\"\n  traits:\n  - analytical\n  - detail-oriented\n  - precise\n</code></pre>"},{"location":"guides/superspec-agent-building/#3-configure-appropriate-evaluation-metrics","title":"3. Configure Appropriate Evaluation Metrics","text":"<pre><code>evaluation:\n  builtin_metrics:\n  - name: answer_correctness\n    threshold: 0.8\n    weight: 2.0\n  - name: analysis_quality\n    threshold: 0.7\n  - name: safety_compliance\n    threshold: 1.0\n    weight: 3.0\n</code></pre>"},{"location":"guides/superspec-agent-building/#4-use-comprehensive-bdd-scenarios","title":"4. Use Comprehensive BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n  - name: basic_functionality\n    description: \"The agent should perform basic tasks correctly\"\n    input:\n      request: \"What is the capital of France?\"\n    expected_output:\n      response: \"Should provide accurate information about Paris\"\n  - name: advanced_functionality\n    description: \"The agent should handle complex scenarios\"\n    input:\n      request: \"Analyze this financial dataset\"\n    expected_output:\n      response: \"Should provide comprehensive analysis\"\n</code></pre>"},{"location":"guides/superspec-agent-building/#5-test-and-validate","title":"5. Test and Validate","text":"<pre><code># Validate your agent specification\nsuper spec validate my_agent_playbook.yaml\n\n# Test with sample inputs\nsuper agent test my_agent_playbook.yaml --input \"test input\"\n\n# Evaluate performance\nsuper agent evaluate my_agent_playbook.yaml\n</code></pre>"},{"location":"guides/superspec-agent-building/#advanced-agent-building-techniques","title":"\ud83d\ude80 Advanced Agent Building Techniques","text":""},{"location":"guides/superspec-agent-building/#1-multi-task-agents","title":"1. Multi-Task Agents","text":"<pre><code>spec:\n  tasks:\n  - name: analyze_data\n    description: \"Analyze data and generate insights\"\n    instruction: \"Perform data analysis...\"\n    inputs: [...]\n    outputs: [...]\n\n  - name: create_visualization\n    description: \"Create data visualizations\"\n    instruction: \"Create charts and graphs...\"\n    inputs: [...]\n    outputs: [...]\n\n  - name: generate_report\n    description: \"Generate comprehensive reports\"\n    instruction: \"Create detailed reports...\"\n    inputs: [...]\n    outputs: [...]\n\n  agentflow:\n  - name: analyze\n    type: Think\n    task: analyze_data\n  - name: visualize\n    type: Generate\n    task: create_visualization\n    depends_on: [\"analyze\"]\n  - name: report\n    type: Generate\n    task: generate_report\n    depends_on: [\"visualize\"]\n</code></pre>"},{"location":"guides/superspec-agent-building/#2-conditional-agentflow","title":"2. Conditional AgentFlow","text":"<pre><code>spec:\n  agentflow:\n  - name: analyze_input\n    type: Think\n    task: analyze_data\n  - name: check_complexity\n    type: Route\n    task: analyze_data\n    depends_on: [\"analyze_input\"]\n  - name: simple_analysis\n    type: Generate\n    task: analyze_data\n    depends_on: [\"check_complexity\"]\n  - name: complex_analysis\n    type: ActWithTools\n    task: analyze_data\n    depends_on: [\"check_complexity\"]\n</code></pre>"},{"location":"guides/superspec-agent-building/#3-tool-specific-agents","title":"3. Tool-Specific Agents","text":"<pre><code>spec:\n  tools:\n    enabled: true\n    categories:\n    - development\n    specific_tools:\n    - code_formatter\n    - debugger\n    - linter\n    - git_manager\n    - docker_manager\n\n  tasks:\n  - name: code_review\n    description: \"Review and improve code\"\n    instruction: |\n      You are a Code Reviewer. Analyze code for quality, security, \n      and best practices. Use available tools to format, lint, and \n      improve the code.\n</code></pre>"},{"location":"guides/superspec-agent-building/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>Agent building with SuperSpec combines the power of DSPy with the simplicity of YAML configuration. By following the declarative approach and leveraging the framework architecture, you can create sophisticated AI agents that are both powerful and maintainable.</p> <p>The key to successful agent building is to: 1. Start simple with basic configurations 2. Iterate gradually adding complexity as needed 3. Test thoroughly with comprehensive scenarios 4. Validate continuously using built-in metrics 5. Optimize performance through DSPy-based improvements</p> <p>\ud83d\udca1 Pro Tip: Use the <code>super spec generate</code> command with domain-specific templates to jumpstart your agent development! </p>"},{"location":"guides/superspec-configuration/","title":"\u2699\ufe0f SuperSpec Configuration","text":"\u2699\ufe0f SuperSpec Configuration <p> Complete configuration reference for SuperSpec DSL         Based on the actual SuperOptiX library implementation     </p>"},{"location":"guides/superspec-configuration/#overview","title":"\ud83d\udccb Overview","text":"<p>SuperSpec configuration covers all aspects of agent specification, from basic metadata to advanced features like memory, tools, and RAG. This page provides detailed configuration options based on the actual library implementation.</p>"},{"location":"guides/superspec-configuration/#configuration-sections","title":"\ud83c\udfaf Configuration Sections","text":""},{"location":"guides/superspec-configuration/#1-metadata-configuration","title":"1. Metadata Configuration","text":"<p>The <code>metadata</code> section defines the agent's identity and basic properties.</p> <pre><code>metadata:\n  name: string                    # REQUIRED - Human-readable agent name\n  id: string                      # REQUIRED - Unique identifier (a-z, 0-9, -, _)\n  version: string                 # REQUIRED - Semantic versioning (e.g., \"1.0.0\")\n  namespace: string               # OPTIONAL - Logical grouping namespace\n  level: oracles|genies          # OPTIONAL - Agent tier level (default: oracles)\n  stage: alpha|beta|stable       # OPTIONAL - Development stage (default: alpha)\n  agent_type: Autonomous|Supervised|Interactive|Reactive|Deliberative|Hybrid  # OPTIONAL\n  description: string             # OPTIONAL - Brief agent description\n  tags: [string]                  # OPTIONAL - Categorization tags\n  created_at: string              # OPTIONAL - ISO timestamp of creation\n  updated_at: string              # OPTIONAL - ISO timestamp of last update\n</code></pre>"},{"location":"guides/superspec-configuration/#valid-namespaces","title":"Valid Namespaces","text":"<pre><code>VALID_NAMESPACES = [\n    \"software\", \"education\", \"healthcare\", \"finance\", \"marketing\",\n    \"legal\", \"consulting\", \"retail\", \"manufacturing\", \"transportation\", \n    \"agriculture_food\", \"energy_utilities\", \"gaming_sports\",\n    \"government_public\", \"hospitality_tourism\", \"human_resources\",\n    \"media_entertainment\", \"real_estate\", \"testing\"\n]\n</code></pre>"},{"location":"guides/superspec-configuration/#valid-agent-types","title":"Valid Agent Types","text":"<pre><code>VALID_AGENT_TYPES = [\n    \"Autonomous\", \"Supervised\", \"Interactive\", \"Reactive\", \"Deliberative\", \"Hybrid\"\n]\n</code></pre>"},{"location":"guides/superspec-configuration/#2-language-model-configuration","title":"2. Language Model Configuration","text":"<p>The <code>language_model</code> section configures the underlying LLM.</p> <pre><code>spec:\n  language_model:\n    location: local|self-hosted|cloud  # REQUIRED - Model hosting location\n    provider: string                   # REQUIRED - Model provider\n    model: string                      # REQUIRED - Specific model identifier\n    model_type: chat|text|completion  # OPTIONAL - Model endpoint type (default: chat)\n    temperature: float                 # OPTIONAL - Randomness control (0.0-2.0, default: 0.0)\n    max_tokens: int                    # OPTIONAL - Max output length (1-100000, default: 4000)\n    top_p: float                       # OPTIONAL - Nucleus sampling (0.0-1.0, default: 1.0)\n    frequency_penalty: float           # OPTIONAL - Frequency penalty (-2.0-2.0, default: 0.0)\n    presence_penalty: float            # OPTIONAL - Presence penalty (-2.0-2.0, default: 0.0)\n    cache: bool                        # OPTIONAL - Enable LLM call caching (default: true)\n    cache_in_memory: bool              # OPTIONAL - Cache in memory vs disk (default: true)\n    num_retries: int                   # OPTIONAL - API failure retry attempts (0-10, default: 3)\n    modalities: [text|image|audio|video]  # OPTIONAL - Supported modalities (default: [text])\n    api_key: string                    # OPTIONAL - API key for cloud providers\n    api_base: string                   # OPTIONAL - Custom API base URL\n    api_version: string                # OPTIONAL - API version (Azure)\n</code></pre>"},{"location":"guides/superspec-configuration/#valid-providers-by-location","title":"Valid Providers by Location","text":"Location Supported Providers local <code>ollama</code>, <code>vllm</code>, <code>sglang</code>, <code>mlx</code>, <code>lm_studio</code> self-hosted <code>custom</code> cloud <code>openai</code>, <code>anthropic</code>, <code>google</code>, <code>azure</code>, <code>mistral</code>, <code>cohere</code>, <code>groq</code>, <code>deepseek</code>"},{"location":"guides/superspec-configuration/#3-persona-configuration","title":"3. Persona Configuration","text":"<p>The <code>persona</code> section defines the agent's personality and behavior.</p> <pre><code>spec:\n  persona:\n    name: string                        # OPTIONAL - Agent's name\n    role: string                        # REQUIRED - Agent's role\n    goal: string                        # OPTIONAL - Primary objective\n    traits: [string]                    # OPTIONAL - Personality characteristics\n    expertise_areas: [string]           # OPTIONAL - Knowledge domains\n    communication_preferences:          # OPTIONAL\n      style: formal|casual|technical|conversational  # OPTIONAL (default: formal)\n      tone: professional|friendly|authoritative|supportive  # OPTIONAL (default: professional)\n      verbosity: concise|detailed|adaptive  # OPTIONAL (default: concise)\n</code></pre>"},{"location":"guides/superspec-configuration/#4-tasks-configuration","title":"4. Tasks Configuration","text":"<p>The <code>tasks</code> section defines the agent's capabilities.</p> <pre><code>spec:\n  tasks:                                # REQUIRED (minimum 1 task)\n    - name: string                      # REQUIRED - Unique task name\n      description: string               # REQUIRED - What the task achieves\n      instruction: string               # REQUIRED - Core LLM instruction\n      inputs:                           # REQUIRED (minimum 1 input)\n        - name: string                  # REQUIRED - Input field name\n          type: str|int|bool|float|list[str]|dict[str,Any]  # REQUIRED - Data type\n          description: string           # REQUIRED - Field description\n          required: bool                # REQUIRED - Whether mandatory\n      outputs:                          # REQUIRED (minimum 1 output)\n        - name: string                  # REQUIRED - Output field name\n          type: str|int|bool|float|list[str]|dict[str,Any]  # REQUIRED - Data type\n          description: string           # REQUIRED - Field description\n      schema:                           # OPTIONAL - Task schema configuration\n        style: chain_of_thought|direct|structured  # OPTIONAL\n        reasoning_traces: bool          # OPTIONAL\n      reasoning_steps: [string]         # OPTIONAL - Reasoning step definitions\n      training_examples: [dict]         # OPTIONAL - Training examples\n      assertions: [dict]                # OPTIONAL - Task assertions\n</code></pre>"},{"location":"guides/superspec-configuration/#5-agentflow-configuration","title":"5. AgentFlow Configuration","text":"<p>The <code>agentflow</code> section defines the execution flow.</p> <pre><code>spec:\n  agentflow:                            # OPTIONAL - Execution flow\n    - name: string                      # REQUIRED - Step name\n      type: string                      # REQUIRED - Step type\n      task: string                      # REQUIRED - Task name to execute\n      depends_on: [string]              # OPTIONAL - Dependencies\n      config: dict                      # OPTIONAL - Step configuration\n      retry_policy: dict                # OPTIONAL - Retry policy\n</code></pre>"},{"location":"guides/superspec-configuration/#valid-agentflow-types-by-tier","title":"Valid AgentFlow Types by Tier","text":"Tier Allowed Types oracles <code>Generate</code>, <code>Think</code>, <code>Compare</code>, <code>Route</code> genies <code>Generate</code>, <code>Think</code>, <code>ActWithTools</code>, <code>Search</code>, <code>Compare</code>, <code>Route</code>"},{"location":"guides/superspec-configuration/#6-memory-configuration-genies-only","title":"6. Memory Configuration (Genies Only)","text":"<p>The <code>memory</code> section configures memory systems for Genies tier agents.</p> <pre><code>spec:\n  memory:\n    enabled: bool                       # REQUIRED - Enable memory system\n    short_term:                         # OPTIONAL - Working memory\n      enabled: bool                     # OPTIONAL (default: true)\n      max_tokens: int                   # OPTIONAL - Maximum tokens\n      window_size: int                  # OPTIONAL - Conversation window size\n    long_term:                          # OPTIONAL - Persistent knowledge\n      enabled: bool                     # OPTIONAL (default: true)\n      storage_type: local               # OPTIONAL - Storage backend\n      max_entries: int                  # OPTIONAL - Maximum entries\n      persistence: bool                 # OPTIONAL - Enable persistence\n    episodic:                           # OPTIONAL - Experience tracking\n      enabled: bool                     # OPTIONAL (default: true)\n      max_episodes: int                 # OPTIONAL - Maximum episodes\n      episode_retention: int            # OPTIONAL - Episode retention days\n    context_manager:                    # OPTIONAL - Context management\n      enabled: bool                     # OPTIONAL (default: true)\n      max_context_length: int           # OPTIONAL - Maximum context length\n      context_strategy: sliding_window  # OPTIONAL - Context strategy\n</code></pre>"},{"location":"guides/superspec-configuration/#valid-memory-backends","title":"Valid Memory Backends","text":"<pre><code>VALID_MEMORY_BACKENDS = [\"file\", \"sqlite\", \"redis\"]\n</code></pre>"},{"location":"guides/superspec-configuration/#7-tools-configuration-genies-only","title":"7. Tools Configuration (Genies Only)","text":"<p>The <code>tools</code> section enables tool integration for Genies tier agents.</p> <pre><code>spec:\n  tools:\n    enabled: bool                       # REQUIRED - Enable tool integration\n    categories: [string]                # OPTIONAL - Tool categories to include\n    specific_tools: [string]            # OPTIONAL - Specific tools to include\n</code></pre>"},{"location":"guides/superspec-configuration/#available-tool-categories","title":"Available Tool Categories","text":"Category Description Examples <code>core</code> Essential tools calculator, file_reader, text_analyzer <code>development</code> Software development code_formatter, debugger, linter <code>utilities</code> General utilities date_time, json_processor, data_processor <code>finance</code> Financial tools financial_calculator, investment_analyzer <code>healthcare</code> Healthcare tools health_assessment, medical_analyzer <code>education</code> Educational tools educational_content, quiz_generator <code>legal</code> Legal tools legal_analyzer, contract_reviewer <code>marketing</code> Marketing tools marketing_analyzer, campaign_planner <code>real_estate</code> Real estate tools property_analyzer, market_researcher <code>retail</code> Retail tools inventory_manager, sales_analyzer <code>transportation</code> Transportation tools route_optimizer, fleet_manager <code>energy</code> Energy tools energy_calculator, efficiency_analyzer <code>agriculture</code> Agriculture tools crop_advisor, weather_analyzer <code>human_resources</code> HR tools hr_analyzer, recruitment_assistant <code>hospitality</code> Hospitality tools hospitality_manager, booking_optimizer <code>manufacturing</code> Manufacturing tools production_planner, quality_inspector <code>gaming_sports</code> Gaming/Sports tools game_analyzer, performance_tracker <code>media_entertainment</code> Media tools content_analyzer, trend_predictor <code>government_public</code> Government tools policy_analyzer, compliance_checker <code>consulting</code> Consulting tools business_consultant, strategy_advisor"},{"location":"guides/superspec-configuration/#8-rag-configuration-genies-only","title":"8. RAG Configuration (Genies Only)","text":"<p>The <code>retrieval</code> section configures Retrieval-Augmented Generation.</p> <pre><code>spec:\n  retrieval:\n    enabled: bool                       # REQUIRED - Enable RAG\n    retriever_type: string              # REQUIRED - Vector store type\n    config:                             # OPTIONAL - Retrieval configuration\n      top_k: int                        # OPTIONAL (default: 5) - Number of results\n      chunk_size: int                   # OPTIONAL (default: 512) - Text chunk size\n      chunk_overlap: int                # OPTIONAL (default: 50) - Chunk overlap\n    vector_store:                       # OPTIONAL - Vector store configuration\n      embedding_model: string           # OPTIONAL - Embedding model\n      collection_name: string           # OPTIONAL - Collection name\n</code></pre>"},{"location":"guides/superspec-configuration/#supported-vector-stores","title":"Supported Vector Stores","text":"Retriever Type Description Use Case <code>chroma</code> ChromaDB vector store Local development, small datasets <code>qdrant</code> Qdrant vector database Production, scalable deployments <code>weaviate</code> Weaviate vector database Enterprise, rich metadata <code>milvus</code> Milvus vector database High-performance, large-scale <code>lancedb</code> LanceDB vector database Fast, embedded vector storage"},{"location":"guides/superspec-configuration/#9-evaluation-configuration","title":"9. Evaluation Configuration","text":"<p>The <code>evaluation</code> section defines quality metrics and validation criteria.</p> <pre><code>spec:\n  evaluation:\n    builtin_metrics:                    # OPTIONAL - Quality metrics\n      - name: string                    # REQUIRED - Metric name\n        threshold: float                # OPTIONAL (default: 0.7) - Quality threshold\n        weight: float                   # OPTIONAL (default: 1.0) - Metric weight\n</code></pre>"},{"location":"guides/superspec-configuration/#available-built-in-metrics","title":"Available Built-in Metrics","text":"<pre><code>VALID_BUILTIN_METRICS = [\n    \"answer_exact_match\", \"answer_passage_match\", \"semantic_f1\", \n    \"rouge_l\", \"bleu\", \"meteor\", \"answer_correctness\", \n    \"faithfulness\", \"context_relevance\"\n]\n</code></pre>"},{"location":"guides/superspec-configuration/#10-feature-specifications-bdd-scenarios","title":"10. Feature Specifications (BDD Scenarios)","text":"<p>The <code>feature_specifications</code> section defines BDD scenarios for testing.</p> <pre><code>spec:\n  feature_specifications:               # OPTIONAL - BDD scenarios\n    scenarios:                          # OPTIONAL\n      - name: string                    # REQUIRED - Scenario name\n        description: string             # REQUIRED - Scenario description\n        input: {}                       # REQUIRED - Input data\n        expected_output: {}             # REQUIRED - Expected output\n        validation_criteria: [string]   # OPTIONAL - Validation hints\n</code></pre>"},{"location":"guides/superspec-configuration/#11-optimization-configuration","title":"11. Optimization Configuration","text":"<p>The <code>optimization</code> section configures DSPy-based performance improvement.</p> <pre><code>spec:\n  optimization:\n    strategy: string                    # OPTIONAL - Optimization strategy\n    metric: string                      # REQUIRED - Metric to optimize for\n    metric_threshold: float             # OPTIONAL - Quality threshold\n    few_shot_bootstrapping_config:      # OPTIONAL - Bootstrap configuration\n      max_bootstrapped_demos: int       # OPTIONAL (default: 4) - Max demos\n      max_rounds: int                   # OPTIONAL (default: 1) - Optimization rounds\n</code></pre>"},{"location":"guides/superspec-configuration/#available-optimization-strategies","title":"Available Optimization Strategies","text":"<pre><code>VALID_OPTIMIZATION_STRATEGIES = [\"few_shot_bootstrapping\"]  # Current version only\n</code></pre>"},{"location":"guides/superspec-configuration/#12-runtime-configuration","title":"12. Runtime Configuration","text":"<p>The <code>runtime</code> section configures runtime behavior.</p> <pre><code>spec:\n  runtime:\n    caching:                            # OPTIONAL - Caching configuration\n      enabled: bool                     # OPTIONAL (default: true)\n      backend: memory|disk|redis        # OPTIONAL - Cache backend\n      ttl: int                          # OPTIONAL - Time to live in seconds\n    monitoring:                         # OPTIONAL - Monitoring configuration\n      log_level: debug|info|warn|error  # OPTIONAL - Log level\n      metrics_collection: bool          # OPTIONAL - Enable metrics collection\n</code></pre>"},{"location":"guides/superspec-configuration/#13-dspy-optimizerteleprompter-support","title":"13. DSPy Optimizer/Teleprompter Support","text":"<p>SuperSpec now supports configuring any DSPy optimizer or teleprompter directly in your agent spec. This enables advanced optimization strategies, including GEPA, SIMBA, COPRO, KNNFewShot, BetterTogether, and more.</p> <p>How to use:</p> <p>Add an <code>optimizer</code> section under <code>spec.optimization</code>:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA  # or SIMBA, COPRO, KNNFewShot, BetterTogether, etc.\n      params:\n        metric: semantic_f1\n        max_iters: 10\n        feedback_fn: my_custom_feedback\n    # ... other optimization config ...\n</code></pre> <ul> <li><code>name</code>: The DSPy optimizer/teleprompter class name (case-sensitive, e.g., GEPA, SIMBA, COPRO, KNNFewShot, BetterTogether, BootstrapFewShot, LabeledFewShot, etc.)</li> <li><code>params</code>: Dictionary of parameters to pass to the optimizer/teleprompter constructor. Keys/values depend on the optimizer.</li> </ul> <p>Examples:</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: semantic_f1\n        max_iters: 10\n        feedback_fn: my_custom_feedback\n</code></pre> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: KNNFewShot\n      params:\n        k: 8\n        retriever: my_knn_retriever\n</code></pre> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: BetterTogether\n      params:\n        prompt_optimizer: BootstrapFewShot\n        weight_optimizer: BootstrapFinetune\n</code></pre> <p>How it works: - The pipeline generator will dynamically instantiate the specified optimizer/teleprompter with the provided parameters. - All DSPy optimizers/teleprompters in your DSPy version are supported. - If not specified, the default optimizer for the agent tier is used (LabeledFewShot for Oracles, BootstrapFewShot for Genies).</p> <p>See also: - DSPy Optimizer Reference - GEPA Optimizer</p>"},{"location":"guides/superspec-configuration/#tier-specific-configuration","title":"\ud83c\udfaf Tier-Specific Configuration","text":""},{"location":"guides/superspec-configuration/#oracles-tier-configuration","title":"Oracles Tier Configuration","text":"<pre><code>metadata:\n  level: oracles\n\nspec:\n  # Basic LLM configuration\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n    temperature: 0.0\n    max_tokens: 2048\n\n  # Simple persona\n  persona:\n    role: \"Assistant\"\n    goal: \"Help with basic tasks\"\n\n  # Single task\n  tasks:\n  - name: answer_question\n    instruction: \"Answer questions clearly and accurately\"\n    inputs:\n    - name: question\n      type: str\n      description: \"The question to answer\"\n      required: true\n    outputs:\n    - name: answer\n      type: str\n      description: \"The answer to the question\"\n\n  # Simple flow\n  agentflow:\n  - name: generate_answer\n    type: Think\n    task: answer_question\n\n  # Basic evaluation\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.8\n</code></pre>"},{"location":"guides/superspec-configuration/#genies-tier-configuration","title":"Genies Tier Configuration","text":"<pre><code>metadata:\n  level: genies\n\nspec:\n  # Advanced LLM configuration\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.7\n    max_tokens: 2048\n\n  # Rich persona\n  persona:\n    name: \"AdvancedBot\"\n    role: \"Advanced Assistant\"\n    goal: \"Provide comprehensive assistance with tools and memory\"\n    traits:\n    - helpful\n    - knowledgeable\n    - precise\n\n  # Multiple tasks\n  tasks:\n  - name: solve_problem\n    instruction: \"Solve complex problems using tools and reasoning\"\n    inputs:\n    - name: problem\n      type: str\n      description: \"Problem to solve\"\n      required: true\n    outputs:\n    - name: solution\n      type: str\n      description: \"Problem solution\"\n\n  # Complex flow\n  agentflow:\n  - name: analyze_problem\n    type: Think\n    task: solve_problem\n  - name: use_tools\n    type: ActWithTools\n    task: solve_problem\n    depends_on: [\"analyze_problem\"]\n  - name: synthesize_solution\n    type: Generate\n    task: solve_problem\n    depends_on: [\"use_tools\"]\n\n  # Advanced features\n  tools:\n    enabled: true\n    categories:\n    - core\n    - utilities\n\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n\n  # Comprehensive evaluation\n  evaluation:\n    builtin_metrics:\n    - name: solution_quality\n      threshold: 0.8\n    - name: tool_usage_efficiency\n      threshold: 0.7\n</code></pre>"},{"location":"guides/superspec-configuration/#configuration-best-practices","title":"\ud83d\ude80 Configuration Best Practices","text":""},{"location":"guides/superspec-configuration/#1-use-proper-validation","title":"1. Use Proper Validation","text":"<pre><code># Validate your configuration\nsuper spec validate my-agent_playbook.yaml\n\n# Check for tier-specific issues\nsuper spec validate my-agent_playbook.yaml --verbose\n</code></pre>"},{"location":"guides/superspec-configuration/#2-follow-tier-constraints","title":"2. Follow Tier Constraints","text":"<pre><code># Correct - Oracles tier without advanced features\nmetadata:\n  level: oracles\nspec:\n  # No memory, tools, or RAG configuration\n  tasks: [...]\n\n# Correct - Genies tier with advanced features\nmetadata:\n  level: genies\nspec:\n  memory:\n    enabled: true\n  tools:\n    enabled: true\n  retrieval:\n    enabled: true\n</code></pre>"},{"location":"guides/superspec-configuration/#3-use-descriptive-configuration","title":"3. Use Descriptive Configuration","text":"<pre><code>metadata:\n  name: \"Financial Data Analyst\"  # Clear, descriptive name\n  description: \"Specialized agent for financial data analysis and reporting\"\n  tags: [\"finance\", \"data-analysis\", \"genies\"]\n\npersona:\n  role: \"Financial Data Analyst\"\n  goal: \"Provide accurate financial analysis and insights\"\n  traits:\n  - analytical\n  - detail-oriented\n  - precise\n</code></pre>"},{"location":"guides/superspec-configuration/#4-configure-appropriate-metrics","title":"4. Configure Appropriate Metrics","text":"<pre><code>evaluation:\n  builtin_metrics:\n  - name: answer_correctness\n    threshold: 0.8\n    weight: 2.0\n  - name: analysis_quality\n    threshold: 0.7\n  - name: safety_compliance\n    threshold: 1.0\n    weight: 3.0\n</code></pre>"},{"location":"guides/superspec-configuration/#5-use-comprehensive-bdd-scenarios","title":"5. Use Comprehensive BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n  - name: basic_functionality\n    description: \"The agent should perform basic tasks correctly\"\n    input:\n      question: \"What is the capital of France?\"\n    expected_output:\n      answer: \"Should provide accurate information about Paris\"\n  - name: advanced_functionality\n    description: \"The agent should handle complex scenarios\"\n    input:\n      problem: \"Analyze this financial dataset\"\n    expected_output:\n      solution: \"Should provide comprehensive analysis\"\n</code></pre> <p>\ud83d\udca1 Pro Tip: Use the <code>super spec schema</code> command to explore all available configuration options for each tier! </p>"},{"location":"guides/superspec-context-engineering/","title":"\ud83e\udde0 SuperSpec Context Engineering","text":"\ud83e\udde0 SuperSpec Context Engineering <p> Master the art of context engineering for optimal agent performance         Based on the actual SuperOptiX library implementation     </p>"},{"location":"guides/superspec-context-engineering/#overview","title":"\ud83d\udccb Overview","text":"<p>Context engineering is the systematic approach to designing dynamic systems that deliver precisely the right information and tools in the optimal format, enabling LLMs to successfully accomplish their intended tasks. When agents fail to perform reliably, the root cause is almost always insufficient or poorly structured context, unclear instructions, or missing tools that haven't been properly communicated to the model.</p>"},{"location":"guides/superspec-context-engineering/#core-principles","title":"\ud83c\udfaf Core Principles","text":""},{"location":"guides/superspec-context-engineering/#1-just-right-context","title":"1. Just-Right Context","text":"<p>The goal is to provide not too much, not too little, but exactly the right amount of context for the agent to perform optimally.</p> <pre><code>graph LR\n    A[Context Engineering] --&gt; B[Just-Right Context]\n    B --&gt; C[Agent Performance]\n    C --&gt; D[Strong Contracts]\n    D --&gt; E[DSPy Signatures]\n    E --&gt; F[Validated Output]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#ffffff\n    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/superspec-context-engineering/#2-context-engineering-philosophy","title":"2. Context Engineering Philosophy","text":"<p>Context engineering represents the next evolution of AI engineering. Rather than developing systems with static, hardcoded logic, engineers now design autonomous, goal-driven entities capable of using tools, accessing memory, engaging in reflective reasoning, and operating within safety constraints.</p>"},{"location":"guides/superspec-context-engineering/#context-engineering-components","title":"\ud83c\udfd7\ufe0f Context Engineering Components","text":""},{"location":"guides/superspec-context-engineering/#1-memory-systems","title":"1. Memory Systems","text":"<p>SuperSpec provides a comprehensive multi-layer memory system:</p> <pre><code>spec:\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 2000\n      window_size: 10\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 500\n      persistence: true\n    episodic:\n      enabled: true\n      max_episodes: 100\n      episode_retention: 30\n    context_manager:\n      enabled: true\n      max_context_length: 4000\n      context_strategy: sliding_window\n</code></pre>"},{"location":"guides/superspec-context-engineering/#memory-types-and-use-cases","title":"Memory Types and Use Cases","text":"Type Purpose Persistence Use Case Short-term Conversation context Session Current interaction Long-term Domain knowledge Permanent Factual information Episodic Interaction history Configurable Learning patterns"},{"location":"guides/superspec-context-engineering/#memory-configuration-options","title":"Memory Configuration Options","text":"<pre><code># Short-term memory for immediate context\nshort_term:\n  enabled: true\n  max_tokens: 2000          # Maximum tokens in working memory\n  window_size: 10           # Number of recent interactions to keep\n\n# Long-term memory for persistent knowledge\nlong_term:\n  enabled: true\n  storage_type: local       # local, sqlite, redis\n  max_entries: 500          # Maximum number of entries\n  persistence: true         # Enable persistence across sessions\n\n# Episodic memory for experience tracking\nepisodic:\n  enabled: true\n  max_episodes: 100         # Maximum episodes to store\n  episode_retention: 30     # Days to retain episodes\n\n# Context manager for overall context handling\ncontext_manager:\n  enabled: true\n  max_context_length: 4000  # Maximum context length\n  context_strategy: sliding_window  # Context management strategy\n</code></pre>"},{"location":"guides/superspec-context-engineering/#2-knowledge-integration","title":"2. Knowledge Integration","text":"<p>Seamless integration with multiple knowledge sources:</p> <pre><code>spec:\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: agent_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#knowledge-sources","title":"Knowledge Sources","text":"Type Description Example Document Store Local file system Company documents API Integration External APIs Market data Structured Data Databases Regulatory info Vector Database Embeddings Semantic search"},{"location":"guides/superspec-context-engineering/#rag-configuration-options","title":"RAG Configuration Options","text":"<pre><code>retrieval:\n  enabled: true\n  retriever_type: chroma    # chroma, qdrant, weaviate, milvus, lancedb\n  config:\n    top_k: 5                # Number of results to retrieve\n    chunk_size: 512         # Text chunk size for processing\n    chunk_overlap: 50       # Overlap between chunks\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    collection_name: agent_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#3-tool-orchestration","title":"3. Tool Orchestration","text":"<p>Structured integration of external tools and APIs:</p> <pre><code>spec:\n  tools:\n    enabled: true\n    categories:\n    - core\n    - development\n    - utilities\n    - finance\n    specific_tools:\n    - calculator\n    - file_reader\n    - text_analyzer\n    - web_search\n    - date_time\n    - code_formatter\n    - financial_calculator\n</code></pre>"},{"location":"guides/superspec-context-engineering/#tool-categories","title":"Tool Categories","text":"Category Description Examples <code>core</code> Essential tools calculator, file_reader, text_analyzer <code>development</code> Software development code_formatter, debugger, linter <code>utilities</code> General utilities date_time, json_processor, data_processor <code>finance</code> Financial tools financial_calculator, investment_analyzer <code>healthcare</code> Healthcare tools health_assessment, medical_analyzer <code>education</code> Educational tools educational_content, quiz_generator <code>legal</code> Legal tools legal_analyzer, contract_reviewer <code>marketing</code> Marketing tools marketing_analyzer, campaign_planner <code>real_estate</code> Real estate tools property_analyzer, market_researcher <code>retail</code> Retail tools inventory_manager, sales_analyzer <code>transportation</code> Transportation tools route_optimizer, fleet_manager <code>energy</code> Energy tools energy_calculator, efficiency_analyzer <code>agriculture</code> Agriculture tools crop_advisor, weather_analyzer <code>human_resources</code> HR tools hr_analyzer, recruitment_assistant <code>hospitality</code> Hospitality tools hospitality_manager, booking_optimizer <code>manufacturing</code> Manufacturing tools production_planner, quality_inspector <code>gaming_sports</code> Gaming/Sports tools game_analyzer, performance_tracker <code>media_entertainment</code> Media tools content_analyzer, trend_predictor <code>government_public</code> Government tools policy_analyzer, compliance_checker <code>consulting</code> Consulting tools business_consultant, strategy_advisor"},{"location":"guides/superspec-context-engineering/#context-engineering-strategies","title":"\ud83c\udfaf Context Engineering Strategies","text":""},{"location":"guides/superspec-context-engineering/#1-persona-driven-context","title":"1. Persona-Driven Context","text":"<p>Define the agent's personality and expertise to guide context selection:</p> <pre><code>spec:\n  persona:\n    name: \"DataBot\"\n    role: \"Data Analyst\"\n    goal: \"Perform comprehensive data analysis and provide insights\"\n    traits:\n    - analytical\n    - detail-oriented\n    - precise\n    - helpful\n    expertise_areas:\n    - data analysis\n    - statistical modeling\n    - financial analysis\n    - visualization\n    communication_preferences:\n      style: technical\n      tone: professional\n      verbosity: detailed\n</code></pre>"},{"location":"guides/superspec-context-engineering/#2-task-specific-context","title":"2. Task-Specific Context","text":"<p>Tailor context to specific task requirements:</p> <pre><code>spec:\n  tasks:\n  - name: analyze_data\n    description: \"Perform comprehensive data analysis\"\n    instruction: |\n      You are a Data Analyst. Analyze the provided data and generate insights, \n      visualizations, and recommendations. Use appropriate statistical methods \n      and create clear, actionable insights.\n    inputs:\n    - name: data_source\n      type: str\n      description: \"Source of data to analyze (file path, URL, or data description)\"\n      required: true\n    - name: analysis_type\n      type: str\n      description: \"Type of analysis to perform (descriptive, predictive, exploratory)\"\n      required: true\n    outputs:\n    - name: analysis_report\n      type: str\n      description: \"Comprehensive analysis report\"\n    - name: visualizations\n      type: list[str]\n      description: \"Generated data visualizations\"\n    - name: recommendations\n      type: str\n      description: \"Actionable recommendations based on analysis\"\n</code></pre>"},{"location":"guides/superspec-context-engineering/#3-agentflow-context-management","title":"3. AgentFlow Context Management","text":"<p>Define how context flows through the agent's execution:</p> <pre><code>spec:\n  agentflow:\n  - name: load_data\n    type: ActWithTools\n    task: analyze_data\n  - name: perform_analysis\n    type: Think\n    task: analyze_data\n    depends_on: [\"load_data\"]\n  - name: generate_visualizations\n    type: Generate\n    task: create_visualization\n    depends_on: [\"perform_analysis\"]\n  - name: synthesize_results\n    type: Generate\n    task: analyze_data\n    depends_on: [\"generate_visualizations\"]\n</code></pre>"},{"location":"guides/superspec-context-engineering/#context-engineering-best-practices","title":"\ud83e\uddea Context Engineering Best Practices","text":""},{"location":"guides/superspec-context-engineering/#1-start-with-clear-persona","title":"1. Start with Clear Persona","text":"<pre><code>persona:\n  role: \"Healthcare Assistant\"\n  goal: \"Provide accurate healthcare information and patient support\"\n  traits:\n  - knowledgeable\n  - empathetic\n  - precise\n  - professional\n  expertise_areas:\n  - general health\n  - medical terminology\n  - patient care\n  - health education\n</code></pre>"},{"location":"guides/superspec-context-engineering/#2-define-task-specific-instructions","title":"2. Define Task-Specific Instructions","text":"<pre><code>tasks:\n- name: health_assessment\n  instruction: |\n    You are a Healthcare Assistant. Assess the provided health information \n    and provide appropriate guidance while maintaining patient privacy and safety. \n    Always err on the side of caution and recommend professional medical attention \n    when appropriate.\n</code></pre>"},{"location":"guides/superspec-context-engineering/#3-configure-appropriate-memory","title":"3. Configure Appropriate Memory","text":"<pre><code>memory:\n  enabled: true\n  short_term:\n    enabled: true\n    max_tokens: 1000        # Keep recent conversation context\n  long_term:\n    enabled: true\n    storage_type: local\n    max_entries: 100        # Store important patient information\n  episodic:\n    enabled: true\n    max_episodes: 50        # Track interaction patterns\n</code></pre>"},{"location":"guides/superspec-context-engineering/#4-select-relevant-tools","title":"4. Select Relevant Tools","text":"<pre><code>tools:\n  enabled: true\n  categories:\n  - core\n  - healthcare\n  specific_tools:\n  - calculator\n  - text_analyzer\n  - date_time\n</code></pre>"},{"location":"guides/superspec-context-engineering/#5-configure-rag-for-knowledge","title":"5. Configure RAG for Knowledge","text":"<pre><code>retrieval:\n  enabled: true\n  retriever_type: chroma\n  config:\n    top_k: 5                # Retrieve relevant medical information\n    chunk_size: 512\n    chunk_overlap: 50\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    collection_name: healthcare_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#context-engineering-examples","title":"\ud83c\udfaf Context Engineering Examples","text":""},{"location":"guides/superspec-context-engineering/#healthcare-context-engineering","title":"Healthcare Context Engineering","text":"<pre><code># Healthcare agent with appropriate context\nspec:\n  persona:\n    role: \"Healthcare Assistant\"\n    goal: \"Provide accurate healthcare information and patient support\"\n    traits: [\"knowledgeable\", \"empathetic\", \"precise\", \"professional\"]\n    expertise_areas: [\"general health\", \"medical terminology\", \"patient care\"]\n    communication_preferences:\n      style: professional\n      tone: supportive\n      verbosity: detailed\n\n  memory:\n    enabled: true\n    short_term:\n      max_tokens: 1000      # Keep recent patient interaction\n    long_term:\n      max_entries: 100      # Store patient preferences\n    episodic:\n      max_episodes: 50      # Track interaction history\n\n  tools:\n    enabled: true\n    categories: [\"core\", \"healthcare\"]\n    specific_tools: [\"calculator\", \"text_analyzer\", \"date_time\"]\n\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5              # Retrieve relevant medical info\n    vector_store:\n      collection_name: healthcare_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#financial-context-engineering","title":"Financial Context Engineering","text":"<pre><code># Financial agent with appropriate context\nspec:\n  persona:\n    role: \"Financial Advisor\"\n    goal: \"Provide sound financial advice and investment guidance\"\n    traits: [\"analytical\", \"conservative\", \"trustworthy\", \"knowledgeable\"]\n    expertise_areas: [\"investment planning\", \"risk assessment\", \"financial analysis\"]\n    communication_preferences:\n      style: professional\n      tone: authoritative\n      verbosity: detailed\n\n  memory:\n    enabled: true\n    short_term:\n      max_tokens: 1000      # Keep recent financial discussion\n    long_term:\n      max_entries: 100      # Store client preferences\n    episodic:\n      max_episodes: 100     # Track financial history\n\n  tools:\n    enabled: true\n    categories: [\"core\", \"finance\", \"utilities\"]\n    specific_tools: [\"calculator\", \"financial_calculator\", \"text_analyzer\"]\n\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5              # Retrieve relevant financial info\n    vector_store:\n      collection_name: financial_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#educational-context-engineering","title":"Educational Context Engineering","text":"<pre><code># Educational agent with appropriate context\nspec:\n  persona:\n    role: \"Educational Tutor\"\n    goal: \"Provide engaging and effective educational support\"\n    traits: [\"patient\", \"encouraging\", \"knowledgeable\", \"adaptive\"]\n    expertise_areas: [\"mathematics\", \"science\", \"language arts\", \"history\"]\n    communication_preferences:\n      style: conversational\n      tone: encouraging\n      verbosity: adaptive\n\n  memory:\n    enabled: true\n    short_term:\n      max_tokens: 1500      # Keep learning session context\n    long_term:\n      max_entries: 200      # Store student progress\n    episodic:\n      max_episodes: 100     # Track learning sessions\n\n  tools:\n    enabled: true\n    categories: [\"core\", \"education\", \"utilities\"]\n    specific_tools: [\"calculator\", \"text_analyzer\", \"date_time\"]\n\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5              # Retrieve educational content\n    vector_store:\n      collection_name: educational_knowledge\n</code></pre>"},{"location":"guides/superspec-context-engineering/#context-engineering-workflow","title":"\ud83d\ude80 Context Engineering Workflow","text":""},{"location":"guides/superspec-context-engineering/#1-analyze-requirements","title":"1. Analyze Requirements","text":"<ul> <li> <p>Domain: What field is the agent working in?</p> </li> <li> <p>Tasks: What specific tasks will the agent perform?</p> </li> <li> <p>Users: Who will interact with the agent?</p> </li> <li> <p>Constraints: What limitations or requirements exist?</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#2-design-persona","title":"2. Design Persona","text":"<ul> <li> <p>Role: Define the agent's professional role</p> </li> <li> <p>Goal: Establish the primary objective</p> </li> <li> <p>Traits: Specify personality characteristics</p> </li> <li> <p>Expertise: Define knowledge areas</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#3-configure-memory","title":"3. Configure Memory","text":"<ul> <li> <p>Short-term: For immediate context</p> </li> <li> <p>Long-term: For persistent knowledge</p> </li> <li> <p>Episodic: For experience tracking</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#4-select-tools","title":"4. Select Tools","text":"<ul> <li> <p>Core tools: Essential functionality</p> </li> <li> <p>Domain-specific tools: Specialized capabilities</p> </li> <li> <p>Utility tools: General-purpose helpers</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#5-configure-rag","title":"5. Configure RAG","text":"<ul> <li> <p>Vector store: Choose appropriate database</p> </li> <li> <p>Embedding model: Select suitable embeddings</p> </li> <li> <p>Retrieval parameters: Tune for optimal results</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#6-test-and-iterate","title":"6. Test and Iterate","text":"<ul> <li> <p>Validate context: Ensure appropriate information flow</p> </li> <li> <p>Measure performance: Assess agent effectiveness</p> </li> <li> <p>Refine configuration: Optimize based on results</p> </li> </ul>"},{"location":"guides/superspec-context-engineering/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>Context engineering is the foundation of effective AI agent design. By carefully crafting the right context through persona definition, memory configuration, tool selection, and knowledge integration, you can create agents that perform reliably and effectively in their intended domains.</p> <p>The key is to provide just-right context - not too much to overwhelm the agent, not too little to leave it unprepared, but exactly what it needs to succeed.</p> <p>\ud83d\udca1 Pro Tip: Use the <code>super spec generate</code> command with appropriate flags to create templates with well-engineered context for your specific domain! </p>"},{"location":"guides/superspec-dsl-examples/","title":"\ud83d\udcd9 SuperSpec DSL Examples","text":"SuperSpec DSL Examples <p>Real-world agent examples across all 6 frameworks</p> \ud83d\udcdd SuperSpec Overview \ud83d\udcd7 DSL Reference \ud83c\udfd7\ufe0f Agent Building \u2699\ufe0f Configuration"},{"location":"guides/superspec-dsl-examples/#framework-examples","title":"\ud83d\udcda Framework Examples","text":"<p>Each example shows the same agent specification adapted for different frameworks.</p>"},{"location":"guides/superspec-dsl-examples/#example-1-simple-ai-assistant","title":"Example 1: Simple AI Assistant","text":"\ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant\n  id: assistant_dspy\n  namespace: demo\n  version: 1.0.0\n  level: oracles\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: Helpful AI Assistant\n    goal: Provide accurate, helpful responses to user queries\n    backstory: |\n      You are a helpful AI assistant.\n    reasoning:\n      steps:\n        - Understand the user's question\n        - Formulate a clear response\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Assistant's response\n\n  feature_specifications:\n    scenarios:\n      - name: Greeting\n        input:\n          query: \"Hello\"\n        expected_output:\n          response: \"greeting\"\n          expected_keywords:\n            - hello\n            - assistant\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium\n</code></pre> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant_openai\n  id: assistant_openai\n  namespace: demo\n  version: 1.0.0\n  level: oracles\n\nspec:\n  target_framework: openai\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    instructions: |\n      You are a helpful AI assistant that provides accurate,\n      clear, and concise responses to user queries.\n\n      Your approach:\n      1. Understand the user's question\n      2. Provide a clear, helpful response\n      3. Be friendly and professional\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Assistant's response\n\n  feature_specifications:\n    scenarios:\n      - name: Greeting\n        input:\n          query: \"Hello\"\n        expected_output:\n          response: \"greeting\"\n          expected_keywords:\n            - hello\n            - assistant\n</code></pre> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: researcher_crew\n  id: researcher_crew\n  namespace: demo\n  version: 1.0.0\n  level: genies\n\nspec:\n  target_framework: crewai\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: AI Research Assistant\n    goal: Conduct thorough research and provide accurate insights\n    backstory: |\n      You are an experienced research assistant with expertise in\n      gathering and analyzing information from multiple sources.\n\n  tasks:\n    - name: research_task\n      description: |\n        Research the given topic thoroughly, gathering information\n        from all available sources.\n      expected_output: |\n        A comprehensive research summary with key findings and insights.\n\n  input_fields:\n    - name: query\n      type: str\n      description: Research topic or question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Research findings\n\n  feature_specifications:\n    scenarios:\n      - name: Research query\n        input:\n          query: \"What is quantum computing?\"\n        expected_output:\n          response: \"quantum computing\"\n          expected_keywords:\n            - quantum\n            - computing\n</code></pre> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant_adk\n  id: assistant_adk\n  namespace: demo\n  version: 1.0.0\n  level: oracles\n\nspec:\n  target_framework: google-adk\n\n  language_model:\n    provider: google\n    model: gemini-2.0-flash\n    api_key: ${GOOGLE_API_KEY}\n\n  persona:\n    instructions: |\n      You are a helpful AI assistant powered by Google's Gemini.\n      Provide accurate, informative responses.\n\n      Your approach:\n      1. Understand the user's question\n      2. Formulate a clear response\n      3. Provide helpful insights\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Assistant's response\n\n  feature_specifications:\n    scenarios:\n      - name: Greeting\n        input:\n          query: \"Hello\"\n        expected_output:\n          response: \"greeting\"\n</code></pre> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: assistant_microsoft\n  id: assistant_microsoft\n  namespace: demo\n  version: 1.0.0\n  level: oracles\n\nspec:\n  target_framework: microsoft\n\n  language_model:\n    provider: ollama\n    model: gpt-oss:20b\n    api_base: http://localhost:11434\n\n  persona:\n    instructions: |\n      You are a helpful AI assistant.\n      Provide accurate, clear responses.\n\n      Your approach:\n      1. Understand the question\n      2. Gather relevant information\n      3. Formulate a clear response\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Assistant's response\n\n  feature_specifications:\n    scenarios:\n      - name: Greeting\n        input:\n          query: \"Hello\"\n        expected_output:\n          response: \"greeting\"\n</code></pre> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: research_agent_deepagents\n  id: research_agent_deepagents\n  namespace: demo\n  version: 1.0.0\n  level: genies\n\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    instructions: |\n      You are an AI research agent.\n      Conduct thorough research and provide insights.\n\n  input_fields:\n    - name: query\n      type: str\n      description: Research query\n\n  output_fields:\n    - name: response\n      type: str\n      description: Research findings\n\n  feature_specifications:\n    scenarios:\n      - name: Research\n        input:\n          query: \"Explain AI optimization\"\n        expected_output:\n          response: \"optimization\"\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-2-agent-with-tools-rag","title":"\ud83e\uddde Example 2: Agent with Tools &amp; RAG","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: genie_with_tools\n  id: genie_with_tools\n  namespace: demo\n  version: 1.0.0\n  level: genies\n  description: Agent with web search and RAG capabilities\n\nspec:\n  target_framework: dspy  # Works with any framework!\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: Information Retrieval Specialist\n    goal: Find and synthesize information from multiple sources\n    backstory: |\n      You are an expert at finding relevant information and\n      presenting it in a clear, actionable format.\n    reasoning:\n      steps:\n        - Understand the information need\n        - Search relevant sources (web + knowledge base)\n        - Synthesize findings\n        - Present clear recommendations\n\n  input_fields:\n    - name: query\n      type: str\n      description: Information request\n\n  output_fields:\n    - name: response\n      type: str\n      description: Synthesized information\n    - name: sources\n      type: list\n      description: List of sources used\n\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n      config:\n        max_results: 5\n\n    - name: calculator\n      type: builtin\n      enabled: true\n\n  rag:\n    enabled: true\n    vector_db:\n      type: chromadb\n      collection_name: knowledge_base\n      persist_directory: ./data/chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n    long_term:\n      enabled: true\n      semantic_search: true\n\n  feature_specifications:\n    scenarios:\n      - name: Web search query\n        input:\n          query: \"Latest developments in AI\"\n        expected_output:\n          response: \"AI developments\"\n          expected_keywords:\n            - AI\n            - developments\n            - recent\n\n      - name: Calculation query\n        input:\n          query: \"What is 15% of 200?\"\n        expected_output:\n          response: \"30\"\n          expected_keywords:\n            - \"30\"\n            - percent\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium\n        reflection_lm: qwen3:8b\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-3-mcp-protocol-agent","title":"\ud83d\udd0c Example 3: MCP Protocol Agent","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: mcp_protocol_agent\n  id: mcp_protocol_agent\n  namespace: protocols\n  version: 1.0.0\n  level: genies\n  description: Agent using Model Context Protocol for tool discovery\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: MCP-Enabled Assistant\n    goal: Use MCP tools to access file systems and repositories\n    backstory: |\n      You are an assistant that can access files and repositories\n      through the Model Context Protocol.\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's request\n\n  output_fields:\n    - name: response\n      type: str\n      description: Response with file/repo information\n\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args:\n            - \"-y\"\n            - \"@modelcontextprotocol/server-filesystem\"\n            - \"/path/to/docs\"\n\n        - name: git\n          command: npx\n          args:\n            - \"-y\"\n            - \"@modelcontextprotocol/server-git\"\n            - \"--repository\"\n            - \"/path/to/repo\"\n    config:\n      top_k: 5\n\n  feature_specifications:\n    scenarios:\n      - name: File access query\n        input:\n          query: \"What files are in the docs folder?\"\n        expected_output:\n          response: \"file list\"\n          expected_keywords:\n            - files\n            - docs\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-4-multi-agent-crew","title":"\ud83d\udcca Example 4: Multi-Agent Crew","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: content_creator_crew\n  id: content_creator_crew\n  namespace: demo\n  version: 1.0.0\n  level: genies\n  description: CrewAI multi-agent content creation system\n\nspec:\n  target_framework: crewai\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: Content Creator\n    goal: Create engaging, well-researched content\n    backstory: |\n      You are an experienced content creator who specializes in\n      creating engaging, informative content.\n\n  tasks:\n    - name: content_creation\n      description: |\n        Create high-quality content on the given topic.\n        Research the topic, outline key points, and write engaging content.\n      expected_output: |\n        A well-structured article with introduction, body, and conclusion.\n        Include key insights and actionable takeaways.\n\n  input_fields:\n    - name: topic\n      type: str\n      description: Content topic\n    - name: target_audience\n      type: str\n      description: Target audience description\n\n  output_fields:\n    - name: response\n      type: str\n      description: Created content\n\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n\n  feature_specifications:\n    scenarios:\n      - name: Content creation\n        input:\n          topic: \"Future of AI\"\n          target_audience: \"Tech enthusiasts\"\n        expected_output:\n          response: \"AI future content\"\n          expected_keywords:\n            - AI\n            - future\n            - technology\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-5-agent-with-memory-optimization","title":"\ud83e\udde0 Example 5: Agent with Memory Optimization","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: memory_optimized_agent\n  id: memory_optimized_agent\n  namespace: demo\n  version: 1.0.0\n  level: genies\n  description: Agent with GEPA-optimized memory selection\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: Conversational AI with Memory\n    goal: Maintain context across conversations\n    backstory: |\n      You are a conversational AI that remembers previous\n      interactions and provides contextual responses.\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's message\n\n  output_fields:\n    - name: response\n      type: str\n      description: Contextual response\n\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    optimization:\n      enabled: true  # GEPA-based memory selection\n      max_context_tokens: 2000\n      relevance_threshold: 0.7\n\n  feature_specifications:\n    scenarios:\n      - name: Context recall\n        input:\n          query: \"What did we discuss earlier?\"\n        expected_output:\n          response: \"previous discussion\"\n          expected_keywords:\n            - discussed\n            - earlier\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-6-rag-optimized-research-agent","title":"\ud83d\udd0d Example 6: RAG-Optimized Research Agent","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: rag_research_agent\n  id: rag_research_agent\n  namespace: research\n  version: 1.0.0\n  level: genies\n  description: Research agent with optimized RAG retrieval\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n\n  persona:\n    role: Research Analyst\n    goal: Provide well-researched, evidence-based answers\n    backstory: |\n      You are a research analyst who excels at finding and\n      synthesizing information from knowledge bases.\n\n  input_fields:\n    - name: research_question\n      type: str\n      description: Research question\n\n  output_fields:\n    - name: response\n      type: str\n      description: Research findings\n    - name: citations\n      type: list\n      description: List of source citations\n\n  rag:\n    enabled: true\n    vector_db:\n      type: lancedb  # or chromadb, weaviate, qdrant, milvus\n      uri: ./data/lancedb\n      table_name: research_docs\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n      reranking: true\n    optimization:\n      enabled: true  # GEPA-based RAG optimization\n      query_expansion: true\n      context_compression: true\n\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n\n  feature_specifications:\n    scenarios:\n      - name: Research with citations\n        input:\n          research_question: \"What are the benefits of RAG?\"\n        expected_output:\n          response: \"RAG benefits\"\n          expected_keywords:\n            - RAG\n            - retrieval\n            - benefits\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: intensive  # Use intensive for research agents\n        reflection_lm: qwen3:8b\n        skip_perfect_score: true\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#example-7-multi-agent-orchestra","title":"\ud83c\udfbc Example 7: Multi-Agent Orchestra","text":"<pre><code>apiVersion: orchestra/v1\nkind: OrchestraSpec\nmetadata:\n  name: research_orchestra\n  id: research_orchestra\n  version: 1.0.0\n  description: Multi-agent research orchestra\n\nspec:\n  agents:\n    - agent_id: researcher\n      role: lead\n    - agent_id: analyst\n      role: support\n    - agent_id: writer\n      role: support\n\n  workflow:\n    mode: sequential\n    steps:\n      - agent: researcher\n        task: gather_information\n      - agent: analyst\n        task: analyze_data\n      - agent: writer\n        task: write_report\n\n  coordination:\n    strategy: hierarchical\n    communication: shared_memory\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#framework-specific-features","title":"\ud83d\udd2c Framework-Specific Features","text":""},{"location":"guides/superspec-dsl-examples/#dspy-specific-configuration","title":"DSPy-Specific Configuration","text":"<pre><code>spec:\n  target_framework: dspy\n  dspy:\n    signature_type: ChainOfThought  # Predict, ChainOfThought, ReAct\n    max_retries: 3\n    include_demos: true\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#crewai-specific-configuration","title":"CrewAI-Specific Configuration","text":"<pre><code>spec:\n  target_framework: crewai\n  crewai:\n    verbose: true\n    process: sequential  # or hierarchical\n    manager_llm: llama3\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#openai-sdk-specific-configuration","title":"OpenAI SDK-Specific Configuration","text":"<pre><code>spec:\n  target_framework: openai\n  openai:\n    parallel_tool_calls: true\n    response_format: text  # or json_object\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#advanced-custom-metrics","title":"\ud83d\udcc8 Advanced: Custom Metrics","text":"<p>Define custom evaluation metrics for domain-specific optimization.</p> <pre><code>spec:\n  evaluation:\n    custom_metrics:\n      - name: factual_accuracy\n        type: llm_judge\n        prompt: |\n          Rate the factual accuracy of the response on a scale of 0-100.\n          Consider: correctness, precision, completeness.\n        judge_model: gpt-4o-mini\n\n      - name: response_length\n        type: python\n        function: len\n        target: response\n        expected_range: [50, 200]\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#complete-real-world-example","title":"\ud83d\udd27 Complete Real-World Example","text":"<p>Here's a production-ready agent with all features:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: production_agent\n  id: production_agent\n  namespace: production\n  version: 2.1.0\n  level: genies\n  stage: stable\n  description: Production-ready agent with all features\n  tags:\n    - production\n    - rag\n    - mcp\n    - memory\n  created_at: \"2025-01-15T10:00:00Z\"\n  updated_at: \"2025-01-20T14:30:00Z\"\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 2000\n\n  persona:\n    role: Production AI Assistant\n    goal: Provide accurate, helpful responses with citations\n    backstory: |\n      You are a production-grade AI assistant with access to\n      extensive knowledge bases, tools, and memory systems.\n    reasoning:\n      steps:\n        - Understand user intent\n        - Search knowledge base (RAG)\n        - Use tools when needed\n        - Recall relevant memories\n        - Formulate comprehensive response\n        - Provide citations\n\n  input_fields:\n    - name: query\n      type: str\n      description: User's question or request\n      required: true\n    - name: context\n      type: str\n      description: Additional context\n      required: false\n\n  output_fields:\n    - name: response\n      type: str\n      description: Agent's response\n    - name: sources\n      type: list\n      description: Citation sources\n    - name: confidence\n      type: float\n      description: Confidence score\n\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n      config:\n        max_results: 5\n        search_engine: duckduckgo\n\n    - name: calculator\n      type: builtin\n      enabled: true\n\n    - name: custom_api\n      type: custom\n      function: tools.custom_api.call_api\n      config:\n        endpoint: https://api.example.com\n        timeout: 30\n\n  rag:\n    enabled: true\n    vector_db:\n      type: chromadb\n      collection_name: production_docs\n      persist_directory: ./data/chroma\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"./docs\"]\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \".\"]\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n      reranking: true\n    optimization:\n      enabled: true\n      query_expansion: true\n      context_compression: true\n\n  memory:\n    enabled: true\n    backend: sqlite\n    database_path: ./data/memory.db\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    optimization:\n      enabled: true\n      max_context_tokens: 2000\n      relevance_threshold: 0.7\n\n  guardrails:\n    enabled: true\n    max_retries: 3\n    timeout: 60\n    content_filters:\n      - no_pii\n      - no_toxic_content\n\n  feature_specifications:\n    scenarios:\n      - name: Knowledge base query\n        input:\n          query: \"What is our refund policy?\"\n        expected_output:\n          response: \"refund policy\"\n          expected_keywords:\n            - refund\n            - policy\n\n      - name: Calculation with tools\n        input:\n          query: \"Calculate compound interest for $1000 at 5% for 3 years\"\n        expected_output:\n          response: \"compound interest\"\n          expected_keywords:\n            - interest\n            - calculation\n\n      - name: Web search query\n        input:\n          query: \"Latest news on AI regulation\"\n        expected_output:\n          response: \"AI regulation news\"\n          expected_keywords:\n            - AI\n            - regulation\n            - news\n\n  evaluation:\n    custom_metrics:\n      - name: citation_quality\n        type: llm_judge\n        judge_model: gpt-4o-mini\n      - name: response_relevance\n        type: embedding_similarity\n        threshold: 0.8\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: intensive\n        reflection_lm: qwen3:8b\n        reflection_minibatch_size: 3\n        skip_perfect_score: true\n        add_format_failure_as_feedback: true\n</code></pre>"},{"location":"guides/superspec-dsl-examples/#next-steps","title":"\ud83d\ude80 Next Steps","text":"\ud83d\udcd7 DSL Reference \ud83c\udfd7\ufe0f Build Your Agent \ud83d\ude80 Quick Start \ud83d\udd2c Framework Guide"},{"location":"guides/superspec-dsl-reference/","title":"\ud83d\udcd7 SuperSpec DSL Reference","text":"SuperSpec DSL Complete Reference <p>Universal agent specification language for all 6 frameworks</p> \ud83d\udcdd SuperSpec Overview \ud83c\udfd7\ufe0f Agent Building \ud83d\udcd9 DSL Examples \u2699\ufe0f Configuration"},{"location":"guides/superspec-dsl-reference/#complete-superspec-schema","title":"\ud83d\udccb Complete SuperSpec Schema","text":""},{"location":"guides/superspec-dsl-reference/#top-level-structure","title":"Top-Level Structure","text":"<pre><code>apiVersion: agent/v1                    # REQUIRED - Schema version\nkind: AgentSpec                        # REQUIRED - Object type\nmetadata:                              # REQUIRED - Agent identity\nspec:                                  # REQUIRED - Agent specification\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#metadata-section","title":"\ud83c\udff7\ufe0f Metadata Section","text":"<p>The <code>metadata</code> section defines the agent's identity and basic properties.</p> Field Required Type Description <code>name</code> Yes string Human-readable agent name <code>id</code> Yes string Unique identifier (a-z, 0-9, -, _) <code>version</code> Yes string Semantic versioning (e.g., \"1.0.0\") <code>namespace</code> Optional string Logical grouping namespace <code>level</code> Optional oracles | genies Agent tier level (default: oracles) <code>description</code> Optional string Brief agent description"},{"location":"guides/superspec-dsl-reference/#example-metadata","title":"Example Metadata","text":"<pre><code>metadata:\n  name: sentiment_analyzer\n  id: sentiment_analyzer\n  namespace: testing\n  version: 1.0.0\n  level: oracles\n  description: Analyzes sentiment of text input\n  tags:\n    - nlp\n    - sentiment-analysis\n    - text-processing\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#spec-section","title":"\ud83c\udfaf Spec Section","text":"<p>The <code>spec</code> section contains all agent configuration.</p>"},{"location":"guides/superspec-dsl-reference/#required-fields","title":"Required Fields","text":"Field Description <code>target_framework</code> Framework choice: <code>dspy</code>, <code>openai</code>, <code>crewai</code>, <code>google-adk</code>, <code>microsoft</code>, <code>deepagents</code> <code>language_model</code> LLM configuration (provider, model, API settings) <code>input_fields</code> List of input field specifications <code>output_fields</code> List of output field specifications <code>feature_specifications</code> BDD scenarios for evaluation (universal across all frameworks)"},{"location":"guides/superspec-dsl-reference/#language-model-configuration","title":"\ud83e\udd16 Language Model Configuration","text":"<p>Configure your LLM for any provider.</p>"},{"location":"guides/superspec-dsl-reference/#ollama-local-recommended","title":"Ollama (Local, Recommended)","text":"<pre><code>spec:\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 1000\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#openai","title":"OpenAI","text":"<pre><code>spec:\n  language_model:\n    provider: openai\n    model: gpt-4o-mini\n    api_key: ${OPENAI_API_KEY}\n    temperature: 0.7\n    max_tokens: 1000\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#google-gemini","title":"Google Gemini","text":"<pre><code>spec:\n  language_model:\n    provider: google\n    model: gemini-2.0-flash\n    api_key: ${GOOGLE_API_KEY}\n    temperature: 0.7\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#azure-openai","title":"Azure OpenAI","text":"<pre><code>spec:\n  language_model:\n    provider: azure\n    model: gpt-4\n    api_key: ${AZURE_OPENAI_API_KEY}\n    api_base: ${AZURE_OPENAI_ENDPOINT}\n    api_version: \"2024-02-15-preview\"\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#mlx-apple-silicon","title":"MLX (Apple Silicon)","text":"<pre><code>spec:\n  language_model:\n    provider: mlx\n    model: mlx-community/Llama-3-8B-Instruct-4bit\n    temperature: 0.7\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#persona-configuration","title":"\ud83d\udc64 Persona Configuration","text":"<p>Define your agent's personality and approach.</p> Field Description <code>role</code> Agent's role (e.g., \"AI Research Assistant\") <code>goal</code> Agent's primary objective <code>backstory</code> Agent's background context (multi-line supported) <code>instructions</code> Direct instructions for the agent (OpenAI SDK, Google ADK, Microsoft style) <code>reasoning.steps</code> List of reasoning steps the agent should follow"},{"location":"guides/superspec-dsl-reference/#example-persona-dspycrewai-style","title":"Example Persona (DSPy/CrewAI Style)","text":"<pre><code>spec:\n  persona:\n    role: AI Research Assistant\n    goal: Help researchers find and analyze academic papers\n    backstory: |\n      You are an experienced research assistant with expertise in\n      literature review and academic analysis. You help researchers\n      discover relevant papers and extract key insights.\n    reasoning:\n      steps:\n        - Understand the research question\n        - Search for relevant papers\n        - Analyze key findings\n        - Summarize insights\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#example-persona-openai-sdkgoogle-adkmicrosoft-style","title":"Example Persona (OpenAI SDK/Google ADK/Microsoft Style)","text":"<pre><code>spec:\n  persona:\n    instructions: |\n      You are a helpful AI assistant that provides accurate,\n      well-researched responses to user queries.\n\n      Your approach:\n      1. Understand the user's question\n      2. Gather relevant information\n      3. Formulate a clear response\n      4. Provide helpful insights\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#input-output-fields","title":"\ud83d\udce5 Input &amp; Output Fields","text":"<p>Define the agent's interface with strongly typed fields.</p>"},{"location":"guides/superspec-dsl-reference/#input-fields","title":"Input Fields","text":"<pre><code>spec:\n  input_fields:\n    - name: query\n      type: str\n      description: User's question or request\n      required: true\n\n    - name: context\n      type: str\n      description: Additional context for the query\n      required: false\n      default: \"\"\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#output-fields","title":"Output Fields","text":"<pre><code>spec:\n  output_fields:\n    - name: response\n      type: str\n      description: Agent's response to the query\n\n    - name: confidence\n      type: float\n      description: Confidence score (0.0-1.0)\n      default: 0.0\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#tasks-configuration-crewaideepagents","title":"\ud83c\udfaf Tasks Configuration (CrewAI/DeepAgents)","text":"<p>Define tasks for multi-agent or complex workflow frameworks.</p> <pre><code>spec:\n  tasks:\n    - name: research_task\n      description: |\n        Research the given topic and gather relevant information\n        from multiple sources.\n      expected_output: |\n        A comprehensive summary with key findings, citations,\n        and recommendations for further reading.\n      agent: researcher  # Optional: assign to specific agent\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#tools-configuration","title":"\ud83d\udd0c Tools Configuration","text":"<p>Add tools for your agent to use.</p>"},{"location":"guides/superspec-dsl-reference/#built-in-tools","title":"Built-in Tools","text":"<pre><code>spec:\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n      config:\n        max_results: 5\n\n    - name: calculator\n      type: builtin\n      enabled: true\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#custom-tools","title":"Custom Tools","text":"<pre><code>spec:\n  tools:\n    - name: custom_api\n      type: custom\n      function: my_module.my_function\n      description: Calls custom API endpoint\n      parameters:\n        endpoint: https://api.example.com\n        api_key: ${API_KEY}\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#memory-configuration","title":"\ud83e\udde0 Memory Configuration","text":"<p>Configure short-term and long-term memory.</p> <pre><code>spec:\n  memory:\n    enabled: true\n    backend: sqlite\n    short_term:\n      max_size: 100\n      retention_policy: lru\n    long_term:\n      enabled: true\n      semantic_search: true\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#rag-configuration","title":"\ud83d\udcda RAG Configuration","text":"<p>Enable Retrieval-Augmented Generation.</p>"},{"location":"guides/superspec-dsl-reference/#vector-database-rag","title":"Vector Database RAG","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    vector_db:\n      type: chromadb\n      collection_name: my_knowledge\n      persist_directory: ./data/chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#mcp-protocol-rag","title":"MCP Protocol RAG","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/repo\"]\n    config:\n      top_k: 5\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#feature-specifications-bdd-scenarios","title":"Feature Specifications (BDD Scenarios)","text":"<p>Define behavior-driven test scenarios (universal across all frameworks).</p> <pre><code>spec:\n  feature_specifications:\n    scenarios:\n      - name: Simple greeting\n        input:\n          query: \"Hello, how are you?\"\n        expected_output:\n          response: \"I am doing well\"\n          expected_keywords:\n            - hello\n            - well\n            - assistant\n\n      - name: Domain-specific query\n        input:\n          query: \"What is quantum entanglement?\"\n        expected_output:\n          response: \"quantum entanglement explanation\"\n          expected_keywords:\n            - quantum\n            - entanglement\n            - particles\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#optimization-configuration","title":"\ud83e\uddec Optimization Configuration","text":"<p>Configure GEPA optimizer (universal across all frameworks).</p> <pre><code>spec:\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium                    # light, medium, intensive\n        reflection_lm: qwen3:8b\n        reflection_minibatch_size: 3\n        skip_perfect_score: true\n        add_format_failure_as_feedback: true\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#optimization-modes","title":"Optimization Modes","text":"Mode Iterations Time Best For light 3-5 5-10 min Quick testing, rapid prototyping medium 10-15 15-30 min Production agents, balanced quality/speed intensive 20-30 30-60 min Critical agents, maximum performance"},{"location":"guides/superspec-dsl-reference/#framework-specific-sections","title":"\ud83d\udd27 Framework-Specific Sections","text":""},{"location":"guides/superspec-dsl-reference/#dspy-specific","title":"DSPy-Specific","text":"<pre><code>spec:\n  target_framework: dspy\n  dspy:\n    signature_type: ChainOfThought  # or Predict, ReAct\n    max_retries: 3\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#crewai-specific","title":"CrewAI-Specific","text":"<pre><code>spec:\n  target_framework: crewai\n  tasks:\n    - name: research\n      description: Conduct research on the topic\n      expected_output: Detailed research summary\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#openai-sdk-specific","title":"OpenAI SDK-Specific","text":"<pre><code>spec:\n  target_framework: openai\n  persona:\n    instructions: |\n      You are a helpful assistant that provides accurate responses.\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#complete-example-multi-framework-agent","title":"\ud83d\udcca Complete Example: Multi-Framework Agent","text":"<p>Here's a complete SuperSpec that can be compiled to ANY framework:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: research_assistant\n  id: research_assistant\n  namespace: research\n  version: 1.0.0\n  level: genies\n  description: AI research assistant that helps find and analyze information\n\nspec:\n  target_framework: dspy  # Change to: openai, crewai, google-adk, microsoft, deepagents\n\n  language_model:\n    provider: ollama\n    model: llama3\n    api_base: http://localhost:11434\n    temperature: 0.7\n\n  persona:\n    role: AI Research Assistant\n    goal: Help users find and analyze relevant information\n    backstory: |\n      You are an experienced research assistant with expertise in\n      information retrieval and analysis.\n    reasoning:\n      steps:\n        - Understand the research question\n        - Search for relevant information\n        - Analyze and synthesize findings\n        - Present clear, actionable insights\n\n  input_fields:\n    - name: query\n      type: str\n      description: Research question or topic\n\n  output_fields:\n    - name: response\n      type: str\n      description: Research findings and analysis\n\n  tools:\n    - name: web_search\n      type: builtin\n      enabled: true\n\n  rag:\n    enabled: true\n    vector_db:\n      type: chromadb\n      collection_name: research_docs\n    config:\n      top_k: 5\n\n  memory:\n    enabled: true\n    short_term:\n      max_size: 50\n\n  feature_specifications:\n    scenarios:\n      - name: Basic research query\n        input:\n          query: \"What is machine learning?\"\n        expected_output:\n          response: \"machine learning explanation\"\n          expected_keywords:\n            - machine learning\n            - algorithms\n            - data\n\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: answer_exact_match\n        auto: medium\n</code></pre>"},{"location":"guides/superspec-dsl-reference/#field-type-reference","title":"\ud83c\udfaf Field Type Reference","text":"Type Python Type Description <code>str</code> <code>str</code> Text string <code>int</code> <code>int</code> Integer number <code>float</code> <code>float</code> Floating point number <code>bool</code> <code>bool</code> Boolean (true/false) <code>list</code> <code>List</code> Array of items"},{"location":"guides/superspec-dsl-reference/#next-steps","title":"\ud83d\ude80 Next Steps","text":"\ud83d\udcd9 See DSL Examples \ud83c\udfd7\ufe0f Build Your Agent \u2699\ufe0f Full Configuration \ud83d\ude80 Quick Start"},{"location":"guides/superspec/","title":"\ud83d\udc8e SuperSpec - Universal Agent Specification","text":"\ud83d\udc8e SuperSpec - Universal Agent Specification <p> SuperSpec is our declarative DSL that works across all 6 major agent frameworks.         Think of it as \"Kubernetes for AI agents\" - you describe what you want, choose your framework, and SuperOptiX builds the entire pipeline.     </p> \ud83d\udcdd Declarative Agent Specs \ud83d\udd27 Multi-Framework Support \ud83e\uddea BDD-Style Testing \u2699\ufe0f GEPA Optimization"},{"location":"guides/superspec/#what-is-superspec","title":"\ud83c\udfaf What is SuperSpec?","text":"<p>SuperSpec (pronounced <code>/su\u02d0.p\u0259r sp\u025bk/</code>) is the universal specification language for AI agents across all major frameworks. It's designed to provide the just-right context to agents so they perform better - not too much, not too little, but striking the perfect balance.</p> <p>\ud83c\udf1f Key Achievement: One specification format works across DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft Agent Framework, and DeepAgents!</p>"},{"location":"guides/superspec/#core-philosophy-framework-freedom","title":"Core Philosophy: Framework Freedom","text":"<p>SuperSpec enables you to: - Choose Your Framework: Select from 6 major frameworks based on your needs - Keep Your Workflow: Same compile \u2192 evaluate \u2192 optimize \u2192 run cycle - Optimize Universally: GEPA works across all frameworks - Version Control Everything: Git-based agent management</p> <pre><code>graph LR\n    A[SuperSpec YAML] --&gt; B[Choose Framework]\n    B --&gt; C[Compile to Native Code]\n    C --&gt; D[GEPA Optimization]\n    D --&gt; E[Production Deployment]\n\n    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#ffffff\n    style B fill:#7c3aed,stroke:#a855f7,stroke-width:2px,color:#ffffff\n    style C fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff\n    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#ffffff\n    style E fill:#059669,stroke:#10b981,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"guides/superspec/#context-engineering","title":"Context Engineering","text":"<p>Context engineering is the systematic approach to designing dynamic systems that deliver precisely the right information and tools in the optimal format, enabling LLMs to successfully accomplish their intended tasks. When agents fail to perform reliably, the root cause is almost always insufficient or poorly structured context, unclear instructions, or missing tools that haven't been properly communicated to the model.</p>"},{"location":"guides/superspec/#agent-engineering","title":"Agent Engineering","text":"<p>Agent engineering represents the next evolution of AI engineering. Rather than developing systems with static, hardcoded logic, engineers now design autonomous, goal-driven entities capable of using tools, accessing memory, engaging in reflective reasoning, and operating within safety constraints.</p>"},{"location":"guides/superspec/#superspec-design-principles","title":"\ud83c\udfd7\ufe0f SuperSpec Design Principles","text":""},{"location":"guides/superspec/#1-declarative-strongly-typed","title":"1. Declarative &amp; Strongly Typed","text":"<p>SuperSpec is declarative and strongly typed to ensure strong contracts between context and LLM output. This contract then converts into DSPy Signatures which validate the output even further.</p>"},{"location":"guides/superspec/#2-kubernetes-inspired","title":"2. Kubernetes-Inspired","text":"<p>Like Kubernetes DSL for declaring pods, deployments, and services, SuperSpec provides a Kubernetes-style declarative specification for AI agents:</p> <pre><code># Kubernetes-style declarative approach\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: my-agent\n  namespace: production\nspec:\n  # Declare what you want, not how to get it\n</code></pre>"},{"location":"guides/superspec/#3-version-controllable","title":"3. Version Controllable","text":"<p>SuperSpec specifications are totally version controllable and context can be versioned, enabling: - Git-based agent management - Rollback capabilities - A/B testing of agent configurations - Team collaboration on agent development</p>"},{"location":"guides/superspec/#superspec-structure-overview","title":"\ud83d\udccb SuperSpec Structure Overview","text":""},{"location":"guides/superspec/#top-level-fields","title":"Top-Level Fields","text":"<pre><code>apiVersion: agent/v1                    # REQUIRED - Schema version\nkind: AgentSpec                        # REQUIRED - Object type\nmetadata:                              # REQUIRED - Agent identity\nspec:                                  # REQUIRED - Agent specification\n  target_framework: string             # REQUIRED - Framework choice (dspy, openai, crewai, google-adk, microsoft, deepagents)\n  language_model:                      # REQUIRED - LLM configuration\n  persona:                             # OPTIONAL - Agent personality\n  tasks:                               # REQUIRED - Agent capabilities (framework-specific)\n  agentflow:                           # OPTIONAL - Execution flow\n  tools:                               # OPTIONAL - Tool integration\n  memory:                              # OPTIONAL - Memory systems\n  rag:                                 # OPTIONAL - Knowledge retrieval\n  evaluation:                          # OPTIONAL - Quality metrics\n  feature_specifications:              # REQUIRED - BDD scenarios (universal)\n  optimization:                        # OPTIONAL - GEPA optimization (universal)\n</code></pre>"},{"location":"guides/superspec/#multi-framework-support","title":"\ud83d\udd27 Multi-Framework Support","text":""},{"location":"guides/superspec/#choosing-your-framework","title":"Choosing Your Framework","text":"<p>SuperSpec supports 6 major agent frameworks. Select based on your needs:</p> \ud83d\udd2c DSPy\ud83e\udd16 OpenAI SDK\ud83d\udc65 CrewAI\ud83d\udd2e Google ADK\ud83c\udfe2 Microsoft\ud83c\udf0a DeepAgents <p><pre><code>spec:\n  target_framework: dspy  # Pure DSPy mode\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n</code></pre> Best For: Complex reasoning, 10+ optimizable variables</p> <p><pre><code>spec:\n  target_framework: openai  # OpenAI Agents SDK\n  language_model:\n    provider: ollama\n    model: gpt-oss:20b\n</code></pre> Best For: Simple &amp; fast, works with Ollama</p> <p><pre><code>spec:\n  target_framework: crewai  # Multi-agent collaboration\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n</code></pre> Best For: Multi-agent teams, role-based agents</p> <p><pre><code>spec:\n  target_framework: google-adk  # Gemini native\n  language_model:\n    provider: google\n    model: gemini-2.0-flash\n</code></pre> Best For: Gemini 2.0, free tier available</p> <p><pre><code>spec:\n  target_framework: microsoft  # Enterprise Azure\n  language_model:\n    provider: ollama\n    model: gpt-oss:20b\n</code></pre> Best For: Enterprise Azure integration</p> <p><pre><code>spec:\n  target_framework: deepagents  # Complex planning\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n</code></pre> Best For: LangGraph planning, advanced reasoning</p>"},{"location":"guides/superspec/#framework-comparison","title":"Framework Comparison","text":"Framework Variables Local Models Best For DSPy 10+ Ollama Complex reasoning, research OpenAI SDK 1 Ollama Simple &amp; fast CrewAI 5 Ollama Multi-agent teams Google ADK 1 Cloud only Gemini native, free tier Microsoft 1 Ollama Enterprise Azure DeepAgents 1 Ollama Complex planning"},{"location":"guides/superspec/#learning-path-beginner-to-advanced","title":"Learning Path: Beginner to Advanced","text":"Level Focus Key Concepts \ud83d\udfe2 Beginner Basic Structure <code>metadata</code>, <code>language_model</code>, <code>persona</code>, <code>tasks</code> \ud83d\udfe1 Intermediate Execution Flow <code>agentflow</code>, <code>evaluation</code>, <code>feature_specifications</code> \ud83d\udd34 Advanced Advanced Features <code>tools</code>, <code>memory</code>, <code>rag</code>, <code>optimization</code>"},{"location":"guides/superspec/#beginner-level-core-components","title":"\ud83c\udfaf Beginner Level: Core Components","text":""},{"location":"guides/superspec/#metadata-section","title":"Metadata Section","text":"<p>The <code>metadata</code> section defines the agent's identity and basic properties.</p>"},{"location":"guides/superspec/#required-fields","title":"Required Fields","text":"<pre><code>metadata:\n  name: string                         # REQUIRED - Human-readable name\n  id: string                           # REQUIRED - Unique identifier\n  version: string                      # REQUIRED - Agent version (e.g., \"1.0.0\")\n</code></pre>"},{"location":"guides/superspec/#optional-fields","title":"Optional Fields","text":"<pre><code>metadata:\n  namespace: string                    # OPTIONAL - Logical grouping (e.g., \"software\", \"finance\")\n  agent_type: Autonomous|Supervised|Interactive|Reactive|Deliberative|Hybrid  # OPTIONAL\n  level: oracles|genies  # OPTIONAL - Capability tier (available: oracles, genies only)\n  description: string                  # OPTIONAL - Brief description\n  tags: [string]                       # OPTIONAL - Categorization tags\n</code></pre>"},{"location":"guides/superspec/#example","title":"Example","text":"<pre><code>metadata:\n  name: Developer Assistant\n  id: developer\n  namespace: software\n  version: \"1.0.0\"\n  agent_type: Supervised\n  level: oracles\n  description: An agent that helps write clean, efficient, and maintainable code.\n  tags: [\"development\", \"coding\", \"software\"]\n</code></pre>"},{"location":"guides/superspec/#language-model-configuration","title":"Language Model Configuration","text":"<p>The <code>language_model</code> section configures the underlying LLM that powers your agent.</p>"},{"location":"guides/superspec/#required-fields_1","title":"Required Fields","text":"<pre><code>spec:\n  language_model:\n    location: local|self-hosted|cloud  # REQUIRED - Model hosting location\n    provider: string                   # REQUIRED - Model provider\n    model: string                      # REQUIRED - Specific model identifier\n</code></pre>"},{"location":"guides/superspec/#location-specific-providers","title":"Location-Specific Providers","text":"Location Supported Providers local <code>ollama</code>, <code>vllm</code>, <code>sg_lang</code>, <code>mlx</code>, <code>lm_studio</code>, <code>custom</code> self-hosted <code>custom</code> cloud <code>openai</code>, <code>anthropic</code>, <code>google</code>, <code>azure</code>, <code>mistral</code>, <code>cohere</code>, <code>groq</code>, <code>deepseek</code>"},{"location":"guides/superspec/#optional-configuration","title":"Optional Configuration","text":"<pre><code>spec:\n  language_model:\n    # Model behavior\n    temperature: float                  # OPTIONAL (default: 0.0) - Randomness control\n    max_tokens: int                     # OPTIONAL (default: 4000) - Max output length\n    top_p: float                        # OPTIONAL (default: 1.0) - Nucleus sampling\n\n    # API configuration (for local/cloud)\n    api_base: string                    # OPTIONAL - API endpoint (e.g., \"http://localhost:11434\")\n    api_key: string                     # OPTIONAL - API key (or use environment variables)\n\n    # Model capabilities\n    modalities: [text|image|audio|video] # OPTIONAL (default: [text])\n</code></pre>"},{"location":"guides/superspec/#examples","title":"Examples","text":""},{"location":"guides/superspec/#oracles-tier-local","title":"Oracles Tier (Local)","text":"<pre><code>spec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n    api_base: http://localhost:11434\n    temperature: 0.0\n    max_tokens: 2048\n</code></pre>"},{"location":"guides/superspec/#genies-tier-local","title":"Genies Tier (Local)","text":"<pre><code>spec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 2048\n</code></pre>"},{"location":"guides/superspec/#cloud-provider","title":"Cloud Provider","text":"<pre><code>spec:\n  language_model:\n    location: cloud\n    provider: openai\n    model: o3-mini\n    temperature: 1.0\n    max_tokens: 20000\n</code></pre>"},{"location":"guides/superspec/#persona-configuration","title":"Persona Configuration","text":"<p>The <code>persona</code> section defines the agent's personality, role, and behavioral characteristics.</p>"},{"location":"guides/superspec/#required-fields_2","title":"Required Fields","text":"<pre><code>spec:\n  persona:\n    role: string                        # REQUIRED - Agent's role (e.g., \"Software Developer\")\n</code></pre>"},{"location":"guides/superspec/#optional-fields_1","title":"Optional Fields","text":"<pre><code>spec:\n  persona:\n    name: string                        # OPTIONAL - Agent's name (e.g., \"DevBot\")\n    goal: string                        # OPTIONAL - Primary objective\n    traits: [string]                    # OPTIONAL - Personality characteristics\n    expertise_areas: [string]           # OPTIONAL - Knowledge domains\n    communication_preferences:          # OPTIONAL\n      style: formal|casual|technical|conversational  # OPTIONAL (default: formal)\n      tone: professional|friendly|authoritative|supportive  # OPTIONAL (default: professional)\n      verbosity: concise|detailed|adaptive  # OPTIONAL (default: concise)\n</code></pre>"},{"location":"guides/superspec/#example_1","title":"Example","text":"<pre><code>spec:\n  persona:\n    name: DevBot\n    role: Software Developer\n    goal: Write clean, efficient, and maintainable code\n    traits:\n    - analytical\n    - detail-oriented\n    - problem-solver\n    expertise_areas:\n    - software development\n    - code review\n    - debugging\n    communication_preferences:\n      style: technical\n      tone: professional\n      verbosity: detailed\n</code></pre>"},{"location":"guides/superspec/#tasks-configuration","title":"Tasks Configuration","text":"<p>The <code>tasks</code> section defines the agent's capabilities and what it can do.</p>"},{"location":"guides/superspec/#required-structure","title":"Required Structure","text":"<pre><code>spec:\n  tasks:                                # REQUIRED (minimum 1 task)\n    - name: string                      # REQUIRED - Unique task name\n      description: string               # REQUIRED - What the task achieves\n      instruction: string               # REQUIRED - Core LLM instruction\n      inputs:                           # REQUIRED (minimum 1 input)\n        - name: string                  # REQUIRED - Input field name\n          type: str|int|bool|float|list[str]|dict[str,Any]  # REQUIRED - Data type\n          description: string           # REQUIRED - Field description\n          required: bool                # REQUIRED - Whether mandatory\n      outputs:                          # REQUIRED (minimum 1 output)\n        - name: string                  # REQUIRED - Output field name\n          type: str|int|bool|float|list[str]|dict[str,Any]  # REQUIRED - Data type\n          description: string           # REQUIRED - Field description\n</code></pre>"},{"location":"guides/superspec/#inputoutput-types","title":"Input/Output Types","text":"Type Description Example <code>str</code> String text <code>\"Hello world\"</code> <code>int</code> Integer number <code>42</code> <code>bool</code> Boolean value <code>true</code> <code>float</code> Decimal number <code>3.14</code> <code>list[str]</code> List of strings <code>[\"item1\", \"item2\"]</code> <code>dict[str,Any]</code> Dictionary/object <code>{\"key\": \"value\"}</code>"},{"location":"guides/superspec/#example_2","title":"Example","text":"<pre><code>spec:\n  tasks:\n  - name: implement_feature\n    description: Implement software features based on requirements\n    instruction: You are a Software Developer. Your goal is to write clean, efficient, and maintainable code. Implement the feature based on the provided requirement.\n    inputs:\n    - name: feature_requirement\n      type: str\n      description: A detailed description of the feature to implement.\n      required: true\n    outputs:\n    - name: implementation\n      type: str\n      description: The code implementation of the feature.\n</code></pre>"},{"location":"guides/superspec/#intermediate-level-execution-flow","title":"\ud83d\udd04 Intermediate Level: Execution Flow","text":""},{"location":"guides/superspec/#agentflow-configuration","title":"AgentFlow Configuration","text":"<p>The <code>agentflow</code> section defines the sequence of steps the agent takes to complete tasks.</p>"},{"location":"guides/superspec/#required-structure_1","title":"Required Structure","text":"<pre><code>spec:\n  agentflow:                            # OPTIONAL - Execution flow\n    - name: string                      # REQUIRED - Step name\n      type: Generate|Think|ActWithTools|Search|Code|Compare|MultiHopSearch|Route  # REQUIRED\n      task: string                      # REQUIRED - Task name to execute\n      depends_on: [string]              # OPTIONAL - Dependencies\n</code></pre>"},{"location":"guides/superspec/#step-types","title":"Step Types","text":"Type Description Use Case <code>Generate</code> Generate content using LLM Text generation, code writing <code>Think</code> Reasoning and analysis Problem solving, decision making <code>ActWithTools</code> Use tools and APIs External integrations <code>Search</code> Information retrieval Knowledge lookup <code>Code</code> Code generation and execution Software development <code>Compare</code> Compare multiple options Evaluation and selection <code>MultiHopSearch</code> Multi-step information gathering Complex research <code>Route</code> Route to different steps Conditional logic"},{"location":"guides/superspec/#example_3","title":"Example","text":"<pre><code>spec:\n  agentflow:\n  - name: analyze_request\n    type: Think\n    task: implement_feature\n  - name: generate_code\n    type: Generate\n    task: implement_feature\n    depends_on: [\"analyze_request\"]\n</code></pre>"},{"location":"guides/superspec/#evaluation-configuration","title":"Evaluation Configuration","text":"<p>The <code>evaluation</code> section defines quality metrics and validation criteria.</p>"},{"location":"guides/superspec/#built-in-metrics","title":"Built-in Metrics","text":"<pre><code>spec:\n  evaluation:\n    builtin_metrics:                    # OPTIONAL - Quality metrics\n      - name: answer_exact_match|answer_passage_match|semantic_f1|rouge_l|bleu|meteor|answer_correctness|faithfulness|context_relevance\n        threshold: float                # OPTIONAL (default: 0.7) - Quality threshold\n        weight: float                   # OPTIONAL (default: 1.0) - Metric weight\n</code></pre>"},{"location":"guides/superspec/#available-metrics","title":"Available Metrics","text":"Metric Description Use Case <code>answer_exact_match</code> Exact text matching Factual accuracy <code>answer_passage_match</code> Passage-level matching Content relevance <code>semantic_f1</code> Semantic similarity Meaning accuracy <code>rouge_l</code> ROUGE-L score Text generation quality <code>bleu</code> BLEU score Translation quality <code>meteor</code> METEOR score Text similarity <code>answer_correctness</code> Overall correctness General quality <code>faithfulness</code> Faithfulness to source Information accuracy <code>context_relevance</code> Context relevance RAG quality"},{"location":"guides/superspec/#example_4","title":"Example","text":"<pre><code>spec:\n  evaluation:\n    builtin_metrics:\n    - name: answer_exact_match\n      threshold: 1.0\n    - name: answer_correctness\n      threshold: 0.8\n      weight: 2.0\n</code></pre>"},{"location":"guides/superspec/#feature-specifications-bdd-scenarios","title":"Feature Specifications (BDD Scenarios)","text":"<p>The <code>feature_specifications</code> section defines BDD scenarios for agent testing and optimization.</p>"},{"location":"guides/superspec/#structure","title":"Structure","text":"<pre><code>spec:\n  feature_specifications:               # OPTIONAL - BDD scenarios\n    scenarios:                          # OPTIONAL\n      - name: string                    # REQUIRED - Scenario name\n        description: string             # REQUIRED - Scenario description\n        input: {}                       # REQUIRED - Input data\n        expected_output: {}             # REQUIRED - Expected output\n        validation_criteria: [string]   # OPTIONAL - Validation hints\n</code></pre>"},{"location":"guides/superspec/#example_5","title":"Example","text":"<pre><code>spec:\n  feature_specifications:\n    scenarios:\n    - name: basic_question_answering\n      description: The agent should answer basic questions accurately.\n      input:\n        question: \"What is artificial intelligence and how does it work?\"\n      expected_output:\n        answer: \"Should include comprehensive explanation of AI concepts\"\n    - name: creative_content_generation\n      description: The agent should generate creative content effectively.\n      input:\n        question: \"Write a short story about a robot learning to paint\"\n      expected_output:\n        answer: \"Should include a creative and engaging story\"\n</code></pre>"},{"location":"guides/superspec/#advanced-level-advanced-features","title":"\ud83d\ude80 Advanced Level: Advanced Features","text":""},{"location":"guides/superspec/#tools-configuration-genies-tier","title":"Tools Configuration (Genies Tier)","text":"<p>The <code>tools</code> section enables tool integration for Genies tier agents.</p>"},{"location":"guides/superspec/#basic-configuration","title":"Basic Configuration","text":"<pre><code>spec:\n  tools:\n    enabled: bool                       # REQUIRED - Enable tool integration\n    categories: [string]                # OPTIONAL - Tool categories to include\n    specific_tools: [string]            # OPTIONAL - Specific tools to include\n</code></pre>"},{"location":"guides/superspec/#available-tool-categories","title":"Available Tool Categories","text":"Category Description Examples <code>core</code> Essential tools calculator, file_reader, text_analyzer <code>development</code> Software development code_formatter, debugger, linter <code>utilities</code> General utilities date_time, json_processor, data_processor <code>finance</code> Financial tools financial_calculator, investment_analyzer <code>healthcare</code> Healthcare tools health_assessment, medical_analyzer <code>education</code> Educational tools educational_content, quiz_generator <code>legal</code> Legal tools legal_analyzer, contract_reviewer <code>marketing</code> Marketing tools marketing_analyzer, campaign_planner <code>real_estate</code> Real estate tools property_analyzer, market_researcher <code>retail</code> Retail tools inventory_manager, sales_analyzer <code>transportation</code> Transportation tools route_optimizer, fleet_manager <code>energy</code> Energy tools energy_calculator, efficiency_analyzer <code>agriculture</code> Agriculture tools crop_advisor, weather_analyzer <code>human_resources</code> HR tools hr_analyzer, recruitment_assistant <code>hospitality</code> Hospitality tools hospitality_manager, booking_optimizer <code>manufacturing</code> Manufacturing tools production_planner, quality_inspector <code>gaming_sports</code> Gaming/Sports tools game_analyzer, performance_tracker <code>media_entertainment</code> Media tools content_analyzer, trend_predictor <code>government_public</code> Government tools policy_analyzer, compliance_checker <code>consulting</code> Consulting tools business_consultant, strategy_advisor"},{"location":"guides/superspec/#example_6","title":"Example","text":"<pre><code>spec:\n  tools:\n    enabled: true\n    categories:\n    - core\n    - development\n    - utilities\n    specific_tools:\n    - calculator\n    - file_reader\n    - text_analyzer\n    - web_search\n    - date_time\n    - code_formatter\n</code></pre>"},{"location":"guides/superspec/#memory-configuration-genies-tier","title":"Memory Configuration (Genies Tier)","text":"<p>The <code>memory</code> section configures memory systems for Genies tier agents.</p>"},{"location":"guides/superspec/#basic-configuration_1","title":"Basic Configuration","text":"<pre><code>spec:\n  memory:\n    enabled: bool                       # REQUIRED - Enable memory system\n    short_term:                         # OPTIONAL - Working memory\n      enabled: bool                     # OPTIONAL (default: true)\n      max_tokens: int                   # OPTIONAL - Maximum tokens\n    long_term:                          # OPTIONAL - Persistent knowledge\n      enabled: bool                     # OPTIONAL (default: true)\n      storage_type: local               # OPTIONAL - Storage backend\n      max_entries: int                  # OPTIONAL - Maximum entries\n    episodic:                           # OPTIONAL - Experience tracking\n      enabled: bool                     # OPTIONAL (default: true)\n      max_episodes: int                 # OPTIONAL - Maximum episodes\n</code></pre>"},{"location":"guides/superspec/#example_7","title":"Example","text":"<pre><code>spec:\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n    episodic:\n      enabled: true\n      max_episodes: 50\n</code></pre>"},{"location":"guides/superspec/#rag-configuration-genies-tier","title":"RAG Configuration (Genies Tier)","text":"<p>The <code>rag</code> section configures Retrieval-Augmented Generation for knowledge retrieval.</p>"},{"location":"guides/superspec/#basic-configuration_2","title":"Basic Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: bool                       # REQUIRED - Enable RAG\n    retriever_type: chroma|qdrant|weaviate|milvus|lancedb  # REQUIRED - Vector store\n    config:                             # OPTIONAL - Retrieval configuration\n      top_k: int                        # OPTIONAL (default: 5) - Number of results\n      chunk_size: int                   # OPTIONAL (default: 512) - Text chunk size\n      chunk_overlap: int                # OPTIONAL (default: 50) - Chunk overlap\n    vector_store:                       # OPTIONAL - Vector store configuration\n      embedding_model: string           # OPTIONAL - Embedding model\n      collection_name: string           # OPTIONAL - Collection name\n</code></pre>"},{"location":"guides/superspec/#supported-vector-stores","title":"Supported Vector Stores","text":"Retriever Type Description Use Case <code>chroma</code> ChromaDB vector store Local development, small datasets <code>qdrant</code> Qdrant vector database Production, scalable deployments <code>weaviate</code> Weaviate vector database Enterprise, rich metadata <code>milvus</code> Milvus vector database High-performance, large-scale <code>lancedb</code> LanceDB vector database Fast, embedded vector storage"},{"location":"guides/superspec/#example_8","title":"Example","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: agent_knowledge\n</code></pre>"},{"location":"guides/superspec/#optimization-configuration","title":"Optimization Configuration","text":"<p>The <code>optimization</code> section configures advanced DSPy-based performance improvement using state-of-the-art optimizers.</p>"},{"location":"guides/superspec/#complete-configuration-schema","title":"Complete Configuration Schema","text":"<pre><code>spec:\n  optimization:\n    enabled: true                       # OPTIONAL (default: false)\n    optimizer:\n      name: GEPA|SIMBA|MIPROv2|BootstrapFewShot|BetterTogether|COPRO|KNNFewShot|LabeledFewShot\n      params:\n        # Common parameters for all optimizers\n        metric: string                  # REQUIRED - Evaluation metric\n        timeout: 300                    # OPTIONAL - Timeout in seconds\n        verbose: false                  # OPTIONAL - Enable detailed logs\n\n        # GEPA-specific parameters\n        auto: minimal|light|medium|heavy            # GEPA budget control\n        reflection_lm: string                       # Reflection model name\n        reflection_minibatch_size: 3                # Reflection batch size\n        skip_perfect_score: true                    # Skip if perfect\n        add_format_failure_as_feedback: true       # Include format errors\n\n        # SIMBA-specific parameters  \n        bsize: 8                                    # SIMBA batch size\n        num_candidates: 5                           # Number of candidates\n        max_steps: 10                               # Maximum steps\n\n        # BootstrapFewShot parameters\n        max_bootstrapped_demos: 8                   # Bootstrap examples\n        max_labeled_demos: 16                       # Labeled examples\n        max_rounds: 2                               # Optimization rounds\n        max_errors: 3                               # Error tolerance\n\n        # MIPROv2 parameters\n        init_temperature: 1.2                      # Initial temperature\n\n    evaluation:\n      enabled: true                               # Enable evaluation\n      metrics: [\"answer_exact_match\"]             # Additional metrics\n      test_split: 0.2                            # Test data fraction\n      cross_validation:\n        enabled: false                            # Enable CV\n        folds: 5                                  # CV folds\n\n    hardware_optimization:\n      target_tier: lightweight|standard|production # Hardware tier\n      memory_limit_gb: 16                         # Memory limit\n      parallel_optimization: false               # Enable parallel\n      max_workers: 2                             # Parallel workers\n</code></pre>"},{"location":"guides/superspec/#dspy-optimizers","title":"DSPy Optimizers","text":"<p>SuperSpec now supports all major DSPy optimizers with universal tier compatibility:</p> Optimizer Description Strengths Time Estimate \ud83e\udde0 GEPA Genetic-Pareto Complex reasoning, math, reflective optimization 3-30 min \u26a1 SIMBA Stochastic Introspective Mini-Batch Ascent Fast optimization, general tasks 1-3 min \ud83d\udcdd MIPROv2 Multi-step Instruction Prompt Optimization Creative writing, sophisticated prompts 2-15 min \ud83d\udd04 BootstrapFewShot Bootstrap few-shot examples Quick optimization, small datasets 30-60 sec \ud83e\udd1d BetterTogether Ensemble few-shot approach Document analysis, balanced accuracy 1-5 min \ud83e\udd16 COPRO Collaborative Prompt Optimization Legal analysis, structured tasks 2-8 min \ud83d\udd0d KNNFewShot K-nearest neighbor few-shot learning Similarity-based, recommendations 1-2 min \ud83c\udff7\ufe0f LabeledFewShot Traditional labeled few-shot Translation, classification 30-90 sec"},{"location":"guides/superspec/#quick-start-examples","title":"Quick Start Examples","text":"<p>GEPA for Mathematical Reasoning: <pre><code>  optimization:\n    enabled: true\n    optimizer:\n      name: GEPA\n      params:\n        metric: advanced_math_feedback\n        auto: light\n        reflection_lm: qwen3:8b\n        reflection_minibatch_size: 3\n</code></pre></p> <p>SIMBA for Fast Optimization: <pre><code>  optimization:\n    enabled: true\n    optimizer:\n      name: SIMBA\n      params:\n        metric: answer_exact_match\n        bsize: 8\n        num_candidates: 5\n</code></pre></p> <p>BootstrapFewShot for Quick Testing: <pre><code>  optimization:\n    enabled: true\n    optimizer:\n      name: BootstrapFewShot\n      params:\n        metric: answer_exact_match\n        max_bootstrapped_demos: 4\n</code></pre></p>"},{"location":"guides/superspec/#advanced-feedback-metrics","title":"Advanced Feedback Metrics","text":"<p>SuperSpec provides specialized metrics for domain-specific optimization:</p> Metric Use Case Description <code>advanced_math_feedback</code> Mathematics Step-by-step reasoning validation <code>multi_component_enterprise_feedback</code> Business Multi-aspect document analysis <code>vulnerability_detection_feedback</code> Security Security analysis with remediation <code>privacy_preservation_feedback</code> Privacy Data privacy compliance <code>medical_accuracy_feedback</code> Healthcare Medical safety validation <code>legal_analysis_feedback</code> Legal Legal compliance verification"},{"location":"guides/superspec/#hardware-tier-configuration","title":"Hardware Tier Configuration","text":"<p>Optimize for different hardware capabilities:</p> <pre><code>  optimization:\n    hardware_optimization:\n      target_tier: lightweight    # 8GB+ RAM, 2-3 min optimization\n      # OR\n      target_tier: standard      # 16GB+ RAM, 5-8 min optimization  \n      # OR\n      target_tier: production    # 32GB+ RAM, 15-30 min optimization\n</code></pre>"},{"location":"guides/superspec/#legacy-support","title":"Legacy Support","text":"<p>For backward compatibility, the old optimization format is still supported:</p> <pre><code>  optimization: few_shot_bootstrapping  # Legacy format - automatically converted\n</code></pre>"},{"location":"guides/superspec/#context-engineering-deep-dive","title":"\ud83c\udfaf Context Engineering Deep Dive","text":""},{"location":"guides/superspec/#memory-systems","title":"Memory Systems","text":"<p>SuperSpec provides a comprehensive multi-layer memory system:</p> <pre><code>context:\n  memory:\n    enabled: true\n    systems:\n      short_term:\n        enabled: true\n        type: \"conversation_buffer\"\n        max_tokens: 4000\n\n      long_term:\n        enabled: true\n        type: \"vector_database\"\n        provider: \"pinecone\"\n        collection: \"agent_memory\"\n\n      episodic:\n        enabled: true\n        type: \"event_store\"\n        persistence: \"redis\"\n        ttl: \"30d\"\n</code></pre>"},{"location":"guides/superspec/#memory-types","title":"Memory Types","text":"Type Purpose Persistence Use Case Short-term Conversation context Session Current interaction Long-term Domain knowledge Permanent Factual information Episodic Interaction history Configurable Learning patterns"},{"location":"guides/superspec/#knowledge-integration","title":"Knowledge Integration","text":"<p>Seamless integration with multiple knowledge sources:</p> <pre><code>context:\n  knowledge:\n    sources:\n      - name: \"company_docs\"\n        type: \"document_store\"\n        path: \"./knowledge/company/\"\n        embedding_model: \"text-embedding-ada-002\"\n\n      - name: \"market_data\"\n        type: \"api_integration\"\n        provider: \"alpha_vantage\"\n        update_frequency: \"1h\"\n\n      - name: \"regulatory_framework\"\n        type: \"structured_data\"\n        format: \"json\"\n        source: \"regulatory_api\"\n\n    retrieval:\n      strategy: \"hybrid\"\n      reranking: true\n      max_results: 10\n</code></pre>"},{"location":"guides/superspec/#knowledge-sources","title":"Knowledge Sources","text":"Type Description Example Document Store Local file system Company documents API Integration External APIs Market data Structured Data Databases Regulatory info Vector Database Embeddings Semantic search"},{"location":"guides/superspec/#tool-orchestration","title":"Tool Orchestration","text":"<p>Structured integration of external tools and APIs:</p> <pre><code>context:\n  tools:\n    enabled: true\n    categories:\n      data_analysis:\n        - name: \"pandas\"\n          version: \"2.0.0\"\n          capabilities: [\"data_manipulation\", \"statistics\"]\n\n        - name: \"matplotlib\"\n          version: \"3.7.0\"\n          capabilities: [\"visualization\"]\n\n      external_apis:\n        - name: \"financial_data_api\"\n          endpoint: \"https://api.financial.com\"\n          authentication: \"api_key\"\n          rate_limit: \"1000/hour\"\n</code></pre>"},{"location":"guides/superspec/#agent-engineering-deep-dive","title":"\ud83d\udd27 Agent Engineering Deep Dive","text":""},{"location":"guides/superspec/#tier-based-architecture","title":"Tier-based Architecture","text":"<p>SuperSpec supports all five tiers with tier-specific configurations. Note: Higher-level configurations for protocols, SuperAgents, and Sovereigns tiers are not included in the current version.</p>"},{"location":"guides/superspec/#oracle-tier-basic","title":"Oracle Tier (Basic)","text":"<pre><code>metadata:\n  tier: oracle\n\nspec:\n  capabilities:\n    chain_of_thought: true\n    template_responses: true\n\n  tasks:\n    - name: \"answer_question\"\n</code></pre>"},{"location":"guides/superspec/#genie-tier-intermediate","title":"Genie Tier (Intermediate)","text":"<pre><code>metadata:\n  tier: genie\n\nspec:\n  context:\n    memory: true\n    tools: true\n    retrieval: true\n\n  reasoning: \"react\"\n  optimization: \"mipro\"\n</code></pre>"},{"location":"guides/superspec/#task-definitions","title":"Task Definitions","text":"<p>Define agent capabilities through declarative task specifications:</p> <pre><code>tasks:\n  - name: \"analyze_financial_data\"\n    description: \"Perform comprehensive financial data analysis\"\n    inputs:\n      - \"data_source\"\n      - \"analysis_type\"\n      - \"time_period\"\n    outputs:\n      - \"analysis_report\"\n      - \"visualizations\"\n      - \"recommendations\"\n    validation:\n      required_inputs: [\"data_source\"]\n      output_format: \"json\"\n</code></pre>"},{"location":"guides/superspec/#agentflow","title":"AgentFlow","text":"<p>Define the execution flow of your agent:</p> <pre><code>agentflow:\n  - name: \"data_collection\"\n    type: \"tool_call\"\n    depends_on: []\n\n  - name: \"data_processing\"\n    type: \"task_execution\"\n    depends_on: [\"data_collection\"]\n\n  - name: \"analysis_generation\"\n    type: \"llm_reasoning\"\n    depends_on: [\"data_processing\"]\n\n  - name: \"insight_synthesis\"\n    type: \"task_execution\"\n    depends_on: [\"analysis_generation\"]\n</code></pre>"},{"location":"guides/superspec/#multi-agent-orchestration","title":"\ud83c\udfad Multi-Agent Orchestration","text":""},{"location":"guides/superspec/#orchestra-specification","title":"Orchestra Specification","text":"<p>Define multi-agent workflows:</p> <pre><code>apiVersion: orchestra/v1\nkind: Orchestra\nmetadata:\n  name: research-team\n  tier: genie\n\nspec:\n  agents:\n    - name: \"researcher\"\n      role: \"information_gathering\"\n      tier: \"genie\"\n\n    - name: \"analyst\"\n      role: \"data_analysis\"\n      tier: \"genie\"\n\n    - name: \"writer\"\n      role: \"report_generation\"\n      tier: \"oracle\"\n\n  workflow:\n    - name: \"research_phase\"\n      agents: [\"researcher\"]\n      type: \"parallel\"\n\n    - name: \"analysis_phase\"\n      agents: [\"analyst\"]\n      depends_on: [\"research_phase\"]\n\n    - name: \"writing_phase\"\n      agents: [\"writer\"]\n      depends_on: [\"analysis_phase\"]\n</code></pre>"},{"location":"guides/superspec/#bdd-integration","title":"\ud83e\uddea BDD Integration","text":""},{"location":"guides/superspec/#feature-specifications","title":"Feature Specifications","text":"<p>Define expected behaviors using BDD scenarios in the <code>feature_specifications</code> section:</p> <pre><code>feature_specifications:\n  scenarios:\n  - name: \"financial_analysis_accuracy\"\n    description: \"The agent should provide accurate financial analysis\"\n    input:\n      data_source: \"sample_financial_data.csv\"\n      analysis_type: \"trend_analysis\"\n      time_period: \"Q1 2024\"\n    expected_output:\n      analysis_report: \"Should include comprehensive financial analysis with insights\"\n      visualizations: \"Should provide relevant charts and graphs\"\n      recommendations: \"Should offer actionable financial recommendations\"\n\n  - name: \"tool_usage_efficiency\"\n    description: \"The agent should use tools efficiently for data processing\"\n    input:\n      data_analysis_request: \"Analyze this dataset and create visualizations\"\n    expected_output:\n      processed_data: \"Should demonstrate efficient use of pandas and matplotlib\"\n      analysis_quality: \"Should provide accurate statistical analysis\"\n</code></pre>"},{"location":"guides/superspec/#scenario-structure","title":"Scenario Structure","text":"<p>Each scenario in <code>feature_specifications</code> follows this structure:</p> <pre><code>feature_specifications:\n  scenarios:\n  - name: string                    # REQUIRED - Unique scenario name\n    description: string             # REQUIRED - What the scenario tests\n    input: {}                       # REQUIRED - Input data matching task inputs\n    expected_output: {}             # REQUIRED - Expected outputs matching task outputs\n    validation_criteria: [string]   # OPTIONAL - Additional validation hints\n</code></pre>"},{"location":"guides/superspec/#evaluation-configuration_1","title":"Evaluation Configuration","text":"<pre><code>evaluation:\n  builtin_metrics:\n  - name: answer_exact_match\n    threshold: 0.8\n  - name: answer_correctness\n    threshold: 0.9\n    weight: 2.0\n\n  optimization:\n    strategy: few_shot_bootstrapping\n    metric: answer_correctness\n    metric_threshold: 0.8\n</code></pre>"},{"location":"guides/superspec/#tier-specific-configurations","title":"\ud83c\udfaf Tier-Specific Configurations","text":"<p>Tier Availability</p> <p>Current Version: This guide covers Oracles and Genies tiers only. Higher-level configurations for Sage, SuperAgents, and Sovereigns tiers (including advanced protocols, multi-agent orchestration, and enterprise features) are not included in the current version.</p>"},{"location":"guides/superspec/#oracles-tier-basic","title":"Oracles Tier (Basic)","text":"<p>Oracles tier agents focus on fundamental capabilities:</p> <pre><code>metadata:\n  level: oracles\n\nspec:\n  # Basic LLM configuration\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n\n  # Simple persona\n  persona:\n    role: \"Assistant\"\n    goal: \"Help with basic tasks\"\n\n  # Single task\n  tasks:\n  - name: answer_question\n    instruction: \"Answer questions clearly and accurately\"\n    inputs:\n    - name: question\n      type: str\n      description: \"The question to answer\"\n      required: true\n    outputs:\n    - name: answer\n      type: str\n      description: \"The answer to the question\"\n\n  # Simple flow\n  agentflow:\n  - name: generate_answer\n    type: Generate\n    task: answer_question\n\n  # Basic evaluation\n  evaluation:\n    builtin_metrics:\n    - name: answer_exact_match\n      threshold: 1.0\n</code></pre>"},{"location":"guides/superspec/#genies-tier-advanced","title":"Genies Tier (Advanced)","text":"<p>Genies tier agents include advanced techniques:</p> <pre><code>metadata:\n  level: genies\n\nspec:\n  # Advanced LLM configuration\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    temperature: 0.7\n    max_tokens: 2048\n\n  # Rich persona\n  persona:\n    name: \"AdvancedBot\"\n    role: \"Advanced Assistant\"\n    goal: \"Provide comprehensive assistance with tools and memory\"\n    traits:\n    - helpful\n    - knowledgeable\n    - precise\n\n  # Multiple tasks\n  tasks:\n  - name: creative_writing\n    instruction: \"Generate creative content\"\n    inputs:\n    - name: writing_request\n      type: str\n      description: \"Creative writing request\"\n      required: true\n    outputs:\n    - name: creative_content\n      type: str\n      description: \"Creative content\"\n\n  # Complex flow\n  agentflow:\n  - name: analyze_request\n    type: Think\n    task: creative_writing\n  - name: generate_content\n    type: Generate\n    task: creative_writing\n    depends_on: [\"analyze_request\"]\n\n  # Advanced techniques\n  tools:\n    enabled: true\n    categories:\n    - core\n    - utilities\n\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n\n  rag:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n\n  # Comprehensive evaluation\n  evaluation:\n    builtin_metrics:\n    - name: response_quality\n      threshold: 0.8\n    - name: creativity_score\n      threshold: 0.7\n</code></pre>"},{"location":"guides/superspec/#best-practices","title":"\ud83d\ude80 Best Practices","text":""},{"location":"guides/superspec/#1-start-simple","title":"1. Start Simple","text":"<p>Begin with Oracle tier for simple use cases:</p> <pre><code># Simple FAQ bot\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: faq-bot\n  tier: oracle\nspec:\n  tasks:\n    - name: \"answer_faq\"\n      template: \"Answer this FAQ: {question}\"\n</code></pre>"},{"location":"guides/superspec/#2-progressive-enhancement","title":"2. Progressive Enhancement","text":"<p>Upgrade to Genie when you need tools and memory:</p> <pre><code># Enhanced customer service\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: customer-service\n  tier: genie\nspec:\n  context:\n    memory: true\n    tools: true\n    retrieval: true\n  tasks:\n    - name: \"handle_inquiry\"\n      description: \"Handle customer inquiries\"\n</code></pre>"},{"location":"guides/superspec/#3-production-considerations","title":"3. Production Considerations","text":"<p>For Protocol tier and above (commercial version):</p> <pre><code># Production-ready agent\napiVersion: agent/v1\nkind: Agent\nmetadata:\n  name: production-agent\n  tier: protocol\nspec:\n  context:\n    memory: true\n    tools: true\n    protocols: [\"mcp\", \"a2a\"]\n  monitoring:\n    tracing: true\n    observability: true\n    alerting: true\n</code></pre>"},{"location":"guides/superspec/#4-validation-and-testing","title":"4. Validation and Testing","text":"<p>Always include BDD feature specifications:</p> <pre><code>feature_specifications:\n  scenarios:\n  - name: \"core_functionality\"\n    description: \"The agent should perform core tasks correctly\"\n    input:\n      question: \"What is the capital of France?\"\n    expected_output:\n      answer: \"Should provide accurate information about Paris\"\n</code></pre>"},{"location":"guides/superspec/#development-workflow","title":"\ud83d\udd27 Development Workflow","text":""},{"location":"guides/superspec/#1-create-superspec-playbook","title":"1. Create SuperSpec Playbook","text":"<pre><code># Generate a new agent specification\nsuper spec generate genies my-agent \\\n  --namespace my-domain \\\n  --tools \\\n  --memory \\\n  --rag\n\n# Or create manually\nvim agents/my-agent/playbook/my-agent_playbook.yaml\n</code></pre>"},{"location":"guides/superspec/#2-validate-specification","title":"2. Validate Specification","text":"<pre><code># Validate the SuperSpec specification\nsuper spec validate my-agent_playbook.yaml\n</code></pre>"},{"location":"guides/superspec/#3-compile-agent","title":"3. Compile Agent","text":"<pre><code># Compile SuperSpec to executable pipeline\nsuper agent compile my-agent\n</code></pre>"},{"location":"guides/superspec/#4-test-and-optimize","title":"4. Test and Optimize","text":"<pre><code># Run BDD evaluation\nsuper agent evaluate my-agent\n\n# Optimize performance\nsuper agent optimize my-agent\n\n# Re-evaluate\nsuper agent evaluate my-agent\n</code></pre>"},{"location":"guides/superspec/#5-deploy","title":"5. Deploy","text":"<pre><code># Run the agent\nsuper agent run my-agent --goal \"Your task here\"\n</code></pre>"},{"location":"guides/superspec/#best-practices_1","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/superspec/#dos","title":"DO's","text":"<ol> <li>Start Simple: Begin with basic Oracles tier configuration</li> <li>Use Descriptive Names: Make task and field names self-documenting</li> <li>Validate Early: Use <code>super spec validate</code> to catch errors</li> <li>Version Control: Commit SuperSpec files to Git for tracking</li> <li>Test Thoroughly: Write comprehensive BDD scenarios</li> <li>Optimize Iteratively: Use evaluation results to guide optimization</li> </ol>"},{"location":"guides/superspec/#donts","title":"DON'Ts","text":"<ol> <li>Don't Over-Configure: Start with minimal configuration</li> <li>Don't Skip Validation: Always validate before compilation</li> <li>Don't Ignore Evaluation: Use BDD scenarios for quality assurance</li> <li>Don't Hardcode Secrets: Use environment variables for API keys</li> <li>Don't Mix Tiers: Stick to tier-appropriate features</li> </ol>"},{"location":"guides/superspec/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>SuperSpec provides a declarative, strongly-typed specification language for AI agents that:</p> <ul> <li>\ud83c\udfaf Strikes the right balance of context for optimal performance</li> <li>\ud83d\udd04 Integrates seamlessly with DSPy for validation and optimization</li> <li>\ud83d\udccb Follows Kubernetes patterns for declarative configuration</li> <li>\ud83e\uddea Supports BDD scenarios for comprehensive testing</li> <li>\ud83d\ude80 Enables version control and team collaboration</li> </ul> <p>Start using SuperSpec today to build reliable, maintainable, and performant AI agents!</p> <p>\ud83d\udca1 Pro Tip: Begin with Oracles tier configurations to understand the basics, then graduate to Genies tier for advanced techniques like tools, memory, and RAG. Always validate your SuperSpec specifications before compilation!</p> <p>Advanced Tiers</p> <p>For Sage, SuperAgents, and Sovereigns tiers with advanced protocols, multi-agent orchestration, and enterprise features, please refer to the commercial version of SuperOptiX. </p>"},{"location":"guides/superspec/#dsl-examples","title":"\ud83d\udd27 DSL Examples","text":""},{"location":"guides/superspec/#basic-oracles-tier-agent","title":"Basic Oracles Tier Agent","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Simple Assistant\n  id: simple-assistant\n  namespace: software\n  version: \"1.0.0\"\n  level: oracles\n  stage: alpha\n  agent_type: Autonomous\n  description: A basic assistant for simple tasks\n  tags: [\"software\", \"oracles\", \"basic\"]\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.2:1b\n    api_base: http://localhost:11434\n    temperature: 0.0\n    max_tokens: 2048\n    cache: true\n  persona:\n    name: SimpleBot\n    role: Assistant\n    goal: Provide helpful assistance for basic tasks\n    traits:\n    - helpful\n    - concise\n    - accurate\n    communication_preferences:\n      style: professional\n      tone: friendly\n      verbosity: concise\n  tasks:\n  - name: answer_question\n    description: Answer user questions clearly and accurately\n    instruction: You are a helpful assistant. Answer the user's question clearly and accurately.\n    inputs:\n    - name: question\n      type: str\n      description: The question to answer\n      required: true\n    outputs:\n    - name: answer\n      type: str\n      description: The answer to the question\n  agentflow:\n  - name: generate_answer\n    type: Think\n    task: answer_question\n    config:\n      reasoning_depth: 2\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.8\n  optimization:\n    strategy: few_shot_bootstrapping\n    metric: answer_correctness\n    few_shot_bootstrapping_config:\n      max_bootstrapped_demos: 4\n      max_rounds: 1\n  feature_specifications:\n    scenarios:\n    - name: basic_question_answering\n      description: The agent should answer basic questions accurately\n      input:\n        question: \"What is the capital of France?\"\n      expected_output:\n        answer: \"Should provide accurate information about Paris\"\n    - name: factual_inquiry\n      description: The agent should handle factual inquiries\n      input:\n        question: \"How many planets are in our solar system?\"\n      expected_output:\n        answer: \"Should provide accurate count of planets\"\n</code></pre>"},{"location":"guides/superspec/#advanced-genies-tier-agent","title":"Advanced Genies Tier Agent","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Advanced Data Analyst\n  id: advanced-data-analyst\n  namespace: finance\n  version: \"1.0.0\"\n  level: genies\n  stage: alpha\n  agent_type: Autonomous\n  description: Advanced data analysis agent with tools and memory\n  tags: [\"finance\", \"genies\", \"data-analysis\", \"advanced\"]\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 2048\n    cache: true\n  persona:\n    name: DataBot\n    role: Data Analyst\n    goal: Perform comprehensive data analysis and provide insights\n    traits:\n    - analytical\n    - detail-oriented\n    - precise\n    - helpful\n    expertise_areas:\n    - data analysis\n    - statistical modeling\n    - financial analysis\n    - visualization\n    communication_preferences:\n      style: technical\n      tone: professional\n      verbosity: detailed\n  tasks:\n  - name: analyze_data\n    description: Perform comprehensive data analysis\n    instruction: You are a Data Analyst. Analyze the provided data and generate insights, visualizations, and recommendations.\n    inputs:\n    - name: data_source\n      type: str\n      description: Source of data to analyze (file path, URL, or data description)\n      required: true\n    - name: analysis_type\n      type: str\n      description: Type of analysis to perform (descriptive, predictive, exploratory)\n      required: true\n    - name: time_period\n      type: str\n      description: Time period for analysis\n      required: false\n    outputs:\n    - name: analysis_report\n      type: str\n      description: Comprehensive analysis report\n    - name: visualizations\n      type: list[str]\n      description: Generated data visualizations\n    - name: recommendations\n      type: str\n      description: Actionable recommendations based on analysis\n  - name: create_visualization\n    description: Create data visualizations\n    instruction: Create appropriate visualizations for the given data and analysis requirements.\n    inputs:\n    - name: data\n      type: dict[str,Any]\n      description: Data to visualize\n      required: true\n    - name: chart_type\n      type: str\n      description: Type of chart to create\n      required: true\n    outputs:\n    - name: visualization\n      type: str\n      description: Generated visualization\n  agentflow:\n  - name: load_data\n    type: ActWithTools\n    task: analyze_data\n  - name: perform_analysis\n    type: Think\n    task: analyze_data\n    depends_on: [\"load_data\"]\n  - name: generate_visualizations\n    type: Generate\n    task: create_visualization\n    depends_on: [\"perform_analysis\"]\n  - name: synthesize_results\n    type: Generate\n    task: analyze_data\n    depends_on: [\"generate_visualizations\"]\n  react_config:\n    max_iters: 5\n    max_tool_calls: 3\n    tool_selection_strategy: automatic\n    reasoning_style: step_by_step\n    error_handling: retry\n    enable_tracing: true\n  tools:\n    enabled: true\n    categories:\n    - core\n    - development\n    - utilities\n    specific_tools:\n    - calculator\n    - file_reader\n    - text_analyzer\n    - web_search\n    - date_time\n    - code_formatter\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 2000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 500\n    episodic:\n      enabled: true\n      max_episodes: 100\n  retrieval:\n    enabled: true\n    retriever_type: chroma\n    config:\n      top_k: 5\n      chunk_size: 512\n      chunk_overlap: 50\n    vector_store:\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      collection_name: data_analysis_knowledge\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.8\n    - name: analysis_quality\n      threshold: 0.7\n    - name: visualization_quality\n      threshold: 0.6\n  feature_specifications:\n    scenarios:\n    - name: financial_data_analysis\n      description: The agent should analyze financial data effectively\n      input:\n        data_source: \"sample_financial_data.csv\"\n        analysis_type: \"descriptive\"\n        time_period: \"Q1 2024\"\n      expected_output:\n        analysis_report: \"Should include comprehensive financial analysis\"\n        visualizations: \"Should provide relevant charts and graphs\"\n        recommendations: \"Should offer actionable financial recommendations\"\n    - name: statistical_analysis\n      description: The agent should perform statistical analysis\n      input:\n        data_source: \"sales_data.json\"\n        analysis_type: \"predictive\"\n      expected_output:\n        analysis_report: \"Should include statistical insights and predictions\"\n        recommendations: \"Should provide data-driven recommendations\"\n  optimization:\n    strategy: few_shot_bootstrapping\n    metric: analysis_quality\n    metric_threshold: 0.7\n    few_shot_bootstrapping_config:\n      max_bootstrapped_demos: 4\n      max_rounds: 1\n</code></pre>"},{"location":"guides/superspec/#healthcare-agent-example","title":"Healthcare Agent Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Healthcare Assistant\n  id: healthcare-assistant\n  namespace: healthcare\n  version: \"1.0.0\"\n  level: genies\n  stage: alpha\n  agent_type: Supervised\n  description: Healthcare assistant for patient information and medical guidance\n  tags: [\"healthcare\", \"genies\", \"medical\", \"patient-care\"]\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n    temperature: 0.3\n    max_tokens: 2048\n  persona:\n    name: HealthBot\n    role: Healthcare Assistant\n    goal: Provide accurate healthcare information and patient support\n    traits:\n    - knowledgeable\n    - empathetic\n    - precise\n    - professional\n    expertise_areas:\n    - general health\n    - medical terminology\n    - patient care\n    - health education\n    communication_preferences:\n      style: professional\n      tone: supportive\n      verbosity: detailed\n  tasks:\n  - name: health_assessment\n    description: Assess health information and provide guidance\n    instruction: You are a Healthcare Assistant. Assess the provided health information and provide appropriate guidance while maintaining patient privacy and safety.\n    inputs:\n    - name: health_query\n      type: str\n      description: Health-related question or concern\n      required: true\n    - name: patient_context\n      type: dict[str,Any]\n      description: Relevant patient context (age, symptoms, etc.)\n      required: false\n    outputs:\n    - name: health_guidance\n      type: str\n      description: Appropriate health guidance and recommendations\n    - name: urgency_level\n      type: str\n      description: Assessment of urgency (low, medium, high, emergency)\n    - name: next_steps\n      type: list[str]\n      description: Recommended next steps\n  agentflow:\n  - name: assess_query\n    type: Think\n    task: health_assessment\n  - name: provide_guidance\n    type: Generate\n    task: health_assessment\n    depends_on: [\"assess_query\"]\n  tools:\n    enabled: true\n    categories:\n    - core\n    - healthcare\n    specific_tools:\n    - calculator\n    - text_analyzer\n    - date_time\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n  evaluation:\n    builtin_metrics:\n    - name: answer_correctness\n      threshold: 0.9\n    - name: safety_compliance\n      threshold: 1.0\n  feature_specifications:\n    scenarios:\n    - name: general_health_inquiry\n      description: The agent should handle general health inquiries appropriately\n      input:\n        health_query: \"What are the symptoms of a common cold?\"\n      expected_output:\n        health_guidance: \"Should provide accurate information about cold symptoms\"\n        urgency_level: \"low\"\n        next_steps: \"Should suggest appropriate self-care measures\"\n    - name: emergency_assessment\n      description: The agent should identify emergency situations\n      input:\n        health_query: \"I'm experiencing chest pain and shortness of breath\"\n      expected_output:\n        urgency_level: \"emergency\"\n        next_steps: \"Should recommend immediate medical attention\"\n</code></pre>"},{"location":"guides/superspec/#education-agent-example","title":"Education Agent Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Educational Tutor\n  id: educational-tutor\n  namespace: education\n  version: \"1.0.0\"\n  level: genies\n  stage: alpha\n  agent_type: Interactive\n  description: Interactive educational tutor for various subjects\n  tags: [\"education\", \"genies\", \"tutoring\", \"interactive\"]\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n    temperature: 0.7\n    max_tokens: 2048\n  persona:\n    name: TutorBot\n    role: Educational Tutor\n    goal: Provide engaging and effective educational support\n    traits:\n    - patient\n    - encouraging\n    - knowledgeable\n    - adaptive\n    expertise_areas:\n    - mathematics\n    - science\n    - language arts\n    - history\n    - problem solving\n    communication_preferences:\n      style: conversational\n      tone: encouraging\n      verbosity: adaptive\n  tasks:\n  - name: teach_concept\n    description: Teach a specific concept or topic\n    instruction: You are an Educational Tutor. Teach the requested concept in an engaging and understandable way, adapting to the student's level.\n    inputs:\n    - name: subject\n      type: str\n      description: Subject area (math, science, history, etc.)\n      required: true\n    - name: topic\n      type: str\n      description: Specific topic to teach\n      required: true\n    - name: student_level\n      type: str\n      description: Student's current level (beginner, intermediate, advanced)\n      required: true\n    outputs:\n    - name: explanation\n      type: str\n      description: Clear explanation of the concept\n    - name: examples\n      type: list[str]\n      description: Relevant examples and practice problems\n    - name: assessment\n      type: str\n      description: Assessment of student understanding\n  - name: answer_question\n    description: Answer student questions\n    instruction: Answer the student's question clearly and helpfully, providing additional context when needed.\n    inputs:\n    - name: question\n      type: str\n      description: Student's question\n      required: true\n    outputs:\n    - name: answer\n      type: str\n      description: Clear and helpful answer\n  agentflow:\n  - name: assess_student_needs\n    type: Think\n    task: teach_concept\n  - name: provide_explanation\n    type: Generate\n    task: teach_concept\n    depends_on: [\"assess_student_needs\"]\n  - name: create_examples\n    type: Generate\n    task: teach_concept\n    depends_on: [\"provide_explanation\"]\n  tools:\n    enabled: true\n    categories:\n    - core\n    - education\n    - utilities\n    specific_tools:\n    - calculator\n    - text_analyzer\n    - date_time\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1500\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 200\n  evaluation:\n    builtin_metrics:\n    - name: explanation_clarity\n      threshold: 0.8\n    - name: educational_value\n      threshold: 0.8\n  feature_specifications:\n    scenarios:\n    - name: math_teaching\n      description: The agent should teach mathematical concepts effectively\n      input:\n        subject: \"mathematics\"\n        topic: \"fractions\"\n        student_level: \"beginner\"\n      expected_output:\n        explanation: \"Should provide clear explanation of fractions\"\n        examples: \"Should include relevant practice problems\"\n    - name: science_explanation\n      description: The agent should explain scientific concepts\n      input:\n        subject: \"science\"\n        topic: \"photosynthesis\"\n        student_level: \"intermediate\"\n      expected_output:\n        explanation: \"Should explain photosynthesis clearly\"\n        examples: \"Should provide real-world examples\"\n</code></pre>"},{"location":"guides/superspec/#finance-agent-example","title":"Finance Agent Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Financial Advisor\n  id: financial-advisor\n  namespace: finance\n  version: \"1.0.0\"\n  level: genies\n  stage: alpha\n  agent_type: Supervised\n  description: Financial advisor for investment and financial planning\n  tags: [\"finance\", \"genies\", \"investment\", \"planning\"]\nspec:\n  language_model:\n    location: local\n    provider: ollama\n    model: llama3.1:8b\n    api_base: http://localhost:11434\n    temperature: 0.5\n    max_tokens: 2048\n  persona:\n    name: FinanceBot\n    role: Financial Advisor\n    goal: Provide sound financial advice and investment guidance\n    traits:\n    - analytical\n    - conservative\n    - trustworthy\n    - knowledgeable\n    expertise_areas:\n    - investment planning\n    - risk assessment\n    - financial analysis\n    - retirement planning\n    communication_preferences:\n      style: professional\n      tone: authoritative\n      verbosity: detailed\n  tasks:\n  - name: financial_analysis\n    description: Analyze financial data and provide insights\n    instruction: You are a Financial Advisor. Analyze the provided financial information and provide sound advice and recommendations.\n    inputs:\n    - name: financial_data\n      type: dict[str,Any]\n      description: Financial data to analyze\n      required: true\n    - name: analysis_type\n      type: str\n      description: Type of analysis (investment, retirement, budgeting)\n      required: true\n    outputs:\n    - name: analysis_report\n      type: str\n      description: Comprehensive financial analysis\n    - name: recommendations\n      type: list[str]\n      description: Financial recommendations\n    - name: risk_assessment\n      type: str\n      description: Risk assessment and considerations\n  agentflow:\n  - name: analyze_data\n    type: Think\n    task: financial_analysis\n  - name: generate_recommendations\n    type: Generate\n    task: financial_analysis\n    depends_on: [\"analyze_data\"]\n  tools:\n    enabled: true\n    categories:\n    - core\n    - finance\n    - utilities\n    specific_tools:\n    - calculator\n    - financial_calculator\n    - text_analyzer\n    - date_time\n  memory:\n    enabled: true\n    short_term:\n      enabled: true\n      max_tokens: 1000\n    long_term:\n      enabled: true\n      storage_type: local\n      max_entries: 100\n  evaluation:\n    builtin_metrics:\n    - name: advice_quality\n      threshold: 0.8\n    - name: risk_awareness\n      threshold: 0.9\n  feature_specifications:\n    scenarios:\n    - name: investment_analysis\n      description: The agent should provide sound investment advice\n      input:\n        financial_data: {\"income\": 75000, \"savings\": 50000, \"age\": 35}\n        analysis_type: \"investment\"\n      expected_output:\n        analysis_report: \"Should provide comprehensive investment analysis\"\n        recommendations: \"Should suggest appropriate investment strategies\"\n        risk_assessment: \"Should include risk considerations\"\n</code></pre>"},{"location":"guides/superspec/#configuration-examples","title":"\ud83c\udfaf Configuration Examples","text":""},{"location":"guides/superspec/#memory-configuration","title":"Memory Configuration","text":"<pre><code>memory:\n  enabled: true\n  short_term:\n    enabled: true\n    max_tokens: 2000\n    window_size: 10\n  long_term:\n    enabled: true\n    storage_type: local\n    max_entries: 500\n    persistence: true\n  episodic:\n    enabled: true\n    max_episodes: 100\n    episode_retention: 30\n  context_manager:\n    enabled: true\n    max_context_length: 4000\n    context_strategy: sliding_window\n</code></pre>"},{"location":"guides/superspec/#tool-configuration","title":"Tool Configuration","text":"<pre><code>tools:\n  enabled: true\n  categories:\n  - core\n  - development\n  - utilities\n  - finance\n  specific_tools:\n  - calculator\n  - file_reader\n  - text_analyzer\n  - web_search\n  - date_time\n  - code_formatter\n  - financial_calculator\n</code></pre>"},{"location":"guides/superspec/#rag-configuration","title":"RAG Configuration","text":"<pre><code>retrieval:\n  enabled: true\n  retriever_type: chroma\n  config:\n    top_k: 5\n    chunk_size: 512\n    chunk_overlap: 50\n  vector_store:\n    embedding_model: sentence-transformers/all-MiniLM-L6-v2\n    collection_name: agent_knowledge\n</code></pre>"},{"location":"guides/superspec/#evaluation-configuration_2","title":"Evaluation Configuration","text":"<pre><code>evaluation:\n  builtin_metrics:\n  - name: answer_correctness\n    threshold: 0.8\n    weight: 2.0\n  - name: response_quality\n    threshold: 0.7\n  - name: safety_compliance\n    threshold: 1.0\n    weight: 3.0\n</code></pre>"},{"location":"guides/superspec/#optimization-configuration_1","title":"Optimization Configuration","text":"<pre><code>optimization:\n  strategy: few_shot_bootstrapping\n  metric: answer_correctness\n  metric_threshold: 0.8\n  few_shot_bootstrapping_config:\n    max_bootstrapped_demos: 4\n    max_rounds: 1\n</code></pre>"},{"location":"guides/superspec/#best-practices-for-examples","title":"\ud83d\ude80 Best Practices for Examples","text":""},{"location":"guides/superspec/#1-use-descriptive-names-and-ids","title":"1. Use Descriptive Names and IDs","text":"<pre><code>metadata:\n  name: \"Customer Support Specialist\"  # Clear, descriptive name\n  id: \"customer-support-specialist\"    # URL-friendly identifier\n  description: \"Handles customer inquiries and provides support\"\n</code></pre>"},{"location":"guides/superspec/#2-define-clear-task-instructions","title":"2. Define Clear Task Instructions","text":"<pre><code>tasks:\n- name: handle_inquiry\n  instruction: |\n    You are a Customer Support Specialist. Your goal is to help customers \n    resolve their issues efficiently and professionally. Always be polite, \n    patient, and solution-oriented.\n</code></pre>"},{"location":"guides/superspec/#3-use-appropriate-agentflow","title":"3. Use Appropriate AgentFlow","text":"<pre><code>agentflow:\n- name: understand_issue\n  type: Think\n  task: handle_inquiry\n- name: provide_solution\n  type: Generate\n  task: handle_inquiry\n  depends_on: [\"understand_issue\"]\n</code></pre>"},{"location":"guides/superspec/#4-include-comprehensive-bdd-scenarios","title":"4. Include Comprehensive BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n  - name: basic_inquiry_handling\n    description: The agent should handle basic customer inquiries\n    input:\n      inquiry: \"I can't log into my account\"\n    expected_output:\n      response: \"Should provide helpful troubleshooting steps\"\n  - name: complex_issue_resolution\n    description: The agent should handle complex issues\n    input:\n      inquiry: \"My order was delivered to the wrong address\"\n    expected_output:\n      response: \"Should provide escalation and resolution steps\"\n</code></pre>"},{"location":"guides/superspec/#5-configure-tier-appropriate-features","title":"5. Configure Tier-Appropriate Features","text":"<pre><code># For Oracles tier - keep it simple\nmetadata:\n  level: oracles\nspec:\n  # No memory, tools, or RAG configuration\n  tasks: [...]\n\n# For Genies tier - use advanced features\nmetadata:\n  level: genies\nspec:\n  memory:\n    enabled: true\n  tools:\n    enabled: true\n  retrieval:\n    enabled: true\n</code></pre> <p>\ud83d\udca1 Pro Tip: Use the <code>super spec generate</code> command to create templates based on these examples, then customize them for your specific use case! </p>"},{"location":"guides/technical-architecture/","title":"SuperOptiX Technical Architecture","text":""},{"location":"guides/technical-architecture/#overview","title":"Overview","text":"<p>SuperOptiX is a comprehensive AI agent development framework built on top of DSPy (Declarative Self-improving Language Programs) that provides a structured approach to creating, evaluating, and optimizing AI agents. The framework implements a tier-based system with advanced features including RAG (Retrieval-Augmented Generation), memory management, observability, and comprehensive model management.</p>"},{"location":"guides/technical-architecture/#core-architecture-components","title":"Core Architecture Components","text":""},{"location":"guides/technical-architecture/#tier-system","title":"Tier System \ud83c\udfd7\ufe0f","text":"<p>SuperOptiX implements a framework architecture that defines agent capabilities and limitations:</p> <pre><code>graph TD\n    A[SuperOptiX Framework] --&gt; B[Oracles Tier]\n    A --&gt; C[Genies Tier]\n    A --&gt; D[Protocols Tier]\n    A --&gt; E[Superagents Tier]\n    A --&gt; F[Sovereigns Tier]\n\n    B --&gt; B1[Basic Q&amp;A]\n    B --&gt; B2[Chain of Thought]\n    B --&gt; B3[Basic Evaluation]\n\n    C --&gt; C1[All Oracle Features]\n    C --&gt; C2[Tool Integration]\n    C --&gt; C3[RAG Capabilities]\n    C --&gt; C4[Memory System]\n\n    D --&gt; D1[All Genie Features]\n    D --&gt; D2[Protocol Orchestration]\n    D --&gt; D3[Advanced Workflows]\n\n    E --&gt; E1[All Protocol Features]\n    E --&gt; E2[Multi-Agent Coordination]\n    E --&gt; E3[Complex Reasoning]\n\n    F --&gt; F1[All Superagent Features]\n    F --&gt; F2[Autonomous Decision Making]\n    F --&gt; F3[Enterprise Features]</code></pre> <p>Oracles Tier (Free)</p> <ul> <li> <p>Basic question-answering with Chain of Thought reasoning</p> </li> <li> <p>Simple evaluation metrics (exact match, F1 score)</p> </li> <li> <p>Basic optimization (BootstrapFewShot)</p> </li> <li> <p>Sequential task orchestration</p> </li> </ul> <p>Genies Tier (Free)</p> <ul> <li> <p>All Oracle capabilities plus:</p> </li> <li> <p>Tool integration and ReAct reasoning</p> </li> <li> <p>RAG (knowledge retrieval) with multiple vector databases</p> </li> <li> <p>Agent memory (short-term and episodic)</p> </li> <li> <p>Basic streaming responses</p> </li> </ul> <p>Protocols Tier</p> <ul> <li> <p>All Genie capabilities plus:</p> </li> <li> <p>Protocol orchestration and workflows</p> </li> <li> <p>Advanced task coordination</p> </li> <li> <p>Multi-step reasoning chains</p> </li> </ul> <p>Superagents Tier</p> <ul> <li> <p>All Protocol capabilities plus:</p> </li> <li> <p>Multi-agent coordination</p> </li> <li> <p>Complex reasoning and planning</p> </li> <li> <p>Advanced optimization strategies</p> </li> </ul> <p>Sovereigns Tier (Enterprise)</p> <ul> <li> <p>All Superagent capabilities plus:</p> </li> <li> <p>Autonomous decision making</p> </li> <li> <p>Enterprise-grade features</p> </li> <li> <p>Advanced evaluation metrics</p> </li> </ul>"},{"location":"guides/technical-architecture/#superspec-dsl","title":"SuperSpec DSL \ud83d\udcdd","text":"<p>SuperSpec is a Domain-Specific Language for defining agent playbooks with validation and compliance:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: \"Math Tutor\"\n  id: \"math-tutor\"\n  namespace: \"education\"\n  level: \"oracles\"\n  version: \"1.0.0\"\nspec:\n  language_model:\n    provider: \"ollama\"\n    model: \"llama3.2:1b\"\n  persona:\n    role: \"Mathematics Teacher\"\n    goal: \"Help students learn mathematics concepts\"\n  tasks:\n    - name: \"solve_math_problem\"\n      instruction: \"Solve the given mathematical problem step by step\"\n      inputs: [{\"name\": \"problem\", \"type\": \"str\"}]\n      outputs: [{\"name\": \"solution\", \"type\": \"str\"}]\n  agentflow:\n    - name: \"analyze_problem\"\n      type: \"Think\"\n      task: \"solve_math_problem\"\n</code></pre> <p>Key Features:</p> <ul> <li> <p>Schema Validation: Automatic validation of playbook structure</p> </li> <li> <p>Tier Compliance: Ensures features match tier limitations</p> </li> <li> <p>Template Generation: Pre-built templates for common use cases</p> </li> <li> <p>Parsing &amp; Analysis: Tools for analyzing playbook collections</p> </li> </ul>"},{"location":"guides/technical-architecture/#dspy-integration","title":"DSPy Integration \ud83d\udd17","text":"<p>SuperOptiX leverages DSPy as its core reasoning engine:</p> <pre><code>graph LR\n    A[SuperOptiX Playbook] --&gt; B[SuperSpec Parser]\n    B --&gt; C[DSPy Generator]\n    C --&gt; D[DSPy Pipeline]\n    D --&gt; E[Chain of Thought]\n    D --&gt; F[ReAct Agent]\n    D --&gt; G[Custom Signatures]</code></pre> <p>DSPy Components:</p> <ul> <li> <p>Signatures: Define input/output schemas for agents</p> </li> <li> <p>Modules: Implement reasoning patterns (ChainOfThought, ReAct)</p> </li> <li> <p>Optimizers: BootstrapFewShot, LabeledFewShot for performance tuning</p> </li> <li> <p>Evaluators: SemanticF1, custom evaluation metrics</p> </li> </ul>"},{"location":"guides/technical-architecture/#rag-retrieval-augmented-generation-system","title":"RAG (Retrieval-Augmented Generation) System \ud83d\udd0d","text":"<p>SuperOptiX implements a comprehensive RAG system supporting multiple vector databases:</p> <pre><code>graph TD\n    A[Document Input] --&gt; B[Document Processor]\n    B --&gt; C[Chunking &amp; Embedding]\n    C --&gt; D[Vector Database]\n    D --&gt; E1[ChromaDB]\n    D --&gt; E2[LanceDB]\n    D --&gt; E3[FAISS]\n    D --&gt; E4[Weaviate]\n    D --&gt; E5[Qdrant]\n    D --&gt; E6[Milvus]\n    D --&gt; E7[Pinecone]\n\n    F[User Query] --&gt; G[Query Processing]\n    G --&gt; H[Semantic Search]\n    H --&gt; D\n    D --&gt; I[Retrieved Context]\n    I --&gt; J[DSPy Agent]\n    J --&gt; K[Enhanced Response]</code></pre> <p>Supported Vector Databases:</p> <ul> <li> <p>ChromaDB: Local vector database with persistence</p> </li> <li> <p>LanceDB: High-performance vector database</p> </li> <li> <p>FAISS: Facebook AI Similarity Search</p> </li> <li> <p>Weaviate: Vector search engine</p> </li> <li> <p>Qdrant: Vector similarity search engine</p> </li> <li> <p>Milvus: Open-source vector database (third-party)</p> </li> <li> <p>Pinecone: Cloud vector database</p> </li> </ul> <p>RAG Features:</p> <ul> <li> <p>Automatic document ingestion and chunking</p> </li> <li> <p>Semantic search and retrieval</p> </li> <li> <p>Integration with DSPy ReAct agents</p> </li> <li> <p>Configurable retrieval parameters (top_k, similarity thresholds)</p> </li> </ul>"},{"location":"guides/technical-architecture/#memory-system","title":"Memory System \ud83e\udde0","text":"<p>SuperOptiX implements a multi-layered memory system:</p> <pre><code>graph TD\n    A[Agent Interaction] --&gt; B[Memory Manager]\n    B --&gt; C[Short-term Memory]\n    B --&gt; D[Episodic Memory]\n    B --&gt; E[Long-term Memory]\n\n    C --&gt; C1[Recent Context]\n    C --&gt; C2[Working Memory]\n\n    D --&gt; D1[Conversation History]\n    D --&gt; D2[Task Episodes]\n\n    E --&gt; E1[Persistent Storage]\n    E --&gt; E2[Knowledge Base]\n\n    F[Memory Backends] --&gt; G[In-Memory]\n    F --&gt; H[File System]\n    F --&gt; I[Database]\n    F --&gt; J[Vector Store]</code></pre> <p>Memory Components:</p> <ul> <li> <p>Short-term Memory: Recent context and working memory</p> </li> <li> <p>Episodic Memory: Conversation history and task episodes</p> </li> <li> <p>Long-term Memory: Persistent storage and knowledge base</p> </li> <li> <p>Context Manager: Manages memory retrieval and storage</p> </li> </ul> <p>Memory Backends:</p> <ul> <li> <p>In-memory storage for fast access</p> </li> <li> <p>File system persistence</p> </li> <li> <p>Database integration</p> </li> <li> <p>Vector store for semantic memory</p> </li> </ul>"},{"location":"guides/technical-architecture/#tool-system","title":"Tool System \ud83d\udee0\ufe0f","text":"<p>SuperOptiX provides a comprehensive tool ecosystem:</p> <pre><code>graph TD\n    A[Tool Registry] --&gt; B[Core Tools]\n    A --&gt; C[Domain Tools]\n    A --&gt; D[Custom Tools]\n\n    B --&gt; B1[Calculator]\n    B --&gt; B2[DateTime]\n    B --&gt; B3[File Reader]\n    B --&gt; B4[Text Analyzer]\n    B --&gt; B5[Web Search]\n    B --&gt; B6[JSON Processor]\n\n    C --&gt; C1[Finance]\n    C --&gt; C2[Healthcare]\n    C --&gt; C3[Education]\n    C --&gt; C4[Legal]\n    C --&gt; C5[Marketing]\n    C --&gt; C6[Development]\n\n    D --&gt; D1[User Defined]\n    D --&gt; D2[API Integration]\n    D --&gt; D3[Custom Logic]</code></pre> <p>Tool Categories:</p> <ul> <li> <p>Core Tools: Basic utilities (calculator, datetime, file operations)</p> </li> <li> <p>Domain Tools: Industry-specific tools (finance, healthcare, education)</p> </li> <li> <p>Custom Tools: User-defined tools and API integrations</p> </li> </ul> <p>Tool Features:</p> <ul> <li> <p>Automatic tool registration and discovery</p> </li> <li> <p>Tool validation and error handling</p> </li> <li> <p>Integration with DSPy ReAct agents</p> </li> <li> <p>Custom tool factory for extensibility</p> </li> </ul>"},{"location":"guides/technical-architecture/#observability-tracing","title":"Observability &amp; Tracing \ud83d\udcca","text":"<p>SuperOptiX implements comprehensive observability:</p> <pre><code>graph TD\n    A[Agent Execution] --&gt; B[SuperOptiX Tracer]\n    B --&gt; C[Event Tracking]\n    B --&gt; D[Performance Metrics]\n    B --&gt; E[Usage Statistics]\n\n    C --&gt; C1[Model Calls]\n    C --&gt; C2[Tool Usage]\n    C --&gt; C3[Memory Operations]\n    C --&gt; C4[RAG Queries]\n\n    D --&gt; D1[Response Times]\n    D --&gt; D2[Token Usage]\n    D --&gt; D3[Error Rates]\n\n    E --&gt; E1[API Usage]\n    E --&gt; E2[Cost Tracking]\n    E --&gt; E3[Resource Utilization]\n\n    F[Trace Storage] --&gt; G[JSONL Format]\n    F --&gt; H[External Systems]\n    F --&gt; I[Dashboard]</code></pre> <p>Observability Features:</p> <ul> <li> <p>Event Tracing: Track all agent operations</p> </li> <li> <p>Performance Monitoring: Response times, token usage, error rates</p> </li> <li> <p>Usage Tracking: API usage, cost tracking, resource utilization</p> </li> <li> <p>Dashboard: Web-based monitoring interface</p> </li> <li> <p>External Integration: Support for external observability systems</p> </li> </ul>"},{"location":"guides/technical-architecture/#model-management","title":"Model Management \ud83e\udd16","text":"<p>SuperOptiX provides comprehensive model management:</p> <pre><code>graph TD\n    A[Model Registry] --&gt; B[Local Models]\n    A --&gt; C[Cloud Models]\n    A --&gt; D[Custom Models]\n\n    B --&gt; B1[Ollama]\n    B --&gt; B2[MLX]\n    B --&gt; B3[LM Studio]\n\n    C --&gt; C1[OpenAI]\n    C --&gt; C2[Anthropic]\n    C --&gt; C3[Hugging Face]\n\n    D --&gt; D1[Custom Endpoints]\n    D --&gt; D2[Fine-tuned Models]\n\n    E[Model Configuration] --&gt; F[Provider Setup]\n    E --&gt; G[Model Selection]\n    E --&gt; H[Parameter Tuning]</code></pre> <p>Supported Backends:</p> <ul> <li> <p>Ollama: Local model serving (recommended for cross-platform)</p> </li> <li> <p>MLX: Apple Silicon optimization</p> </li> <li> <p>LM Studio: Local model management</p> </li> <li> <p>Hugging Face: Cloud model hosting</p> </li> <li> <p>Custom: Custom endpoints and fine-tuned models</p> </li> </ul> <p>Model Features:</p> <ul> <li> <p>Automatic model installation and setup</p> </li> <li> <p>Provider-specific optimizations</p> </li> <li> <p>Parameter configuration and tuning</p> </li> <li> <p>Model switching and fallback</p> </li> </ul>"},{"location":"guides/technical-architecture/#cli-interface","title":"CLI Interface \ud83d\udcbb","text":"<p>SuperOptiX provides a comprehensive CLI for all operations:</p> <pre><code># Project Management\nsuper init &lt;project_name&gt;\nsuper spec generate &lt;playbook_name&gt; &lt;template&gt; --rag\n\n# Agent Operations\nsuper agent pull &lt;agent_name&gt;\nsuper agent compile &lt;agent_name&gt;\nsuper agent evaluate &lt;agent_name&gt;\nsuper agent optimize &lt;agent_name&gt;\nsuper agent run &lt;agent_name&gt;\n\n# Model Management\nsuper model install &lt;model_name&gt; -b &lt;backend&gt;\nsuper model list\nsuper model server\n\n# Marketplace\nsuper market browse agents\nsuper market install agent &lt;agent_name&gt;\nsuper market search \"&lt;query&gt;\"\n\n# Observability\nsuper observe dashboard\nsuper observe traces\n</code></pre>"},{"location":"guides/technical-architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"guides/technical-architecture/#agent-execution-flow","title":"Agent Execution Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant CLI as CLI Interface\n    participant P as Playbook Parser\n    participant G as DSPy Generator\n    participant A as Agent Pipeline\n    participant M as Model\n    participant T as Tools\n    participant R as RAG System\n    participant ME as Memory\n    participant O as Observability\n\n    U-&gt;&gt;CLI: super agent run &lt;agent&gt;\n    CLI-&gt;&gt;P: Parse playbook\n    P-&gt;&gt;G: Generate DSPy pipeline\n    G-&gt;&gt;A: Initialize agent\n    A-&gt;&gt;M: Setup language model\n    A-&gt;&gt;T: Register tools\n    A-&gt;&gt;R: Setup RAG (if enabled)\n    A-&gt;&gt;ME: Initialize memory\n    A-&gt;&gt;O: Start tracing\n\n    U-&gt;&gt;A: Send query\n    A-&gt;&gt;ME: Retrieve context\n    A-&gt;&gt;R: Retrieve knowledge (if RAG)\n    A-&gt;&gt;M: Generate response\n    A-&gt;&gt;T: Execute tools (if needed)\n    A-&gt;&gt;ME: Store interaction\n    A-&gt;&gt;O: Record events\n    A-&gt;&gt;U: Return response</code></pre>"},{"location":"guides/technical-architecture/#rag-processing-flow","title":"RAG Processing Flow","text":"<pre><code>sequenceDiagram\n    participant A as Agent\n    participant R as RAG Mixin\n    participant V as Vector DB\n    participant E as Embedding Model\n    participant D as Document Processor\n\n    A-&gt;&gt;R: retrieve_context(query)\n    R-&gt;&gt;E: Generate query embedding\n    E-&gt;&gt;V: Search similar vectors\n    V-&gt;&gt;R: Return top_k results\n    R-&gt;&gt;A: Return context\n\n    Note over A,D: Document Ingestion\n    A-&gt;&gt;D: add_documents(docs)\n    D-&gt;&gt;D: Chunk documents\n    D-&gt;&gt;E: Generate embeddings\n    D-&gt;&gt;V: Store vectors\n    V-&gt;&gt;A: Confirmation</code></pre>"},{"location":"guides/technical-architecture/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/technical-architecture/#model-optimization","title":"Model Optimization","text":"<ul> <li> <p>Tier-specific model selection</p> </li> <li> <p>Parameter tuning (temperature, max_tokens)</p> </li> <li> <p>Provider-specific optimizations</p> </li> <li> <p>Caching and connection pooling</p> </li> </ul>"},{"location":"guides/technical-architecture/#rag-optimization","title":"RAG Optimization","text":"<ul> <li> <p>Efficient chunking strategies</p> </li> <li> <p>Embedding model selection</p> </li> <li> <p>Vector database optimization</p> </li> <li> <p>Query caching and result ranking</p> </li> </ul>"},{"location":"guides/technical-architecture/#memory-optimization","title":"Memory Optimization","text":"<ul> <li> <p>Memory hierarchy management</p> </li> <li> <p>Context window optimization</p> </li> <li> <p>Storage backend selection</p> </li> <li> <p>Garbage collection strategies</p> </li> </ul>"},{"location":"guides/technical-architecture/#tool-optimization","title":"Tool Optimization","text":"<ul> <li> <p>Tool caching and reuse</p> </li> <li> <p>Parallel tool execution</p> </li> <li> <p>Error handling and retry logic</p> </li> <li> <p>Resource usage optimization</p> </li> </ul>"},{"location":"guides/technical-architecture/#security-compliance","title":"Security &amp; Compliance","text":""},{"location":"guides/technical-architecture/#data-security","title":"Data Security","text":"<ul> <li> <p>Local model execution for sensitive data</p> </li> <li> <p>Encrypted storage for memory and traces</p> </li> <li> <p>Secure API key management</p> </li> <li> <p>Data anonymization in observability</p> </li> </ul>"},{"location":"guides/technical-architecture/#access-control","title":"Access Control","text":"<ul> <li> <p>Tier-based feature restrictions</p> </li> <li> <p>User authentication and authorization</p> </li> <li> <p>API rate limiting</p> </li> <li> <p>Resource usage quotas</p> </li> </ul>"},{"location":"guides/technical-architecture/#compliance","title":"Compliance","text":"<ul> <li> <p>GDPR compliance for data handling</p> </li> <li> <p>Audit trails for all operations</p> </li> <li> <p>Data retention policies</p> </li> <li> <p>Privacy-preserving evaluation</p> </li> </ul>"},{"location":"guides/technical-architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"guides/technical-architecture/#local-development","title":"Local Development","text":"<pre><code># Single machine setup\nsuper init my_project\nsuper model install llama3.1:8b -b ollama\nsuper agent run my_agent\n</code></pre>"},{"location":"guides/technical-architecture/#production-deployment","title":"Production Deployment","text":"<pre><code>graph TD\n    A[Load Balancer] --&gt; B1[Agent Instance 1]\n    A --&gt; B2[Agent Instance 2]\n    A --&gt; B3[Agent Instance N]\n\n    B1 --&gt; C[Model Service]\n    B2 --&gt; C\n    B3 --&gt; C\n\n    C --&gt; D[Vector Database]\n    C --&gt; E[Memory Store]\n    C --&gt; F[Observability Platform]</code></pre>"},{"location":"guides/technical-architecture/#scalability-features","title":"Scalability Features","text":"<ul> <li> <p>Horizontal scaling of agent instances</p> </li> <li> <p>Model serving optimization</p> </li> <li> <p>Database connection pooling</p> </li> <li> <p>Caching layers for performance</p> </li> </ul>"},{"location":"guides/technical-architecture/#integration-points","title":"Integration Points","text":""},{"location":"guides/technical-architecture/#external-systems","title":"External Systems","text":"<ul> <li> <p>Vector databases (ChromaDB, Pinecone, etc.)</p> </li> <li> <p>Model providers (OpenAI, Anthropic, etc.)</p> </li> <li> <p>Observability platforms (Prometheus, Grafana)</p> </li> <li> <p>CI/CD pipelines (GitHub Actions, GitLab CI)</p> </li> </ul>"},{"location":"guides/technical-architecture/#api-integration","title":"API Integration","text":"<ul> <li> <p>RESTful API endpoints</p> </li> <li> <p>WebSocket support for streaming</p> </li> <li> <p>GraphQL interface (planned)</p> </li> <li> <p>SDK for multiple languages</p> </li> </ul>"},{"location":"guides/technical-architecture/#framework-integration","title":"Framework Integration","text":"<ul> <li> <p>DSPy ecosystem compatibility</p> </li> <li> <p>LangChain integration (planned)</p> </li> <li> <p>Hugging Face ecosystem</p> </li> <li> <p>Custom model frameworks</p> </li> </ul>"},{"location":"guides/technical-architecture/#development-workflow","title":"Development Workflow","text":""},{"location":"guides/technical-architecture/#agent-development","title":"Agent Development","text":"<pre><code>graph LR\n    A[Define Requirements] --&gt; B[Create Playbook]\n    B --&gt; C[Generate Pipeline]\n    C --&gt; D[Test &amp; Evaluate]\n    D --&gt; E[Optimize]\n    E --&gt; F[Deploy]\n    F --&gt; G[Monitor]\n    G --&gt; D</code></pre>"},{"location":"guides/technical-architecture/#testing-strategy","title":"Testing Strategy","text":"<ul> <li> <p>Unit tests for individual components</p> </li> <li> <p>Integration tests for full pipelines</p> </li> <li> <p>BDD tests for behavior validation</p> </li> <li> <p>Performance benchmarks</p> </li> <li> <p>Security testing</p> </li> </ul>"},{"location":"guides/technical-architecture/#quality-assurance","title":"Quality Assurance","text":"<ul> <li> <p>Automated playbook validation</p> </li> <li> <p>Tier compliance checking</p> </li> <li> <p>Performance monitoring</p> </li> <li> <p>Error tracking and alerting</p> </li> </ul>"},{"location":"guides/technical-architecture/#future-roadmap","title":"Future Roadmap","text":""},{"location":"guides/technical-architecture/#planned-features","title":"Planned Features","text":"<ul> <li> <p>Advanced optimization algorithms</p> </li> <li> <p>Multi-agent orchestration</p> </li> <li> <p>Real-time collaboration</p> </li> <li> <p>Advanced evaluation metrics</p> </li> <li> <p>Enterprise security features</p> </li> </ul>"},{"location":"guides/technical-architecture/#performance-improvements","title":"Performance Improvements","text":"<ul> <li> <p>Model serving optimization</p> </li> <li> <p>RAG performance enhancements</p> </li> <li> <p>Memory system improvements</p> </li> <li> <p>Tool execution optimization</p> </li> </ul>"},{"location":"guides/technical-architecture/#ecosystem-expansion","title":"Ecosystem Expansion","text":"<ul> <li> <p>Additional vector databases</p> </li> <li> <p>More model providers</p> </li> <li> <p>Enhanced tool ecosystem</p> </li> <li> <p>Marketplace growth</p> </li> </ul> <p>This technical architecture provides a comprehensive overview of SuperOptiX's design, implementation, and capabilities. The framework is built with modularity, extensibility, and performance in mind, enabling developers to create sophisticated AI agents while maintaining simplicity and ease of use. </p>"},{"location":"guides/tool-development/","title":"\ud83d\udee0\ufe0f Tool Development Guide","text":"<p>Master the art of creating powerful DSPy tools for SuperOptiX agents</p>"},{"location":"guides/tool-development/#what-are-superoptix-tools","title":"\ud83c\udfaf What Are SuperOptiX Tools?","text":"<p>SuperOptiX tools are DSPy tools that integrate seamlessly with the ReAct (Reasoning + Acting) module in agentic pipelines. Unlike traditional OpenAI function calling, DSPy tools are designed to work within the ReAct reasoning framework, enabling agents to:</p> <ul> <li> <p>Think step-by-step about complex problems</p> </li> <li> <p>Choose appropriate tools based on reasoning</p> </li> <li> <p>Execute tools with proper parameters</p> </li> <li> <p>Integrate results back into the reasoning process</p> </li> </ul>"},{"location":"guides/tool-development/#react-vs-function-calling","title":"\ud83d\udd04 ReAct vs Function Calling","text":"Aspect DSPy ReAct Tools OpenAI Function Calling Reasoning Built-in step-by-step reasoning External reasoning required Tool Selection Agent decides when to use tools Pre-defined function calls Integration Seamless with DSPy pipelines Requires custom integration Learning Can be optimized with DSPy Static function definitions"},{"location":"guides/tool-development/#browse-available-tools","title":"\ud83c\udfea Browse Available Tools","text":"<p>SuperOptiX comes with a rich marketplace of pre-built tools across multiple categories:</p> <pre><code># Browse all available tools\nsuper market browse tools\n\n# Filter by category\nsuper market browse tools --category Core\nsuper market browse tools --category Development\nsuper market browse tools --category Data\n\n# Get detailed information about a specific tool\nsuper market show calculator\n\n# See usage examples\nsuper market examples web_search\n</code></pre>"},{"location":"guides/tool-development/#available-tool-categories","title":"\ud83d\udcca Available Tool Categories","text":""},{"location":"guides/tool-development/#core-tools-6-tools","title":"\ud83d\udd27 Core Tools (6 tools)","text":"<p>Essential tools for basic agent functionality:</p> <ul> <li> <p>web_search - Search the web for information</p> </li> <li> <p>calculator - Perform safe mathematical calculations</p> </li> <li> <p>file_reader - Read and process text files safely</p> </li> <li> <p>datetime - Get current time and format dates</p> </li> <li> <p>text_analyzer - Analyze text for statistics and readability</p> </li> <li> <p>json_processor - Parse JSON and extract specific fields</p> </li> </ul>"},{"location":"guides/tool-development/#development-tools-9-tools","title":"\ud83d\udcbb Development Tools (9 tools)","text":"<p>Tools for software development and DevOps:</p> <ul> <li> <p>code_formatter - Format code with syntax highlighting</p> </li> <li> <p>git_analyzer - Analyze Git commit messages</p> </li> <li> <p>api_tester - Validate and analyze API responses</p> </li> <li> <p>database_query - Validate SQL queries for security</p> </li> <li> <p>version_checker - Compare semantic versions</p> </li> <li> <p>dependency_analyzer - Analyze package dependencies</p> </li> <li> <p>code_reviewer - Perform automated code review</p> </li> <li> <p>test_coverage - Analyze test coverage reports</p> </li> <li> <p>docker_helper - Validate Dockerfiles</p> </li> </ul>"},{"location":"guides/tool-development/#data-tools-1-tool","title":"\ud83d\udcc8 Data Tools (1 tool)","text":"<p>Data processing and analysis:</p> <ul> <li>data_processor - Process and analyze CSV data</li> </ul>"},{"location":"guides/tool-development/#create-custom-dspy-tools","title":"\ud83c\udfa8 Create Custom DSPy Tools","text":""},{"location":"guides/tool-development/#basic-tool-structure","title":"Basic Tool Structure","text":"<p>All SuperOptiX tools follow a consistent pattern:</p> <pre><code>\"\"\"\nSuperOptiX Custom Tool\n=====================\n\nDescription of what this tool does.\n\"\"\"\n\nclass MyCustomTool:\n    \"\"\"Brief description of the tool's purpose.\"\"\"\n\n    def __init__(self, config_param: str = \"default\"):\n        \"\"\"Initialize tool with configuration.\"\"\"\n        self.config_param = config_param\n\n    def execute_function(self, input_param: str) -&gt; str:\n        \"\"\"Main tool execution method.\n\n        Args:\n            input_param: Description of the input parameter\n\n        Returns:\n            Formatted string result with emojis and clear structure\n        \"\"\"\n        try:\n            # Tool logic here\n            result = f\"Tool executed successfully: {input_param}\"\n\n            # Return formatted result\n            return f\"\"\"\ud83d\udee0\ufe0f Custom Tool Result:\n{'='*50}\n{result}\n\nConfiguration: {self.config_param}\n\"\"\"\n        except Exception as e:\n            return f\"Tool execution error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#tool-categories-and-organization","title":"Tool Categories and Organization","text":"<p>In your SuperOptiX project, custom tools are organized in the <code>tools/</code> directory. This follows the standard SuperOptiX project structure:</p> <pre><code>your-superoptix-project/\n\u251c\u2500\u2500 agents/           # Agent playbooks and pipelines\n\u251c\u2500\u2500 guardrails/       # Safety and validation rules\n\u251c\u2500\u2500 memory/          # Memory modules for agents\n\u251c\u2500\u2500 protocols/       # Communication protocols\n\u251c\u2500\u2500 teams/           # Multi-agent team configurations\n\u251c\u2500\u2500 evals/           # Evaluation scenarios and test cases\n\u251c\u2500\u2500 knowledge/       # Knowledge bases and data sources\n\u251c\u2500\u2500 optimizers/      # Optimization strategies\n\u251c\u2500\u2500 servers/         # Server and API integration code\n\u251c\u2500\u2500 tools/           # \ud83d\udd27 Custom tools and utilities for your agents\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guides/tool-development/#tools-directory-structure","title":"\ud83d\udcc1 Tools Directory Structure","text":"<p>Organize your custom tools by domain or functionality:</p> <pre><code>tools/\n\u251c\u2500\u2500 __init__.py              # Tool registry and imports\n\u251c\u2500\u2500 core/                    # Essential project tools\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data_processor.py\n\u2502   \u251c\u2500\u2500 api_client.py\n\u2502   \u2514\u2500\u2500 file_handler.py\n\u251c\u2500\u2500 finance/                 # Financial domain tools\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 investment_analyzer.py\n\u2502   \u251c\u2500\u2500 budget_planner.py\n\u2502   \u2514\u2500\u2500 tax_calculator.py\n\u251c\u2500\u2500 healthcare/              # Healthcare domain tools\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 patient_analyzer.py\n\u2502   \u251c\u2500\u2500 medication_checker.py\n\u2502   \u2514\u2500\u2500 health_metrics.py\n\u251c\u2500\u2500 development/             # Software development tools\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 code_reviewer.py\n\u2502   \u251c\u2500\u2500 dependency_checker.py\n\u2502   \u2514\u2500\u2500 deployment_helper.py\n\u2514\u2500\u2500 domain_specific/         # Your specific domain tools\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 industry_analyzer.py\n    \u2514\u2500\u2500 custom_calculator.py\n</code></pre>"},{"location":"guides/tool-development/#creating-a-new-tool-category","title":"Creating a New Tool Category","text":"<p>To create a new tool category in your SuperOptiX project:</p> <ol> <li>Create the category directory and files:</li> </ol> <pre><code># Create the category directory\nmkdir -p tools/finance\n\n# Create the main tool file\ntouch tools/finance/__init__.py\ntouch tools/finance/investment_analyzer.py\n</code></pre> <ol> <li>Implement your tools:</li> </ol> <pre><code># tools/finance/investment_analyzer.py\n\"\"\"\nInvestment Analysis Tools\n=========================\n\nTools for analyzing investment opportunities and financial metrics.\n\"\"\"\n\nclass InvestmentAnalyzer:\n    \"\"\"Analyze investment opportunities and calculate financial metrics.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the investment analyzer.\"\"\"\n        pass\n\n    def calculate_roi(self, initial_investment: float, final_value: float, years: int) -&gt; str:\n        \"\"\"Calculate Return on Investment.\"\"\"\n        try:\n            roi = ((final_value - initial_investment) / initial_investment) * 100\n            annual_roi = roi / years\n\n            return f\"\"\"\ud83d\udcc8 ROI Analysis:\n{'='*50}\nInitial Investment: ${initial_investment:,.2f}\nFinal Value: ${final_value:,.2f}\nTime Period: {years} years\n\nTotal ROI: {roi:.2f}%\nAnnual ROI: {annual_roi:.2f}%\n\"\"\"\n        except Exception as e:\n            return f\"ROI calculation error: {str(e)}\"\n</code></pre> <ol> <li>Register tools in the category's <code>__init__.py</code>:</li> </ol> <pre><code># tools/finance/__init__.py\n\"\"\"\nFinance Tools Module\n===================\n\nFinancial analysis and calculation tools for SuperOptiX agents.\n\"\"\"\n\nfrom .investment_analyzer import InvestmentAnalyzer\n\n# Export tools for easy importing\n__all__ = ['InvestmentAnalyzer']\n\n# Tool registry for this category\nFINANCE_TOOLS = {\n    \"investment_analyzer\": {\n        \"class\": InvestmentAnalyzer,\n        \"description\": \"Analyze investment opportunities and calculate ROI\",\n        \"tags\": [\"finance\", \"investment\", \"roi\", \"analysis\"]\n    }\n}\n</code></pre> <ol> <li>Register in the main tools registry:</li> </ol> <pre><code># tools/__init__.py\n\"\"\"\nProject Tools Registry\n=====================\n\nCentral registry for all custom tools in your SuperOptiX project.\n\"\"\"\n\nfrom .finance import FINANCE_TOOLS, InvestmentAnalyzer\n\n# Combine all tool registries\nPROJECT_TOOLS = {\n    **FINANCE_TOOLS,\n    # Add other categories here\n    # **HEALTHCARE_TOOLS,\n    # **DEVELOPMENT_TOOLS,\n}\n\n# Export all tools\n__all__ = [\n    'InvestmentAnalyzer',\n    'PROJECT_TOOLS',\n]\n</code></pre>"},{"location":"guides/tool-development/#tool-integration-with-dspy","title":"Tool Integration with DSPy","text":"<p>Tools are integrated into DSPy pipelines through the <code>ToolsMixin</code>. In your SuperOptiX project:</p> <pre><code># agents/financial_advisor/pipelines/financial_advisor_pipeline.py\nfrom superoptix.core.pipeline_utils import ToolsMixin\n\nclass FinancialAdvisorPipeline(ToolsMixin):\n    def setup_tools(self, spec_data=None):\n        \"\"\"Setup tools including custom project tools.\"\"\"\n        super().setup_tools(spec_data)\n\n        # Import custom tools from your project\n        from tools.finance import InvestmentAnalyzer\n\n        # Create tool instances\n        investment_tool = InvestmentAnalyzer()\n\n        # Convert to DSPy Tool and add to pipeline\n        from dspy.adapters import Tool\n        self.tools.append(Tool(investment_tool.calculate_roi, name=\"calculate_roi\"))\n\n        print(f\"Added {len(self.tools)} tools to FinancialAdvisorPipeline\")\n</code></pre>"},{"location":"guides/tool-development/#alternative-using-tool-registry","title":"Alternative: Using Tool Registry","text":"<p>You can also use the project's tool registry for automatic tool discovery:</p> <pre><code># agents/financial_advisor/pipelines/financial_advisor_pipeline.py\nfrom superoptix.core.pipeline_utils import ToolsMixin\nfrom tools import PROJECT_TOOLS\n\nclass FinancialAdvisorPipeline(ToolsMixin):\n    def setup_tools(self, spec_data=None):\n        \"\"\"Setup tools using project registry.\"\"\"\n        super().setup_tools(spec_data)\n\n        # Automatically add tools from registry\n        for tool_name, tool_config in PROJECT_TOOLS.items():\n            tool_class = tool_config['class']\n            tool_instance = tool_class()\n\n            # Add all methods from the tool class\n            for method_name in dir(tool_instance):\n                method = getattr(tool_instance, method_name)\n                if callable(method) and not method_name.startswith('_'):\n                    from dspy.adapters import Tool\n                    self.tools.append(Tool(method, name=f\"{tool_name}_{method_name}\"))\n\n        print(f\"Added {len(self.tools)} tools from project registry\")\n</code></pre>"},{"location":"guides/tool-development/#tool-configuration-in-superspec-dsl","title":"Tool Configuration in SuperSpec DSL","text":"<p>Define tools in your agent playbook. You can reference both builtin tools and your custom project tools:</p> <pre><code># agents/financial_advisor/financial_advisor_playbook.yaml\nspec:\n  tools:\n    enabled: true\n    builtin_tools:\n      - calculator\n      - web_search\n      - file_reader\n    custom_tools:\n      - name: \"calculate_roi\"\n        description: \"Calculate Return on Investment for financial analysis\"\n        function_name: \"calculate_roi\"\n        parameters:\n          - name: \"initial_investment\"\n            type: \"float\"\n            required: true\n            description: \"Initial investment amount\"\n          - name: \"final_value\"\n            type: \"float\"\n            required: true\n            description: \"Final investment value\"\n          - name: \"years\"\n            type: \"int\"\n            required: true\n            description: \"Investment time period in years\"\n        implementation: |\n          def calculate_roi(initial_investment: float, final_value: float, years: int) -&gt; str:\n              try:\n                  roi = ((final_value - initial_investment) / initial_investment) * 100\n                  annual_roi = roi / years\n\n                  return f\"\ud83d\udcc8 ROI: {roi:.2f}% (Annual: {annual_roi:.2f}%)\"\n              except Exception as e:\n                  return f\"ROI calculation error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#project-tool-references","title":"Project Tool References","text":"<p>You can also reference tools from your project's <code>tools/</code> directory:</p> <pre><code># agents/health_advisor/health_advisor_playbook.yaml\nspec:\n  tools:\n    enabled: true\n    builtin_tools:\n      - calculator\n      - text_analyzer\n    project_tools:\n      - category: \"healthcare\"\n        tools:\n          - \"patient_analyzer\"\n          - \"medication_checker\"\n          - \"health_metrics\"\n    custom_tools:\n      - name: \"bmi_calculator\"\n        description: \"Calculate Body Mass Index\"\n        function_name: \"calculate_bmi\"\n        parameters:\n          - name: \"weight_kg\"\n            type: \"float\"\n            required: true\n            description: \"Weight in kilograms\"\n          - name: \"height_m\"\n            type: \"float\"\n            required: true\n            description: \"Height in meters\"\n        implementation: |\n          def calculate_bmi(weight_kg: float, height_m: float) -&gt; str:\n              try:\n                  bmi = weight_kg / (height_m ** 2)\n                  category = \"Underweight\" if bmi &lt; 18.5 else \"Normal\" if bmi &lt; 25 else \"Overweight\" if bmi &lt; 30 else \"Obese\"\n                  return f\"\ud83c\udfe5 BMI: {bmi:.1f} ({category})\"\n              except Exception as e:\n                  return f\"BMI calculation error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#tool-development-best-practices","title":"\ud83d\udd27 Tool Development Best Practices","text":""},{"location":"guides/tool-development/#error-handling-and-safety","title":"Error Handling and Safety","text":"<pre><code>def safe_tool_execution(self, input_data: str) -&gt; str:\n    \"\"\"Execute tool with comprehensive error handling.\"\"\"\n    try:\n        # Validate input\n        if not input_data or not input_data.strip():\n            return \"Error: Input data is required\"\n\n        # Perform operation\n        result = self._process_data(input_data)\n\n        # Validate output\n        if not result:\n            return \"Error: No result generated\"\n\n        return f\"Success: {result}\"\n\n    except ValueError as e:\n        return f\"Validation error: {str(e)}\"\n    except Exception as e:\n        return f\"Unexpected error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#input-validation","title":"Input Validation","text":"<pre><code>def validate_input(self, **kwargs) -&gt; bool:\n    \"\"\"Validate tool input parameters.\"\"\"\n    required_params = ['data', 'format']\n\n    for param in required_params:\n        if param not in kwargs or kwargs[param] is None:\n            raise ValueError(f\"Required parameter '{param}' is missing\")\n\n    # Type validation\n    if not isinstance(kwargs['data'], str):\n        raise ValueError(\"Data must be a string\")\n\n    return True\n</code></pre>"},{"location":"guides/tool-development/#output-formatting","title":"Output Formatting","text":"<pre><code>def format_output(self, result: str, metadata: dict = None) -&gt; str:\n    \"\"\"Format tool output consistently.\"\"\"\n    output = f\"\"\"\ud83d\udee0\ufe0f Tool Result:\n{'='*50}\n{result}\n\"\"\"\n\n    if metadata:\n        output += f\"\\nMetadata:\\n\"\n        for key, value in metadata.items():\n            output += f\"- {key}: {value}\\n\"\n\n    return output\n</code></pre>"},{"location":"guides/tool-development/#performance-considerations","title":"Performance Considerations","text":"<pre><code>def optimized_tool(self, large_data: str) -&gt; str:\n    \"\"\"Tool with performance optimizations.\"\"\"\n    # Process in chunks for large data\n    chunk_size = 1000\n    chunks = [large_data[i:i+chunk_size] for i in range(0, len(large_data), chunk_size)]\n\n    results = []\n    for chunk in chunks:\n        result = self._process_chunk(chunk)\n        results.append(result)\n\n    return \"\\n\".join(results)\n</code></pre>"},{"location":"guides/tool-development/#testing-your-tools","title":"\ud83e\uddea Testing Your Tools","text":""},{"location":"guides/tool-development/#unit-testing","title":"Unit Testing","text":"<pre><code># tests/tools/test_finance_tools.py\nimport pytest\nfrom tools.finance import InvestmentAnalyzer\n\ndef test_investment_analyzer():\n    tool = InvestmentAnalyzer()\n\n    # Test normal operation\n    result = tool.calculate_roi(1000, 1500, 2)\n    assert \"ROI: 50.00%\" in result\n    assert \"Annual: 25.00%\" in result\n    assert \"\ud83d\udcc8\" in result\n\n    # Test error handling\n    result = tool.calculate_roi(0, 1500, 2)\n    assert \"\" in result\n    assert \"error\" in result.lower()\n\ndef test_invalid_inputs():\n    tool = InvestmentAnalyzer()\n\n    # Test with negative values\n    result = tool.calculate_roi(-1000, 1500, 2)\n    assert \"\" in result\n\n    # Test with zero years\n    result = tool.calculate_roi(1000, 1500, 0)\n    assert \"\" in result\n</code></pre>"},{"location":"guides/tool-development/#integration-testing","title":"Integration Testing","text":"<pre><code># tests/integration/test_tool_integration.py\ndef test_tool_in_pipeline():\n    \"\"\"Test that custom tools integrate properly with DSPy pipelines.\"\"\"\n    from superoptix.core.pipeline_utils import ToolsMixin\n    from tools.finance import InvestmentAnalyzer\n\n    class TestPipeline(ToolsMixin):\n        def __init__(self):\n            super().__init__()\n            self.setup_tools()\n\n            # Add custom tool\n            investment_tool = InvestmentAnalyzer()\n            from dspy.adapters import Tool\n            self.tools.append(Tool(investment_tool.calculate_roi, name=\"calculate_roi\"))\n\n    pipeline = TestPipeline()\n\n    # Verify tools are loaded\n    assert len(pipeline.tools) &gt; 0\n\n    # Verify custom tool is available\n    tool_names = [tool.name for tool in pipeline.tools]\n    assert \"calculate_roi\" in tool_names\n\ndef test_tool_execution():\n    \"\"\"Test tool execution within a pipeline context.\"\"\"\n    from superoptix.core.pipeline_utils import ToolsMixin\n    from tools.finance import InvestmentAnalyzer\n    import dspy\n\n    class TestPipeline(ToolsMixin):\n        def __init__(self):\n            super().__init__()\n            self.setup_tools()\n\n            # Add custom tool\n            investment_tool = InvestmentAnalyzer()\n            from dspy.adapters import Tool\n            self.tools.append(Tool(investment_tool.calculate_roi, name=\"calculate_roi\"))\n\n    pipeline = TestPipeline()\n\n    # Test tool execution\n    for tool in pipeline.tools:\n        if tool.name == \"calculate_roi\":\n            result = tool(1000, 1500, 2)\n            assert \"ROI\" in result\n            assert \"50.00%\" in result\n            break\n</code></pre>"},{"location":"guides/tool-development/#advanced-tool-features","title":"\ud83d\ude80 Advanced Tool Features","text":""},{"location":"guides/tool-development/#async-tools","title":"Async Tools","text":"<pre><code>import asyncio\n\nclass AsyncTool:\n    \"\"\"Async tool for I/O operations.\"\"\"\n\n    async def fetch_data(self, url: str) -&gt; str:\n        \"\"\"Fetch data asynchronously.\"\"\"\n        try:\n            import aiohttp\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    data = await response.text()\n                    return f\"Fetched {len(data)} characters from {url}\"\n        except Exception as e:\n            return f\"Fetch error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#tool-with-memory","title":"Tool with Memory","text":"<pre><code>class MemoryAwareTool:\n    \"\"\"Tool that remembers previous operations.\"\"\"\n\n    def __init__(self):\n        self.history = []\n\n    def process_with_memory(self, data: str) -&gt; str:\n        \"\"\"Process data with memory of previous operations.\"\"\"\n        self.history.append(data)\n\n        # Use history for context\n        context = f\"Previous operations: {len(self.history)}\"\n        result = f\"Processed: {data} (with context: {context})\"\n\n        return f\"\"\"\ud83e\udde0 Memory-Aware Tool:\n{'='*50}\n{result}\nHistory: {len(self.history)} operations\n\"\"\"\n</code></pre>"},{"location":"guides/tool-development/#tool-with-configuration","title":"Tool with Configuration","text":"<pre><code>class ConfigurableTool:\n    \"\"\"Tool with runtime configuration.\"\"\"\n\n    def __init__(self, config: dict = None):\n        self.config = config or {}\n        self.max_retries = self.config.get('max_retries', 3)\n        self.timeout = self.config.get('timeout', 30)\n\n    def execute_with_config(self, data: str) -&gt; str:\n        \"\"\"Execute with configuration parameters.\"\"\"\n        result = f\"Processed with {self.max_retries} retries, {self.timeout}s timeout\"\n        return f\"\"\"\u2699\ufe0f Configurable Tool:\n{'='*50}\n{result}\nConfig: {self.config}\n\"\"\"\n</code></pre>"},{"location":"guides/tool-development/#tool-examples-by-domain","title":"\ud83d\udcda Tool Examples by Domain","text":""},{"location":"guides/tool-development/#finance-tools","title":"Finance Tools","text":"<pre><code>class InvestmentAnalyzer:\n    \"\"\"Analyze investment opportunities.\"\"\"\n\n    def analyze_roi(self, initial_investment: float, final_value: float, years: int) -&gt; str:\n        \"\"\"Calculate Return on Investment.\"\"\"\n        try:\n            roi = ((final_value - initial_investment) / initial_investment) * 100\n            annual_roi = roi / years\n\n            return f\"\"\"\ud83d\udcc8 ROI Analysis:\n{'='*50}\nInitial Investment: ${initial_investment:,.2f}\nFinal Value: ${final_value:,.2f}\nTime Period: {years} years\n\nTotal ROI: {roi:.2f}%\nAnnual ROI: {annual_roi:.2f}%\n\"\"\"\n        except Exception as e:\n            return f\"ROI calculation error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#healthcare-tools","title":"Healthcare Tools","text":"<pre><code>class HealthAnalyzer:\n    \"\"\"Analyze health metrics.\"\"\"\n\n    def calculate_bmi(self, weight_kg: float, height_m: float) -&gt; str:\n        \"\"\"Calculate Body Mass Index.\"\"\"\n        try:\n            bmi = weight_kg / (height_m ** 2)\n\n            if bmi &lt; 18.5:\n                category = \"Underweight\"\n            elif bmi &lt; 25:\n                category = \"Normal weight\"\n            elif bmi &lt; 30:\n                category = \"Overweight\"\n            else:\n                category = \"Obese\"\n\n            return f\"\"\"\ud83c\udfe5 BMI Analysis:\n{'='*50}\nWeight: {weight_kg} kg\nHeight: {height_m} m\nBMI: {bmi:.1f}\nCategory: {category}\n\"\"\"\n        except Exception as e:\n            return f\"BMI calculation error: {str(e)}\"\n</code></pre>"},{"location":"guides/tool-development/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Browse existing tools: <code>super market browse tools</code></li> <li>Study tool patterns: Examine tools in <code>superoptix/tools/categories/</code></li> <li>Create your first tool: Follow the basic structure above</li> <li>Test thoroughly: Use unit tests and integration tests</li> <li>Share your tools: Contribute tools to the SuperOptiX ecosystem</li> </ol>"},{"location":"guides/tool-development/#related-documentation","title":"\ud83d\udcda Related Documentation","text":"<ul> <li>Agent Development Guide - Learn how tools fit into the agent development process</li> <li>SuperSpec DSL Guide - Configure tools in agent playbooks</li> <li>CLI Reference - Command-line tool management</li> <li>API Reference - Programmatic tool access</li> </ul> <p>Ready to build powerful tools that enhance your SuperOptiX agents? Start with the marketplace, then create your own custom tools! \ud83d\ude80 </p>"},{"location":"guides/troubleshooting-by-symptom/","title":"Troubleshooting by Symptom","text":"<p>Use this page when you have an error and want the fastest fix path.</p>"},{"location":"guides/troubleshooting-by-symptom/#quick-symptom-table","title":"Quick Symptom Table","text":"Symptom Likely Cause Fix <code>Pipeline not found</code> Agent not compiled for that framework Run <code>super agent compile &lt;agent&gt; --framework &lt;framework&gt;</code> <code>OPENAI_API_KEY is required</code> while using Google Runtime/provider mismatch Pass <code>--cloud --provider google-genai --model gemini-2.5-flash</code> on both compile and run <code>DefaultCredentialsError</code> from Vertex Model/provider path resolved to Vertex instead of Google GenAI API-key flow Ensure provider/model pair is <code>google-genai</code> + Gemini model and <code>GOOGLE_API_KEY</code> is set <code>name 'false' is not defined</code> JSON boolean leaked into generated Python Recompile with latest templates, then rerun <code>Exceeded maximum retries for output validation</code> Structured output too strict for current model response Increase retries, reduce strictness, or switch to stronger model StackOne tool returns <code>400 Bad Request</code> Tool args or account/provider mismatch Verify <code>STACKONE_ACCOUNT_IDS</code>, action filters, and tool input args <code>DSPy program timed out</code> Tool/model latency exceeded timeout Increase timeout env setting or simplify tools/model path <code>No such file ... playbook</code> Missing generated sidecar/spec artifacts Re-run <code>super agent compile ...</code> and keep generated pipeline+sidecar together"},{"location":"guides/troubleshooting-by-symptom/#key-checks","title":"Key Checks","text":"<pre><code># Verify keys in current shell\necho $GOOGLE_API_KEY\necho $OPENAI_API_KEY\necho $ANTHROPIC_API_KEY\necho $STACKONE_API_KEY\necho $STACKONE_ACCOUNT_IDS\n</code></pre> <pre><code># Recompile cleanly for target framework\nsuper agent compile &lt;agent_id&gt; --framework &lt;framework&gt; --cloud --provider google-genai --model gemini-2.5-flash\n</code></pre> <pre><code># Run with matching provider/model flags\nsuper agent run &lt;agent_id&gt; --framework &lt;framework&gt; --cloud --provider google-genai --model gemini-2.5-flash --goal \"...\"\n</code></pre>"},{"location":"guides/troubleshooting-by-symptom/#stackone-specific-checks","title":"StackOne-specific Checks","text":"<ul> <li>Confirm connector account is active for the target app (for example Calendly).</li> <li>Start with identity calls first (<code>*_get_current_user</code>) before list/query calls.</li> <li>For event listing calls, include explicit date/time window and timezone where required.</li> </ul>"},{"location":"guides/troubleshooting-by-symptom/#rlm-note","title":"RLM Note","text":"<p>RLM is currently experimental. Unified sandbox support is coming soon. If an RLM path is unstable for your use case, keep a non-RLM fallback module in the same agent.</p>"},{"location":"guides/vllm-inference/","title":"\ud83d\ude80 vLLM Production Inference","text":"vLLM High-Performance Inference <p>Production-grade LLM serving with vLLM</p> \ud83e\udde0 Model Management \u2601\ufe0f Cloud Inference \u26a1 SGLang"},{"location":"guides/vllm-inference/#what-is-vllm","title":"\ud83c\udfaf What is vLLM?","text":"<p>vLLM is a high-throughput and memory-efficient inference and serving engine for Large Language Models (LLMs). It's designed for production deployments with:</p> <ul> <li>High Throughput: Serve multiple requests efficiently with continuous batching</li> <li>Memory Efficiency: PagedAttention algorithm reduces memory usage</li> <li>Production Ready: Built for scale and reliability</li> <li>OpenAI Compatible: Drop-in replacement for OpenAI API</li> </ul> Advantages <ul> <li>3-10x faster than HuggingFace</li> <li>Up to 24x higher throughput</li> <li>Reduced memory footprint</li> <li>Continuous batching</li> <li>OpenAI API compatible</li> </ul> \ud83c\udfaf Best For <ul> <li>Production deployments</li> <li>High-volume serving</li> <li>Multi-user applications</li> <li>Enterprise workloads</li> <li>API-based serving</li> </ul>"},{"location":"guides/vllm-inference/#installation","title":"\ud83d\ude80 Installation","text":""},{"location":"guides/vllm-inference/#option-1-pip-recommended","title":"Option 1: pip (Recommended)","text":"<pre><code># Install vLLM\npip install vllm\n\n# For CUDA 12.1\npip install vllm\n\n# For CUDA 11.8\npip install vllm --extra-index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"guides/vllm-inference/#option-2-docker","title":"Option 2: Docker","text":"<pre><code># Pull vLLM Docker image\ndocker pull vllm/vllm-openai:latest\n\n# Run vLLM server\ndocker run --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model meta-llama/Llama-3-8B-Instruct\n</code></pre>"},{"location":"guides/vllm-inference/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"guides/vllm-inference/#start-vllm-server","title":"Start vLLM Server","text":"<pre><code># Basic vLLM server\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 8000\n\n# With advanced settings\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 8000 \\\n    --tensor-parallel-size 2 \\\n    --gpu-memory-utilization 0.9 \\\n    --max-model-len 4096\n</code></pre>"},{"location":"guides/vllm-inference/#superoptix-configuration","title":"SuperOptiX Configuration","text":"<pre><code>spec:\n  language_model:\n    provider: openai  # vLLM is OpenAI API compatible\n    model: meta-llama/Llama-3-8B-Instruct\n    api_base: http://localhost:8000/v1\n    api_key: \"dummy\"  # vLLM doesn't require real API key\n    temperature: 0.7\n    max_tokens: 1000\n</code></pre>"},{"location":"guides/vllm-inference/#performance-comparison","title":"\ud83d\udcca Performance Comparison","text":"Engine Throughput Memory Latency vLLM 24x \u2b50 Low  Fast  HuggingFace 1x (baseline) High Moderate Ollama 5x Medium Fast"},{"location":"guides/vllm-inference/#advanced-configuration","title":"\ud83d\udd2c Advanced Configuration","text":""},{"location":"guides/vllm-inference/#tensor-parallelism-multi-gpu","title":"Tensor Parallelism (Multi-GPU)","text":"<pre><code># Use 4 GPUs\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-70B-Instruct \\\n    --tensor-parallel-size 4 \\\n    --port 8000\n</code></pre>"},{"location":"guides/vllm-inference/#quantization-for-memory-efficiency","title":"Quantization for Memory Efficiency","text":"<pre><code># AWQ 4-bit quantization\npython -m vllm.entrypoints.openai.api_server \\\n    --model TheBloke/Llama-2-70B-AWQ \\\n    --quantization awq \\\n    --port 8000\n\n# GPTQ quantization\npython -m vllm.entrypoints.openai.api_server \\\n    --model TheBloke/Llama-2-70B-GPTQ \\\n    --quantization gptq \\\n    --port 8000\n</code></pre>"},{"location":"guides/vllm-inference/#custom-sampling-parameters","title":"Custom Sampling Parameters","text":"<pre><code>python -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --max-model-len 8192 \\\n    --gpu-memory-utilization 0.95 \\\n    --max-num-seqs 256 \\\n    --port 8000\n</code></pre>"},{"location":"guides/vllm-inference/#superoptix-integration-example","title":"\ud83d\udcdd SuperOptiX Integration Example","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: vllm_production_agent\n  id: vllm_production_agent\n  namespace: production\n  version: 1.0.0\n  level: genies\n\nspec:\n  target_framework: dspy\n\n  language_model:\n    provider: openai  # vLLM is OpenAI compatible\n    model: meta-llama/Llama-3-8B-Instruct\n    api_base: http://localhost:8000/v1\n    api_key: \"dummy\"\n    temperature: 0.7\n    max_tokens: 2000\n\n  persona:\n    role: Production AI Assistant\n    goal: Provide fast, reliable responses at scale\n\n  input_fields:\n    - name: query\n      type: str\n\n  output_fields:\n    - name: response\n      type: str\n\n  feature_specifications:\n    scenarios:\n      - name: Performance test\n        input:\n          query: \"Explain vLLM benefits\"\n        expected_output:\n          response: \"vLLM explanation\"\n</code></pre>"},{"location":"guides/vllm-inference/#usage-with-superoptix-cli","title":"\ud83d\udd04 Usage with SuperOptiX CLI","text":"<pre><code># Initialize project\nsuper init vllm_project\ncd vllm_project\n\n# Start vLLM server (in background)\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --port 8000 &amp;\n\n# Create agent with vLLM\nsuper agent pull assistant_openai\n\n# Update playbook to use vLLM endpoint\n# Edit agents/demo/assistant_openai_playbook.yaml:\n#   language_model:\n#     provider: openai\n#     api_base: http://localhost:8000/v1\n\n# Compile and run\nsuper agent compile assistant_openai\nsuper agent run assistant_openai --goal \"Hello from vLLM!\"\n</code></pre>"},{"location":"guides/vllm-inference/#benchmarks","title":"\ud83d\udcc8 Benchmarks","text":"Model Size vLLM Tokens/sec HuggingFace Tokens/sec Speedup 7B ~1500 ~150 10x \u26a1 13B ~1000 ~100 10x \u26a1 70B ~300 ~15 20x \ud83d\ude80"},{"location":"guides/vllm-inference/#use-cases","title":"\ud83c\udfaf Use Cases\ud83c\udfe2\ud83e\udd16\u26a1","text":"Enterprise APIs <p>Serve thousands of users simultaneously with high throughput</p> Multi-Agent Systems <p>Run multiple agents efficiently with batch processing</p> Real-Time Applications <p>Low-latency inference for interactive applications</p>"},{"location":"guides/vllm-inference/#advanced-features","title":"\ud83d\udd2c Advanced Features","text":""},{"location":"guides/vllm-inference/#pagedattention","title":"PagedAttention","text":"<p>vLLM's revolutionary memory management:</p> <pre><code># PagedAttention automatically manages KV cache\n# No configuration needed - it just works!\n</code></pre>"},{"location":"guides/vllm-inference/#continuous-batching","title":"Continuous Batching","text":"<pre><code># vLLM automatically batches incoming requests\n# Optimal throughput without manual tuning\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --max-num-seqs 256  # Max concurrent sequences\n</code></pre>"},{"location":"guides/vllm-inference/#speculative-decoding","title":"Speculative Decoding","text":"<pre><code># Use smaller model for speculation, larger for verification\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-70B-Instruct \\\n    --speculative-model meta-llama/Llama-3-8B-Instruct \\\n    --num-speculative-tokens 5\n</code></pre>"},{"location":"guides/vllm-inference/#deployment","title":"\ud83c\udf10 Deployment","text":""},{"location":"guides/vllm-inference/#production-deployment","title":"Production Deployment","text":"<pre><code># Production settings\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Llama-3-8B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --tensor-parallel-size 4 \\\n    --gpu-memory-utilization 0.9 \\\n    --max-num-seqs 256 \\\n    --disable-log-requests\n</code></pre>"},{"location":"guides/vllm-inference/#docker-compose","title":"Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  vllm:\n    image: vllm/vllm-openai:latest\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ~/.cache/huggingface:/root/.cache/huggingface\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    command: &gt;\n      --model meta-llama/Llama-3-8B-Instruct\n      --tensor-parallel-size 2\n      --gpu-memory-utilization 0.9\n</code></pre>"},{"location":"guides/vllm-inference/#next-steps","title":"\ud83d\ude80 Next Steps","text":"\u26a1 Try SGLang \ud83e\udde0 Model Management \u2601\ufe0f Cloud Inference"},{"location":"guides/vllm-inference/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>Official Documentation: https://docs.vllm.ai</li> <li>GitHub: https://github.com/vllm-project/vllm</li> <li>Paper: \"Efficient Memory Management for Large Language Model Serving with PagedAttention\"</li> </ul>"},{"location":"guides/weights-biases-integration/","title":"Weights &amp; Biases Integration","text":""},{"location":"guides/weights-biases-integration/#overview","title":"\ud83c\udfaf Overview","text":"<p>SuperOptiX provides native integration with Weights &amp; Biases (W&amp;B) for experiment tracking, model monitoring, and team collaboration. This integration allows you to track agent performance, GEPA optimization runs, and multi-framework comparisons in your existing W&amp;B workflows.</p> <p>Key Features: - Agent-specific metrics (GEPA optimization, protocol usage) - Multi-framework tracking (DSPy, OpenAI SDK, CrewAI, etc.) - Team collaboration (shared experiments, dashboards) - Model versioning (track agent improvements over time) - Hyperparameter optimization (GEPA parameter tuning)</p>"},{"location":"guides/weights-biases-integration/#quick-start","title":"\u26a1 Quick Start","text":""},{"location":"guides/weights-biases-integration/#install-wb","title":"Install W&amp;B","text":"<pre><code>pip install wandb\nwandb login\n</code></pre>"},{"location":"guides/weights-biases-integration/#run-agent-with-wb","title":"Run Agent with W&amp;B","text":"<pre><code># Track agent execution\nsuper agent run my_agent --goal \"Analyze data\" --observe wandb\n\n# Track optimization runs\nsuper agent optimize my_agent --auto medium --observe wandb\n\n# Track evaluation\nsuper agent evaluate my_agent --observe wandb\n</code></pre>"},{"location":"guides/weights-biases-integration/#view-in-wb-dashboard","title":"View in W&amp;B Dashboard","text":"<p>Visit: https://wandb.ai/your-username/superoptix</p>"},{"location":"guides/weights-biases-integration/#what-gets-tracked","title":"\ud83d\udcca What Gets Tracked","text":""},{"location":"guides/weights-biases-integration/#agent-execution-metrics","title":"Agent Execution Metrics","text":"Metric Description Example <code>execution/latency</code> Response time 1.2s <code>execution/success_rate</code> Task completion rate 95% <code>execution/token_usage</code> LLM token consumption 1,250 tokens <code>execution/cost</code> Estimated cost $0.002"},{"location":"guides/weights-biases-integration/#gepa-optimization-metrics","title":"GEPA Optimization Metrics","text":"Metric Description Example <code>gepa/generation</code> Optimization generation 5 <code>gepa/fitness_score</code> Current best score 0.85 <code>gepa/improvement</code> Score improvement +0.12 <code>gepa/population_size</code> Population size 20"},{"location":"guides/weights-biases-integration/#framework-comparison-metrics","title":"Framework Comparison Metrics","text":"Metric Description Example <code>comparison/dspy/accuracy</code> DSPy framework accuracy 0.80 <code>comparison/openai/accuracy</code> OpenAI SDK accuracy 0.95 <code>comparison/crewai/accuracy</code> CrewAI accuracy 0.88"},{"location":"guides/weights-biases-integration/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"guides/weights-biases-integration/#basic-configuration","title":"Basic Configuration","text":"<pre><code># In your playbook\nspec:\n  observability:\n    backend: wandb\n    config:\n      project: \"my-agent-project\"\n      entity: \"my-team\"  # Optional\n      tags: [\"production\", \"v2\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from superoptix.observability import get_observability\n\n# Custom W&amp;B configuration\nobs = get_observability(\n    agent_name=\"my_agent\",\n    backend=\"wandb\",\n    config={\n        \"project\": \"superoptix-agents\",\n        \"entity\": \"my-company\",\n        \"tags\": [\"production\", \"customer-support\"],\n        \"group\": \"agent-optimization\",\n        \"job_type\": \"gepa-optimization\"\n    }\n)\n</code></pre>"},{"location":"guides/weights-biases-integration/#dashboard-setup","title":"\ud83d\udcc8 Dashboard Setup","text":""},{"location":"guides/weights-biases-integration/#create-custom-dashboard","title":"Create Custom Dashboard","text":"<p>In W&amp;B, create a new dashboard with these panels:</p> <p>Agent Performance Panel: <pre><code># Query: agent_name = \"my_agent\"\n# Metrics: execution/latency, execution/success_rate\n# Chart: Line plot over time\n</code></pre></p> <p>GEPA Optimization Panel: <pre><code># Query: run.tags contains \"gepa\"\n# Metrics: gepa/fitness_score, gepa/improvement\n# Chart: Scatter plot (generation vs fitness)\n</code></pre></p> <p>Framework Comparison Panel: <pre><code># Query: run.tags contains \"comparison\"\n# Metrics: comparison/*/accuracy\n# Chart: Bar chart by framework\n</code></pre></p>"},{"location":"guides/weights-biases-integration/#automated-reports","title":"Automated Reports","text":"<pre><code># Generate weekly performance report\nsuper agent report my_agent --format wandb --period weekly\n</code></pre>"},{"location":"guides/weights-biases-integration/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":""},{"location":"guides/weights-biases-integration/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<pre><code># Track GEPA parameter tuning\nsuper agent optimize my_agent \\\n  --auto intensive \\\n  --observe wandb \\\n  --wandb-sweep \\\n  --sweep-config sweep_config.yaml\n</code></pre> <p>sweep_config.yaml: <pre><code>program: \"super agent optimize\"\nmethod: bayes\nmetric:\n  name: \"gepa/fitness_score\"\n  goal: maximize\nparameters:\n  reflection_lm:\n    values: [\"qwen3:8b\", \"llama3:8b\", \"gemma2:9b\"]\n  reflection_minibatch_size:\n    distribution: int_uniform\n    min: 2\n    max: 8\n  auto:\n    values: [\"light\", \"medium\", \"intensive\"]\n</code></pre></p>"},{"location":"guides/weights-biases-integration/#model-versioning","title":"Model Versioning","text":"<pre><code># Track model improvements\nsuper agent run my_agent \\\n  --goal \"Process documents\" \\\n  --observe wandb \\\n  --model-version \"v2.1\" \\\n  --tags [\"production\", \"document-processing\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#team-collaboration","title":"Team Collaboration","text":"<pre><code># Share experiments with team\nsuper agent run my_agent \\\n  --goal \"Customer support\" \\\n  --observe wandb \\\n  --entity \"my-company\" \\\n  --project \"customer-agents\" \\\n  --tags [\"team-shared\", \"customer-support\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"guides/weights-biases-integration/#common-issues","title":"Common Issues","text":"<p>Issue: \"wandb authentication failed\" <pre><code># Solution\nwandb login\n# Enter API key from: https://wandb.ai/authorize\n</code></pre></p> <p>Issue: \"Project not found\" <pre><code># Solution: Create project first\nwandb init --project \"my-superoptix-project\"\n</code></pre></p> <p>Issue: \"Entity not found\" <pre><code># Solution: Check entity name\nwandb whoami\n# Use correct entity name or omit for personal account\n</code></pre></p>"},{"location":"guides/weights-biases-integration/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging\nexport WANDB_DEBUG=true\nsuper agent run my_agent --observe wandb --verbose\n</code></pre>"},{"location":"guides/weights-biases-integration/#example-workflows","title":"\ud83d\udcca Example Workflows","text":""},{"location":"guides/weights-biases-integration/#agent-development-workflow","title":"Agent Development Workflow","text":"<pre><code># Initial development\nsuper agent run my_agent --observe wandb --tags [\"development\"]\n\n# Optimization\nsuper agent optimize my_agent --observe wandb --tags [\"optimization\"]\n\n# Evaluation\nsuper agent evaluate my_agent --observe wandb --tags [\"evaluation\"]\n\n# Production deployment\nsuper agent run my_agent --observe wandb --tags [\"production\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#multi-framework-comparison","title":"Multi-Framework Comparison","text":"<pre><code># Compare frameworks\nsuper agent run sentiment_analyzer --observe wandb --tags [\"dspy\", \"comparison\"]\nsuper agent run assistant_openai --observe wandb --tags [\"openai\", \"comparison\"]\nsuper agent run researcher_crew --observe wandb --tags [\"crewai\", \"comparison\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#ab-testing","title":"A/B Testing","text":"<pre><code># Test different configurations\nsuper agent run my_agent \\\n  --observe wandb \\\n  --tags [\"ab-test\", \"config-a\"] \\\n  --config config_a.yaml\n\nsuper agent run my_agent \\\n  --observe wandb \\\n  --tags [\"ab-test\", \"config-b\"] \\\n  --config config_b.yaml\n</code></pre>"},{"location":"guides/weights-biases-integration/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"guides/weights-biases-integration/#project-organization","title":"Project Organization","text":"<pre><code>\ud83d\udcc1 W&amp;B Projects Structure:\n\u251c\u2500\u2500 superoptix-agents/          # Main project\n\u251c\u2500\u2500 superoptix-gepa/            # GEPA optimization runs\n\u251c\u2500\u2500 superoptix-comparison/       # Framework comparisons\n\u2514\u2500\u2500 superoptix-production/      # Production monitoring\n</code></pre>"},{"location":"guides/weights-biases-integration/#tagging-strategy","title":"Tagging Strategy","text":"<pre><code># Use consistent tags\n--tags [\"framework:dspy\", \"tier:genies\", \"stage:production\"]\n--tags [\"optimization:gepa\", \"auto:medium\", \"run:001\"]\n--tags [\"comparison\", \"framework:openai\", \"metric:accuracy\"]\n</code></pre>"},{"location":"guides/weights-biases-integration/#metric-naming","title":"Metric Naming","text":"<pre><code># Use hierarchical naming\n\"execution/latency\"\n\"execution/success_rate\"\n\"gepa/fitness_score\"\n\"gepa/improvement\"\n\"comparison/dspy/accuracy\"\n\"comparison/openai/accuracy\"\n</code></pre>"},{"location":"guides/weights-biases-integration/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"guides/weights-biases-integration/#with-mlflow","title":"With MLFlow","text":"<pre><code># Log to both W&amp;B and MLFlow\nsuper agent run my_agent --observe all\n</code></pre>"},{"location":"guides/weights-biases-integration/#with-langfuse","title":"With LangFuse","text":"<pre><code># Use W&amp;B for metrics, LangFuse for traces\nsuper agent run my_agent --observe wandb\nsuper agent run my_agent --observe langfuse\n</code></pre>"},{"location":"guides/weights-biases-integration/#with-custom-metrics","title":"With Custom Metrics","text":"<pre><code>import wandb\n\n# Add custom metrics\nwandb.log({\n    \"custom/business_metric\": calculate_business_value(),\n    \"custom/user_satisfaction\": get_user_rating(),\n    \"custom/cost_per_interaction\": calculate_cost()\n})\n</code></pre>"},{"location":"guides/weights-biases-integration/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>W&amp;B Documentation: https://docs.wandb.ai/</li> <li>SuperOptiX Observability: Enhanced Observability Guide</li> <li>GEPA Optimization: GEPA Guide</li> <li>Multi-Framework Support: Multi-Framework Guide</li> </ul>"},{"location":"guides/weights-biases-integration/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<ol> <li>Set up W&amp;B account: https://wandb.ai/signup</li> <li>Install and login: <code>pip install wandb &amp;&amp; wandb login</code></li> <li>Run your first tracked agent: <code>super agent run my_agent --observe wandb</code></li> <li>Create custom dashboard in W&amp;B</li> <li>Set up team collaboration with shared projects</li> </ol> <p>Ready to track your agent experiments? Start with the Quick Start section above!</p>"},{"location":"guides/agent-optimization/datasets/","title":"Dataset-Driven Optimization","text":""},{"location":"guides/agent-optimization/datasets/#what-is-dataset-driven-optimization","title":"What is Dataset-Driven Optimization?","text":"<p>Dataset-driven optimization is the process of training agents on large-scale, real-world examples (100s to 1000s) instead of just a handful of manual RSpec-style BDD scenarios. GEPA learns patterns, edge cases, and domain-specific knowledge from actual data.</p> <p>Key Insight: While RSpec-style BDD scenarios define expected behavior, large datasets teach the agent how real-world problems actually look and how experts actually solve them.</p>"},{"location":"guides/agent-optimization/datasets/#the-dataset-optimization-problem","title":"The Dataset Optimization Problem","text":""},{"location":"guides/agent-optimization/datasets/#without-datasets-manual-scenarios-only","title":"Without Datasets (Manual Scenarios Only)","text":"<p>Scenario: Training code review agent</p> <pre><code>Training Data: 5-10 manual RSpec-style BDD scenarios\nCoverage: Limited patterns, idealized examples\nResult: Agent works on test cases but fails on real code\n</code></pre> <p>Problems: - Limited pattern exposure - Idealized examples don't match real-world complexity - Edge cases not covered - Manual scenario creation is time-consuming</p>"},{"location":"guides/agent-optimization/datasets/#with-dataset-driven-optimization","title":"With Dataset-Driven Optimization","text":"<p>Scenario: Same code review agent</p> <pre><code>Training Data: \n- 10 RSpec-style BDD scenarios (expected behavior)\n- 100 real GitHub code reviews (real-world examples)\n\nCoverage: Diverse patterns, real complexity, edge cases\nResult: Agent handles real-world code variations\n</code></pre> <p>Solution: Broader pattern recognition, robust to edge cases, production-ready</p>"},{"location":"guides/agent-optimization/datasets/#what-gepa-learns-from-datasets","title":"What GEPA Learns from Datasets","text":""},{"location":"guides/agent-optimization/datasets/#real-world-patterns","title":"Real-World Patterns","text":"<p>What It Is: Learning how real issues actually manifest in code</p> <p>What GEPA Learns: - Common vulnerability patterns - Typical code smell variations - Industry-specific conventions - Edge case handling</p> <p>Example: SQL Injection Patterns</p> <p>Manual RSpec-Style BDD Scenario (Idealized): <pre><code>scenarios:\n  - name: sql_injection\n    input:\n      code: 'query = \"SELECT * FROM users WHERE id = \" + user_id'\n    expected_output:\n      review: \"SQL injection detected\"\n</code></pre></p> <p>Dataset Examples (Real-World): <pre><code>code,review\n\"query = f'SELECT * FROM t WHERE x = {val}'\",\"SQL injection via f-string\"\n\"sql = 'DELETE FROM items WHERE ' + condition\",\"SQL injection in DELETE\"\n\"db.execute('UPDATE users SET name = ' + name)\",\"SQL injection in UPDATE\"\n\"cursor.execute(query % (user_input,))\",\"SQL injection via % formatting\"\n</code></pre></p> <p>What GEPA Learns: - SQL injection appears in SELECT, DELETE, UPDATE, INSERT - Multiple string formatting methods (concat, f-string, % formatting) - Variables named differently (query, sql, cmd) - Real code uses diverse patterns</p> <p>Impact: Recognizes SQL injection in ANY form, not just idealized examples</p>"},{"location":"guides/agent-optimization/datasets/#expert-solution-phrasing","title":"Expert Solution Phrasing","text":"<p>What It Is: Learning how domain experts actually write recommendations</p> <p>What GEPA Learns from Real Reviews: - Professional terminology - Actionable solution format - Appropriate severity levels - Citation styles</p> <p>Example: Learning from Real GitHub Code Reviews</p> <p>Dataset Examples: <pre><code>code,review\n\"[complexity 8]\",\"Cyclomatic complexity (8) exceeds maintainability threshold (4). Recommend refactoring with early returns pattern. See: Clean Code, Chapter 3.\"\n\"[hardcoded key]\",\"CRITICAL: Hardcoded API key detected (line 23). Never commit secrets. Use environment variables: os.environ.get('API_KEY'). Reference: OWASP ASVS 2.7.1\"\n\"[O(n\u00b2) loop]\",\"Performance: Nested loops create O(n\u00b2) complexity. For 10k items, that's 100M iterations. Optimize to O(n) using set lookup. Benchmark: 1000x faster.\"\n</code></pre></p> <p>What GEPA Learns: - Quantify impact: \"8 exceeds 4\", \"100M iterations\", \"1000x faster\" - Provide specific solutions: \"Use environment variables\", \"set lookup\" - Cite standards: \"Clean Code Chapter 3\", \"OWASP ASVS 2.7.1\" - Use severity markers: \"CRITICAL:\", \"Performance:\", \"Maintainability:\"</p> <p>Impact: Professional, detailed reviews vs. generic \"this is bad\" responses</p>"},{"location":"guides/agent-optimization/datasets/#edge-case-handling","title":"Edge Case Handling","text":"<p>What It Is: Learning to handle unusual or complex scenarios</p> <p>What GEPA Learns from Dataset: - Rare vulnerability patterns - Complex code structures - Multiple simultaneous issues - Language-specific idioms</p> <p>Example: Complex Security Patterns</p> <p>Dataset Includes: <pre><code># Edge Case 1: SQL injection in WHERE clause builder\ndef build_query(filters):\n    where = \" AND \".join([f\"{k} = '{v}'\" for k, v in filters.items()])\n    return f\"SELECT * FROM table WHERE {where}\"\n\n# Edge Case 2: Second-order SQL injection\ndef log_query(query):\n    log = f\"INSERT INTO logs VALUES ('{query}')\"  # Injection here!\n    db.execute(log)\n\n# Edge Case 3: Blind SQL injection via time delay\ndef check_user(username):\n    query = f\"SELECT IF(username='{username}', SLEEP(5), 0)\"\n</code></pre></p> <p>What GEPA Learns: - SQL injection isn't just in obvious SELECT statements - Can occur in logging, error handling, query builders - Can be second-order (stored then executed) - Various attack vectors (WHERE, INSERT, time-based)</p> <p>Impact: Detects vulnerabilities in unusual contexts</p>"},{"location":"guides/agent-optimization/datasets/#domain-specific-knowledge","title":"Domain-Specific Knowledge","text":"<p>What It Is: Learning industry standards, best practices, conventions</p> <p>What GEPA Learns from Dataset: - Industry terminology - Standard references (OWASP, CWE, PEP) - Severity classification - Remediation priorities</p> <p>Example: Security Domain Dataset</p> <pre><code>vulnerability,severity,standard_reference\n\"SQL Injection\",CRITICAL,\"OWASP Top 10 #1, CWE-89\"\n\"XSS\",CRITICAL,\"OWASP Top 10 #3, CWE-79\"\n\"Hardcoded Secrets\",CRITICAL,\"OWASP ASVS 2.7.1, CWE-798\"\n\"High Complexity\",MEDIUM,\"Clean Code, Cyclomatic Complexity\"\n\"Missing Error Handling\",MEDIUM,\"PEP 8, Exception Handling\"\n</code></pre> <p>What GEPA Learns: - Map each issue to severity level - Cite appropriate standards - Use industry-standard terminology</p> <p>Impact: Professional, standards-compliant reviews</p>"},{"location":"guides/agent-optimization/datasets/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/datasets/#scenario-training-on-real-github-code-reviews","title":"Scenario: Training on Real GitHub Code Reviews","text":"<p>Training Data: - Without Dataset: 5 manual RSpec-style BDD scenarios - With Dataset: 5 RSpec-style BDD scenarios + 100 real GitHub reviews</p> <p>Input Code: <pre><code># Complex real-world code\nclass UserManager:\n    def __init__(self):\n        self.db_password = \"admin123\"  # Issue 1: Hardcoded\n\n    def get_user(self, user_id):\n        # Issue 2: SQL injection via f-string\n        query = f\"SELECT * FROM users WHERE id = {user_id}\"\n        return self.db.execute(query)\n\n    def verify_login(self, username, password):\n        # Issue 3: Timing attack vulnerability\n        user = self.get_user(username)\n        if user and user.password == password:  # Issue 4: Plaintext comparison\n            return True\n        return False\n</code></pre></p> <p>Before Dataset Optimization (5 scenarios only): <pre><code>Agent Review:\n\"Security issues detected:\n- Line 6: SQL injection\n- Line 12: Password issue\n\nPlease fix these problems.\"\n</code></pre> - Vague - Missed 2 issues (hardcoded secret, timing attack) - No specific solutions - No severity classification - No standards cited</p> <p>After Dataset Optimization (5 scenarios + 100 real reviews): <pre><code>Agent Review:\n\"Security Audit Results\n\nCRITICAL Issues (Fix Immediately):\n\n1. Hardcoded Database Credential (Line 4)\n   Code: self.db_password = \"admin123\"\n   Risk: Database compromise if code exposed\n   Solution:\n   ```python\n   self.db_password = os.environ.get('DB_PASSWORD')\n   ```\n   Reference: OWASP ASVS 2.7.1, CWE-798\n\n2. SQL Injection via F-String (Line 8)\n   Code: f'SELECT * FROM users WHERE id = {user_id}'\n   Risk: Database takeover possible\n   Attack: user_id = '1 OR 1=1'\n   Solution:\n   ```python\n   query = 'SELECT * FROM users WHERE id = ?'\n   self.db.execute(query, (user_id,))\n   ```\n   Reference: OWASP Top 10 #1, CWE-89\n\n3. Timing Attack Vulnerability (Line 13)\n   Code: if user and user.password == password\n   Risk: Password length leakage via timing\n   Solution: Use constant-time comparison\n   Reference: CWE-208\n\n4. Plaintext Password Storage (Line 13)\n   Code: Direct password comparison\n   Risk: Passwords must be hashed\n   Solution:\n   ```python\n   import bcrypt\n   if bcrypt.checkpw(password.encode(), user.password_hash):\n   ```\n   Reference: OWASP ASVS 2.4.1\n\nSeverity Distribution:\n- CRITICAL: 4 issues\n- HIGH: 0\n- MEDIUM: 0\n\nPriority: Address all CRITICAL issues before deployment\n\nStandards Compliance:\n- OWASP Top 10: 2 violations\n- OWASP ASVS: 2 violations\n- CWE: 4 weaknesses\n\nRecommended Next Steps:\n1. Immediate: Fix SQL injection + hardcoded password\n2. Before deployment: Add password hashing + timing-safe comparison\n3. Security audit: Review all authentication code\"\n</code></pre></p> <p>Learned from 100 Real Reviews: - Caught all 4 issues (100% vs. 50%) - Specific attack examples - Executable solutions - Severity classification - Standards compliance check - Actionable priority order</p> <p>Improvement: From incomplete \u2192 Comprehensive professional audit</p>"},{"location":"guides/agent-optimization/datasets/#how-gepa-learns-from-datasets","title":"How GEPA Learns from Datasets","text":""},{"location":"guides/agent-optimization/datasets/#the-optimization-process","title":"The Optimization Process","text":"<ol> <li> <p>Dataset Loading <pre><code>Loading: 100 GitHub code reviews\nFormat: CSV with (code, review, severity, tags)\n</code></pre></p> </li> <li> <p>Pattern Extraction <pre><code>GEPA Analyzes Dataset:\n- 30% of reviews mention \"SQL injection\" with \"parameterized\" solution\n- 25% mention \"hardcoded\" with \"environment variables\" solution\n- 40% cite \"OWASP\" standards\n- 80% provide code examples in solutions\n- Reviews average 200 words with structured format\n</code></pre></p> </li> <li> <p>Reflection Phase <pre><code>GEPA Reflection:\n\"Real expert reviews always:\n 1. Identify specific vulnerability type (not generic 'security issue')\n 2. Provide executable code solution (not just 'fix it')\n 3. Cite standards (OWASP, CWE)\n 4. Classify severity (CRITICAL, HIGH, MEDIUM)\n\n Agent should learn these patterns.\"\n</code></pre></p> </li> <li> <p>Strategy Generation <pre><code>GEPA Creates Instructions:\n\"For security issues:\n - Specify exact vulnerability (SQL injection, XSS, etc.)\n - Provide code solution with imports and context\n - Cite relevant OWASP/CWE reference\n - Classify severity based on exploitability\n - Structure: Issue \u2192 Risk \u2192 Solution \u2192 Reference\"\n</code></pre></p> </li> <li> <p>Validation <pre><code>Test on held-out dataset:\n- Agent now matches expert review format\n- 90% of responses include code solutions\n- 85% cite appropriate standards\n- Severity classification 95% accurate\n</code></pre></p> </li> </ol> <p>Result: Agent learns professional review style from real experts</p>"},{"location":"guides/agent-optimization/datasets/#best-practices","title":"Best Practices","text":""},{"location":"guides/agent-optimization/datasets/#combine-rspec-style-bdd-scenarios-datasets","title":"Combine RSpec-Style BDD Scenarios + Datasets","text":"<pre><code>spec:\n  # RSpec-style BDD for expected behavior (10 scenarios)\n  feature_specifications:\n    scenarios:\n      - name: sql_injection_detection\n        input: {code: \"[SQL injection]\"}\n        expected_output: {review: \"SQL injection detected\"}\n\n  # Dataset for real-world patterns (100+ examples)\n  datasets:\n    - name: github_reviews\n      source: ./data/real_reviews.csv\n      limit: 100\n</code></pre> <p>Why: RSpec-style BDD defines expectations, datasets teach real-world execution</p>"},{"location":"guides/agent-optimization/datasets/#use-diverse-high-quality-datasets","title":"Use Diverse, High-Quality Datasets","text":"<pre><code>datasets:\n  # Real code reviews from GitHub\n  - name: github_reviews\n    source: ./data/github_code_reviews.csv\n    limit: 100\n\n  # Security-specific examples\n  - name: security_dataset\n    source: huggingface:code_security_vulnerabilities\n    format: huggingface\n    limit: 500\n\n  # Performance optimization examples\n  - name: performance_dataset\n    source: ./data/performance_reviews.csv\n    limit: 50\n</code></pre>"},{"location":"guides/agent-optimization/datasets/#balance-dataset-size","title":"Balance Dataset Size","text":"<pre><code># Development (fast iteration)\ndatasets:\n  - limit: 20  # Small for quick optimization\n\n# Production (best quality)\ndatasets:\n  - limit: 500  # Large for comprehensive learning\n</code></pre>"},{"location":"guides/agent-optimization/datasets/#ensure-dataset-quality","title":"Ensure Dataset Quality","text":"<p>Good Dataset: - Diverse examples - Expert-quality annotations - Consistent format - Real-world complexity</p> <p>Bad Dataset: - Repetitive examples - Low-quality annotations - Inconsistent format - Artificial simplicity</p>"},{"location":"guides/agent-optimization/datasets/#dataset-formats-supported","title":"Dataset Formats Supported","text":""},{"location":"guides/agent-optimization/datasets/#csv-most-common","title":"CSV (Most Common)","text":"<pre><code>datasets:\n  - name: code_reviews\n    source: ./data/reviews.csv\n    format: csv\n    mapping:\n      input: code\n      output: review\n</code></pre>"},{"location":"guides/agent-optimization/datasets/#huggingface-100000-datasets","title":"HuggingFace (100,000+ Datasets)","text":"<pre><code>datasets:\n  - name: security_dataset\n    source: huggingface:code_security\n    format: huggingface\n    mapping:\n      input: code_snippet\n      output: vulnerability_report\n    limit: 1000\n</code></pre>"},{"location":"guides/agent-optimization/datasets/#jsonjsonl","title":"JSON/JSONL","text":"<pre><code>datasets:\n  - name: reviews\n    source: ./data/reviews.jsonl\n    format: jsonl\n    mapping:\n      input: code\n      output: expert_review\n</code></pre> <p>See Dataset Import Guide for complete details.</p>"},{"location":"guides/agent-optimization/datasets/#scaling-benefits","title":"Scaling Benefits","text":""},{"location":"guides/agent-optimization/datasets/#small-dataset-10-examples","title":"Small Dataset (10 examples)","text":"<p>Training Time: 2-3 minutes Pattern Coverage: 30% Accuracy: 60% Best For: Quick prototypes</p>"},{"location":"guides/agent-optimization/datasets/#medium-dataset-100-examples","title":"Medium Dataset (100 examples)","text":"<p>Training Time: 10-15 minutes Pattern Coverage: 80% Accuracy: 85% Best For: Production agents</p>"},{"location":"guides/agent-optimization/datasets/#large-dataset-1000-examples","title":"Large Dataset (1000+ examples)","text":"<p>Training Time: 60-90 minutes Pattern Coverage: 95% Accuracy: 92% Best For: Mission-critical agents</p> <p>Recommendation: Start with 100 examples for best balance.</p>"},{"location":"guides/agent-optimization/datasets/#beforeafter-comparison_1","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/datasets/#scenario-security-code-review-training","title":"Scenario: Security Code Review Training","text":"<p>Agent Task: Detect security vulnerabilities in code</p> <p>Training Approach 1: Manual RSpec-Style BDD Only</p> <pre><code>feature_specifications:\n  scenarios:\n    - name: sql_injection\n      input: {code: 'query = \"SELECT * WHERE id = \" + user_id'}\n      expected_output: {finding: \"SQL injection\"}\n\n    - name: xss_vulnerability\n      input: {code: 'html = \"&lt;div&gt;\" + user_input + \"&lt;/div&gt;\"'}\n      expected_output: {finding: \"XSS vulnerability\"}\n\n    # ... 8 more idealized scenarios\n</code></pre> <p>Coverage: 10 patterns (idealized)</p> <p>Test on Real Code: <pre><code># Real-world complex code\nasync def search_products(filters: dict):\n    where_clauses = []\n    for field, value in filters.items():\n        where_clauses.append(f\"{field} = '{value}'\")  # SQL injection!\n    query = \"SELECT * FROM products WHERE \" + \" AND \".join(where_clauses)\n    return await db.fetch(query)\n</code></pre></p> <p>Agent Result (RSpec-Style BDD Only): <pre><code>\"Potential SQL issue detected.\"\n</code></pre> - Vague - Didn't recognize f-string SQL injection in loop - No specific solution - Pass Rate: 40%</p> <p>Training Approach 2: RSpec-Style BDD + Dataset</p> <pre><code>feature_specifications:\n  scenarios: [Same 10 scenarios as above]\n\ndatasets:\n  - name: github_security_reviews\n    source: ./data/real_security_reviews.csv\n    limit: 100\n</code></pre> <p>Dataset Examples Include: <pre><code>code,review,severity\n\"f'{col} = {val}'\",\"SQL injection via f-string formatting\",CRITICAL\n\"' AND '.join(clauses)\",\"SQL injection in dynamic WHERE builder\",CRITICAL\n\"async def query(sql)\",\"Async SQL execution requires parameterization\",HIGH\n</code></pre></p> <p>Coverage: 10 RSpec-style BDD patterns + 100 real-world variations</p> <p>Test on Same Real Code:</p> <p>Agent Result (RSpec-Style BDD + Dataset): <pre><code>\"CRITICAL: SQL Injection in Dynamic Query Builder\n\nIssue: F-string formatting in WHERE clause construction (Line 4)\nCode: where_clauses.append(f\"{field} = '{value}'\")\n\nAttack Vector:\n  filters = {'id': \"1' OR '1'='1\"}\n  \u2192 Query: SELECT * FROM products WHERE id = '1' OR '1'='1'\n  \u2192 Returns ALL products (authentication bypass)\n\nRoot Cause: User-controlled 'filters' dict values inserted into SQL without sanitization\n\nSolution (Parameterized Queries):\n```python\nasync def search_products(filters: dict):\n    placeholders = []\n    values = []\n    for field, value in filters.items():\n        placeholders.append(f\"{field} = ?\")\n        values.append(value)\n    query = \"SELECT * FROM products WHERE \" + \" AND \".join(placeholders)\n    return await db.fetch(query, *values)\n</code></pre></p> <p>Why This Works: - Parameterized queries separate SQL logic from data - Database driver handles escaping automatically - Prevents all SQL injection attacks</p> <p>Additional Recommendation: - Validate field names against whitelist (prevent column injection) - Add input sanitization for defense-in-depth</p> <p>Reference: OWASP Top 10 #1 (2024), CWE-89 Severity: CRITICAL (exploitable remote code execution)\" <pre><code>**Learned from Dataset**:\n- Recognized f-string SQL injection in loop context\n- Identified attack vector with example\n- Provided async-compatible solution\n- Added field validation recommendation\n- Cited multiple standards\n\n**Improvement**: 40% \u2192 95% pass rate, professional-quality reviews\n\n---\n\n## How GEPA Learns from Datasets\n\n### The Dataset Optimization Process\n\n1. **Dataset Analysis**\n   ```\n   GEPA Analyzes 100 Real Reviews:\n\n   Patterns Discovered:\n   - 85% of security reviews cite OWASP or CWE\n   - 90% provide executable code solutions\n   - 75% include attack examples\n   - 95% classify severity\n   - 80% mention multiple related issues\n   ```\n\n2. **Reflection on Gaps**\n   ```\n   GEPA Reflection:\n   \"Current agent (trained on 10 RSpec-style BDD only):\n    - Cites standards: 20% (vs. 85% in real reviews)\n    - Provides code: 40% (vs. 90% in real reviews)\n    - Shows attacks: 10% (vs. 75% in real reviews)\n\n    Need to learn from real expert patterns.\"\n   ```\n\n3. **Strategy Learning**\n   ```\n   GEPA Extracts Strategies:\n   - \"Always cite OWASP for web security issues\"\n   - \"Provide executable solution with imports\"\n   - \"Show attack example for critical vulnerabilities\"\n   - \"Classify severity based on exploitability\"\n   - \"Mention related issues (defense-in-depth)\"\n   ```\n\n4. **Validation on Test Set**\n   ```\n   Hold out 20 reviews for testing:\n\n   Agent trained on dataset:\n   - Cites standards: 80% \u2190 Learned!\n   - Provides code: 85% \u2190 Learned!\n   - Shows attacks: 70% \u2190 Learned!\n   - Severity correct: 90% \u2190 Learned!\n   ```\n\n**Result**: Agent learned professional review style from real experts\n\n---\n\n## Combining Datasets with Other Layers\n\n### Dataset + Prompts\n</code></pre> Dataset teaches: How experts phrase recommendations Prompts optimize: How to structure responses Combined: Professional, well-structured reviews <pre><code>### Dataset + RAG\n</code></pre> Dataset teaches: Which knowledge to cite RAG optimizes: When to retrieve which docs Combined: Accurate citations of relevant standards <pre><code>### Dataset + Tools\n</code></pre> Dataset teaches: When experts use metrics (complexity, performance) Tools optimize: Which tool for which metric Combined: Metric-driven, quantified recommendations <pre><code>### Dataset + Memory\n</code></pre> Dataset teaches: How to reference similar past issues Memory optimizes: Which past issues to recall Combined: \"Similar to issue #47\" with consistent handling <pre><code>---\n\n## Metrics and Results\n\n### What Gets Measured\n\n- **Pattern Coverage**: % of real-world patterns recognized\n- **Expert Alignment**: Similarity to expert reviews (style, content, format)\n- **Edge Case Handling**: % of unusual scenarios handled correctly\n- **Generalization**: Performance on unseen examples\n\n### Typical Improvements\n\n&lt;table style=\"width: 100%; border-collapse: collapse; margin: 20px 0;\"&gt;\n    &lt;tr style=\"background: rgba(33, 150, 243, 0.15); font-weight: bold;\"&gt;\n        &lt;td style=\"padding: 15px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Metric&lt;/td&gt;\n        &lt;td style=\"padding: 15px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;RSpec-Style BDD Only (10)&lt;/td&gt;\n        &lt;td style=\"padding: 15px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;RSpec-Style BDD + Dataset (100)&lt;/td&gt;\n        &lt;td style=\"padding: 15px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Improvement&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Pattern Coverage&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;30%&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;&lt;strong&gt;85%&lt;/strong&gt;&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;+55%&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Expert Alignment&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;40%&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;&lt;strong&gt;90%&lt;/strong&gt;&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;+50%&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Edge Case Handling&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;25%&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;&lt;strong&gt;80%&lt;/strong&gt;&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;+55%&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;Generalization&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;50%&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;&lt;strong&gt;88%&lt;/strong&gt;&lt;/td&gt;\n        &lt;td style=\"padding: 12px; border: 2px solid rgba(128, 128, 128, 0.3);\"&gt;+38%&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n\n---\n\n## Quick Start\n\n### Enable Dataset-Driven Optimization\n\n```yaml\nspec:\n  # RSpec-style BDD scenarios (define expected behavior)\n  feature_specifications:\n    scenarios:\n      - name: security_detection\n        input: {code: \"[vulnerable code]\"}\n        expected_output: {finding: \"vulnerability type\"}\n\n  # Dataset (teach real-world patterns)\n  datasets:\n    - name: real_reviews\n      source: ./data/expert_reviews.csv\n      format: csv\n      mapping:\n        input: code\n        output: review\n      limit: 100\n\n  # GEPA learns from both!\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre></p> <pre><code>super agent compile your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA trains on both RSpec-style BDD and dataset examples!</p>"},{"location":"guides/agent-optimization/datasets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/datasets/#issue-dataset-not-loading","title":"Issue: Dataset Not Loading","text":"<p>Symptoms: \"No dataset found\" error</p> <p>Solutions: 1. Check file path is correct 2. Verify format matches (csv, json, jsonl, parquet) 3. Check column mapping matches actual columns 4. See Dataset Import Guide</p>"},{"location":"guides/agent-optimization/datasets/#issue-low-quality-after-dataset-training","title":"Issue: Low Quality After Dataset Training","text":"<p>Symptoms: No improvement despite large dataset</p> <p>Solutions: 1. Validate dataset quality (are examples expert-level?) 2. Check mapping is correct (input/output fields match) 3. Ensure dataset diversity (not all same pattern) 4. Increase optimization iterations (<code>--auto intensive</code>)</p>"},{"location":"guides/agent-optimization/datasets/#issue-agent-overfits-to-dataset","title":"Issue: Agent Overfits to Dataset","text":"<p>Symptoms: Perfect on training data, poor on new examples</p> <p>Solutions: 1. Add more diverse RSpec-style BDD scenarios 2. Use dataset shuffling (<code>shuffle: true</code>) 3. Split dataset (train/test) 4. Reduce dataset size if too repetitive</p>"},{"location":"guides/agent-optimization/datasets/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udcac Prompt Optimization - Optimize instructions</li> <li>\ud83d\udd0d RAG Optimization - Optimize retrieval</li> <li>\ud83d\udee0\ufe0f Tool Optimization - Optimize tools</li> <li>\ud83e\udde0 Memory Optimization - Optimize context</li> <li>\ud83d\udd0c Protocol Optimization - Optimize protocols</li> <li>\ud83c\udfaf Full-Stack Example - See all layers</li> <li>Dataset Import Guide - How to import datasets</li> <li>RSpec-Style BDD Testing - BDD methodology</li> </ul> <p>Next: See how all layers work together in the Full-Stack Example \u2192</p>"},{"location":"guides/agent-optimization/full-stack-example/","title":"Full-Stack Optimization: Complete Example","text":""},{"location":"guides/agent-optimization/full-stack-example/#overview","title":"Overview","text":"<p>This guide shows how all 6 optimization layers work together in a real production agent. We'll use a Code Review Agent as our example because it demonstrates every optimization layer naturally.</p> <p>What You'll See: - How each layer contributes to the final result - The compound effect of multi-layer optimization - Before/after comparisons at each layer - Complete end-to-end workflow</p>"},{"location":"guides/agent-optimization/full-stack-example/#the-use-case-code-review-agent","title":"The Use Case: Code Review Agent","text":""},{"location":"guides/agent-optimization/full-stack-example/#agent-purpose","title":"Agent Purpose","text":"<p>Analyze code for security vulnerabilities, performance issues, and code quality problems, providing actionable recommendations with code examples.</p>"},{"location":"guides/agent-optimization/full-stack-example/#why-this-example","title":"Why This Example?","text":"<p>Code review requires ALL optimization layers: - Prompts: How to structure comprehensive reviews - RAG: When to search security/performance documentation - Tools: Which analysis tools to use (complexity, security scanning) - Memory: Recall similar past findings - Protocols: Use MCP for advanced file operations - Datasets: Learn from 100 real GitHub code reviews</p>"},{"location":"guides/agent-optimization/full-stack-example/#the-agent-configuration","title":"The Agent Configuration","text":""},{"location":"guides/agent-optimization/full-stack-example/#full-playbook-structure","title":"Full Playbook Structure","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Code Review Assistant\n  level: genies\n\nspec:\n  # Language Model\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n\n  # Layer 1: Prompts (always optimized)\n  persona:\n    role: Senior Software Engineer &amp; Security Reviewer\n    goal: Provide thorough, actionable code reviews\n    traits: [detail-oriented, security-conscious, constructive]\n\n  # Layer 2: RAG (knowledge retrieval)\n  rag:\n    enabled: true\n    knowledge_base:\n      - ./knowledge/security/*.md\n      - ./knowledge/performance/*.md\n      - ./knowledge/best_practices/*.md\n    top_k: 5\n\n  # Layer 3: Tools (analysis capabilities)\n  tools:\n    enabled: true\n    specific_tools:\n      - complexity_calculator\n      - security_scanner\n      - performance_analyzer\n\n  # Layer 4: Memory (context from past reviews)\n  memory:\n    enabled: true\n    enable_context_optimization: true\n    max_context_tokens: 2000\n\n  # Layer 5: Protocols (MCP for advanced ops)\n  # (Can be added if needed)\n\n  # Layer 6: Datasets (learn from real reviews)\n  datasets:\n    - name: github_reviews\n      source: ./data/real_code_reviews.csv\n      limit: 100\n\n  # GEPA optimizes ALL layers\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#the-test-input-complex-real-world-code","title":"The Test Input: Complex Real-World Code","text":"<pre><code># Production authentication code with multiple issues\npassword = \"admin123\"  # Issue 1: Hardcoded credential\n\ndef authenticate_user(username):\n    # Issue 2: SQL injection via string concatenation\n    query = \"SELECT * FROM users WHERE username='\" + username + \"'\"\n    result = db.execute(query)\n\n    # Issue 3: High cyclomatic complexity (nested conditionals)\n    if result:\n        if result['password'] == password:  # Issue 4: Plaintext password\n            if result['active']:\n                if result['verified']:\n                    if result['subscription'] == 'premium':\n                        return True\n    return False\n</code></pre> <p>Issues Present: 1. Hardcoded password (CRITICAL) 2. SQL injection (CRITICAL) 3. Plaintext password comparison (CRITICAL) 4. High cyclomatic complexity (MEDIUM)</p>"},{"location":"guides/agent-optimization/full-stack-example/#baseline-performance-before-optimization","title":"Baseline Performance (Before Optimization)","text":""},{"location":"guides/agent-optimization/full-stack-example/#initial-agent-behavior","title":"Initial Agent Behavior","text":"<pre><code>super agent compile code_review_assistant\nsuper agent evaluate code_review_assistant\n</code></pre> <p>Result: <pre><code>Testing 8 RSpec-style BDD scenarios:\n\nSQL Injection Detection: FAIL\n   Expected: Specific vulnerability with solution\n   Got: \"Check your SQL queries\"\n\nHardcoded Credentials: FAIL\n   Expected: Identify hardcoded secret with env var solution\n   Got: Generic \"use better practices\"\n\nComplexity Analysis: PASS\n   (Agent happened to mention complexity)\n\nSecurity Comprehensive: FAIL\n   Expected: All 3 security issues identified\n   Got: Only 1 issue found\n\nOverall: 1/8 PASS (12.5%)\n</code></pre></p> <p>Agent Output (Baseline): <pre><code>\"Your code has some security issues and could be improved.\"\n</code></pre></p> <p>Problems: - Extremely vague - No specific issues identified - No solutions provided - Not actionable - No tool usage - No knowledge retrieval - No past context referenced</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-by-layer-optimization","title":"Layer-by-Layer Optimization","text":""},{"location":"guides/agent-optimization/full-stack-example/#step-1-run-gepa-optimization","title":"Step 1: Run GEPA Optimization","text":"<pre><code>super agent optimize code_review_assistant --auto medium --fresh\n</code></pre> <p>What Happens: GEPA optimizes all 6 layers simultaneously</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-1-prompt-optimization-in-action","title":"Layer 1: Prompt Optimization in Action","text":"<p>GEPA Reflection (Iteration 3): <pre><code>\"Agent gave vague 'security issues' response. Need specific vulnerability \n identification. Optimize persona to include security expertise and goal \n to include 'specific findings with severity classification'.\"\n</code></pre></p> <p>Prompt Evolution:</p> <p>Before: <pre><code>persona:\n  role: Code Reviewer\n  goal: Find code issues\n</code></pre></p> <p>After GEPA: <pre><code>persona:\n  role: Senior Software Engineer &amp; Security Reviewer with expertise in \n       OWASP Top 10, secure coding, and performance optimization\n  goal: Identify specific security vulnerabilities, performance bottlenecks, \n       and code quality issues with severity classification and executable \n       solutions\n</code></pre></p> <p>Result: Agent now knows to be specific and provide solutions</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-2-rag-optimization-in-action","title":"Layer 2: RAG Optimization in Action","text":"<p>GEPA Reflection (Iteration 5): <pre><code>\"Agent should search security documentation BEFORE analyzing SQL queries.\n Pattern: String concatenation in SQL context \u2192 Retrieve sql_injection.md\"\n</code></pre></p> <p>RAG Strategy Evolution:</p> <p>Before: <pre><code>Query: Generic \"code review\"\nRetrieved: random_doc.md, naming_conventions.md, testing.md\nRelevance: 25% (wrong topics)\n</code></pre></p> <p>After GEPA: <pre><code>Learned Strategy:\n- Detect: String concatenation + SQL keywords\n- Query: \"SQL injection prevention parameterized queries OWASP\"\n- Retrieved: sql_injection.md, database_security.md, owasp_a03.md\n- Relevance: 95% (perfect match)\n</code></pre></p> <p>Result: Agent retrieves exact security docs needed</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-3-tool-optimization-in-action","title":"Layer 3: Tool Optimization in Action","text":"<p>GEPA Reflection (Iteration 7): <pre><code>\"Agent should use complexity_calculator for nested conditions.\n Pattern: &gt;3 nested if statements \u2192 Use complexity_calculator\n Then cite threshold violation in review.\"\n</code></pre></p> <p>Tool Usage Evolution:</p> <p>Before: <pre><code>Tools Available: complexity_calculator, security_scanner\nTools Used: None\nResult: No metrics, vague assessment\n</code></pre></p> <p>After GEPA: <pre><code>Learned Strategy:\n1. Detect nested conditionals \u2192 Use complexity_calculator\n2. Detect string concatenation in SQL \u2192 Use security_scanner\n3. Combine results for comprehensive review\n\nTools Used:\n- complexity_calculator \u2192 Returns: complexity = 5\n- security_scanner \u2192 Returns: [SQL injection, hardcoded secret]\n</code></pre></p> <p>Result: Metric-driven, tool-backed findings</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-4-memory-optimization-in-action","title":"Layer 4: Memory Optimization in Action","text":"<p>GEPA Reflection (Iteration 9): <pre><code>\"Agent should reference similar past SQL injection findings.\n Memory optimization should prioritize high-importance security memories.\"\n</code></pre></p> <p>Memory Selection Evolution:</p> <p>Before: <pre><code>Memories: Last 10 reviews chronologically\nRelevance: 30% (mostly irrelevant)\n</code></pre></p> <p>After GEPA: <pre><code>Learned Strategy:\nQuery: \"SQL injection\"\nSelected Memories:\n- Memory #47: Previous SQL injection in auth code (0.95 similarity)\n- Memory #23: Parameterized query pattern used before (0.88 similarity)\n- Memory #61: Team security standard (0.82 similarity, high importance)\n\nResult: Highly relevant past context\n</code></pre></p> <p>Result: Agent says \"Similar to previous finding #47. Use parameterized queries as recommended before.\"</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-5-protocol-optimization-in-action","title":"Layer 5: Protocol Optimization in Action","text":"<p>GEPA Reflection (Iteration 11): <pre><code>\"For simple code snippets, use built-in tools (faster).\n For file system operations, use MCP when recursive or complex.\"\n</code></pre></p> <p>Protocol Strategy Evolution:</p> <p>Before: <pre><code>Always initialize MCP \u2192 Slow startup (500ms overhead)\n</code></pre></p> <p>After GEPA: <pre><code>Learned Strategy:\n- Single code snippet \u2192 Use built-in tools (fast)\n- Directory analysis \u2192 Use MCP filesystem (powerful)\n- Git context needed \u2192 Use MCP github (required)\n\nCurrent query: Single code snippet\nTool: built-in security_scanner (50ms vs. 500ms)\n</code></pre></p> <p>Result: Optimal tool source selection</p>"},{"location":"guides/agent-optimization/full-stack-example/#layer-6-dataset-learning-in-action","title":"Layer 6: Dataset Learning in Action","text":"<p>GEPA Learning from 100 Real Reviews:</p> <pre><code>Pattern Extraction:\n- 85% of expert reviews cite OWASP standards\n- 90% provide executable code solutions\n- 75% include attack examples for critical issues\n- 95% classify severity based on exploitability\n- 80% prioritize security over code quality\n\nAgent Learns:\n\u2192 Always cite OWASP for web security\n\u2192 Provide code solutions with imports\n\u2192 Show attack vector for CRITICAL issues\n\u2192 Classify: CRITICAL &gt; HIGH &gt; MEDIUM &gt; LOW\n\u2192 Prioritize security in recommendations\n</code></pre> <p>Result: Agent review style matches expert reviews</p>"},{"location":"guides/agent-optimization/full-stack-example/#final-optimized-performance","title":"Final Optimized Performance","text":""},{"location":"guides/agent-optimization/full-stack-example/#re-evaluation-after-optimization","title":"Re-Evaluation After Optimization","text":"<pre><code>super agent evaluate code_review_assistant  # automatically loads optimized weights\n</code></pre> <p>Result: <pre><code>Testing 8 RSpec-style BDD scenarios:\n\nSQL Injection Detection: PASS\n   \u2713 Identified SQL injection with specific attack example\n   \u2713 Provided parameterized query solution\n   \u2713 Cited OWASP Top 10 #1\n\nHardcoded Credentials: PASS\n   \u2713 Found hardcoded password\n   \u2713 Recommended environment variables\n   \u2713 Cited CWE-798\n\nComplexity Analysis: PASS\n   \u2713 Used complexity_calculator tool\n   \u2713 Reported complexity 5/4 threshold\n   \u2713 Suggested early returns pattern\n\nSecurity Comprehensive: PASS\n   \u2713 Found all 3 security issues\n   \u2713 Prioritized by severity\n   \u2713 Cited OWASP standards\n\nPerformance Issues: PASS\nMemory Reference: PASS\nMulti-Issue Handling: PASS\nProfessional Format: PASS\n\nOverall: 8/8 PASS (100%)\nImprovement: +87.5% (12.5% \u2192 100%)\n</code></pre></p>"},{"location":"guides/agent-optimization/full-stack-example/#complete-optimized-agent-output","title":"Complete Optimized Agent Output","text":""},{"location":"guides/agent-optimization/full-stack-example/#live-query-result","title":"Live Query Result","text":"<pre><code>super agent run code_review_assistant --code \"[authentication code above]\"\n</code></pre> <p>Complete Agent Response (All Layers Working Together):</p> <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nSECURITY CODE REVIEW RESULTS\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nOVERALL SEVERITY: \ud83d\udd34 CRITICAL\n\n\ud83d\udccb CRITICAL ISSUES (Fix Before Deployment):\n\n1. Hardcoded Database Credential (Line 2)\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Code: password = \"admin123\"\n\n   \ud83c\udfaf Issue [Tool: security_scanner]:\n   Sensitive credential stored directly in source code\n\n   \u26a0\ufe0f  Risk:\n   - Credential exposed if code repository accessed\n   - Security breach if code deployed to production\n   - Violates security compliance standards\n\n   Solution:\n   ```python\n   import os\n   password = os.environ.get('DB_PASSWORD')\n   if not password:\n       raise ValueError(\"DB_PASSWORD environment variable not set\")\n   ```\n\n   \ud83d\udcda Reference [Retrieved from: knowledge/security/hardcoded_secrets.md]:\n   - OWASP ASVS 2.7.1: Credentials must not be hardcoded\n   - CWE-798: Use of Hard-coded Credentials\n   - Similar to: Previous finding #61 (resolved with env vars)\n\n2. SQL Injection Vulnerability (Line 5)\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Code: query = \"SELECT * FROM users WHERE username='\" + username + \"'\"\n\n   \ud83c\udfaf Issue [Tool: security_scanner]:\n   User input concatenated directly into SQL query\n\n   \u26a0\ufe0f  Attack Example:\n   Input: username = \"admin' OR '1'='1\"\n   Result: SELECT * FROM users WHERE username='admin' OR '1'='1'\n   Impact: Returns ALL users, authentication bypass\n\n   Solution:\n   ```python\n   def authenticate_user(username):\n       query = \"SELECT * FROM users WHERE username = ?\"\n       result = db.execute(query, (username,))\n   ```\n\n   \ud83d\udcda Reference [Retrieved from: knowledge/security/sql_injection.md]:\n   - OWASP Top 10 2024: #1 Injection\n   - CWE-89: SQL Injection\n   - Parameterized queries prevent all injection attacks\n   - Similar to: Previous finding #47 (parameterized queries recommended)\n\n3. Plaintext Password Comparison (Line 7)\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Code: if result['password'] == password\n\n   \ud83c\udfaf Issue [Tool: security_scanner]:\n   Direct password comparison without hashing\n\n   \u26a0\ufe0f  Risk:\n   - Passwords stored in plaintext in database\n   - Timing attack vulnerability (password length leak)\n   - Complete compromise if database breached\n\n   Solution:\n   ```python\n   import bcrypt\n\n   # When storing password:\n   hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n\n   # When verifying:\n   if bcrypt.checkpw(password.encode(), result['password_hash']):\n       return True\n   ```\n\n   \ud83d\udcda Reference [Retrieved from: knowledge/security/password_hashing.md]:\n   - OWASP ASVS 2.4.1: Passwords must be hashed with bcrypt/Argon2\n   - Use constant-time comparison to prevent timing attacks\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udcca MEDIUM PRIORITY ISSUES:\n\n4. High Cyclomatic Complexity (Lines 7-12)\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Function: authenticate_user()\n\n   \ud83c\udfaf Metrics [Tool: complexity_calculator]:\n   - Cyclomatic Complexity: 5\n   - Recommended Threshold: 4\n   - Nesting Depth: 5 levels\n\n   \u26a0\ufe0f  Impact:\n   - Hard to test (2^5 = 32 test cases needed)\n   - Difficult to maintain\n   - Error-prone modifications\n\n   Solution (Early Returns Pattern):\n   ```python\n   def authenticate_user(username):\n       query = \"SELECT * FROM users WHERE username = ?\"\n       result = db.execute(query, (username,))\n\n       if not result:\n           return False\n       if not bcrypt.checkpw(password.encode(), result['password_hash']):\n           return False\n       if not result['active']:\n           return False\n       if not result['verified']:\n           return False\n       if result['subscription'] != 'premium':\n           return False\n\n       return True\n   ```\n\n   \ud83d\udcda Reference [Retrieved from: knowledge/best_practices/code_smells.md]:\n   - Clean Code: Cyclomatic complexity should be &lt;4\n   - Early returns reduce nesting and improve readability\n   - Similar to: Issue #23 (refactored using early returns)\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udcca ANALYSIS METRICS:\n\nLines Analyzed: 13\nIssues Found: 4\n  - CRITICAL: 3\n  - HIGH: 0\n  - MEDIUM: 1\n  - LOW: 0\n\nTool Usage:\n  - security_scanner: 3 findings\n  - complexity_calculator: 1 finding\n\nKnowledge Retrieved:\n  - security/sql_injection.md (0.94 relevance)\n  - security/hardcoded_secrets.md (0.91 relevance)\n  - security/password_hashing.md (0.89 relevance)\n  - best_practices/code_smells.md (0.87 relevance)\n\nMemory References:\n  - Issue #47: SQL injection (similar pattern)\n  - Issue #61: Hardcoded credentials (team standard)\n  - Issue #23: Complexity refactoring (same solution)\n\nAnalysis Time: 2.8 seconds\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83c\udfaf RECOMMENDATIONS (Priority Order):\n\nIMMEDIATE (Deploy Blockers):\n1. Fix SQL injection (Line 5) - CRITICAL security risk\n2. Remove hardcoded password (Line 2) - CRITICAL exposure\n3. Implement password hashing (Line 7) - CRITICAL crypto failure\n\nBEFORE NEXT RELEASE:\n4. Refactor complexity (Lines 7-12) - Maintainability\n\nSTANDARDS COMPLIANCE CHECK:\nOWASP Top 10 #1: Injection - VIOLATED\nOWASP ASVS 2.4.1: Password Hashing - VIOLATED\nOWASP ASVS 2.7.1: Hardcoded Credentials - VIOLATED\n\u26a0\ufe0f  Clean Code: Complexity Threshold - EXCEEDED\n\nRISK LEVEL: \ud83d\udd34 DEPLOY BLOCKED (3 critical security issues)\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udcda REFERENCES &amp; RESOURCES:\n\nSecurity:\n- OWASP Top 10 2024: https://owasp.org/Top10\n- SQL Injection Prevention: [Local: knowledge/security/sql_injection.md]\n- Password Security: [Local: knowledge/security/password_hashing.md]\n\nCode Quality:\n- Clean Code (Martin): Chapter 3 (Functions)\n- Cyclomatic Complexity: https://en.wikipedia.org/wiki/Cyclomatic_complexity\n\nTeam Standards:\n- Previous similar findings: #23, #47, #61\n- Security review checklist: ./docs/security_checklist.md\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#layer-by-layer-breakdown","title":"Layer-by-Layer Breakdown","text":""},{"location":"guides/agent-optimization/full-stack-example/#how-each-layer-contributed","title":"How Each Layer Contributed","text":"<p>1. Prompts \ud83d\udcac: - Learned to structure reviews: Severity \u2192 Findings \u2192 Solutions \u2192 References - Learned to quantify impact: \"2^5 = 32 test cases needed\" - Learned to prioritize: CRITICAL before MEDIUM</p> <p>2. RAG \ud83d\udd0d: - Searched security docs when detecting SQL concatenation - Retrieved 4 highly relevant docs (&gt;0.87 similarity) - Integrated citations naturally: \"[Retrieved from: sql_injection.md]\"</p> <p>3. Tools \ud83d\udee0\ufe0f: - Used security_scanner for vulnerability detection - Used complexity_calculator for metrics - Combined tool outputs: \"3 security + 1 complexity findings\"</p> <p>4. Memory \ud83e\udde0: - Recalled similar past findings (#23, #47, #61) - Referenced team standards and previous solutions - Selected only relevant memories (3/50 selected)</p> <p>5. Protocols \ud83d\udd0c: - Used built-in tools (code snippet analysis, no MCP needed) - Would use MCP for directory scanning or Git context</p> <p>6. Datasets \ud83d\udcca: - Learned professional phrasing from 100 real reviews - Learned to provide attack examples (from dataset) - Learned severity classification (CRITICAL for exploitable) - Learned to cite standards (90% of dataset cites OWASP)</p>"},{"location":"guides/agent-optimization/full-stack-example/#optimization-metrics","title":"Optimization Metrics","text":""},{"location":"guides/agent-optimization/full-stack-example/#performance-improvement","title":"Performance Improvement","text":"Metric Before After Improvement Overall Accuracy 12.5% 100% +87.5% Security Detection 33% (1/3) 100% (3/3) +67% RAG Relevance 25% 95% +70% Tool Usage Accuracy 0% 100% +100% Memory Relevance 30% 90% +60% Response Actionability 20% 95% +75% Token Efficiency 5,000 tokens 2,000 tokens -60%"},{"location":"guides/agent-optimization/full-stack-example/#the-compound-effect","title":"The Compound Effect","text":""},{"location":"guides/agent-optimization/full-stack-example/#single-layer-vs-full-stack","title":"Single-Layer vs. Full-Stack","text":"<p>Optimizing ONLY Prompts: <pre><code>Result: Better instructions, but...\n- Still retrieves wrong knowledge docs\n- Still doesn't use tools correctly\n- Still includes irrelevant memories\n- Still uses generic phrasing\n\nFinal Accuracy: ~45%\n</code></pre></p> <p>Optimizing ALL Layers: <pre><code>Result: Better instructions AND\n- Retrieves perfect security docs\n- Uses tools strategically\n- Recalls relevant past findings\n- Uses expert-level phrasing from dataset\n\nFinal Accuracy: 100%\n</code></pre></p> <p>Key Insight: Each layer multiplies the effect of others. This is why full-stack optimization produces 2-3x better results than prompt-only optimization.</p>"},{"location":"guides/agent-optimization/full-stack-example/#complete-workflow","title":"Complete Workflow","text":""},{"location":"guides/agent-optimization/full-stack-example/#end-to-end-optimization-process","title":"End-to-End Optimization Process","text":"<pre><code># Initialize project\nsuper init code_review_project\ncd code_review_project\n\n# Create knowledge base\nmkdir -p knowledge/security knowledge/performance knowledge/best_practices\n# ... add your documentation files\n\n# Prepare dataset (optional but recommended)\n# Download or create real code review examples\ncp your_real_reviews.csv data/code_reviews.csv\n\n# Create agent playbook\ncat &gt; agents/code_reviewer/playbook.yaml &lt;&lt; 'EOF'\nspec:\n  persona:\n    role: Code Reviewer\n    goal: Find issues in code\n\n  rag:\n    enabled: true\n    knowledge_base: [./knowledge/**/*.md]\n\n  tools:\n    enabled: true\n    specific_tools: [complexity_calculator, security_scanner]\n\n  memory:\n    enabled: true\n    enable_context_optimization: true\n\n  datasets:\n    - source: ./data/code_reviews.csv\n      limit: 100\n\n  optimization:\n    optimizer: {name: GEPA, params: {auto: medium}}\nEOF\n\n# Compile\nsuper agent compile code_reviewer\n# \u2192 Generates pipeline with all layers enabled\n\n# Baseline evaluation\nsuper agent evaluate code_reviewer\n# \u2192 Shows baseline: 12.5% accuracy\n\n# Optimize ALL layers\nsuper agent optimize code_reviewer --auto medium --fresh\n# \u2192 GEPA optimizes: prompts + RAG + tools + memory + dataset learning\n# \u2192 Takes 10-15 minutes\n# \u2192 Shows progress for each layer\n\n# Re-evaluate\nsuper agent evaluate code_reviewer  # automatically loads optimized weights\n# \u2192 Shows improvement: 100% accuracy (+87.5%)\n\n# Test on real code\nsuper agent run code_reviewer --code \"$(cat suspicious_code.py)\"\n# \u2192 Professional, comprehensive security audit\n\n# Deploy\nsuper agent run code_reviewer --interactive\n# \u2192 Production-ready code review agent!\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#what-makes-this-production-ready","title":"What Makes This Production-Ready?","text":""},{"location":"guides/agent-optimization/full-stack-example/#quality-indicators","title":"Quality Indicators","text":"<p>1. Comprehensive Coverage - Detects all vulnerability types (SQL, XSS, secrets, etc.) - Analyzes performance and code quality - Handles edge cases and complex patterns</p> <p>2. Actionable Output - Specific issue identification - Executable code solutions - Clear priority ordering - Standards compliance</p> <p>3. Professional Quality - Expert-level phrasing (learned from dataset) - Proper citations (OWASP, CWE, Clean Code) - Severity classification - Risk assessment</p> <p>4. Efficient Operation - Strategic knowledge retrieval (95% relevance) - Correct tool usage (100% accuracy) - Optimized memory selection (60% fewer tokens) - Fast response time (&lt;3 seconds)</p> <p>5. Robust Performance - Handles unseen code variations (dataset generalization) - References past findings (memory consistency) - Graceful error handling (tool/protocol failures)</p>"},{"location":"guides/agent-optimization/full-stack-example/#applying-to-your-agents","title":"Applying to Your Agents","text":""},{"location":"guides/agent-optimization/full-stack-example/#step-1-identify-your-layers","title":"Step 1: Identify Your Layers","text":"<p>Which layers does your agent need?</p> <pre><code>Customer Support:\n  - Prompts: (response quality)\n  - RAG: (product knowledge)\n  - Tools: \u26a0\ufe0f (maybe ticketing system)\n  - Memory: (conversation history)\n  - Protocols: (not needed)\n  - Datasets: (real support tickets)\n\nResearch Agent:\n  - Prompts: (research methodology)\n  - RAG: (academic papers)\n  - Tools: (search, citations)\n  - Memory: \u26a0\ufe0f (maybe past searches)\n  - Protocols: (MCP for data sources)\n  - Datasets: (research examples)\n\nCode Generator:\n  - Prompts: (coding standards)\n  - RAG: (API docs, patterns)\n  - Tools: (syntax validators)\n  - Memory: \u26a0\ufe0f (maybe past code)\n  - Protocols: \u26a0\ufe0f (maybe MCP filesystem)\n  - Datasets: (code examples)\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#step-2-enable-your-layers","title":"Step 2: Enable Your Layers","text":"<pre><code>spec:\n  # Enable only what you need\n  rag:\n    enabled: true  # If you have knowledge base\n\n  tools:\n    enabled: true  # If you need tools\n\n  memory:\n    enabled: true  # If you need context\n    enable_context_optimization: true\n\n  datasets:\n    - source: your_data.csv  # If you have examples\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#step-3-optimize","title":"Step 3: Optimize","text":"<pre><code>super agent optimize your_agent --auto medium\n</code></pre> <p>GEPA automatically optimizes all enabled layers!</p>"},{"location":"guides/agent-optimization/full-stack-example/#key-takeaways","title":"Key Takeaways","text":""},{"location":"guides/agent-optimization/full-stack-example/#what-you-learned","title":"What You Learned","text":"<ol> <li>Full-Stack &gt; Prompts Only: All 6 layers working together produces 2-3x better results</li> <li>Each Layer Compounds: RAG finds better docs, tools provide better metrics, memory recalls better context</li> <li>GEPA Learns Strategies: Not just what to do, but WHEN, WHICH, and HOW</li> <li>Datasets Amplify: Real examples teach real-world patterns and expert-level quality</li> <li>Production-Ready: 12% \u2192 100% accuracy demonstrates deployment readiness</li> </ol>"},{"location":"guides/agent-optimization/full-stack-example/#the-full-stack-advantage","title":"The Full-Stack Advantage","text":"Optimization Approach Accuracy Quality Production-Ready Prompts Only 45% Basic No Prompts + RAG 65% Better \u26a0\ufe0f Maybe Prompts + RAG + Tools 75% Good \u26a0\ufe0f Close Full-Stack (All 6 Layers) 100% Expert Yes"},{"location":"guides/agent-optimization/full-stack-example/#next-steps","title":"Next Steps","text":""},{"location":"guides/agent-optimization/full-stack-example/#explore-individual-layers","title":"Explore Individual Layers","text":"<p>Deep-dive into specific layers: - \ud83d\udcac Prompt Optimization - \ud83d\udd0d RAG Optimization - \ud83d\udee0\ufe0f Tool Optimization - \ud83e\udde0 Memory Optimization - \ud83d\udd0c Protocol Optimization - \ud83d\udcca Dataset Optimization</p>"},{"location":"guides/agent-optimization/full-stack-example/#try-the-workflow","title":"Try the Workflow","text":"<pre><code># Create an agent with multiple layers\n# Optimize with GEPA\n# See the compound effect!\n</code></pre>"},{"location":"guides/agent-optimization/full-stack-example/#learn-gepa-internals","title":"Learn GEPA Internals","text":"<ul> <li>GEPA Optimizer Guide</li> <li>GEPA + --fresh Flag</li> </ul>"},{"location":"guides/agent-optimization/full-stack-example/#scale-with-datasets","title":"Scale with Datasets","text":"<ul> <li>Dataset Import Guide</li> <li>Browse HuggingFace: https://huggingface.co/datasets</li> </ul> <p>Related Guides:</p> <ul> <li>Agent Optimization Overview</li> <li>GEPA Optimization</li> <li>Memory Optimization</li> <li>Dataset Import</li> <li>Tool Development</li> <li>RAG Configuration</li> <li>MCP Protocol</li> </ul>"},{"location":"guides/agent-optimization/memory/","title":"Memory Optimization","text":""},{"location":"guides/agent-optimization/memory/#what-is-memory-optimization","title":"What is Memory Optimization?","text":"<p>Memory optimization is the process of improving which memories an agent includes in its context, how it scores relevance, and how it manages token budgets. While traditional approaches include all memories (leading to context overflow), GEPA learns intelligent, context-aware memory selection.</p> <p>Key Insight: After 20+ interactions, an agent can't include all memories. GEPA learns which memories matter most for each query, balancing relevance, importance, and recency within token constraints.</p>"},{"location":"guides/agent-optimization/memory/#the-memory-optimization-problem","title":"The Memory Optimization Problem","text":""},{"location":"guides/agent-optimization/memory/#without-optimization","title":"Without Optimization","text":"<p>Scenario: Customer support agent after 30 interactions</p> <pre><code>1. Agent has 30 memories (15,000 tokens total)\n2. Context limit: 2,000 tokens\n3. Traditional approach: Include all \u2192 Context overflow\n4. Agent: Error or truncated response\n</code></pre> <p>Problem: Too many memories, context overflow, poor performance</p>"},{"location":"guides/agent-optimization/memory/#with-gepa-optimization","title":"With GEPA Optimization","text":"<p>Scenario: Same agent, same 30 memories</p> <pre><code>1. Agent has 30 memories (15,000 tokens total)\n2. Context limit: 2,000 tokens\n3. GEPA-learned strategy: Select 6 most relevant (1,800 tokens)\n4. Agent: High-quality response with perfect context\n</code></pre> <p>Solution: Optimized selection, perfect fit, better performance</p>"},{"location":"guides/agent-optimization/memory/#what-gepa-optimizes-in-memory","title":"What GEPA Optimizes in Memory","text":""},{"location":"guides/agent-optimization/memory/#context-selection-which-memories-to-include","title":"Context Selection (Which Memories to Include)","text":"<p>What It Is: Learning which memories are relevant for each query</p> <p>What GEPA Learns: - Relevance scoring (semantic similarity to query) - Importance weighting (critical vs. minor info) - Recency balance (recent vs. older memories) - Task-specific patterns</p> <p>Example Configuration:</p> <pre><code>spec:\n  memory:\n    enabled: true\n    enable_context_optimization: true  # GEPA optimizes selection!\n    max_context_tokens: 2000\n    short_term_capacity: 100\n</code></pre> <p>Before Optimization: <pre><code>Strategy: Include all recent memories chronologically\nResult: \n- Context overflow (15,000 tokens &gt; 2,000 limit)\n- Irrelevant memories included\n- Important older memories excluded\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategy:\nFor security query:\n  1. Search for memories matching \"security\", \"SQL\", \"vulnerability\"\n  2. Prioritize: High importance memories (0.8-1.0)\n  3. Include: Recent similar findings (last 7 days)\n  4. Summarize: Older related memories\n  5. Stop at: 1,800 tokens (buffer for response)\n\nResult:\n- 6 highly relevant memories selected\n- Fits perfectly in 2,000 token budget\n- Includes critical past findings\n</code></pre></p> <p>Impact: 60% fewer tokens, 55% higher relevance, better performance</p>"},{"location":"guides/agent-optimization/memory/#relevance-scoring-how-to-score-memories","title":"Relevance Scoring (How to Score Memories)","text":"<p>What It Is: Calculating how relevant each memory is to current query</p> <p>What GEPA Learns: - Semantic similarity weights - Keyword matching importance - Category matching - Task-specific relevance</p> <p>Scoring Algorithm (GEPA-Optimized):</p> <pre><code># Pseudocode showing what GEPA learns\n\ndef score_memory(memory, query, task_type):\n    # GEPA learns optimal weights for each task type\n    weights = get_task_weights(task_type)  # GEPA optimizes these!\n\n    relevance = calculate_similarity(query, memory.content)\n    importance = memory.importance\n    recency = calculate_recency(memory.timestamp)\n\n    # GEPA learns optimal combination\n    final_score = (\n        relevance * weights['relevance'] +      # GEPA learned: 0.5\n        importance * weights['importance'] +    # GEPA learned: 0.3\n        recency * weights['recency']            # GEPA learned: 0.2\n    )\n\n    return final_score\n</code></pre> <p>Before Optimization: <pre><code>Weights: Equal (0.33, 0.33, 0.33)\nResult: Recent but irrelevant memories rank high\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Weights (Security Task):\n- relevance: 0.6 (prioritize semantic match)\n- importance: 0.3 (critical findings matter)\n- recency: 0.1 (older security patterns still valid)\n\nLearned Weights (Conversation Task):\n- relevance: 0.4\n- importance: 0.2\n- recency: 0.4 (recent context matters more)\n</code></pre></p> <p>Impact: Task-specific scoring produces better context</p>"},{"location":"guides/agent-optimization/memory/#token-budget-management","title":"Token Budget Management","text":"<p>What It Is: Fitting memories within context window constraints</p> <p>What GEPA Learns: - How many memories to include - When to summarize vs. include full memory - Buffer allocation for response - Compression strategies</p> <p>Before Optimization: <pre><code>Approach: Include memories until limit reached\nResult:\n- Memory 1: 500 tokens (full)\n- Memory 2: 500 tokens (full)\n- Memory 3: 500 tokens (full)\n- Memory 4: 500 tokens (full)\n- Total: 2,000 tokens (no buffer for response!)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategy:\n- Reserve 200 tokens for response buffer\n- Budget: 1,800 tokens for memories\n- Strategy: Include top 3 full (450 tokens each)\n- Summarize next 3 (150 tokens each)\n- Total: 1,800 tokens (perfect fit!)\n</code></pre></p> <p>Impact: Better token allocation, room for quality responses</p>"},{"location":"guides/agent-optimization/memory/#summarization-strategies","title":"Summarization Strategies","text":"<p>What It Is: Learning when and how to compress memories</p> <p>What GEPA Learns: - When to summarize (old, low-importance, large) - How much to compress - What information to preserve - Summary format</p> <p>Before Optimization: <pre><code>Memory: \"Customer John reported shipping issue with order #12345 on \n        Oct 15. He lives in California, ordered 3 items (laptop, mouse, \n        keyboard), paid $1,500, wants refund, mentioned he's traveling \n        next week...\"\nLength: 800 tokens\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Summarization:\nSummary: \"Customer John: Order #12345 shipping issue, refund requested (Oct 15)\"\nLength: 50 tokens\nPreserved: Customer name, order ID, issue type, request, date\nRemoved: Address, item details, payment (not relevant to current query)\n</code></pre></p> <p>Impact: 16x compression while preserving key info</p>"},{"location":"guides/agent-optimization/memory/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/memory/#scenario-customer-support-query","title":"Scenario: Customer Support Query","text":"<p>Agent Memory (30 interactions, 15,000 tokens): - Memories 1-5: Recent small talk (low importance) - Memories 6-10: Previous product questions (medium importance) - Memory 11: Shipping issue with order #12345 (high importance) - Memory 12: Customer prefers email contact (high importance) - Memories 13-30: Various unrelated interactions</p> <p>Current Query: \"What happened with my shipping issue?\"</p> <p>Before Memory Optimization: <pre><code>Selection: Last 10 memories chronologically (memories 21-30)\nIncluded:\n- Recent small talk about weather\n- Question about product features\n- Unrelated order status check\n- More irrelevant recent context\n\nMissing:\n- Memory 11: Actual shipping issue details!\n- Memory 12: Contact preference\n\nResult: Agent says \"I don't have information about shipping issues\"\n</code></pre></p> <p>After GEPA Memory Optimization: <pre><code>GEPA-Learned Selection Strategy:\n\n1. Semantic Search: \"shipping issue\" matches Memory 11 (0.95 similarity)\n2. Importance Filter: Memory 11 (0.9) and Memory 12 (0.8) ranked high\n3. Recency Boost: Memory 11 is recent enough (5 days ago)\n4. Token Allocation: Memory 11 (500 tokens) + Memory 12 (200 tokens) = 700 tokens\n\nSelected Memories:\n- Memory 11: Shipping issue with order #12345 (full text)\n- Memory 12: Email contact preference (full text)\n- Memory 28: Recent interaction (summarized, 100 tokens)\nTotal: 800 tokens (well under 2,000 limit)\n\nAgent Response:\n\"I found information about your shipping issue.\n\nOrder #12345 Status:\n- Reported: Oct 15\n- Issue: Package delayed at distribution center\n- Expected: Oct 25\n- Tracking: Updated yesterday\n\nI'll send detailed status to your email (your preferred contact method).\n\nWould you like me to check current tracking status or escalate for faster delivery?\"\n</code></pre></p> <p>Improvement: From \"no information\" \u2192 Complete, accurate response</p>"},{"location":"guides/agent-optimization/memory/#how-gepa-learns-memory-strategies","title":"How GEPA Learns Memory Strategies","text":""},{"location":"guides/agent-optimization/memory/#the-optimization-process","title":"The Optimization Process","text":"<ol> <li> <p>Analysis Phase <pre><code>GEPA Observes:\n- Query about shipping issue\n- Agent selected recent irrelevant memories\n- Agent missed Memory 11 (actual shipping issue details)\n- Response was \"I don't have that information\" (FAIL)\n</code></pre></p> </li> <li> <p>Reflection Phase <pre><code>GEPA Reflection:\n\"Agent failed because it selected recent memories chronologically \n instead of semantically relevant memories. Query contained 'shipping \n issue' which matches Memory 11 with 0.95 similarity. Need to prioritize \n semantic relevance over recency for factual queries.\"\n</code></pre></p> </li> <li> <p>Mutation Phase <pre><code>GEPA Tests:\n- Strategy 1: Pure recency (chronological)\n- Strategy 2: Pure relevance (semantic)\n- Strategy 3: Balanced (0.6 relevance + 0.3 importance + 0.1 recency)\n</code></pre></p> </li> <li> <p>Evaluation Phase <pre><code>Results:\n- Strategy 1: 30% (misses key info)\n- Strategy 2: 70% (good but ignores recent context)\n- Strategy 3: 95% (balanced!) \u2190 Winner!\n</code></pre></p> </li> <li> <p>Selection Phase <pre><code>GEPA Keeps: Strategy 3 (balanced approach)\nFine-Tunes: Adjust weights per task type\n</code></pre></p> </li> </ol> <p>Result: Learned task-specific memory selection strategies</p>"},{"location":"guides/agent-optimization/memory/#best-practices","title":"Best Practices","text":""},{"location":"guides/agent-optimization/memory/#enable-context-optimization","title":"Enable Context Optimization","text":"<pre><code>memory:\n  enabled: true\n  enable_context_optimization: true  # Critical!\n  max_context_tokens: 2000\n</code></pre>"},{"location":"guides/agent-optimization/memory/#set-appropriate-token-budgets","title":"Set Appropriate Token Budgets","text":"<pre><code>memory:\n  max_context_tokens: 2000   # For general agents\n  max_context_tokens: 4000   # For complex reasoning agents\n  max_context_tokens: 1000   # For simple Q&amp;A agents\n</code></pre>"},{"location":"guides/agent-optimization/memory/#use-memory-importance-levels","title":"Use Memory Importance Levels","text":"<pre><code># In code\nmemory.remember(\n    content=\"Critical security finding: SQL injection\",\n    memory_type=\"long_term\",\n    importance=0.9  # High importance!\n)\n\nmemory.remember(\n    content=\"Small talk about weather\",\n    memory_type=\"short_term\",\n    importance=0.1  # Low importance\n)\n</code></pre> <p>GEPA learns to prioritize high-importance memories.</p>"},{"location":"guides/agent-optimization/memory/#define-memory-aware-rspec-style-bdd-scenarios","title":"Define Memory-Aware RSpec-Style BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: memory_recall\n      description: Agent should recall similar past issues\n      given_memory:\n        - content: \"Previous SQL injection in authentication\"\n          importance: 0.9\n      input:\n        code: [SQL injection code]\n      expected_output:\n        review: Must mention \"similar to previous finding\"\n</code></pre>"},{"location":"guides/agent-optimization/memory/#common-memory-strategies-gepa-learns","title":"Common Memory Strategies GEPA Learns","text":""},{"location":"guides/agent-optimization/memory/#strategy-1-semantic-matching","title":"Strategy 1: Semantic Matching","text":"<p>Before: Chronological selection After: \"For factual queries, prioritize memories with &gt;0.80 semantic similarity\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-2-importance-weighting","title":"Strategy 2: Importance Weighting","text":"<p>Before: All memories equal weight After: \"Always include high-importance memories (&gt;0.8) regardless of age\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-3-task-specific-balancing","title":"Strategy 3: Task-Specific Balancing","text":"<p>Before: Same strategy for all tasks After: \"Security queries: 60% relevance, 30% importance, 10% recency. Conversation: 40% relevance, 20% importance, 40% recency\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-4-dynamic-summarization","title":"Strategy 4: Dynamic Summarization","text":"<p>Before: Include full text or skip After: \"Summarize memories &gt;7 days old with &lt;0.7 relevance to save tokens\"</p>"},{"location":"guides/agent-optimization/memory/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"guides/agent-optimization/memory/#what-gets-measured","title":"What Gets Measured","text":"<ul> <li>Selection Accuracy: % of scenarios where right memories selected</li> <li>Token Efficiency: Average tokens used vs. budget</li> <li>Relevance Score: Average similarity of selected memories</li> <li>Response Quality: Agent performance with optimized context</li> </ul>"},{"location":"guides/agent-optimization/memory/#typical-improvements","title":"Typical Improvements","text":"Metric Before After Improvement Token Usage 5,000 (overflow) 1,800 -64% Memory Relevance 30% 85% +55% Response Accuracy 45% 90% +45% Task Success Rate 50% 85% +35%"},{"location":"guides/agent-optimization/memory/#quick-start","title":"Quick Start","text":""},{"location":"guides/agent-optimization/memory/#enable-memory-optimization","title":"Enable Memory Optimization","text":"<pre><code>spec:\n  # Memory configuration\n  memory:\n    enabled: true\n    enable_context_optimization: true  # GEPA optimizes!\n    max_context_tokens: 2000\n    short_term_capacity: 100\n\n  # GEPA automatically optimizes memory selection\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre> <pre><code>super agent compile your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA will learn optimal memory selection strategies!</p>"},{"location":"guides/agent-optimization/memory/#advanced-memory-specific-configuration","title":"Advanced: Memory-Specific Configuration","text":""},{"location":"guides/agent-optimization/memory/#fine-tune-memory-behavior","title":"Fine-Tune Memory Behavior","text":"<pre><code>memory:\n  enabled: true\n  enable_context_optimization: true\n\n  # Token budget\n  max_context_tokens: 2000\n\n  # Capacity limits\n  short_term_capacity: 100\n  long_term_capacity: 1000\n\n  # Embeddings for semantic search\n  enable_embeddings: true\n  embedding_model: sentence-transformers/all-MiniLM-L6-v2\n\n  # Retention policy\n  retention_policy: lru  # Least Recently Used\n</code></pre> <p>GEPA learns optimal configurations through optimization.</p>"},{"location":"guides/agent-optimization/memory/#common-memory-strategies-gepa-learns_1","title":"Common Memory Strategies GEPA Learns","text":""},{"location":"guides/agent-optimization/memory/#strategy-1-task-specific-weighting","title":"Strategy 1: Task-Specific Weighting","text":"<p>Before: Same weights for all queries After: - \"Security queries: Prioritize high-importance memories (importance weight: 0.4)\" - \"Conversation queries: Prioritize recent context (recency weight: 0.5)\" - \"Knowledge queries: Prioritize semantic relevance (relevance weight: 0.6)\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-2-dynamic-summarization","title":"Strategy 2: Dynamic Summarization","text":"<p>Before: Include full memories or exclude them After: \"Memories &gt;7 days old: Summarize to 20% of original length if relevance &lt;0.7\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-3-category-aware-selection","title":"Strategy 3: Category-Aware Selection","text":"<p>Before: Ignore memory categories After: \"For security query, prioritize memories with category='security_patterns'\"</p>"},{"location":"guides/agent-optimization/memory/#strategy-4-similarity-clustering","title":"Strategy 4: Similarity Clustering","text":"<p>Before: Select memories independently After: \"If selecting memory about SQL injection, also include related memories about database security (cluster similar topics)\"</p>"},{"location":"guides/agent-optimization/memory/#integration-with-other-layers","title":"Integration with Other Layers","text":"<p>Memory optimization enhances other layers:</p> <p>Memory + Prompts: <pre><code>Optimized Memory: Includes past security finding\nOptimized Prompt: \"Reference similar past issues when available\"\n\u2192 Agent says: \"Similar to previous finding #47, this is SQL injection\"\n</code></pre></p> <p>Memory + RAG: <pre><code>Optimized Memory: Recalls \"We used sql_injection.md doc before\"\nOptimized RAG: Retrieves same doc again for consistency\n\u2192 Consistent security recommendations across sessions\n</code></pre></p> <p>Memory + Tools: <pre><code>Optimized Memory: \"Last time complexity was 7, we refactored\"\nOptimized Tool: Calculates current complexity = 8\n\u2192 Agent says: \"Similar to previous issue #23 (complexity 7). Recommend same refactoring approach\"\n</code></pre></p>"},{"location":"guides/agent-optimization/memory/#real-world-example","title":"Real-World Example","text":""},{"location":"guides/agent-optimization/memory/#use-case-customer-support-with-memory","title":"Use Case: Customer Support with Memory","text":"<p>Agent Memory (50 interactions over 2 weeks):</p> ID Content Type Importance Age Tokens M1 Small talk about weather short_term 0.1 1h 50 M2 Product feature question short_term 0.3 2h 150 M3 Order #12345 shipping issue reported long_term 0.9 5 days 400 M4 Customer prefers email contact long_term 0.8 5 days 100 M5 Unrelated billing question short_term 0.4 1 day 200 ... ... ... ... ... ... M50 Small talk yesterday short_term 0.1 1 day 50 <p>Total: 15,000 tokens, Budget: 2,000 tokens</p> <p>Query: \"What's the status of my shipping issue?\"</p> <p>Before Memory Optimization: <pre><code>Selection: Last 20 memories chronologically (M31-M50)\nTotal Tokens: 2,100 (overflow!)\nRelevance: 25% (mostly irrelevant recent chat)\n\nIncluded:\n- M50: Small talk about weather (irrelevant)\n- M49: Product question (irrelevant)\n- M48: Another product question (irrelevant)\n- ... more irrelevant memories\n- M31: Unrelated topic\n\nMissing:\n- M3: Actual shipping issue! (excluded because too old)\n- M4: Contact preference (excluded)\n\nResult: Agent can't answer the question!\n</code></pre></p> <p>After GEPA Memory Optimization: <pre><code>GEPA-Learned Selection Strategy:\n\nStep 1: Semantic Search\n- Query: \"shipping issue\"\n- M3 matches with 0.95 similarity \u2190 Highly relevant!\n- M4 matches with 0.60 similarity \u2190 Contact info\n\nStep 2: Importance Weighting\n- M3: importance = 0.9 (high priority!)\n- M4: importance = 0.8 (high priority!)\n\nStep 3: Token Allocation\n- M3: 400 tokens (full, most important)\n- M4: 100 tokens (full, contact pref)\n- M47-M50: 300 tokens (recent context, summarized)\n- Total: 800 tokens (40% of budget)\n\nStep 4: Final Selection\nSelected: M3, M4, M47-M50 (6 memories)\nTokens: 800 (under budget)\nRelevance: 85% (highly relevant)\n\nAgent Response:\n\"Your order #12345 shipping issue status:\n\nOriginal Report (Oct 15):\n- Package delayed at distribution center\n- Expected delivery: Oct 25\n\nCurrent Status:\n- Tracking updated yesterday\n- Package in transit\n- Estimated arrival: Oct 24 (1 day early!)\n\nI'll send detailed tracking to your email (your preferred contact method).\n\nWould you like me to set up delivery notifications?\"\n</code></pre></p> <p>Improvement: From \"no information\" \u2192 Complete, personalized response</p>"},{"location":"guides/agent-optimization/memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/memory/#issue-context-overflow","title":"Issue: Context Overflow","text":"<p>Symptoms: Agent errors or truncated responses</p> <p>Solutions: 1. Reduce <code>max_context_tokens</code> 2. Enable <code>enable_context_optimization: true</code> 3. Increase summarization aggressiveness 4. Use shorter memory formats</p>"},{"location":"guides/agent-optimization/memory/#issue-irrelevant-memories-selected","title":"Issue: Irrelevant Memories Selected","text":"<p>Symptoms: Agent includes off-topic memories</p> <p>Solutions: 1. Add RSpec-style BDD scenarios with <code>given_memory</code> showing expected selection 2. Increase importance scores for critical memories 3. Tune semantic similarity threshold 4. Optimize with more diverse scenarios</p>"},{"location":"guides/agent-optimization/memory/#issue-important-memories-excluded","title":"Issue: Important Memories Excluded","text":"<p>Symptoms: Agent misses key information</p> <p>Solutions: 1. Increase <code>importance</code> score when storing critical info 2. Use <code>memory_type: long_term</code> for persistent info 3. Add category tags for better organization 4. Increase <code>max_context_tokens</code> budget</p>"},{"location":"guides/agent-optimization/memory/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udcac Prompt Optimization - Optimize instructions</li> <li>\ud83d\udd0d RAG Optimization - Optimize knowledge retrieval</li> <li>\ud83d\udee0\ufe0f Tool Optimization - Optimize tool usage</li> <li>\ud83d\udd0c Protocol Optimization - Optimize protocols</li> <li>\ud83c\udfaf Full-Stack Example - See all layers</li> <li>Memory Optimization Guide - Implementation details</li> <li>Memory Systems Guide - Memory architecture</li> </ul> <p>Next: Learn how GEPA optimizes protocol usage patterns (MCP) \u2192</p>"},{"location":"guides/agent-optimization/overview/","title":"Agent Optimization: Beyond Prompts","text":""},{"location":"guides/agent-optimization/overview/#what-is-agent-optimization","title":"What is Agent Optimization?","text":"<p>Agent optimization is the process of improving an AI agent's performance across all layers of the agentic pipeline, not just prompts. While traditional approaches focus solely on prompt engineering, SuperOptiX optimizes the entire stack: prompts, RAG retrieval, tool usage, memory selection, protocol handling, and dataset-driven learning.</p> <p>Key Insight: A production-ready agent requires optimization at every layer. Optimizing only prompts while leaving other layers unoptimized is like tuning a car's engine while ignoring the transmission, brakes, and steering.</p>"},{"location":"guides/agent-optimization/overview/#the-problem-with-prompt-only-optimization","title":"The Problem with Prompt-Only Optimization","text":"<p>Traditional AI optimization focuses exclusively on prompt engineering:</p> Traditional Approach <p>What Gets Optimized:</p> <ul> <li>Prompt instructions only</li> </ul> <p>What's Ignored:</p> <ul> <li>When to retrieve knowledge (RAG)</li> <li>Which tools to use</li> <li>How to select relevant memories</li> <li>How to combine information</li> </ul> <p>Result: Suboptimal performance, manual tuning required for each layer</p> <p>Example Problem: An agent might have a perfect prompt for code review, but if it doesn't know WHEN to search security documentation or WHICH analysis tools to use, it will still produce mediocre results.</p>"},{"location":"guides/agent-optimization/overview/#the-superoptix-full-stack-approach","title":"The SuperOptiX Full-Stack Approach","text":"<p>SuperOptiX optimizes 6 distinct layers of the agentic pipeline:</p> Layer Traditional Approach SuperOptiX Approach Impact \ud83d\udcac Prompts Manual tuning GEPA learns optimal instructions High \ud83d\udd0d RAG Fixed retrieval GEPA learns retrieval strategy High \ud83d\udee0\ufe0f Tools Hardcoded selection GEPA learns tool selection Medium \ud83e\udde0 Memory All memories included GEPA optimizes context selection High \ud83d\udd0c Protocols (MCP) Static patterns GEPA adapts protocol usage Medium \ud83d\udcca Datasets Small manual examples GEPA trains on 100s-1000s examples Very High"},{"location":"guides/agent-optimization/overview/#how-gepa-optimizes-each-layer","title":"How GEPA Optimizes Each Layer","text":"<p>GEPA (Genetic-Pareto) doesn't just optimize prompts. It learns strategies for each layer through reflection and iteration:</p>"},{"location":"guides/agent-optimization/overview/#the-optimization-process","title":"The Optimization Process","text":"<pre><code>graph TD\n    A[Agent Playbook] --&gt; B[Initial Evaluation]\n    B --&gt; C[GEPA Reflection]\n    C --&gt; D{Which Layer Needs Improvement?}\n    D --&gt;|Prompts| E[Optimize Instructions]\n    D --&gt;|RAG| F[Optimize Retrieval Strategy]\n    D --&gt;|Tools| G[Optimize Tool Selection]\n    D --&gt;|Memory| H[Optimize Context Selection]\n    D --&gt;|Protocols| I[Optimize Protocol Usage]\n    D --&gt;|All| J[Optimize Integration]\n    E --&gt; K[Re-Evaluate]\n    F --&gt; K\n    G --&gt; K\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K\n    K --&gt; L{Improved?}\n    L --&gt;|Yes| M[Next Iteration]\n    L --&gt;|No| N[Try Different Strategy]\n    M --&gt; C\n    N --&gt; C</code></pre> <p>Key Feature: GEPA automatically identifies which layer needs improvement and applies targeted optimizations.</p>"},{"location":"guides/agent-optimization/overview/#quick-results-preview","title":"Quick Results Preview","text":"<p>Here's what full-stack optimization looks like in practice:</p> <p>Use Case: Code Review Agent</p> Metric Before Optimization After Optimization Improvement Overall Accuracy 37.5% 87.5% +50% Security Detection 33% 100% +67% RAG Relevance Random Strategic Contextual Tool Usage Wrong/None Correct Tools 100% Accurate Memory Efficiency All memories (overflow) Optimized selection -60% tokens Response Quality Vague suggestions Actionable solutions With code examples <p>Key Observation: The compound effect of optimizing all layers produces production-ready results.</p>"},{"location":"guides/agent-optimization/overview/#the-six-optimization-layers","title":"The Six Optimization Layers","text":"<p>Click on any layer to learn how GEPA optimizes it:</p>"},{"location":"guides/agent-optimization/overview/#prompt-optimization","title":"\ud83d\udcac Prompt Optimization","text":"<p>What Gets Optimized: - Persona and role definition - Task instructions - Reasoning patterns - Response formatting</p> <p>GEPA Learns: - How to structure clear instructions - When to use chain-of-thought - How to format comprehensive responses</p> <p>Example: \"Provide thorough, actionable code reviews\" \u2192 \"Analyze code for security (SQL injection, XSS), performance (O(n\u00b2) complexity), and maintainability (cyclomatic complexity &gt; 4). Provide specific solutions with code examples.\"</p>"},{"location":"guides/agent-optimization/overview/#rag-optimization","title":"\ud83d\udd0d RAG Optimization","text":"<p>What Gets Optimized: - When to search knowledge base - Which documents to retrieve - How to integrate context - Relevance scoring</p> <p>GEPA Learns: - \"Search security docs BEFORE analyzing SQL queries\" - \"Retrieve performance patterns for loop analysis\" - \"Check best practices for naming conventions\"</p> <p>Example: From random document retrieval \u2192 Strategic, issue-specific retrieval with 85% relevance</p>"},{"location":"guides/agent-optimization/overview/#tool-optimization","title":"\ud83d\udee0\ufe0f Tool Optimization","text":"<p>What Gets Optimized: - Tool selection for scenarios - Tool invocation order - Output combination - Multi-tool orchestration</p> <p>GEPA Learns: - \"Use complexity_calculator for nested conditions\" - \"Run security_scanner on string concatenation\" - \"Combine findings from multiple tools\"</p> <p>Example: From no tool usage \u2192 Correct tool selection 100% of the time</p>"},{"location":"guides/agent-optimization/overview/#memory-optimization","title":"\ud83e\udde0 Memory Optimization","text":"<p>What Gets Optimized: - Context selection - Relevance scoring - Token budgeting - Summarization strategies</p> <p>GEPA Learns: - \"Include similar past security findings\" - \"Prioritize recent review patterns\" - \"Summarize older memories to save tokens\"</p> <p>Example: From all memories (context overflow) \u2192 Optimized selection (60% fewer tokens, better relevance)</p>"},{"location":"guides/agent-optimization/overview/#protocol-optimization-mcp","title":"\ud83d\udd0c Protocol Optimization (MCP)","text":"<p>What Gets Optimized: - MCP tool selection - Protocol invocation patterns - Result processing - Error handling</p> <p>GEPA Learns: - When to use MCP tools vs built-in tools - How to structure protocol calls - How to handle tool errors gracefully</p> <p>Example: From generic protocol usage \u2192 Optimized MCP-specific patterns</p>"},{"location":"guides/agent-optimization/overview/#dataset-driven-optimization","title":"\ud83d\udcca Dataset-Driven Optimization","text":"<p>What Gets Optimized: - Pattern recognition from 100s-1000s examples - Edge case handling - Real-world solution phrasing - Domain-specific knowledge</p> <p>GEPA Learns: - Common security vulnerability patterns - Typical code smell indicators - Effective recommendation phrasing from real examples</p> <p>Example: From 5 manual scenarios \u2192 Training on 100 real GitHub code reviews</p>"},{"location":"guides/agent-optimization/overview/#compound-effect-why-full-stack-matters","title":"Compound Effect: Why Full-Stack Matters","text":"<p>Each layer optimization compounds with others:</p> <p>Example Scenario: Code with SQL injection</p> <ol> <li>Prompts (optimized): Agent knows to check for security issues</li> <li>RAG (optimized): Retrieves SQL injection documentation</li> <li>Tools (optimized): Runs security_scanner to confirm</li> <li>Memory (optimized): Recalls similar past findings</li> <li>Datasets (optimized): Uses phrasing from real GitHub reviews</li> </ol> <p>Result: Comprehensive, actionable review with specific solutions</p> <p>vs. Prompt-Only Optimization: - Prompts (optimized): Agent knows to check security - RAG (not optimized): Retrieves random docs - Tools (not optimized): Doesn't use security_scanner - Memory (not optimized): Includes irrelevant memories - Datasets (not optimized): Generic responses</p> <p>Result: Vague \"check your SQL\" response</p>"},{"location":"guides/agent-optimization/overview/#getting-started","title":"Getting Started","text":""},{"location":"guides/agent-optimization/overview/#understand-each-layer","title":"Understand Each Layer","text":"<p>Read through each layer's guide to understand what GEPA optimizes:</p> <ul> <li>\ud83d\udcac Prompt Optimization</li> <li>\ud83d\udd0d RAG Optimization</li> <li>\ud83d\udee0\ufe0f Tool Optimization</li> <li>\ud83e\udde0 Memory Optimization</li> <li>\ud83d\udd0c Protocol Optimization</li> <li>\ud83d\udcca Dataset-Driven Optimization</li> </ul>"},{"location":"guides/agent-optimization/overview/#see-the-full-stack-example","title":"See the Full-Stack Example","text":"<p>Check out the comprehensive \ud83c\udfaf Full-Stack Example showing all layers working together.</p>"},{"location":"guides/agent-optimization/overview/#apply-to-your-agents","title":"Apply to Your Agents","text":"<p>Enable optimization layers in your playbook:</p> <pre><code>spec:\n  # Prompts (always optimized)\n  persona:\n    role: Your agent role\n    goal: Your agent goal\n\n  # RAG optimization\n  rag:\n    enabled: true\n    knowledge_base:\n      - ./knowledge/**/*.md\n\n  # Tool optimization\n  tools:\n    enabled: true\n    categories:\n      - your_category\n\n  # Memory optimization\n  memory:\n    enabled: true\n    enable_context_optimization: true\n\n  # Dataset optimization\n  datasets:\n    - name: training_data\n      source: ./data/examples.csv\n      limit: 100\n\n  # GEPA optimizes ALL layers\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre>"},{"location":"guides/agent-optimization/overview/#run-optimization","title":"Run Optimization","text":"<pre><code>super agent compile your_agent\nsuper agent evaluate your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA will automatically optimize all enabled layers!</p>"},{"location":"guides/agent-optimization/overview/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Full-Stack &gt; Prompts Only: Optimizing all layers produces compound improvements</li> <li>GEPA Learns Strategies: Not just prompt text, but WHEN, WHICH, and HOW for each layer</li> <li>Production-Ready Results: 37% \u2192 87% accuracy through comprehensive optimization</li> <li>Automatic: GEPA handles all layers, you just enable them in the playbook</li> <li>Framework-Agnostic: Works across DSPy, OpenAI, CrewAI, and all supported frameworks</li> </ol>"},{"location":"guides/agent-optimization/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Deep Dive: Read individual layer guides</li> <li>See Example: Check out the Full-Stack Example</li> <li>Try It: Enable layers in your agent and run optimization</li> <li>Learn More: GEPA Optimizer Guide</li> </ul> <p>Related Guides:</p> <ul> <li>GEPA Optimization</li> <li>Memory Optimization</li> <li>Dataset Import</li> <li>Tool Development</li> <li>RAG Configuration</li> <li>MCP Protocol</li> </ul>"},{"location":"guides/agent-optimization/prompts/","title":"Prompt Optimization","text":""},{"location":"guides/agent-optimization/prompts/#what-is-prompt-optimization","title":"What is Prompt Optimization?","text":"<p>Prompt optimization is the process of improving an agent's core instructions, persona definition, reasoning patterns, and response formatting to produce better outputs. This is the foundation layer of agent optimization.</p> <p>Key Insight: GEPA doesn't just tweak prompt text. It learns strategies for how to structure instructions, when to use reasoning, and how to format responses for maximum clarity and effectiveness.</p>"},{"location":"guides/agent-optimization/prompts/#what-gepa-optimizes-in-prompts","title":"What GEPA Optimizes in Prompts","text":""},{"location":"guides/agent-optimization/prompts/#persona-and-role-definition","title":"Persona and Role Definition","text":"<p>What It Is: The agent's identity, role, and behavioral guidelines</p> <p>What GEPA Learns: - Role clarity and specificity - Goal articulation - Communication style - Domain expertise framing</p> <p>Example Configuration:</p> <pre><code>spec:\n  persona:\n    role: Senior Software Engineer &amp; Security Reviewer\n    goal: Provide thorough, actionable code reviews\n    traits:\n      - detail-oriented\n      - security-conscious\n      - constructive\n</code></pre> <p>Before Optimization: <pre><code>role: Code Reviewer\ngoal: Review code\n</code></pre></p> <p>After GEPA Optimization: <pre><code>role: Senior Software Engineer &amp; Security Reviewer\ngoal: Provide thorough, actionable code reviews that improve security, \n     performance, and maintainability with specific solutions\ntraits: [detail-oriented, security-conscious, constructive, pragmatic]\n</code></pre></p> <p>Impact: More focused, professional reviews with clear authority</p>"},{"location":"guides/agent-optimization/prompts/#task-instructions","title":"Task Instructions","text":"<p>What It Is: Core instructions for each agent task</p> <p>What GEPA Learns: - Instruction clarity - Step specification - Output requirements - Success criteria</p> <p>Example:</p> <pre><code>tasks:\n  - name: review_code\n    instruction: |\n      Analyze the code for:\n      1. Security vulnerabilities (SQL injection, XSS, hardcoded secrets)\n      2. Performance issues (O(n\u00b2) complexity, inefficient loops)\n      3. Code quality (cyclomatic complexity, duplication, naming)\n\n      For each issue:\n      - Identify the specific line/pattern\n      - Explain why it's problematic\n      - Provide a concrete solution with code example\n      - Cite relevant documentation or standards\n</code></pre> <p>Before Optimization: <pre><code>instruction: Review the code and find problems\n</code></pre></p> <p>After GEPA Optimization: <pre><code>instruction: |\n  Analyze code systematically:\n  1. Security: Check for OWASP Top 10 vulnerabilities\n  2. Performance: Identify O(n\u00b2) or worse complexity\n  3. Quality: Calculate cyclomatic complexity (threshold: 4)\n\n  For each finding:\n  - Specify exact line and pattern\n  - Explain impact (security risk, performance cost, etc.)\n  - Provide executable solution with code\n  - Reference standards (OWASP, PEP 8, etc.)\n</code></pre></p> <p>Impact: Structured, comprehensive reviews vs. vague suggestions</p>"},{"location":"guides/agent-optimization/prompts/#reasoning-patterns","title":"Reasoning Patterns","text":"<p>What It Is: Chain-of-thought, step-by-step thinking process</p> <p>What GEPA Learns: - When to use chain-of-thought - How many reasoning steps - Depth of analysis - Thinking structure</p> <p>Example:</p> <pre><code>reasoning:\n  style: chain_of_thought\n  steps:\n    - Scan code for security patterns\n    - Calculate complexity metrics\n    - Check against best practices\n    - Prioritize findings by severity\n    - Formulate actionable recommendations\n</code></pre> <p>Before Optimization: <pre><code>Agent thinks implicitly, inconsistent analysis\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Agent follows explicit reasoning steps:\n1. Scan for security patterns (SQL concatenation, hardcoded secrets)\n2. Run complexity analysis (cyclomatic complexity, nesting depth)\n3. Check best practices (naming, error handling, DRY)\n4. Prioritize by severity (critical &gt; high &gt; medium &gt; low)\n5. Generate solutions with code examples\n</code></pre></p> <p>Impact: Consistent, thorough analysis every time</p>"},{"location":"guides/agent-optimization/prompts/#response-formatting","title":"Response Formatting","text":"<p>What It Is: How the agent structures its output</p> <p>What GEPA Learns: - Response organization - Level of detail - Code example formatting - Citation style</p> <p>Example Output Structure:</p> <pre><code>output_fields:\n  - name: review\n    type: str\n    description: Comprehensive code review with structured findings\n  - name: severity\n    type: str\n    description: Overall severity (critical, high, medium, low)\n  - name: recommendations\n    type: list\n    description: Prioritized action items\n</code></pre> <p>Before Optimization: <pre><code>\"Your code has some issues. Check for SQL injection and complexity.\"\n</code></pre></p> <p>After GEPA Optimization: <pre><code>SEVERITY: CRITICAL\n\nFINDINGS:\n\n1. SQL Injection Vulnerability (CRITICAL)\n   Line: query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n   Issue: User input concatenated directly into SQL query\n   Solution:\n   ```python\n   query = \"SELECT * FROM users WHERE name = ?\"\n   result = db.execute(query, (username,))\n   ```\n   Reference: OWASP Top 10 #1\n\n2. High Cyclomatic Complexity (MEDIUM)\n   Function: login()\n   Complexity: 8 (threshold: 4)\n   Issue: Too many nested conditions\n   Solution: Use early returns\n   ```python\n   if not result: return False\n   if not result['active']: return False\n   return True\n   ```\n\nRECOMMENDATIONS:\n1. Fix SQL injection immediately (security risk)\n2. Refactor login() to reduce complexity (maintainability)\n</code></pre></p> <p>Impact: Actionable, professional reviews vs. vague suggestions</p>"},{"location":"guides/agent-optimization/prompts/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/prompts/#scenario-code-review-request","title":"Scenario: Code Review Request","text":"<p>Input Code: <pre><code>def get_user(username):\n    query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n    return db.execute(query)\n</code></pre></p> <p>Before Prompt Optimization: <pre><code>\"Your code might have security issues. Consider using better practices.\"\n</code></pre> - Vague - No specific issue identified - No solution provided - Not actionable</p> <p>After GEPA Prompt Optimization: <pre><code>SEVERITY: CRITICAL\n\nFINDING: SQL Injection Vulnerability\n\nLine: query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n\nIssue: User input (username) is concatenated directly into SQL query without \n       sanitization, allowing attackers to inject malicious SQL code.\n\nAttack Example:\n  username = \"admin' OR '1'='1\"\n  \u2192 Returns all users, bypassing authentication\n\nSolution:\n```python\ndef get_user(username):\n    query = \"SELECT * FROM users WHERE name = ?\"\n    return db.execute(query, (username,))\n</code></pre></p> <p>Why This Works: - Parameterized queries prevent SQL injection - Database driver handles escaping automatically - Industry standard (OWASP recommendation)</p> <p>Reference: OWASP Top 10 2024 - Injection Attacks (#1) <pre><code>- Specific issue identified\n- Attack scenario explained\n- Executable solution provided\n- Standards cited\n\n**Improvement**: From 0% helpful \u2192 100% actionable\n\n---\n\n## How GEPA Learns Prompt Strategies\n\n### The Optimization Process\n\n1. **Analysis Phase**\n   - GEPA evaluates agent responses\n   - Identifies vague or incomplete reviews\n   - Notes missing elements (severity, solutions, citations)\n\n2. **Reflection Phase**\n   ```\n   GEPA Reflection:\n   \"The agent identified 'security issue' but didn't specify SQL injection.\n    It didn't provide a code solution or cite OWASP standards.\n    Need more specific instructions for security findings.\"\n   ```\n\n3. **Mutation Phase**\n   - Generates improved prompt variations\n   - Tests: \"Always specify exact vulnerability type (SQL injection, XSS, etc.)\"\n   - Tests: \"Provide executable code solutions\"\n   - Tests: \"Cite security standards (OWASP, CWE)\"\n\n4. **Selection Phase**\n   - Evaluates each variation\n   - Selects best-performing prompts\n   - Keeps improvements, discards regressions\n\n5. **Iteration**\n   - Repeats process\n   - Compounds improvements\n   - Converges on optimal prompts\n\n**Result**: Learned prompt strategies, not just text tweaks\n\n---\n\n## Best Practices\n\n### Start with Clear Base Prompts\n\n```yaml\n# Good starting point\npersona:\n  role: [Specific role with expertise level]\n  goal: [Clear, measurable objective]\n  traits: [Relevant characteristics]\n\ntasks:\n  - instruction: [Step-by-step process, not vague request]\n</code></pre></p>"},{"location":"guides/agent-optimization/prompts/#define-expected-output-format","title":"Define Expected Output Format","text":"<pre><code>output_fields:\n  - name: finding\n    description: Specific issue identified\n  - name: severity\n    description: Impact level\n  - name: solution\n    description: Executable fix with code\n</code></pre> <p>GEPA will learn to match this structure consistently.</p>"},{"location":"guides/agent-optimization/prompts/#provide-rspec-style-bdd-scenarios","title":"Provide RSpec-Style BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: security_detection\n      description: Agent should detect and explain security issues\n      input:\n        code: [Code with SQL injection]\n      expected_output:\n        review: Must mention \"SQL injection\" and \"parameterized queries\"\n        severity: critical\n</code></pre> <p>GEPA optimizes prompts to match these specifications.</p>"},{"location":"guides/agent-optimization/prompts/#use-datasets-for-real-world-patterns","title":"Use Datasets for Real-World Patterns","text":"<pre><code>datasets:\n  - source: ./data/real_reviews.csv\n    limit: 100\n</code></pre> <p>GEPA learns effective phrasing from real examples.</p>"},{"location":"guides/agent-optimization/prompts/#common-patterns-gepa-learns","title":"Common Patterns GEPA Learns","text":""},{"location":"guides/agent-optimization/prompts/#pattern-1-specificity-over-generality","title":"Pattern 1: Specificity Over Generality","text":"<p>Before: \"Check for issues\" After: \"Check for: SQL injection, XSS, hardcoded secrets, complexity &gt; 4\"</p>"},{"location":"guides/agent-optimization/prompts/#pattern-2-actionability","title":"Pattern 2: Actionability","text":"<p>Before: \"This could be better\" After: \"Replace X with Y. Here's the code: [executable solution]\"</p>"},{"location":"guides/agent-optimization/prompts/#pattern-3-citations","title":"Pattern 3: Citations","text":"<p>Before: \"This is bad practice\" After: \"Violates SOLID principles (reference: Clean Code, Chapter 3)\"</p>"},{"location":"guides/agent-optimization/prompts/#pattern-4-structured-output","title":"Pattern 4: Structured Output","text":"<p>Before: Freeform text response After: Severity \u2192 Findings \u2192 Solutions \u2192 References</p>"},{"location":"guides/agent-optimization/prompts/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"guides/agent-optimization/prompts/#what-gets-measured","title":"What Gets Measured","text":"<ul> <li>Specificity: % of responses with specific issue identification</li> <li>Actionability: % of responses with executable solutions</li> <li>Citation Rate: % of responses with references</li> <li>Format Compliance: % matching expected output structure</li> </ul>"},{"location":"guides/agent-optimization/prompts/#typical-improvements","title":"Typical Improvements","text":"Metric Before After Improvement Response Specificity 30% 90% +60% Actionable Solutions 20% 85% +65% Citation Rate 10% 75% +65% Format Compliance 40% 95% +55%"},{"location":"guides/agent-optimization/prompts/#quick-start","title":"Quick Start","text":""},{"location":"guides/agent-optimization/prompts/#enable-prompt-optimization","title":"Enable Prompt Optimization","text":"<p>Prompts are automatically optimized when you run GEPA:</p> <pre><code># Create agent with good base prompts\nsuper spec generate code_reviewer --template genie\n\n# Compile\nsuper agent compile code_reviewer\n\n# Evaluate baseline\nsuper agent evaluate code_reviewer\n\n# Optimize prompts (and other enabled layers)\nsuper agent optimize code_reviewer --auto medium\n\n# See improvement\nsuper agent evaluate code_reviewer  # automatically loads optimized weights\n</code></pre> <p>GEPA automatically optimizes all prompts in your playbook!</p>"},{"location":"guides/agent-optimization/prompts/#advanced-prompt-specific-tuning","title":"Advanced: Prompt-Specific Tuning","text":""},{"location":"guides/agent-optimization/prompts/#control-what-gepa-optimizes","title":"Control What GEPA Optimizes","text":"<pre><code>optimization:\n  optimizer:\n    name: GEPA\n    params:\n      # Focus on prompts\n      variables_to_optimize:\n        - persona.role\n        - persona.goal\n        - tasks[*].instruction\n</code></pre>"},{"location":"guides/agent-optimization/prompts/#optimization-intensity","title":"Optimization Intensity","text":"<pre><code>optimization:\n  optimizer:\n    params:\n      auto: light      # Quick prompt tweaks (3-5 iterations)\n      auto: medium     # Balanced optimization (10-15 iterations)\n      auto: intensive  # Thorough exploration (20-30 iterations)\n</code></pre>"},{"location":"guides/agent-optimization/prompts/#common-prompt-improvements","title":"Common Prompt Improvements","text":""},{"location":"guides/agent-optimization/prompts/#improvement-1-adding-specificity","title":"Improvement 1: Adding Specificity","text":"<p>Before: \"Analyze the code\" After: \"Analyze code for: (1) Security vulnerabilities per OWASP Top 10, (2) Performance issues with O(n) or worse complexity, (3) Code quality issues with cyclomatic complexity &gt; 4\"</p>"},{"location":"guides/agent-optimization/prompts/#improvement-2-adding-structure","title":"Improvement 2: Adding Structure","text":"<p>Before: \"Provide feedback\" After: \"Structure review as: Severity \u2192 Findings (line number, issue, impact) \u2192 Solutions (code example) \u2192 References (standards)\"</p>"},{"location":"guides/agent-optimization/prompts/#improvement-3-adding-examples","title":"Improvement 3: Adding Examples","text":"<p>Before: \"Suggest improvements\" After: \"Suggest improvements with before/after code examples showing exact changes needed\"</p>"},{"location":"guides/agent-optimization/prompts/#improvement-4-adding-context","title":"Improvement 4: Adding Context","text":"<p>Before: \"Review code quality\" After: \"Review code quality considering: project type, team size, production criticality, industry standards\"</p>"},{"location":"guides/agent-optimization/prompts/#integration-with-other-layers","title":"Integration with Other Layers","text":"<p>Prompt optimization works best when combined with other layers:</p> <p>Prompt + RAG: <pre><code>Prompt: \"Search security documentation before analyzing SQL queries\"\n\u2192 Combines optimized instructions with optimized retrieval\n</code></pre></p> <p>Prompt + Tools: <pre><code>Prompt: \"Use complexity_calculator for nested conditions exceeding 3 levels\"\n\u2192 Combines optimized instructions with optimized tool selection\n</code></pre></p> <p>Prompt + Memory: <pre><code>Prompt: \"Reference similar past findings when identifying patterns\"\n\u2192 Combines optimized instructions with optimized memory retrieval\n</code></pre></p>"},{"location":"guides/agent-optimization/prompts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/prompts/#issue-prompts-not-improving","title":"Issue: Prompts Not Improving","text":"<p>Symptoms: Optimization runs but prompts stay similar</p> <p>Solutions: 1. Check RSpec-style BDD scenarios are specific enough 2. Ensure datasets have diverse examples 3. Increase iterations: <code>--auto intensive</code> 4. Add reflection_lm: <code>--reflection-lm llama3.1:8b</code></p>"},{"location":"guides/agent-optimization/prompts/#issue-prompts-too-long","title":"Issue: Prompts Too Long","text":"<p>Symptoms: Optimized prompts exceed token limits</p> <p>Solutions: 1. Set max_tokens constraint in optimization config 2. Use summarization in optimization params 3. Focus on key instructions, not exhaustive lists</p>"},{"location":"guides/agent-optimization/prompts/#issue-inconsistent-format","title":"Issue: Inconsistent Format","text":"<p>Symptoms: Agent responses vary in structure</p> <p>Solutions: 1. Define strict output_fields schema 2. Add format examples in RSpec-style BDD scenarios 3. Use structured output in task configuration</p>"},{"location":"guides/agent-optimization/prompts/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udd0d RAG Optimization - Optimize knowledge retrieval</li> <li>\ud83d\udee0\ufe0f Tool Optimization - Optimize tool selection</li> <li>\ud83e\udde0 Memory Optimization - Optimize context selection</li> <li>\ud83d\udcca Dataset-Driven Optimization - Train on large-scale data</li> <li>\ud83c\udfaf Full-Stack Example - See all layers together</li> <li>GEPA Optimizer Guide - Technical details</li> <li>SuperSpec DSL - Playbook configuration</li> </ul> <p>Next: Learn how GEPA optimizes RAG retrieval strategies \u2192</p>"},{"location":"guides/agent-optimization/protocols/","title":"Protocol Optimization (MCP)","text":""},{"location":"guides/agent-optimization/protocols/#what-is-protocol-optimization","title":"What is Protocol Optimization?","text":"<p>Protocol optimization is the process of improving how agents use standardized communication protocols like MCP (Model Context Protocol) for tool interaction. GEPA learns optimal protocol invocation patterns, tool selection within protocols, and error handling strategies.</p> <p>Key Insight: Protocols like MCP provide powerful tool ecosystems, but agents must learn WHEN to use protocol tools vs. built-in tools, HOW to structure protocol calls, and HOW to handle protocol-specific errors.</p>"},{"location":"guides/agent-optimization/protocols/#what-is-mcp","title":"What is MCP?","text":"<p>MCP (Model Context Protocol) is an open protocol for connecting AI models with external tools and data sources. It standardizes how agents discover, invoke, and interact with tools.</p> <p>MCP Benefits: - Standardized tool interface - Automatic tool discovery - Rich tool ecosystems - Cross-platform compatibility</p> <p>SuperOptiX + MCP: GEPA optimizes how agents use MCP tools for maximum effectiveness.</p>"},{"location":"guides/agent-optimization/protocols/#the-protocol-optimization-problem","title":"The Protocol Optimization Problem","text":""},{"location":"guides/agent-optimization/protocols/#without-optimization","title":"Without Optimization","text":"<p>Scenario: Agent with both built-in and MCP tools</p> <pre><code>1. Agent has: built-in calculator + MCP math_tools\n2. Query: \"Calculate 2^10\"\n3. Agent: Doesn't know which to use\n4. Agent: Uses wrong tool or no tool\n</code></pre> <p>Problem: Tool confusion, suboptimal usage</p>"},{"location":"guides/agent-optimization/protocols/#with-gepa-optimization","title":"With GEPA Optimization","text":"<p>Scenario: Same tools, same query</p> <pre><code>1. Agent has: built-in calculator + MCP math_tools\n2. GEPA-learned strategy: \"Use MCP math_tools for advanced math, built-in for simple\"\n3. Query: \"Calculate 2^10\"\n4. Agent: Uses built-in calculator (simpler, faster)\n5. Agent: Returns 1024\n</code></pre> <p>Solution: Optimal tool selection based on query complexity</p>"},{"location":"guides/agent-optimization/protocols/#what-gepa-optimizes-in-mcp","title":"What GEPA Optimizes in MCP","text":""},{"location":"guides/agent-optimization/protocols/#protocol-tool-selection","title":"Protocol Tool Selection","text":"<p>What It Is: Learning when to use MCP tools vs. built-in tools</p> <p>What GEPA Learns: - Complexity thresholds (simple \u2192 built-in, complex \u2192 MCP) - Capability mapping (which tool for which task) - Performance tradeoffs (latency vs. capability)</p> <p>Example Configuration:</p> <pre><code>spec:\n  tools:\n    enabled: true\n    protocol: mcp\n    mcp_servers:\n      - name: filesystem\n        command: uvx mcp-server-filesystem\n      - name: github\n        command: uvx mcp-server-github\n\n    # Built-in tools also available\n    categories:\n      - utilities\n</code></pre> <p>Before Optimization: <pre><code>Strategy: Always prefer MCP tools (slower, more overhead)\nResult: Simple tasks take longer than needed\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategies:\n- \"Simple file read \u2192 Use built-in file_reader (fast)\"\n- \"Git operations \u2192 Use MCP github server (required)\"\n- \"Complex filesystem ops \u2192 Use MCP filesystem server (powerful)\"\n- \"Calculations \u2192 Use built-in calculator unless advanced math needed\"\n</code></pre></p> <p>Impact: 3x faster for simple tasks, same power for complex tasks</p>"},{"location":"guides/agent-optimization/protocols/#protocol-invocation-patterns","title":"Protocol Invocation Patterns","text":"<p>What It Is: Learning how to structure and sequence protocol calls</p> <p>What GEPA Learns: - Call structure optimization - Parameter selection - Error recovery patterns - Result caching strategies</p> <p>Before Optimization: <pre><code>MCP Call: Generic parameters, no error handling\n\nResult: Failures on edge cases, no retry logic\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Patterns:\n1. Validate inputs before MCP call\n2. Use specific parameters based on context\n3. Implement retry logic for transient failures\n4. Cache results for repeated queries\n5. Fall back to built-in tools if MCP unavailable\n</code></pre></p> <p>Impact: 90% error recovery rate, robust performance</p>"},{"location":"guides/agent-optimization/protocols/#multi-protocol-orchestration","title":"Multi-Protocol Orchestration","text":"<p>What It Is: Learning to coordinate multiple MCP servers</p> <p>What GEPA Learns: - Which server for which task - Cross-server workflows - Result aggregation from multiple protocols</p> <p>Example: Code review with MCP</p> <p>Before Optimization: <pre><code>Agent: Uses only one MCP server, misses context\nResult: Incomplete analysis\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Orchestration:\n1. MCP filesystem \u2192 Read code file\n2. MCP github \u2192 Get commit history and PR comments\n3. Built-in security_scanner \u2192 Analyze code\n4. Synthesize: File content + Git context + Security analysis\n\nResult: Comprehensive review with version control context\n</code></pre></p> <p>Impact: Multi-dimensional analysis vs. single-source review</p>"},{"location":"guides/agent-optimization/protocols/#error-handling-and-fallbacks","title":"Error Handling and Fallbacks","text":"<p>What It Is: Learning how to handle protocol failures</p> <p>What GEPA Learns: - When to retry - When to fall back to alternatives - How to communicate errors - Graceful degradation</p> <p>Before Optimization: <pre><code>MCP server down \u2192 Agent fails completely\nResponse: \"Error: Unable to process request\"\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Fallback Strategy:\n1. Try MCP tool\n2. If timeout \u2192 Retry once\n3. If still failing \u2192 Fall back to built-in tool\n4. If no fallback \u2192 Explain limitation gracefully\n\nResponse: \"MCP server temporarily unavailable. Using built-in tools for analysis. \n           Results may be less comprehensive but still actionable.\"\n</code></pre></p> <p>Impact: Graceful degradation vs. complete failure</p>"},{"location":"guides/agent-optimization/protocols/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/protocols/#scenario-file-analysis-with-mcp","title":"Scenario: File Analysis with MCP","text":"<p>Setup: - Built-in tool: <code>file_reader</code> (simple, fast) - MCP tool: <code>mcp-server-filesystem</code> (advanced, slower)</p> <p>Query: \"Read config.json and check for hardcoded secrets\"</p> <p>Before Protocol Optimization: <pre><code>Tool Selection: Always MCP (overkill for simple read)\nSteps:\n1. Initialize MCP connection (300ms)\n2. Discover tools (200ms)\n3. Read file via MCP (150ms)\n4. Analyze (agent processing)\nTotal: 650ms + analysis\n\nResult: Correct but slow\n</code></pre></p> <p>After GEPA Protocol Optimization: <pre><code>GEPA-Learned Strategy: \"Simple file read \u2192 Use built-in (faster)\"\n\nTool Selection: built-in file_reader\nSteps:\n1. Read file (50ms)\n2. Analyze (agent processing)\nTotal: 50ms + analysis\n\nResult: Same correctness, 13x faster\n\nBUT, if query was: \"Read all JSON files in directory recursively\"\nGEPA learns: \"Complex filesystem ops \u2192 Use MCP (more powerful)\"\nTool Selection: mcp-server-filesystem\n</code></pre></p> <p>Impact: Optimal tool choice based on task complexity</p>"},{"location":"guides/agent-optimization/protocols/#how-gepa-learns-protocol-strategies","title":"How GEPA Learns Protocol Strategies","text":""},{"location":"guides/agent-optimization/protocols/#the-optimization-process","title":"The Optimization Process","text":"<ol> <li> <p>Analysis Phase <pre><code>GEPA Observes:\n- Agent used MCP for simple file read\n- Operation took 650ms (slow)\n- Built-in tool could have done it in 50ms\n- RSpec-style BDD scenario passed but performance poor\n</code></pre></p> </li> <li> <p>Reflection Phase <pre><code>GEPA Reflection:\n\"MCP overhead (500ms) not justified for simple file read.\n Built-in file_reader could handle this faster.\n Learn pattern: Simple reads \u2192 built-in, Complex ops \u2192 MCP\"\n</code></pre></p> </li> <li> <p>Mutation Phase <pre><code>GEPA Tests:\n- Strategy 1: \"Always use built-in\" (fails on complex ops)\n- Strategy 2: \"Always use MCP\" (slow on simple ops)\n- Strategy 3: \"Built-in for single file, MCP for directory/recursive\"\n</code></pre></p> </li> <li> <p>Evaluation Phase <pre><code>Results:\n- Strategy 1: 60% (fast but limited)\n- Strategy 2: 70% (powerful but slow)\n- Strategy 3: 95% (optimal!) \u2190 Winner!\n</code></pre></p> </li> <li> <p>Selection Phase <pre><code>GEPA Keeps: Strategy 3 (complexity-based selection)\nGeneralizes: Pattern applies to other tool types\n</code></pre></p> </li> </ol> <p>Result: Learned when to use protocol tools vs. built-in tools</p>"},{"location":"guides/agent-optimization/protocols/#best-practices","title":"Best Practices","text":""},{"location":"guides/agent-optimization/protocols/#configure-both-mcp-and-built-in-tools","title":"Configure Both MCP and Built-in Tools","text":"<pre><code>tools:\n  enabled: true\n  protocol: mcp\n\n  # MCP servers\n  mcp_servers:\n    - name: filesystem\n      command: uvx mcp-server-filesystem\n    - name: github  \n      command: uvx mcp-server-github\n\n  # Built-in tools as fallbacks\n  categories:\n    - utilities\n    - code_analysis\n</code></pre> <p>GEPA learns when to use each type.</p>"},{"location":"guides/agent-optimization/protocols/#define-protocol-aware-rspec-style-bdd-scenarios","title":"Define Protocol-Aware RSpec-Style BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: git_operations\n      description: Agent should use MCP github server for Git ops\n      input:\n        task: \"Get latest commit message\"\n      expected_output:\n        result: Must use MCP github tool (not built-in)\n</code></pre>"},{"location":"guides/agent-optimization/protocols/#enable-error-recovery","title":"Enable Error Recovery","text":"<pre><code>tools:\n  retry_on_failure: true\n  fallback_to_builtin: true\n  timeout: 30\n</code></pre> <p>GEPA optimizes retry and fallback strategies.</p>"},{"location":"guides/agent-optimization/protocols/#monitor-protocol-performance","title":"Monitor Protocol Performance","text":"<pre><code>observability:\n  enabled: true\n  track_tool_latency: true\n  track_protocol_usage: true\n</code></pre> <p>Helps GEPA learn performance-optimal patterns.</p>"},{"location":"guides/agent-optimization/protocols/#common-protocol-strategies-gepa-learns","title":"Common Protocol Strategies GEPA Learns","text":""},{"location":"guides/agent-optimization/protocols/#strategy-1-complexity-based-selection","title":"Strategy 1: Complexity-Based Selection","text":"<p>Before: Random tool choice After: \"Simple operations \u2192 built-in tools. Complex operations \u2192 MCP tools\"</p>"},{"location":"guides/agent-optimization/protocols/#strategy-2-capability-aware-routing","title":"Strategy 2: Capability-Aware Routing","text":"<p>Before: Try built-in first, fail, then MCP After: \"Git operations always require MCP github server. Don't try built-in.\"</p>"},{"location":"guides/agent-optimization/protocols/#strategy-3-performance-optimization","title":"Strategy 3: Performance Optimization","text":"<p>Before: Always initialize all MCP servers After: \"Lazy-load MCP servers only when needed. Cache connections.\"</p>"},{"location":"guides/agent-optimization/protocols/#strategy-4-graceful-degradation","title":"Strategy 4: Graceful Degradation","text":"<p>Before: MCP fails \u2192 Agent fails After: \"MCP unavailable \u2192 Use built-in tools with disclaimer about limitations\"</p>"},{"location":"guides/agent-optimization/protocols/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"guides/agent-optimization/protocols/#what-gets-measured","title":"What Gets Measured","text":"<ul> <li>Protocol Selection Accuracy: % correct protocol choice</li> <li>Latency: Average tool execution time</li> <li>Error Recovery Rate: % of protocol failures recovered</li> <li>Capability Utilization: % of protocol features used effectively</li> </ul>"},{"location":"guides/agent-optimization/protocols/#typical-improvements","title":"Typical Improvements","text":"Metric Before After Improvement Protocol Selection Accuracy 40% 95% +55% Average Latency 850ms 200ms 4x faster Error Recovery 10% 90% +80% Feature Utilization 30% 85% +55%"},{"location":"guides/agent-optimization/protocols/#quick-start","title":"Quick Start","text":""},{"location":"guides/agent-optimization/protocols/#enable-protocol-optimization","title":"Enable Protocol Optimization","text":"<pre><code>spec:\n  # MCP protocol configuration\n  tools:\n    enabled: true\n    protocol: mcp\n    mcp_servers:\n      - name: filesystem\n        command: uvx mcp-server-filesystem\n\n  # GEPA automatically optimizes protocol usage\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre> <pre><code>super agent compile your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA will learn optimal MCP usage patterns!</p>"},{"location":"guides/agent-optimization/protocols/#advanced-mcp-specific-configuration","title":"Advanced: MCP-Specific Configuration","text":""},{"location":"guides/agent-optimization/protocols/#multiple-mcp-servers","title":"Multiple MCP Servers","text":"<pre><code>tools:\n  protocol: mcp\n  mcp_servers:\n    - name: filesystem\n      command: uvx mcp-server-filesystem\n      capabilities: [read, write, list, search]\n\n    - name: github\n      command: uvx mcp-server-github\n      env:\n        GITHUB_TOKEN: ${GITHUB_TOKEN}\n      capabilities: [repos, issues, prs]\n\n    - name: database\n      command: uvx mcp-server-postgres\n      capabilities: [query, schema]\n</code></pre> <p>GEPA learns which server for which operation.</p>"},{"location":"guides/agent-optimization/protocols/#what-gepa-optimizes","title":"What GEPA Optimizes","text":""},{"location":"guides/agent-optimization/protocols/#tool-source-selection-mcp-vs-built-in","title":"Tool Source Selection (MCP vs. Built-in)","text":"<p>What GEPA Learns: - When MCP tools provide value over built-in - Performance tradeoffs - Capability requirements</p> <p>Example:</p> <p>Before Optimization: <pre><code>All tools \u2192 MCP (slow initialization for simple tasks)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Decision Tree:\n- File exists check \u2192 built-in (1ms)\n- Read single file \u2192 built-in (10ms)\n- Read directory recursively \u2192 MCP filesystem (powerful)\n- Git operations \u2192 MCP github (required)\n- Database queries \u2192 MCP postgres (specialized)\n</code></pre></p> <p>Impact: Optimal tool source for each operation type</p>"},{"location":"guides/agent-optimization/protocols/#protocol-call-optimization","title":"Protocol Call Optimization","text":"<p>What GEPA Learns: - Optimal parameter structuring - Efficient call sequencing - Connection reuse</p> <p>Before Optimization: <pre><code>Each call:\n1. Initialize MCP connection\n2. Discover tools\n3. Call tool\n4. Close connection\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Pattern:\n1. Initialize connection once\n2. Cache tool discovery\n3. Reuse connection for multiple calls\n4. Close after session\n\nResult: 5x faster for multi-call scenarios\n</code></pre></p>"},{"location":"guides/agent-optimization/protocols/#error-handling-patterns","title":"Error Handling Patterns","text":"<p>What GEPA Learns: - MCP-specific error types - Recovery strategies - Fallback approaches</p> <p>Before Optimization: <pre><code>MCP server not responding \u2192 Agent fails\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Error Handling:\n1. Connection timeout \u2192 Retry with backoff\n2. Tool not found \u2192 Check MCP server status\n3. Server crashed \u2192 Fall back to built-in tools\n4. Permission denied \u2192 Explain and suggest alternatives\n</code></pre></p> <p>Impact: Robust operation even with protocol issues</p>"},{"location":"guides/agent-optimization/protocols/#beforeafter-comparison_1","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/protocols/#scenario-code-file-analysis","title":"Scenario: Code File Analysis","text":"<p>Query: \"Analyze all Python files in src/ directory for security issues\"</p> <p>Available Tools: - Built-in: <code>file_reader</code>, <code>security_scanner</code> - MCP: <code>mcp-server-filesystem</code> (recursive read), <code>mcp-server-github</code> (git context)</p> <p>Before Protocol Optimization: <pre><code>Tool Selection: Random or all built-in\nSteps:\n1. Try file_reader on \"src/\" \u2192 Fails (not recursive)\n2. Agent gives up\nResponse: \"Unable to analyze directory\"\n</code></pre></p> <p>After GEPA Protocol Optimization: <pre><code>GEPA-Learned Strategy:\n\nStep 1: Recognize \"all files in directory\" \u2192 Requires recursive read\nTool: MCP filesystem server\n\nStep 2: Read all .py files recursively\nResult: 15 Python files found\n\nStep 3: For each file \u2192 Use built-in security_scanner (faster)\nResults: 3 files with SQL injection, 2 with hardcoded secrets\n\nStep 4: Use MCP github \u2192 Get git context\nResult: Files with issues were recently modified\n\nAgent Response:\n\"Security Analysis Complete\n\nAnalyzed: 15 Python files in src/ directory\n\nCritical Issues Found:\n1. auth.py (Line 45): SQL injection\n   [Modified: 2 days ago by dev@team.com]\n\n2. config.py (Line 12): Hardcoded API key\n   [Modified: 1 week ago by admin@team.com]\n\n3. database.py (Line 89): SQL injection\n   [Modified: 3 days ago by dev@team.com]\n\nRecommendation: Review recent commits for security practices\n\nFiles Analyzed: 15\nIssues Found: 5\nHigh Priority: 3\nScan Time: 2.3s\"\n</code></pre></p> <p>Improvement: From directory analysis failure \u2192 Comprehensive security audit</p>"},{"location":"guides/agent-optimization/protocols/#common-mcp-optimization-patterns","title":"Common MCP Optimization Patterns","text":""},{"location":"guides/agent-optimization/protocols/#pattern-1-lazy-loading","title":"Pattern 1: Lazy Loading","text":"<p>Before: Initialize all MCP servers at startup After: \"Initialize MCP servers only when needed, cache for session\"</p>"},{"location":"guides/agent-optimization/protocols/#pattern-2-capability-based-routing","title":"Pattern 2: Capability-Based Routing","text":"<p>Before: Try built-in first, then MCP After: \"Check required capability first. If MCP-only (like Git), go direct to MCP\"</p>"},{"location":"guides/agent-optimization/protocols/#pattern-3-batch-operations","title":"Pattern 3: Batch Operations","text":"<p>Before: One MCP call per item After: \"Batch similar operations into single MCP call when possible\"</p>"},{"location":"guides/agent-optimization/protocols/#pattern-4-connection-pooling","title":"Pattern 4: Connection Pooling","text":"<p>Before: New connection per request After: \"Reuse connections across requests, maintain connection pool\"</p>"},{"location":"guides/agent-optimization/protocols/#integration-with-other-layers","title":"Integration with Other Layers","text":"<p>Protocol optimization works synergistically:</p> <p>Protocols + Prompts: <pre><code>Optimized Prompt: \"For Git operations, use MCP github server\"\nOptimized Protocol: Correct MCP server selection\n\u2192 Agent efficiently handles Git workflows\n</code></pre></p> <p>Protocols + RAG: <pre><code>Optimized Protocol: MCP github \u2192 Get PR comments\nOptimized RAG: Retrieves review guidelines doc\n\u2192 Agent provides review consistent with team standards + Git context\n</code></pre></p> <p>Protocols + Tools: <pre><code>Optimized Protocol: MCP filesystem \u2192 Read files\nOptimized Tools: Built-in security_scanner \u2192 Analyze content\n\u2192 Best of both: MCP power + Built-in speed\n</code></pre></p>"},{"location":"guides/agent-optimization/protocols/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/protocols/#issue-mcp-tools-always-used-performance-penalty","title":"Issue: MCP Tools Always Used (Performance Penalty)","text":"<p>Symptoms: Even simple operations use MCP</p> <p>Solutions: 1. Add RSpec-style BDD scenarios showing when NOT to use MCP 2. Enable built-in tools as alternatives 3. Add latency metrics to optimization 4. Use <code>--fresh</code> flag to see optimization decisions</p>"},{"location":"guides/agent-optimization/protocols/#issue-mcp-connection-failures","title":"Issue: MCP Connection Failures","text":"<p>Symptoms: Protocol tools fail frequently</p> <p>Solutions: 1. Implement retry logic in tool configuration 2. Enable fallback to built-in tools 3. Add connection health checks 4. Optimize error handling through GEPA</p>"},{"location":"guides/agent-optimization/protocols/#issue-inefficient-mcp-usage","title":"Issue: Inefficient MCP Usage","text":"<p>Symptoms: Multiple connections for same session</p> <p>Solutions: 1. Enable connection pooling 2. Cache tool discovery results 3. Batch similar operations 4. GEPA learns these patterns automatically</p>"},{"location":"guides/agent-optimization/protocols/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udcac Prompt Optimization - Optimize instructions</li> <li>\ud83d\udd0d RAG Optimization - Optimize retrieval</li> <li>\ud83d\udee0\ufe0f Tool Optimization - Optimize tool selection</li> <li>\ud83e\udde0 Memory Optimization - Optimize context</li> <li>\ud83c\udfaf Full-Stack Example - See all layers</li> <li>MCP Protocol Guide - MCP setup</li> <li>MCP + RAG Guide - Combined optimization</li> <li>MCP Optimization Tutorial - Step-by-step</li> </ul> <p>Next: Learn how GEPA leverages large-scale datasets \u2192</p>"},{"location":"guides/agent-optimization/rag/","title":"RAG Optimization","text":""},{"location":"guides/agent-optimization/rag/#what-is-rag-optimization","title":"What is RAG Optimization?","text":"<p>RAG (Retrieval-Augmented Generation) optimization is the process of improving when, which, and how an agent retrieves knowledge from its knowledge base. While traditional RAG uses fixed retrieval strategies, GEPA learns dynamic, context-aware retrieval patterns.</p> <p>Key Insight: It's not enough to have a knowledge base. The agent must learn WHEN to search it, WHICH documents to retrieve, and HOW to integrate that knowledge into its response.</p>"},{"location":"guides/agent-optimization/rag/#the-rag-optimization-problem","title":"The RAG Optimization Problem","text":""},{"location":"guides/agent-optimization/rag/#without-optimization","title":"Without Optimization","text":"<p>Scenario: Agent reviewing code for SQL injection</p> <pre><code>1. Agent receives code with SQL injection\n2. Agent retrieves random documents from knowledge base\n3. Agent might get: \"Python naming conventions.md\" (irrelevant)\n4. Agent gives vague response: \"Check your code\"\n</code></pre> <p>Problem: Wrong documents retrieved, no actionable solution</p>"},{"location":"guides/agent-optimization/rag/#with-gepa-optimization","title":"With GEPA Optimization","text":"<p>Scenario: Same code review</p> <pre><code>1. Agent receives code with SQL injection\n2. GEPA-learned strategy: \"Search security docs for SQL patterns\"\n3. Agent retrieves: \"sql_injection.md\" (highly relevant)\n4. Agent gives specific response: \"SQL injection detected. Use parameterized queries: query = 'SELECT * WHERE id = ?'\"\n</code></pre> <p>Solution: Right documents at right time, actionable solution</p>"},{"location":"guides/agent-optimization/rag/#what-gepa-optimizes-in-rag","title":"What GEPA Optimizes in RAG","text":""},{"location":"guides/agent-optimization/rag/#retrieval-strategy-when-to-search","title":"Retrieval Strategy (When to Search)","text":"<p>What It Is: Learning when to query the knowledge base</p> <p>What GEPA Learns: - Which scenarios require knowledge retrieval - When to search before analysis vs. after - When to skip retrieval (already have knowledge)</p> <p>Example Configuration:</p> <pre><code>spec:\n  rag:\n    enabled: true\n    vector_database: chromadb\n    collection: code_review_knowledge\n    knowledge_base:\n      - ./knowledge/security/*.md\n      - ./knowledge/python/*.md\n      - ./knowledge/performance/*.md\n</code></pre> <p>Before Optimization: <pre><code>Strategy: Always search all knowledge sources for every query\nResult: Slow, often irrelevant docs retrieved\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategies:\n- \"Search security/*.md when code contains string concatenation in SQL\"\n- \"Search performance/*.md when detecting loops or recursion\"\n- \"Search python/*.md for naming and style issues\"\n- \"Skip search for simple syntax errors\"\n</code></pre></p> <p>Impact: 3x faster, 85% relevance vs. 30% relevance</p>"},{"location":"guides/agent-optimization/rag/#document-selection-which-to-retrieve","title":"Document Selection (Which to Retrieve)","text":"<p>What It Is: Choosing the most relevant documents from the knowledge base</p> <p>What GEPA Learns: - Query formulation for semantic search - Relevance threshold tuning - Number of documents to retrieve (top_k)</p> <p>Configuration:</p> <pre><code>rag:\n  top_k: 5\n  similarity_threshold: 0.7\n</code></pre> <p>Before Optimization: <pre><code>Query: Generic \"code review\"\nRetrieved: Random 5 docs, low relevance\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Query Strategies:\n- For SQL code: \"SQL injection prevention parameterized queries OWASP\"\n- For loops: \"performance optimization time complexity O(n)\"\n- For naming: \"Python naming conventions PEP 8\"\n\nRetrieved: Top 5 highly relevant docs (&gt;0.85 similarity)\n</code></pre></p> <p>Impact: Precision increased from 40% \u2192 90%</p>"},{"location":"guides/agent-optimization/rag/#context-integration-how-to-use","title":"Context Integration (How to Use)","text":"<p>What It Is: Incorporating retrieved knowledge into agent responses</p> <p>What GEPA Learns: - How to cite retrieved documents - How to combine multiple sources - When to quote vs. paraphrase - How to attribute knowledge</p> <p>Before Optimization: <pre><code>Response: \"This is a security issue.\"\n[Retrieved doc not cited or used effectively]\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Response: \"SQL Injection Vulnerability (CRITICAL)\n\nAccording to OWASP Top 10 2024 [Retrieved from: security/sql_injection.md]:\n'Injection attacks occur when untrusted data is sent to an interpreter as \npart of a command or query.'\n\nSolution (from best practices):\n```python\nquery = \"SELECT * FROM users WHERE name = ?\"\ndb.execute(query, (username,))\n</code></pre></p> <p>Reference: OWASP Top 10 #1, CWE-89\" <pre><code>**Impact**: Clear attribution, professional citations, actionable solutions\n\n---\n\n### Relevance Scoring\n\n**What It Is**: Filtering and ranking retrieved documents\n\n**What GEPA Learns**:\n- Optimal similarity thresholds\n- Re-ranking strategies\n- Filtering irrelevant results\n\n**Configuration**:\n\n```yaml\nrag:\n  similarity_threshold: 0.7\n  rerank: true\n</code></pre></p> <p>Before Optimization: <pre><code>Retrieved 5 docs with similarities: [0.71, 0.68, 0.65, 0.64, 0.62]\nUsed all 5 (including low-relevance ones)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Threshold: 0.75 for security queries, 0.65 for general\nRetrieved 5 docs, filtered to top 2 with &gt;0.85 similarity\nRe-ranked by: recency + domain match + keyword overlap\n</code></pre></p> <p>Impact: Higher quality context, less noise</p>"},{"location":"guides/agent-optimization/rag/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/rag/#scenario-security-code-review","title":"Scenario: Security Code Review","text":"<p>Input Code: <pre><code>def get_user(username):\n    query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n    return db.execute(query)\n</code></pre></p> <p>Before RAG Optimization: <pre><code>RAG Behavior:\n- Query: \"code review\"\n- Retrieved: naming_conventions.md, file_organization.md, testing.md\n- Relevance: Low (wrong topic)\n\nAgent Response:\n\"This code could be improved. Consider better practices.\"\n</code></pre></p> <p>After GEPA RAG Optimization: <pre><code>RAG Behavior:\n- Learned Strategy: \"Detect SQL pattern \u2192 Search security docs\"\n- Query: \"SQL injection prevention parameterized queries\"\n- Retrieved: sql_injection.md, database_security.md\n- Relevance: High (&gt;0.90 similarity)\n\nAgent Response:\n\"SQL Injection Vulnerability (CRITICAL)\n\nIssue: String concatenation in SQL query allows injection attacks.\n\nAccording to OWASP Top 10 [Retrieved from: sql_injection.md]:\nThis is the #1 web security risk in 2024.\n\nSolution:\n```python\ndef get_user(username):\n    query = \"SELECT * FROM users WHERE name = ?\"\n    return db.execute(query, (username,))\n</code></pre></p> <p>Why: Parameterized queries prevent injection by treating user input as data, not code.</p> <p>Reference: OWASP A03:2021 - Injection\" <pre><code>**Improvement**: From irrelevant docs \u2192 Precise security knowledge\n\n---\n\n## How GEPA Learns RAG Strategies\n\n### The Optimization Process\n\n1. **Analysis Phase**\n   ```\n   GEPA Observes:\n   - Agent retrieved \"naming_conventions.md\" for SQL injection code\n   - Document wasn't relevant to security issue\n   - Agent gave vague response without specific solution\n   ```\n\n2. **Reflection Phase**\n   ```\n   GEPA Reflection:\n   \"The agent should search security documentation BEFORE analyzing \n    SQL queries. String concatenation in SQL context is a security \n    pattern that requires security knowledge retrieval.\"\n   ```\n\n3. **Mutation Phase**\n   ```\n   GEPA Tests:\n   - Strategy 1: \"Always search security docs for any SQL code\"\n   - Strategy 2: \"Search security docs when detecting string concatenation in SQL\"\n   - Strategy 3: \"Search security docs after finding potential injection\"\n   ```\n\n4. **Evaluation Phase**\n   ```\n   Results:\n   - Strategy 1: 70% (too broad, slow)\n   - Strategy 2: 95% (precise, fast) \u2190 Winner!\n   - Strategy 3: 60% (too late, misses context)\n   ```\n\n5. **Selection Phase**\n   ```\n   GEPA Keeps: Strategy 2\n   Next Iteration: Build on this strategy for other patterns\n   ```\n\n**Result**: Learned when and what to retrieve for maximum relevance\n\n---\n\n## Best Practices\n\n### Organize Knowledge Base by Topic\n\n```yaml\nrag:\n  knowledge_base:\n    - ./knowledge/security/*.md      # Security topics\n    - ./knowledge/performance/*.md   # Performance topics\n    - ./knowledge/best_practices/*.md # General practices\n</code></pre></p> <p>GEPA learns which directory to search for which scenario.</p>"},{"location":"guides/agent-optimization/rag/#use-descriptive-document-names","title":"Use Descriptive Document Names","text":"<pre><code>Good:\n- sql_injection_prevention.md\n- xss_mitigation.md\n- password_hashing_best_practices.md\n\nBad:\n- doc1.md\n- security.md\n- notes.md\n</code></pre>"},{"location":"guides/agent-optimization/rag/#structure-documents-consistently","title":"Structure Documents Consistently","text":"<pre><code># SQL Injection Prevention\n\n## What is SQL Injection?\n[Clear explanation]\n\n## How to Prevent\n[Specific solutions with code]\n\n## References\n[OWASP, CWE links]\n</code></pre> <p>Consistent structure helps GEPA learn effective retrieval patterns.</p>"},{"location":"guides/agent-optimization/rag/#combine-with-rspec-style-bdd-scenarios","title":"Combine with RSpec-Style BDD Scenarios","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: sql_injection_detection\n      description: Agent should use security docs for SQL analysis\n      input:\n        code: [SQL injection code]\n      expected_output:\n        review: Must mention \"SQL injection\" and cite \"OWASP\"\n</code></pre> <p>GEPA optimizes RAG to pass these scenarios.</p>"},{"location":"guides/agent-optimization/rag/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"guides/agent-optimization/rag/#what-gets-measured","title":"What Gets Measured","text":"<ul> <li>Retrieval Precision: % of retrieved docs that are relevant</li> <li>Retrieval Recall: % of relevant docs that are retrieved</li> <li>Response Relevance: % of responses using retrieved knowledge</li> <li>Citation Accuracy: % of citations that are correct</li> </ul>"},{"location":"guides/agent-optimization/rag/#typical-improvements","title":"Typical Improvements","text":"Metric Before After Improvement Retrieval Precision 30% 85% +55% Response Relevance 40% 90% +50% Citation Accuracy 25% 95% +70% Retrieval Speed 2.5s 0.8s 3x faster"},{"location":"guides/agent-optimization/rag/#quick-start","title":"Quick Start","text":""},{"location":"guides/agent-optimization/rag/#enable-rag-optimization","title":"Enable RAG Optimization","text":"<pre><code>spec:\n  # RAG configuration\n  rag:\n    enabled: true\n    vector_database: chromadb\n    knowledge_base:\n      - ./knowledge/**/*.md\n    top_k: 5\n\n  # GEPA automatically optimizes RAG strategies\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre> <pre><code>super agent compile your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA will learn optimal retrieval strategies!</p>"},{"location":"guides/agent-optimization/rag/#advanced-rag-specific-configuration","title":"Advanced: RAG-Specific Configuration","text":""},{"location":"guides/agent-optimization/rag/#fine-tune-retrieval-parameters","title":"Fine-Tune Retrieval Parameters","text":"<pre><code>rag:\n  # Semantic search config\n  top_k: 5                    # Number of docs to retrieve\n  similarity_threshold: 0.7   # Minimum similarity score\n  rerank: true                # Re-rank results\n\n  # Document processing\n  chunk_size: 512             # Tokens per chunk\n  chunk_overlap: 50           # Overlap between chunks\n\n  # Embedding model\n  embedding_model: sentence-transformers/all-MiniLM-L6-v2\n</code></pre> <p>GEPA learns optimal values for these parameters through optimization.</p>"},{"location":"guides/agent-optimization/rag/#common-rag-strategies-gepa-learns","title":"Common RAG Strategies GEPA Learns","text":""},{"location":"guides/agent-optimization/rag/#strategy-1-topic-aware-retrieval","title":"Strategy 1: Topic-Aware Retrieval","text":"<p>Before: Search all knowledge indiscriminately After: \"Search security/.md for SQL/XSS patterns, performance/.md for loops/recursion\"</p>"},{"location":"guides/agent-optimization/rag/#strategy-2-pre-emptive-retrieval","title":"Strategy 2: Pre-emptive Retrieval","text":"<p>Before: Retrieve after analysis (too late) After: \"Search BEFORE analyzing SQL queries to have security context\"</p>"},{"location":"guides/agent-optimization/rag/#strategy-3-contextual-queries","title":"Strategy 3: Contextual Queries","text":"<p>Before: Generic query \"code review\" After: Specific query \"SQL injection prevention parameterized queries OWASP\"</p>"},{"location":"guides/agent-optimization/rag/#strategy-4-multi-source-combination","title":"Strategy 4: Multi-Source Combination","text":"<p>Before: Use only top result After: \"Combine security doc + code example doc + OWASP reference for comprehensive answer\"</p>"},{"location":"guides/agent-optimization/rag/#integration-with-other-layers","title":"Integration with Other Layers","text":"<p>RAG optimization amplifies other layer optimizations:</p> <p>RAG + Prompts: <pre><code>Optimized Prompt: \"Search security docs for SQL patterns\"\nOptimized RAG: Retrieves sql_injection.md with 0.95 similarity\n\u2192 Agent has perfect context for security analysis\n</code></pre></p> <p>RAG + Tools: <pre><code>Optimized RAG: Retrieves complexity best practices\nOptimized Tools: Uses complexity_calculator\n\u2192 Agent cites doc: \"Per Clean Code guidelines, complexity should be &lt;4\"\n\u2192 Then shows: \"Your code: 8 (calculated)\"\n</code></pre></p> <p>RAG + Memory: <pre><code>Optimized Memory: \"Similar SQL injection found in previous review\"\nOptimized RAG: Retrieves same security doc used before\n\u2192 Consistent, high-quality security recommendations\n</code></pre></p>"},{"location":"guides/agent-optimization/rag/#real-world-example","title":"Real-World Example","text":""},{"location":"guides/agent-optimization/rag/#use-case-security-code-review","title":"Use Case: Security Code Review","text":"<p>Knowledge Base Structure: <pre><code>knowledge/\n\u251c\u2500\u2500 security/\n\u2502   \u251c\u2500\u2500 sql_injection.md          (OWASP Top 10 #1)\n\u2502   \u251c\u2500\u2500 xss_prevention.md          (OWASP Top 10 #3)\n\u2502   \u251c\u2500\u2500 hardcoded_secrets.md       (Security best practice)\n\u2502   \u2514\u2500\u2500 password_hashing.md        (Cryptography)\n\u251c\u2500\u2500 performance/\n\u2502   \u251c\u2500\u2500 time_complexity.md         (Big O notation)\n\u2502   \u2514\u2500\u2500 optimization_patterns.md   (Performance tips)\n\u2514\u2500\u2500 best_practices/\n    \u2514\u2500\u2500 solid_principles.md        (Clean code)\n</code></pre></p> <p>Input: Code with SQL injection</p> <p>Before RAG Optimization: <pre><code>Retrieved:\n1. solid_principles.md (similarity: 0.68) - Wrong topic\n2. optimization_patterns.md (similarity: 0.65) - Wrong topic\n3. time_complexity.md (similarity: 0.63) - Wrong topic\n\nResponse: \"Your code needs improvement.\"\n</code></pre></p> <p>After GEPA RAG Optimization: <pre><code>Learned Strategy: \"SQL string concatenation \u2192 security domain\"\n\nRetrieved:\n1. sql_injection.md (similarity: 0.94) - Perfect!\n2. hardcoded_secrets.md (similarity: 0.87) - Relevant!\n3. password_hashing.md (similarity: 0.82) - Related!\n\nResponse: \"SQL Injection Vulnerability (CRITICAL)\n\nIssue: User input concatenated into SQL query (OWASP Top 10 #1)\n[Retrieved from: security/sql_injection.md]\n\nAttack Example: username = \\\"admin' OR '1'='1\\\"\n\u2192 Returns all users, bypasses authentication\n\nSolution (from OWASP guidelines):\n```python\nquery = \"SELECT * FROM users WHERE name = ?\"\nresult = db.execute(query, (username,))\n</code></pre></p> <p>Reference: OWASP A03:2021 - Injection, CWE-89\" ```</p> <p>Impact: 0% helpful \u2192 100% actionable with professional citations</p>"},{"location":"guides/agent-optimization/rag/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/rag/#issue-low-retrieval-relevance","title":"Issue: Low Retrieval Relevance","text":"<p>Symptoms: Agent retrieves wrong documents</p> <p>Solutions: 1. Add more RSpec-style BDD scenarios showing expected retrieval 2. Increase <code>top_k</code> to give GEPA more options 3. Improve document organization by topic 4. Use more specific document titles</p>"},{"location":"guides/agent-optimization/rag/#issue-slow-retrieval","title":"Issue: Slow Retrieval","text":"<p>Symptoms: RAG queries take too long</p> <p>Solutions: 1. GEPA learns to skip retrieval when not needed 2. Reduce <code>top_k</code> (GEPA finds optimal value) 3. Use smaller embedding model 4. Enable caching for repeated queries</p>"},{"location":"guides/agent-optimization/rag/#issue-documents-not-used-in-response","title":"Issue: Documents Not Used in Response","text":"<p>Symptoms: Docs retrieved but not cited</p> <p>Solutions: 1. Add citation requirements to RSpec-style BDD scenarios 2. Optimize prompts to include \"cite sources\" 3. Add examples showing proper citation format</p>"},{"location":"guides/agent-optimization/rag/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udcac Prompt Optimization - Optimize instructions</li> <li>\ud83d\udee0\ufe0f Tool Optimization - Optimize tool usage</li> <li>\ud83e\udde0 Memory Optimization - Optimize context</li> <li>\ud83d\udcca Dataset-Driven Optimization - Train on data</li> <li>\ud83c\udfaf Full-Stack Example - See all layers</li> <li>RAG Configuration Guide - RAG setup details</li> <li>MCP + RAG Complete Guide - Advanced RAG</li> </ul> <p>Next: Learn how GEPA optimizes tool selection and usage \u2192</p>"},{"location":"guides/agent-optimization/tools/","title":"Tool Optimization","text":""},{"location":"guides/agent-optimization/tools/#what-is-tool-optimization","title":"What is Tool Optimization?","text":"<p>Tool optimization is the process of improving an agent's tool selection, invocation order, and output combination strategies. While traditional approaches hardcode which tools to use, GEPA learns dynamic, context-aware tool usage patterns.</p> <p>Key Insight: Having tools isn't enough. The agent must learn WHICH tools to use for WHICH scenarios, in WHAT order, and HOW to combine their outputs for comprehensive results.</p>"},{"location":"guides/agent-optimization/tools/#the-tool-optimization-problem","title":"The Tool Optimization Problem","text":""},{"location":"guides/agent-optimization/tools/#without-optimization","title":"Without Optimization","text":"<p>Scenario: Agent reviewing code with high complexity</p> <pre><code>1. Agent receives nested conditional code\n2. Agent has complexity_calculator tool available\n3. Agent doesn't use it (doesn't know when to use tools)\n4. Agent says: \"Your code might be complex\"\n</code></pre> <p>Problem: Tool available but not used, vague response</p>"},{"location":"guides/agent-optimization/tools/#with-gepa-optimization","title":"With GEPA Optimization","text":"<p>Scenario: Same code review</p> <pre><code>1. Agent receives nested conditional code\n2. GEPA-learned strategy: \"Use complexity_calculator for nested conditions\"\n3. Agent uses tool: complexity_calculator(code)\n4. Tool returns: complexity = 8\n5. Agent says: \"High complexity detected (8/4 threshold). Refactor to reduce nesting.\"\n</code></pre> <p>Solution: Correct tool used, specific metric provided, actionable feedback</p>"},{"location":"guides/agent-optimization/tools/#what-gepa-optimizes-in-tool-usage","title":"What GEPA Optimizes in Tool Usage","text":""},{"location":"guides/agent-optimization/tools/#tool-selection-which-tools-to-use","title":"Tool Selection (Which Tools to Use)","text":"<p>What It Is: Learning which tool(s) to use for each scenario</p> <p>What GEPA Learns: - Pattern \u2192 Tool mapping - When to use specific tools - When to use multiple tools - When to skip tools</p> <p>Example Configuration:</p> <pre><code>spec:\n  tools:\n    enabled: true\n    specific_tools:\n      - complexity_calculator    # Calculates cyclomatic complexity\n      - security_scanner         # Detects vulnerabilities\n      - performance_analyzer     # Identifies performance issues\n      - code_smell_detector      # Finds code smells\n</code></pre> <p>Before Optimization: <pre><code>Strategy: Random tool selection or no tool usage\nResult: Tools used incorrectly or not at all\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategies:\n- \"Use complexity_calculator when seeing nested if/for/while (&gt;3 levels)\"\n- \"Use security_scanner when detecting string concatenation in SQL/HTML\"\n- \"Use performance_analyzer for loops with nested loops\"\n- \"Use code_smell_detector for duplicated code patterns\"\n</code></pre></p> <p>Impact: 0% \u2192 100% correct tool selection</p>"},{"location":"guides/agent-optimization/tools/#tool-invocation-order-orchestration","title":"Tool Invocation Order (Orchestration)","text":"<p>What It Is: Learning the optimal sequence of tool calls</p> <p>What GEPA Learns: - Which tools to call first - Dependencies between tools - Parallel vs. sequential execution - When to stop tool chain</p> <p>Before Optimization: <pre><code>Random order:\n1. code_smell_detector (finds duplication)\n2. complexity_calculator (finds complexity: 8)\n3. security_scanner (finds SQL injection)\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Priority Order:\n1. security_scanner FIRST (critical issues)\n   \u2192 Finds: SQL injection (CRITICAL)\n2. complexity_calculator SECOND (maintainability)\n   \u2192 Finds: Complexity 8 (MEDIUM)\n3. code_smell_detector THIRD (code quality)\n   \u2192 Finds: DRY violation (LOW)\n\nResult: Prioritized by severity automatically\n</code></pre></p> <p>Impact: Logical ordering, priority-based findings</p>"},{"location":"guides/agent-optimization/tools/#output-combination-multi-tool-integration","title":"Output Combination (Multi-Tool Integration)","text":"<p>What It Is: Learning how to combine outputs from multiple tools</p> <p>What GEPA Learns: - How to merge tool results - How to resolve conflicts - How to synthesize comprehensive findings - Citation format for tool outputs</p> <p>Example Scenario: Code with multiple issues</p> <p>Before Optimization: <pre><code>Tool 1: complexity_calculator \u2192 \"Complexity: 8\"\nTool 2: security_scanner \u2192 \"Potential injection\"\n\nAgent Response: \"Complexity is 8. Potential injection found.\"\n[Disconnected findings, no synthesis]\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Tool 1: complexity_calculator \u2192 \"Complexity: 8\"\nTool 2: security_scanner \u2192 \"SQL injection detected\"\n\nAgent Response:\n\"Multiple issues detected:\n\n1. SQL Injection (CRITICAL) [Tool: security_scanner]\n   Line: query = 'SELECT...'\n   Impact: Security breach possible\n\n2. High Complexity (MEDIUM) [Tool: complexity_calculator]\n   Complexity: 8/4 threshold\n   Impact: Hard to maintain and test\n\nRECOMMENDATION: Fix SQL injection first (security), then refactor (maintainability)\"\n</code></pre></p> <p>Impact: Synthesized, prioritized findings vs. disconnected results</p>"},{"location":"guides/agent-optimization/tools/#error-handling-and-fallbacks","title":"Error Handling and Fallbacks","text":"<p>What It Is: Learning how to handle tool failures gracefully</p> <p>What GEPA Learns: - What to do when tool fails - Fallback strategies - Error message interpretation - Alternative tool selection</p> <p>Before Optimization: <pre><code>Tool fails \u2192 Agent gives up\n\"Unable to analyze code.\"\n</code></pre></p> <p>After GEPA Optimization: <pre><code>Learned Strategies:\n- Tool fails \u2192 Try alternative tool\n- All tools fail \u2192 Use pattern matching from training data\n- Partial failure \u2192 Continue with available results\n\nResponse: \"Complexity tool unavailable. Based on pattern analysis: \n           Function has 6 nested conditions, likely high complexity. \n           Recommend refactoring.\"\n</code></pre></p> <p>Impact: Robust performance even when tools fail</p>"},{"location":"guides/agent-optimization/tools/#beforeafter-comparison","title":"Before/After Comparison","text":""},{"location":"guides/agent-optimization/tools/#scenario-performance-code-review","title":"Scenario: Performance Code Review","text":"<p>Input Code: <pre><code>def find_duplicates(items):\n    duplicates = []\n    for i in range(len(items)):\n        for j in range(len(items)):\n            if i != j and items[i] == items[j]:\n                if items[i] not in duplicates:\n                    duplicates.append(items[i])\n    return duplicates\n</code></pre></p> <p>Before Tool Optimization: <pre><code>Tool Usage: None (agent doesn't know to use performance_analyzer)\n\nAgent Response:\n\"This code could be more efficient.\"\n</code></pre> - Vague - No metrics - No solution</p> <p>After GEPA Tool Optimization: <pre><code>Tool Usage:\n1. performance_analyzer(code)\n   \u2192 Detects: Nested loops, O(n\u00b2) complexity\n\n2. complexity_calculator(code)\n   \u2192 Returns: Cyclomatic complexity = 4\n\nLearned Strategy: \"Use both tools for loop analysis\"\n\nAgent Response:\n\"Performance Issue Detected\n\nComplexity Analysis [Tool: complexity_calculator]:\n- Cyclomatic Complexity: 4\n- Nested loops detected\n\nPerformance Analysis [Tool: performance_analyzer]:\n- Time Complexity: O(n\u00b2)\n- Space Complexity: O(n)\n- Issue: Nested iteration over same list\n\nImpact: For 1000 items, 1,000,000 iterations\n\nOptimized Solution (O(n)):\n```python\ndef find_duplicates(items):\n    seen = set()\n    duplicates = set()\n    for item in items:\n        if item in seen:\n            duplicates.add(item)\n        seen.add(item)\n    return list(duplicates)\n</code></pre></p> <p>Performance Improvement: O(n\u00b2) \u2192 O(n) (1000x faster for large lists)\" <pre><code>**Improvement**: From vague \u2192 Specific metrics + Executable solution\n\n---\n\n## How GEPA Learns Tool Strategies\n\n### The Optimization Process\n\n1. **Analysis Phase**\n   ```\n   GEPA Observes:\n   - Agent saw nested loops but didn't use performance_analyzer\n   - Agent gave vague \"could be better\" response\n   - RSpec-style BDD scenario expected O(n\u00b2) mention and solution\n   - Scenario FAILED\n   ```\n\n2. **Reflection Phase**\n   ```\n   GEPA Reflection:\n   \"The agent should use performance_analyzer when detecting nested loops.\n    The tool would have identified O(n\u00b2) complexity and suggested O(n) solution.\n    Need to learn: nested loops \u2192 performance_analyzer\"\n   ```\n\n3. **Mutation Phase**\n   ```\n   GEPA Tests:\n   - Strategy 1: \"Always use performance_analyzer\"\n   - Strategy 2: \"Use performance_analyzer when detecting loops\"\n   - Strategy 3: \"Use performance_analyzer for nested loops only\"\n   ```\n\n4. **Evaluation Phase**\n   ```\n   Results:\n   - Strategy 1: 50% (too many false positives)\n   - Strategy 2: 75% (good but misses single loops)\n   - Strategy 3: 95% (precise!) \u2190 Winner!\n   ```\n\n5. **Selection Phase**\n   ```\n   GEPA Keeps: Strategy 3\n   Next: Learn to combine with complexity_calculator\n   ```\n\n**Result**: Learned precise tool selection patterns\n\n---\n\n## Best Practices\n\n### Provide Diverse Tool Categories\n\n```yaml\ntools:\n  categories:\n    - code_analysis    # Static analysis tools\n    - security         # Security scanning tools\n    - utilities        # General utilities\n  specific_tools:\n    - complexity_calculator\n    - security_scanner\n    - performance_analyzer\n</code></pre></p> <p>GEPA learns which category for which scenario.</p>"},{"location":"guides/agent-optimization/tools/#define-tool-purposes-in-rspec-style-bdd","title":"Define Tool Purposes in RSpec-Style BDD","text":"<pre><code>feature_specifications:\n  scenarios:\n    - name: complexity_detection\n      description: Agent should use complexity_calculator for nested code\n      input:\n        code: [Nested conditionals]\n      expected_output:\n        review: Must include \"complexity: 8\" (from tool)\n</code></pre> <p>GEPA learns: This scenario requires complexity_calculator.</p>"},{"location":"guides/agent-optimization/tools/#show-tool-usage-in-datasets","title":"Show Tool Usage in Datasets","text":"<pre><code>code,review\n\"[nested loops]\",\"Performance: O(n\u00b2) [Tool: performance_analyzer]\"\n\"[SQL concat]\",\"SQL Injection [Tool: security_scanner]\"\n</code></pre> <p>GEPA learns tool patterns from real examples.</p>"},{"location":"guides/agent-optimization/tools/#enable-tool-reflection","title":"Enable Tool Reflection","text":"<pre><code>tools:\n  max_iterations: 5    # Allow multi-step tool usage\n  timeout: 30          # Per-tool timeout\n</code></pre>"},{"location":"guides/agent-optimization/tools/#common-tool-patterns-gepa-learns","title":"Common Tool Patterns GEPA Learns","text":""},{"location":"guides/agent-optimization/tools/#pattern-1-conditional-tool-usage","title":"Pattern 1: Conditional Tool Usage","text":"<p>Before: Use tools randomly After: \"Use complexity_calculator only for functions &gt;5 lines with conditionals\"</p>"},{"location":"guides/agent-optimization/tools/#pattern-2-tool-chaining","title":"Pattern 2: Tool Chaining","text":"<p>Before: Use one tool in isolation After: \"Run security_scanner \u2192 If findings, use vulnerability_db for severity \u2192 Cite from knowledge base\"</p>"},{"location":"guides/agent-optimization/tools/#pattern-3-tool-result-validation","title":"Pattern 3: Tool Result Validation","text":"<p>Before: Trust tool output blindly After: \"If complexity_calculator returns unexpected value, verify with manual pattern check\"</p>"},{"location":"guides/agent-optimization/tools/#pattern-4-multi-tool-synthesis","title":"Pattern 4: Multi-Tool Synthesis","text":"<p>Before: Report each tool result separately After: \"Combine security_scanner + complexity_calculator + code_smell_detector for comprehensive review\"</p>"},{"location":"guides/agent-optimization/tools/#metrics-and-results","title":"Metrics and Results","text":""},{"location":"guides/agent-optimization/tools/#what-gets-measured","title":"What Gets Measured","text":"<ul> <li>Tool Selection Accuracy: % of scenarios where correct tools used</li> <li>Tool Usage Rate: % of scenarios where tools should be used and are</li> <li>Output Synthesis Quality: % of multi-tool outputs properly combined</li> <li>Error Handling: % of tool failures handled gracefully</li> </ul>"},{"location":"guides/agent-optimization/tools/#typical-improvements","title":"Typical Improvements","text":"Metric Before After Improvement Tool Selection Accuracy 25% 100% +75% Tool Usage Rate 30% 95% +65% Multi-Tool Synthesis 20% 85% +65% Error Handling 0% 90% +90%"},{"location":"guides/agent-optimization/tools/#quick-start","title":"Quick Start","text":""},{"location":"guides/agent-optimization/tools/#enable-tool-optimization","title":"Enable Tool Optimization","text":"<pre><code>spec:\n  # Tool configuration\n  tools:\n    enabled: true\n    categories:\n      - code_analysis\n      - security\n    specific_tools:\n      - complexity_calculator\n      - security_scanner\n\n  # GEPA automatically optimizes tool usage\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        auto: medium\n</code></pre> <pre><code>super agent compile your_agent\nsuper agent optimize your_agent --auto medium\n</code></pre> <p>GEPA will learn optimal tool selection strategies!</p>"},{"location":"guides/agent-optimization/tools/#advanced-tool-specific-configuration","title":"Advanced: Tool-Specific Configuration","text":""},{"location":"guides/agent-optimization/tools/#fine-tune-tool-behavior","title":"Fine-Tune Tool Behavior","text":"<pre><code>tools:\n  enabled: true\n  max_iterations: 5        # Allow multi-step tool usage\n  timeout: 30              # Per-tool timeout\n  retry_on_failure: true   # Retry failed tools\n\n  # Tool categories\n  categories:\n    - code_analysis\n    - security\n    - utilities\n\n  # Specific tools with configs\n  specific_tools:\n    - name: complexity_calculator\n      params:\n        threshold: 4\n    - name: security_scanner\n      params:\n        rules: [\"sql-injection\", \"xss\", \"secrets\"]\n</code></pre> <p>GEPA learns optimal parameters through optimization.</p>"},{"location":"guides/agent-optimization/tools/#common-tool-strategies-gepa-learns","title":"Common Tool Strategies GEPA Learns","text":""},{"location":"guides/agent-optimization/tools/#strategy-1-pattern-based-selection","title":"Strategy 1: Pattern-Based Selection","text":"<p>Before: Use all tools for everything After: \"Use security_scanner only when code contains: string concatenation, user input, DB queries, file operations\"</p>"},{"location":"guides/agent-optimization/tools/#strategy-2-progressive-tool-usage","title":"Strategy 2: Progressive Tool Usage","text":"<p>Before: Use all tools upfront After: \"Start with security_scanner. If issues found, then use vulnerability_db for details. If no security issues, check complexity_calculator\"</p>"},{"location":"guides/agent-optimization/tools/#strategy-3-tool-result-interpretation","title":"Strategy 3: Tool Result Interpretation","text":"<p>Before: Report tool output verbatim After: \"Complexity: 8 \u2192 Explain: 'This exceeds threshold of 4, making code hard to test. Recommend refactoring.'\"</p>"},{"location":"guides/agent-optimization/tools/#strategy-4-multi-tool-combination","title":"Strategy 4: Multi-Tool Combination","text":"<p>Before: Report each tool separately After: \"Combine security_scanner + complexity_calculator \u2192 Comprehensive review: 'Code has CRITICAL security issue (priority 1) and MEDIUM complexity issue (priority 2)'\"</p>"},{"location":"guides/agent-optimization/tools/#integration-with-other-layers","title":"Integration with Other Layers","text":"<p>Tool optimization amplifies other optimizations:</p> <p>Tools + Prompts: <pre><code>Optimized Prompt: \"Use complexity_calculator for nested conditions\"\nOptimized Tool: Correctly identifies nested conditions \u2192 Calls tool\n\u2192 Agent provides metric-driven feedback\n</code></pre></p> <p>Tools + RAG: <pre><code>Optimized RAG: Retrieves complexity best practices doc\nOptimized Tool: Calculates actual complexity = 8\n\u2192 Agent says: \"Your complexity (8) exceeds recommended (4) per Clean Code guidelines\"\n</code></pre></p> <p>Tools + Memory: <pre><code>Optimized Memory: Recalls \"Previous similar code had complexity 7, we refactored\"\nOptimized Tool: Calculates current complexity = 8\n\u2192 Agent says: \"Similar to issue #47 (complexity 7). Recommend same refactoring approach\"\n</code></pre></p>"},{"location":"guides/agent-optimization/tools/#real-world-example","title":"Real-World Example","text":""},{"location":"guides/agent-optimization/tools/#use-case-comprehensive-code-analysis","title":"Use Case: Comprehensive Code Analysis","text":"<p>Available Tools: - <code>complexity_calculator</code> - Cyclomatic complexity - <code>security_scanner</code> - Vulnerability detection - <code>performance_analyzer</code> - Performance patterns - <code>code_smell_detector</code> - Code quality issues</p> <p>Input Code: <pre><code>password = \"admin123\"  # Hardcoded secret\n\ndef login(user):\n    # SQL injection vulnerability\n    query = \"SELECT * FROM users WHERE username='\" + user + \"'\"\n    result = db.execute(query)\n\n    # High complexity (nested conditionals)\n    if result:\n        if result['password'] == password:\n            if result['active']:\n                if result['verified']:\n                    return True\n    return False\n</code></pre></p> <p>Before Tool Optimization: <pre><code>Tool Usage: None (doesn't know which tools to use)\n\nResponse: \"Your code has some issues.\"\n</code></pre></p> <p>After GEPA Tool Optimization: <pre><code>Learned Multi-Tool Strategy:\n\nStep 1: security_scanner\n\u2192 Detects: Hardcoded password, SQL injection, plaintext password check\n\nStep 2: complexity_calculator  \n\u2192 Calculates: Cyclomatic complexity = 5\n\nStep 3: Synthesis\n\u2192 Prioritizes: Security (CRITICAL) before complexity (MEDIUM)\n\nAgent Response:\n\"Multiple Critical Issues Detected\n\n1. Hardcoded Credentials (CRITICAL) [Tool: security_scanner]\n   Line: password = 'admin123'\n   Risk: Security breach if code exposed\n\n2. SQL Injection (CRITICAL) [Tool: security_scanner]\n   Line: query = 'SELECT...' + user + '...'\n   Risk: Database compromise possible\n\n3. Plaintext Password Comparison (CRITICAL) [Tool: security_scanner]\n   Line: result['password'] == password\n   Risk: Passwords should be hashed\n\n4. High Cyclomatic Complexity (MEDIUM) [Tool: complexity_calculator]\n   Complexity: 5/4 threshold\n   Issue: Nested conditionals reduce maintainability\n\nPRIORITY:\n1. Fix security issues immediately (3 CRITICAL)\n2. Refactor complexity after security fixed\"\n</code></pre></p> <p>Improvement: From no tool usage \u2192 Strategic multi-tool orchestration</p>"},{"location":"guides/agent-optimization/tools/#how-gepa-learns-tool-strategies","title":"How GEPA Learns Tool Strategies","text":""},{"location":"guides/agent-optimization/tools/#example-learning-to-use-complexity_calculator","title":"Example: Learning to Use complexity_calculator","text":"<p>Iteration 1: <pre><code>Scenario: Code with nested conditionals\nAgent: Doesn't use complexity_calculator\nResult: FAIL (expected \"complexity: 5\" in output)\n\nGEPA Reflection:\n\"Agent failed to use complexity_calculator for nested conditionals.\n Tool is available but not invoked. Need to learn pattern.\"\n</code></pre></p> <p>Iteration 2: <pre><code>Modified Prompt: \"For code with if/else statements, use complexity_calculator\"\nAgent: Uses tool for ALL if statements (even simple ones)\nResult: PARTIAL (too broad, slow)\n\nGEPA Reflection:\n\"Tool usage too aggressive. Should only use for nested (&gt;3 levels)\"\n</code></pre></p> <p>Iteration 3: <pre><code>Refined Prompt: \"Use complexity_calculator when detecting &gt;3 nested conditionals\"\nAgent: Uses tool selectively for complex code only\nResult: PASS (100% accuracy)\n\nGEPA: Strategy learned! Keep this pattern.\n</code></pre></p> <p>Result: Learned precise tool invocation pattern through reflection</p>"},{"location":"guides/agent-optimization/tools/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/agent-optimization/tools/#issue-tools-not-being-used","title":"Issue: Tools Not Being Used","text":"<p>Symptoms: Agent has tools but doesn't call them</p> <p>Solutions: 1. Add RSpec-style BDD scenarios requiring tool outputs 2. Include tool usage examples in datasets 3. Increase optimization iterations 4. Check tool registration is working</p>"},{"location":"guides/agent-optimization/tools/#issue-wrong-tools-used","title":"Issue: Wrong Tools Used","text":"<p>Symptoms: Agent uses inappropriate tools for scenarios</p> <p>Solutions: 1. Add more specific RSpec-style BDD scenarios 2. Increase training examples showing correct tool usage 3. Use tool categories to group related tools 4. Add tool descriptions to help GEPA understand purpose</p>"},{"location":"guides/agent-optimization/tools/#issue-tool-outputs-not-integrated","title":"Issue: Tool Outputs Not Integrated","text":"<p>Symptoms: Tool called but output not used in response</p> <p>Solutions: 1. Require tool outputs in expected_output of RSpec-style BDD scenarios 2. Optimize prompts to include \"cite tool results\" 3. Add examples showing proper tool citation</p>"},{"location":"guides/agent-optimization/tools/#related-guides","title":"Related Guides","text":"<ul> <li>\ud83d\udcac Prompt Optimization - Optimize instructions</li> <li>\ud83d\udd0d RAG Optimization - Optimize knowledge retrieval</li> <li>\ud83e\udde0 Memory Optimization - Optimize context</li> <li>\ud83d\udd0c Protocol Optimization - Optimize MCP usage</li> <li>\ud83c\udfaf Full-Stack Example - See all layers</li> <li>Tool Development Guide - Create custom tools</li> <li>MCP Optimization Tutorial - Advanced tool protocols</li> </ul> <p>Next: Learn how GEPA optimizes memory and context selection \u2192</p>"},{"location":"techniques/","title":"\ud83e\uddea Advanced Techniques","text":"<p>SuperOptiX provides a comprehensive suite of advanced techniques for building production-ready AI agents. This section covers the core methodologies that make SuperOptiX the most powerful agent framework available.</p>"},{"location":"techniques/#core-techniques","title":"\ud83e\udde0 Core Techniques","text":"<ul> <li>Memory Systems - Implement persistent agent memory</li> <li>RAG Integration - Enhance agents with retrieval-augmented generation</li> <li>Model Intelligence - Advanced model management and optimization</li> </ul>"},{"location":"techniques/#performance-optimization","title":"\u26a1 Performance &amp; Optimization","text":"<ul> <li>Optimization - Optimize agent performance with DSPy</li> <li>Evaluation &amp; Testing - Test and validate your agents</li> <li>Observability - Monitor and debug your agents</li> </ul>"},{"location":"techniques/#development-methodologies","title":"\ud83c\udfd7\ufe0f Development Methodologies","text":"<ul> <li>BDD Scenarios - Behavior-driven development for agents</li> <li>CI/CD Integration - Automate testing and deployment</li> <li>API Reference - REST API for integration</li> </ul>"},{"location":"techniques/#getting-started","title":"\ud83c\udfaf Getting Started","text":"<p>Each technique includes: - Step-by-step tutorials with real examples - Best practices and common pitfalls - Integration guides for existing systems - Performance benchmarks and optimization tips</p> <p>Choose a technique that aligns with your current needs, or follow the recommended learning path for comprehensive mastery of SuperOptiX. </p>"},{"location":"tutorials/","title":"\ud83c\udf93 Tutorials","text":"<p>Welcome to the SuperOptiX tutorials! These step-by-step guides will take you from complete beginner to building sophisticated AI agents and orchestras.</p>"},{"location":"tutorials/#getting-started-tutorials","title":"\ud83d\ude80 Getting Started Tutorials","text":""},{"location":"tutorials/#quick-start-workflow","title":"\ud83c\udfaf Quick Start Workflow","text":"<p>Perfect for beginners. Build your first agent and run compile/evaluate/optimize.</p> <p>What you'll learn: - Initialize a SuperOptiX project - Pull and compile an agent - Compile and understand the agent pipeline - Set up for evaluation and optimization</p> <p>Time: 10-15 minutes Difficulty: Beginner Prerequisites: None</p>"},{"location":"tutorials/#openai-sdk-gepa-optimization","title":"\ud83d\udd27 OpenAI SDK + GEPA Optimization","text":"<p>Build custom agents with native OpenAI SDK. Create a code reviewer using official OpenAI Agents SDK patterns and optimize it with GEPA.</p> <p>What you'll learn: - Write agents using official OpenAI SDK patterns - Integrate native SDK agents with SuperOptiX - Define BDD test scenarios for measurable metrics - Run GEPA optimization to improve performance - Implement automatic optimization loading</p> <p>Time: 30-45 minutes Difficulty: Intermediate Prerequisites: Basic Python, Ollama installed</p>"},{"location":"tutorials/#multi-agent-orchestra","title":"\ud83c\udfad Multi-Agent Orchestra","text":"<p>Advanced tutorial - Coordinate multiple agents to solve complex problems.</p> <p>What you'll learn: - Design multi-agent workflows - Implement agent communication - Handle complex orchestration scenarios - Deploy real-world systems</p> <p>Time: 30-45 minutes Difficulty: Advanced Prerequisites: First Agent Tutorial</p>"},{"location":"tutorials/#prerequisites","title":"\ud83c\udfaf Prerequisites","text":"<p>Before starting the tutorials, ensure you have:</p> <ol> <li>SuperOptix installed - See our Setup Guide</li> <li>Python 3.8+ - Latest stable version recommended</li> <li>Basic Python knowledge - Familiarity with functions and classes</li> <li>Text editor - VS Code, PyCharm, or any code editor</li> </ol>"},{"location":"tutorials/#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":""},{"location":"tutorials/#tutorial-1-genies-agent","title":"Tutorial 1: Genies Agent","text":"<ul> <li>A developer agent</li> <li>Tool calling capabilities (web search, calculator, file operations)</li> <li>RAG system integration</li> <li>Memory and observability features</li> </ul>"},{"location":"tutorials/#tutorial-2-openai-sdk-code-reviewer","title":"Tutorial 2: OpenAI SDK Code Reviewer","text":"<ul> <li>Code reviewer agent</li> <li>Native OpenAI Agents SDK implementation</li> <li>GEPA-optimized prompts (75% \u2192 100% pass rate)</li> <li>BDD test scenarios for validation</li> <li>Automatic optimization loading</li> </ul>"},{"location":"tutorials/#tutorial-3-multi-agent-orchestra","title":"Tutorial 3: Multi-Agent Orchestra","text":"<ul> <li>Customer service workflow</li> <li>Multiple specialized agents</li> <li>Coordinated problem-solving</li> </ul>"},{"location":"tutorials/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<p>After completing the tutorials, explore:</p> <ul> <li>Core Concepts - Deep dive into SuperOptix fundamentals</li> <li>Guides - Techniques and integrations</li> <li>Reference - API documentation and CLI reference</li> </ul>"},{"location":"tutorials/#need-help","title":"\ud83c\udd98 Need Help?","text":"<ul> <li>Stuck? Check our Troubleshooting Guide</li> <li>Questions? Visit our FAQ</li> </ul> <p>Ready to build your first AI agent? Start with the Quick Start \ud83d\ude80</p>"},{"location":"tutorials/deepagents-backends-tutorial/","title":"\ud83d\uddc4\ufe0f DeepAgents Backends Tutorial","text":"<p>Complete hands-on tutorial for DeepAgents 0.2.0 pluggable backends. Learn how to build agents with persistent memory, filesystem access, and hybrid storage strategies.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#what-youll-learn","title":"\ud83d\udcd6 What You'll Learn","text":"<p>By the end of this tutorial, you'll know how to:</p> <ul> <li>Configure all 4 backend types (state, store, filesystem, composite)</li> <li>Build a chatbot with persistent memory</li> <li>Create a code review agent with real file access</li> <li>Design hybrid storage strategies</li> <li>Optimize backends for GEPA</li> <li>Deploy production-ready agents</li> </ul> <p>Time: 30 minutes Level: Intermediate</p>"},{"location":"tutorials/deepagents-backends-tutorial/#prerequisites","title":"\ud83c\udfaf Prerequisites","text":""},{"location":"tutorials/deepagents-backends-tutorial/#required","title":"Required","text":"<pre><code># Install SuperOptiX with DeepAgents\npip install superoptix[frameworks-deepagents]\n\n# Verify version (must be 0.2.0+)\npython -c \"import deepagents; print(deepagents.__version__)\"\n\n# Set Gemini API key (FREE)\nexport GOOGLE_API_KEY=\"your-gemini-key\"\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#initialize-project","title":"Initialize Project","text":"<pre><code>super init deepagents_tutorial\ncd deepagents_tutorial\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-1-persistent-chatbot-storebackend","title":"\ud83d\udcda Tutorial 1: Persistent Chatbot (StoreBackend)","text":""},{"location":"tutorials/deepagents-backends-tutorial/#goal","title":"Goal","text":"<p>Build a chatbot that remembers users across conversations.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-1-create-playbook","title":"Step 1: Create Playbook","text":"<pre><code># Save as: agents/my_chatbot/playbook/my_chatbot_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: My Persistent Chatbot\n  id: my_chatbot\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash  # Check provider docs for latest models\n\n  # Enable persistent memory\n  backend:\n    type: store\n\n  input_fields:\n    - name: query\n      type: str\n\n  output_fields:\n    - name: response\n      type: str\n\n  persona:\n    system_prompt: |\n      You are a personal assistant with PERSISTENT MEMORY.\n\n      Files you should use:\n      - /user_profile.txt - User's name, preferences, interests\n      - /history.txt - Topics discussed before\n\n      WORKFLOW:\n      1. Read /user_profile.txt first (use read_file)\n      2. Personalize response based on what you know\n      3. Update /user_profile.txt if you learn something new\n\n      Remember: Files PERSIST FOREVER across all conversations!\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-2-compile","title":"Step 2: Compile","text":"<pre><code>super agent compile my_chatbot --framework deepagents\n</code></pre> <p>What happened: - Template generates Python code - Includes <code>_create_backend()</code> method - Configures StoreBackend</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-3-first-conversation","title":"Step 3: First Conversation","text":"<pre><code>super agent run my_chatbot --goal \"Hi! My name is Bob and I'm a Python developer.\"\n</code></pre> <p>Agent's internal actions: 1. Tries to read <code>/user_profile.txt</code> (doesn't exist yet) 2. Processes query 3. Writes to <code>/user_profile.txt</code>: <pre><code>Name: Bob\nProfession: Python developer\nFirst contact: 2025-10-29\n</code></pre> 4. Responds: \"Nice to meet you, Bob! I see you're a Python developer...\"</p> <p>File location: Stored in LangGraph store (persistent database)</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-4-second-conversation-hours-later","title":"Step 4: Second Conversation (Hours Later)","text":"<pre><code>super agent run my_chatbot --goal \"What's my name?\"\n</code></pre> <p>Agent's internal actions: 1. Reads <code>/user_profile.txt</code> (still there! ) 2. Finds: \"Name: Bob\" 3. Responds: \"Your name is Bob!\"</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-5-build-on-memory","title":"Step 5: Build on Memory","text":"<pre><code>super agent run my_chatbot --goal \"Suggest a Python project for me\"\n</code></pre> <p>Agent's internal actions: 1. Reads <code>/user_profile.txt</code> 2. Sees: \"Profession: Python developer\" 3. Personalizes response: \"Given your Python background, I suggest...\"</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-6-verify-persistence","title":"Step 6: Verify Persistence","text":"<pre><code># Restart everything, new terminal, next day...\nsuper agent run my_chatbot --goal \"Who am I?\"\n# Response: \"You're Bob, a Python developer!\" \n# The memory persisted!\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-2-code-review-agent-filesystembackend","title":"\ud83d\udcc1 Tutorial 2: Code Review Agent (FilesystemBackend)","text":""},{"location":"tutorials/deepagents-backends-tutorial/#goal_1","title":"Goal","text":"<p>Build an agent that analyzes actual project files.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-1-setup-test-project","title":"Step 1: Setup Test Project","text":"<pre><code># Create a sample project to review\nmkdir -p /tmp/demo_project/src\ncat &gt; /tmp/demo_project/src/app.py &lt;&lt; 'EOF'\ndef login(username, password):\n    # TODO: Add input validation\n    query = f\"SELECT * FROM users WHERE username='{username}'\"  # SQL injection!\n    result = db.execute(query)\n    return result\n\ndef process_data(data):\n    # Memory leak - list grows indefinitely\n    global_cache.append(data)\n    return data\nEOF\n\ncat &gt; /tmp/demo_project/README.md &lt;&lt; 'EOF'\n# Demo Project\nA sample Python application for testing code review agents.\nEOF\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-2-create-playbook","title":"Step 2: Create Playbook","text":"<pre><code># Save as: agents/code_reviewer/playbook/code_reviewer_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Code Reviewer\n  id: code_reviewer\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-pro  # Pro for better analysis\n    temperature: 0.3\n\n  # Access real project files\n  backend:\n    type: filesystem\n    root_dir: /tmp/demo_project\n\n  input_fields:\n    - name: query\n      type: str\n\n  output_fields:\n    - name: report\n      type: str\n\n  persona:\n    system_prompt: |\n      You are a senior code reviewer with REAL FILESYSTEM ACCESS.\n\n      Available in /tmp/demo_project:\n      - /src/ - Source code\n      - /README.md - Documentation\n\n      Available tools:\n      - ls /src/ - List files\n      - read_file /src/app.py - Read code\n      - grep_search \"TODO\" /src/ - Find patterns\n      - write_file /review.md - Write reports\n\n      SECURITY CHECKLIST:\n      - SQL injection\n      - XSS vulnerabilities\n      - Memory leaks\n      - Input validation\n      - Error handling\n\n      Always provide:\n      - Specific file paths and line numbers\n      - Severity (Critical/High/Medium/Low)\n      - Code examples\n      - Recommended fixes\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-3-compile-review","title":"Step 3: Compile &amp; Review","text":"<pre><code>super agent compile code_reviewer --framework deepagents\n\n# Review the code\nsuper agent run code_reviewer --goal \"Review src/app.py for security issues\"\n</code></pre> <p>Agent's internal actions: 1. Reads actual file: <code>/tmp/demo_project/src/app.py</code> 2. Analyzes code 3. Finds issues:    - SQL injection (line 3)    - Missing input validation    - Memory leak (line 10) 4. Responds with detailed findings</p> <p>Example response: <pre><code>Found 2 CRITICAL and 1 HIGH severity issues:\n\n1. SQL INJECTION - CRITICAL\n   File: /src/app.py, Line 3\n   Issue: Unsanitized user input in SQL query\n\n   Vulnerable code:\n   query = f\"SELECT * FROM users WHERE username='{username}'\"\n\n   Fix: Use parameterized queries:\n   query = \"SELECT * FROM users WHERE username=?\"\n   result = db.execute(query, (username,))\n\n2. MEMORY LEAK - HIGH\n   File: /src/app.py, Line 10\n   Issue: global_cache grows indefinitely\n   ...\n</code></pre></p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-4-generate-report","title":"Step 4: Generate Report","text":"<pre><code>super agent run code_reviewer --goal \"Analyze all files and write a complete report to /security_report.md\"\n</code></pre> <p>Agent's internal actions: 1. Runs: <code>ls /src/</code> 2. Reads each file 3. Analyzes all code 4. Writes to: <code>/tmp/demo_project/security_report.md</code> (REAL FILE!)</p> <p>Verify it: <pre><code>cat /tmp/demo_project/security_report.md\n# You'll see the full security report!\n\n# It's a real file - use it with git, share it, etc.\n</code></pre></p>"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-3-hybrid-research-agent-compositebackend","title":"\ud83d\udd00 Tutorial 3: Hybrid Research Agent (CompositeBackend)","text":""},{"location":"tutorials/deepagents-backends-tutorial/#goal_2","title":"Goal","text":"<p>Build a research agent with optimal storage for each data type.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-1-setup-research-workspace","title":"Step 1: Setup Research Workspace","text":"<pre><code># Create directory structure\nmkdir -p /tmp/research_workspace/papers\n\n# Add a sample paper (or use your own PDFs)\necho \"# Sample Research Paper\nThis is a sample academic paper about AI agents.\n\" &gt; /tmp/research_workspace/papers/agents_paper.txt\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-2-create-playbook_1","title":"Step 2: Create Playbook","text":"<pre><code># Save as: agents/researcher/playbook/researcher_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Advanced Researcher\n  id: researcher\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash  # Check provider docs for latest models\n\n  # Hybrid storage strategy\n  backend:\n    type: composite\n    default: state                # Scratch space\n    routes:\n      /memories/: store          # Persistent findings\n      /papers/: filesystem       # Academic papers\n      /cache/: state             # Temporary data\n    root_dir: /tmp/research_workspace\n\n  input_fields:\n    - name: query\n      type: str\n\n  output_fields:\n    - name: report\n      type: str\n\n  persona:\n    system_prompt: |\n      You are a research agent with HYBRID STORAGE:\n\n      \ud83d\udcda /memories/ (DATABASE - Persistent)\n      - Research findings that should last forever\n      - Literature reviews\n      - Key insights\n      Example: write_file /memories/ai_research.txt \"Summary: ...\"\n\n      \ud83d\udcc2 /papers/ (FILESYSTEM - Real Files)\n      - Access actual papers in /tmp/research_workspace/papers/\n      - Read PDFs and documents\n      Example: read_file /papers/agents_paper.txt\n\n      \ud83d\udcbe /cache/ (STATE - Temporary)\n      - Internet search results\n      - Temporary calculations\n      Example: write_file /cache/search.txt \"Results: ...\"\n\n      \ud83d\uddc2\ufe0f / (STATE - Scratch)\n      - Current conversation only\n      Example: write_file /draft.txt \"Work in progress...\"\n\n      RESEARCH WORKFLOW:\n      1. Check /memories/research_index.txt for prior work\n      2. Search internet \u2192 save to /cache/\n      3. Read /papers/ for academic sources\n      4. Save important findings \u2192 /memories/\n      5. Keep /memories/research_index.txt updated\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-3-compile","title":"Step 3: Compile","text":"<pre><code>super agent compile researcher --framework deepagents\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#step-4-first-research-session","title":"Step 4: First Research Session","text":"<pre><code>super agent run researcher --goal \"Research LangGraph. Save important findings for future reference.\"\n</code></pre> <p>Agent's actions: 1. Checks <code>/memories/research_index.txt</code> (empty - first time) 2. Searches for information about LangGraph 3. Saves results to <code>/cache/search_results.txt</code> (temporary) 4. Checks <code>/papers/</code> for relevant papers 5. Writes to <code>/memories/langgraph_research.txt</code> (PERSISTS!) 6. Updates <code>/memories/research_index.txt</code>: <pre><code>Research Topics:\n- LangGraph: See /memories/langgraph_research.txt (2025-10-29)\n</code></pre></p> <p>File locations: - <code>/memories/</code> \u2192 LangGraph store (database) - <code>/papers/</code> \u2192 <code>/tmp/research_workspace/papers/</code> (real files) - <code>/cache/</code> \u2192 LangGraph state (ephemeral)</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-5-access-papers","title":"Step 5: Access Papers","text":"<pre><code>super agent run researcher --goal \"Summarize the paper in /papers/agents_paper.txt\"\n</code></pre> <p>Agent's actions: 1. Reads REAL file: <code>/tmp/research_workspace/papers/agents_paper.txt</code> 2. Summarizes content 3. May save summary to <code>/memories/</code> for future reference</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-6-week-later-recall-research","title":"Step 6: Week Later - Recall Research","text":"<pre><code># New conversation, days later\nsuper agent run researcher --goal \"What did I research about LangGraph?\"\n</code></pre> <p>Agent's actions: 1. Reads <code>/memories/research_index.txt</code> (STILL THERE! ) 2. Sees: \"LangGraph: See /memories/langgraph_research.txt\" 3. Reads <code>/memories/langgraph_research.txt</code> (PERSISTS!) 4. Responds: \"Based on your research from October 29<sup>th</sup>, LangGraph is...\"</p>"},{"location":"tutorials/deepagents-backends-tutorial/#step-7-verify-storage-locations","title":"Step 7: Verify Storage Locations","text":"<pre><code># Check ephemeral cache (will be empty - it's gone)\n# /cache/ files don't persist\n\n# Check persistent memories (still there!)\n# /memories/ files persist in database\n\n# Check filesystem papers (real files)\nls /tmp/research_workspace/papers/\n# agents_paper.txt  \u2190 Still there!\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-4-gepa-optimization-with-backends","title":"\ud83c\udf93 Tutorial 4: GEPA Optimization with Backends","text":""},{"location":"tutorials/deepagents-backends-tutorial/#does-gepa-work-with-backends","title":"Does GEPA Work with Backends?","text":"<p>YES! GEPA optimizes the <code>system_prompt</code> regardless of backend type.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#example-optimize-persistent-chatbot","title":"Example: Optimize Persistent Chatbot","text":"<pre><code># Pull demo agent\nsuper agent pull chatbot_persistent\n\n# Compile\nsuper agent compile chatbot_persistent --framework deepagents\n\n# Evaluate baseline\nsuper agent evaluate chatbot_persistent\n\n# Optimize with GEPA\nsuper agent optimize chatbot_persistent --auto medium\n\n# Test optimized version\nsuper agent evaluate chatbot_persistent  # automatically loads optimized weights\n</code></pre> <p>What GEPA optimizes: - The <code>system_prompt</code> instruction - Memory management instructions - File organization strategy - Response personalization</p> <p>Example improvement:</p> <p>Before GEPA: <pre><code>system_prompt: |\n  You are a personal assistant with long-term memory.\n  Save info to /user_profile.txt.\n</code></pre></p> <p>After GEPA: <pre><code>system_prompt: |\n  You are a dedicated personal assistant with advanced persistent memory capabilities.\n\n  MEMORY MANAGEMENT PROTOCOL:\n\n  1. ALWAYS read /user_profile.txt before responding\n  2. Extract user's name, preferences, and context\n  3. Personalize every response using this information\n  4. After each interaction, update files with new learnings:\n     - /user_profile.txt: User details, preferences\n     - /conversation_topics.txt: Discussion history\n     - /reminders.txt: User's to-dos and future plans\n\n  RESPONSE GUIDELINES:\n  - Use user's name naturally\n  - Reference past conversations\n  - Build relationships through memory\n  - Proactively suggest based on preferences\n</code></pre></p> <p>Result: +25% improvement in user satisfaction scores!</p>"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-5-backend-performance-comparison","title":"\ud83d\udd0d Tutorial 5: Backend Performance Comparison","text":""},{"location":"tutorials/deepagents-backends-tutorial/#setup-same-agent-different-backends","title":"Setup: Same Agent, Different Backends","text":"<p>Let's create the same agent with different backends and compare:</p> <pre><code># Test 1: StateBackend (default)\nsuper agent pull research_agent_deepagents\nmv research_agent_deepagents research_state\n\n# Test 2: StoreBackend (persistent)\nsuper agent pull research_agent_deepagents  \nmv research_agent_deepagents research_store\n# Edit playbook: backend.type = store\n\n# Test 3: FilesystemBackend\nsuper agent pull research_agent_deepagents\nmv research_agent_deepagents research_filesystem\n# Edit playbook: backend.type = filesystem, root_dir = /tmp/agent_files\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#compile-all","title":"Compile All","text":"<pre><code>for agent in research_state research_store research_filesystem; do\n  super agent compile $agent --framework deepagents\ndone\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#test-performance","title":"Test Performance","text":"<pre><code># Test each with same query\nQUERY=\"Research machine learning and save findings to /notes.txt\"\n\ntime super agent run research_state --goal \"$QUERY\"\ntime super agent run research_store --goal \"$QUERY\"\ntime super agent run research_filesystem --goal \"$QUERY\"\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#check-persistence","title":"Check Persistence","text":"<pre><code># New conversation - which agent still has /notes.txt?\n\nsuper agent run research_state --goal \"Show me /notes.txt\"\n# File not found (ephemeral)\n\nsuper agent run research_store --goal \"Show me /notes.txt\"\n# File found! (persistent in database)\n\nsuper agent run research_filesystem --goal \"Show me /notes.txt\"\n# File found! (on actual filesystem)\n\ncat /tmp/agent_files/notes.txt\n# Real file exists on disk\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#results","title":"Results","text":"Backend Speed Persistence Visible in IDE Best For state \u26a1\u26a1\u26a1 1.2s No No Speed store \u26a1\u26a1 1.5s Yes No Memory filesystem \u26a1\u26a1 1.4s Yes Yes Real files"},{"location":"tutorials/deepagents-backends-tutorial/#tutorial-6-production-ready-hybrid-agent","title":"\ud83c\udfaf Tutorial 6: Production-Ready Hybrid Agent","text":""},{"location":"tutorials/deepagents-backends-tutorial/#goal_3","title":"Goal","text":"<p>Build a production-ready development assistant with optimal storage.</p>"},{"location":"tutorials/deepagents-backends-tutorial/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Development Assistant Agent         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                         \u2502\n\u2502  /memories/ \u2192 StoreBackend             \u2502\n\u2502  \u251c\u2500 /user_preferences.txt (persistent)  \u2502\n\u2502  \u251c\u2500 /project_context.txt (persistent)   \u2502\n\u2502  \u2514\u2500 /code_patterns.txt (persistent)     \u2502\n\u2502                                         \u2502\n\u2502  /project/ \u2192 FilesystemBackend         \u2502\n\u2502  \u251c\u2500 /src/*.py (real files)              \u2502\n\u2502  \u251c\u2500 /tests/*.py (real files)            \u2502\n\u2502  \u2514\u2500 /docs/*.md (real files)             \u2502\n\u2502                                         \u2502\n\u2502  /cache/ \u2192 StateBackend                \u2502\n\u2502  \u251c\u2500 /search_results.txt (temp)          \u2502\n\u2502  \u2514\u2500 /temp_analysis.txt (temp)           \u2502\n\u2502                                         \u2502\n\u2502  / \u2192 StateBackend                       \u2502\n\u2502  \u2514\u2500 /workspace.txt (scratch)            \u2502\n\u2502                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#complete-playbook","title":"Complete Playbook","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Dev Assistant\n  id: dev_assistant\n  description: Production-ready development assistant\nspec:\n  target_framework: deepagents\n\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash  # Check provider docs for latest models\n    temperature: 0.7\n\n  # Hybrid storage for production\n  backend:\n    type: composite\n    default: state\n    routes:\n      /memories/: store\n      /project/: filesystem\n      /cache/: state\n    root_dir: /Users/local/my_project\n\n  input_fields:\n    - name: query\n      type: str\n\n  output_fields:\n    - name: response\n      type: str\n\n  persona:\n    system_prompt: |\n      You are a professional development assistant with HYBRID STORAGE.\n\n      \ud83e\udde0 STORAGE STRATEGY:\n\n      /memories/ (Database - Persistent Forever)\n      \u251c\u2500 /user_preferences.txt - User's coding style, preferences\n      \u251c\u2500 /project_context.txt - Project architecture, key decisions\n      \u251c\u2500 /code_patterns.txt - Common patterns in this codebase\n      \u2514\u2500 /error_solutions.txt - Previously solved errors\n      \u2192 Use for: Long-term memory, project knowledge\n\n      /project/ (Real Files - Your Actual Project)\n      \u251c\u2500 /src/*.py - Source code\n      \u251c\u2500 /tests/*.py - Tests\n      \u251c\u2500 /docs/*.md - Documentation\n      \u2514\u2500 /requirements.txt - Dependencies\n      \u2192 Use for: Reading code, making changes, analysis\n\n      /cache/ (State - Temporary)\n      \u251c\u2500 /search_results.txt - Recent searches\n      \u251c\u2500 /analysis_temp.txt - Intermediate analysis\n      \u2514\u2500 /todo_list.txt - Current session todos\n      \u2192 Use for: Temporary data, cleared each conversation\n\n      / (State - Scratch)\n      \u251c\u2500 /workspace.txt - Current work\n      \u2514\u2500 /draft.txt - Draft responses\n      \u2192 Use for: Current conversation only\n\n      \ud83c\udfaf WORKFLOW:\n\n      1. CONTEXT LOADING\n         - Read /memories/user_preferences.txt\n         - Read /memories/project_context.txt\n         - Understand the project and user\n\n      2. TASK EXECUTION\n         - Use ls /project/src/ to explore\n         - Use read_file /project/src/app.py for analysis\n         - Use grep_search for finding patterns\n         - Save temp results to /cache/\n\n      3. CODE CHANGES (if requested)\n         - Read current file\n         - Make changes\n         - Write back to /project/ (REAL file modified!)\n         - Explain changes clearly\n\n      4. MEMORY UPDATE\n         - Save new learnings to /memories/\n         - Update project context if needed\n         - Record error solutions\n\n      \ud83d\udd12 SAFETY:\n      - Never delete files without confirmation\n      - Always explain changes before making them\n      - Back up important files before editing\n      - Use edit_file for surgical changes\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#usage-examples","title":"Usage Examples","text":"<p>Example 1: Code Review <pre><code>super agent run dev_assistant --goal \"Review all Python files for issues\"\n\n# Agent:\n# Lists /project/src/*.py\n# Reads each file\n# Analyzes code\n# Writes report to /project/code_review.md (REAL FILE)\n</code></pre></p> <p>Example 2: Add Feature <pre><code>super agent run dev_assistant --goal \"Add input validation to login function in src/auth.py\"\n\n# Agent:\n# Reads /project/src/auth.py\n# Identifies login function\n# Adds validation\n# Writes back to /project/src/auth.py (MODIFIED!)\n# Saves pattern to /memories/code_patterns.txt\n</code></pre></p> <p>Example 3: Recall Context <pre><code># Week later, new conversation\nsuper agent run dev_assistant --goal \"What changes have we made to auth.py?\"\n\n# Agent:\n# Reads /memories/project_context.txt\n# Finds record of auth.py changes\n# Responds with history\n</code></pre></p>"},{"location":"tutorials/deepagents-backends-tutorial/#security-best-practices","title":"\ud83d\udee1\ufe0f Security Best Practices","text":""},{"location":"tutorials/deepagents-backends-tutorial/#limit-filesystem-scope","title":"Limit Filesystem Scope","text":"<pre><code># BAD: Too broad\nbackend:\n  type: filesystem\n  root_dir: /  # Can access ENTIRE system!\n\n# GOOD: Specific directory\nbackend:\n  type: filesystem\n  root_dir: /Users/local/my_project/src  # Limited scope\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#use-read-only-for-sensitive-data","title":"Use Read-Only for Sensitive Data","text":"<pre><code># Composite with read-only paper access\nbackend:\n  type: composite\n  default: state\n  routes:\n    /papers/: filesystem  # Agent can READ papers\n    /output/: state       # Agent can WRITE outputs (not to real files)\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#validate-changes","title":"Validate Changes","text":"<p>Add validation to system prompt: <pre><code>persona:\n  system_prompt: |\n    Before modifying any file:\n    1. Show the user the planned changes\n    2. Wait for confirmation\n    3. Only then write to /project/\n</code></pre></p>"},{"location":"tutorials/deepagents-backends-tutorial/#quick-reference","title":"\ud83d\udcca Quick Reference","text":""},{"location":"tutorials/deepagents-backends-tutorial/#when-to-use-each-backend","title":"When to Use Each Backend","text":"<pre><code># Quick Q&amp;A, no memory needed\nbackend:\n  type: state\n\n# Chatbot, personal assistant\nbackend:\n  type: store\n\n# Code review, file analysis\nbackend:\n  type: filesystem\n  root_dir: /path/to/project\n\n# Production agent, complex needs\nbackend:\n  type: composite\n  default: state\n  routes:\n    /memories/: store\n    /project/: filesystem\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#configuration-template","title":"Configuration Template","text":"<pre><code>spec:\n  backend:\n    type: composite\n    default: state\n    routes:\n      /memories/: store          # What should persist\n      /project/: filesystem      # What's real files\n      /cache/: state             # What's temporary\n    root_dir: /path/to/workspace  # For filesystem routes\n</code></pre>"},{"location":"tutorials/deepagents-backends-tutorial/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<ol> <li>Try all 3 tutorials above</li> <li>Pull demo agents: <pre><code>super agent pull chatbot_persistent\nsuper agent pull code_reviewer\nsuper agent pull researcher_hybrid\n</code></pre></li> <li>Build your own agent with the right backend</li> <li>Read the complete guide: DeepAgents Backends</li> <li>Optimize with GEPA: <pre><code>super agent optimize your_agent --auto medium\n</code></pre></li> </ol>"},{"location":"tutorials/deepagents-backends-tutorial/#resources","title":"\ud83d\udcda Resources","text":"<ul> <li>DeepAgents 0.2.0 Announcement</li> <li>SuperOptiX DeepAgents Guide</li> <li>Backend Configuration Reference</li> <li>Gemini Setup Guide</li> </ul> <p>Ready to build production agents? Start with the chatbot tutorial and work your way up! \ud83d\ude80</p>"},{"location":"tutorials/deepagents-complete-workflow/","title":"\ud83c\udfaf DeepAgents Complete End-to-End Workflow","text":"<p>A comprehensive, step-by-step guide to building, running, evaluating, and optimizing DeepAgents with SuperOptiX. Follow along and build production-ready agents with persistent memory, real file access, and GEPA optimization - all using FREE Gemini models!</p>"},{"location":"tutorials/deepagents-complete-workflow/#table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Step-by-Step Workflow</li> <li>Backend Configuration</li> <li>Advanced Examples</li> <li>Troubleshooting</li> <li>Production Deployment</li> </ol> <p>Time to Complete: 30-45 minutes Difficulty: Intermediate Cost: $0.00 (FREE tier with Gemini!)</p>"},{"location":"tutorials/deepagents-complete-workflow/#introduction","title":"\ud83c\udfaf Introduction","text":""},{"location":"tutorials/deepagents-complete-workflow/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this tutorial, you'll have:</p> <ul> <li>A fully functional DeepAgents research assistant</li> <li>Real Gemini API integration (FREE tier)</li> <li>Automated evaluation with BDD scenarios</li> <li>GEPA-optimized system prompts (+20-30% improvement)</li> <li>Production-ready agent deployment</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#what-is-deepagents","title":"What is DeepAgents?","text":"<p>DeepAgents 0.2.0 is LangChain's framework for building \"deep agents\" - sophisticated, long-running agents that can:</p> <ul> <li>\ud83d\udccb Plan complex tasks with <code>write_todos</code></li> <li>\ud83d\udcc1 Manage files with 6 filesystem tools</li> <li>\ud83d\udc65 Spawn subagents for specialized tasks</li> <li>\ud83d\uddc4\ufe0f Persist memory across conversations (NEW in 0.2.0!)</li> <li>\ud83d\udcc2 Access real files on your computer (NEW in 0.2.0!)</li> </ul> <p>Source: LangChain Blog - Doubling Down on DeepAgents</p>"},{"location":"tutorials/deepagents-complete-workflow/#prerequisites","title":"\ud83d\udccb Prerequisites","text":""},{"location":"tutorials/deepagents-complete-workflow/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.11+ (required)</li> <li>SuperOptiX installed (see below)</li> <li>Internet connection (for Gemini API)</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#install-superoptix","title":"Install SuperOptiX","text":"<pre><code># Install SuperOptiX with DeepAgents support\npip install superoptix[frameworks-deepagents]\n\n# REQUIRED: Install Gemini integration for LangChain\npip install langchain-google-genai\n</code></pre> <p>What gets installed: - SuperOptiX core - DeepAgents 0.2.0+ with backend support - LangChain, LangGraph integration - GEPA optimizer - Google Gemini integration for LangChain</p>"},{"location":"tutorials/deepagents-complete-workflow/#get-free-gemini-api-key","title":"Get FREE Gemini API Key","text":"<p>Why Gemini? - FREE tier with generous quotas - Function-calling support (required for DeepAgents) - Fast (1-3 second responses) - GPT-4 class quality</p> <p>Steps: 1. Go to Google AI Studio 2. Sign in with your Google account 3. Click \"Create API Key\" or \"Get API Key\" 4. Copy the key (format: <code>AIzaSy...</code>)</p> <p>Free Tier Limits: - 15 requests per minute - 1,500 requests per day - 1M tokens per minute - More than enough for development and testing!</p>"},{"location":"tutorials/deepagents-complete-workflow/#set-environment-variable","title":"Set Environment Variable","text":"Fish ShellBash/ZshTemporary (Current Session Only) <pre><code># Add to ~/.config/fish/config.fish\nset -x GOOGLE_API_KEY \"AIzaSy-your-actual-key-here\"\n\n# Reload config\nsource ~/.config/fish/config.fish\n\n# Verify\necho $GOOGLE_API_KEY\n</code></pre> <pre><code># Add to ~/.bashrc or ~/.zshrc\nexport GOOGLE_API_KEY=\"AIzaSy-your-actual-key-here\"\n\n# Reload config\nsource ~/.bashrc  # or source ~/.zshrc\n\n# Verify\necho $GOOGLE_API_KEY\n</code></pre> <pre><code># Set for current terminal session\nexport GOOGLE_API_KEY=\"AIzaSy-your-actual-key-here\"\n\n# Verify\necho $GOOGLE_API_KEY\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#verify-installation","title":"Verify Installation","text":"<pre><code># Check SuperOptiX\nsuper --version\n\n# Check DeepAgents backends\npython -c \"from superoptix.vendor.deepagents.backends import StateBackend, StoreBackend, FilesystemBackend, CompositeBackend; print('All backends available!')\"\n\n# Check Gemini integration\npython -c \"from langchain_google_genai import ChatGoogleGenerativeAI; print('Gemini integration ready!')\"\n</code></pre> <p>Expected Output: <pre><code>SuperOptiX version 0.1.4\nAll backends available!\nGemini integration ready!\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#step-by-step-workflow","title":"\ud83d\ude80 Step-by-Step Workflow","text":""},{"location":"tutorials/deepagents-complete-workflow/#step-1-initialize-project","title":"Step 1: Initialize Project","text":"<p>Create a new SuperOptiX project:</p> <pre><code># Create project\nsuper init my_deepagents_project\n\n# Navigate to project directory\ncd my_deepagents_project\n\n# Verify structure\nls -la\n</code></pre> <p>Expected Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 SUCCESS! Your full-blown shippable Agentic System 'my_deepagents_project' \u2502\n\u2502 is ready!                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nYour project structure:\nmy_deepagents_project/\n\u251c\u2500\u2500 .super                 # Project metadata\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 my_deepagents_project/\n    \u251c\u2500\u2500 agents/            # Your agents go here\n    \u251c\u2500\u2500 guardrails/\n    \u251c\u2500\u2500 memory/\n    \u251c\u2500\u2500 protocols/\n    \u251c\u2500\u2500 teams/\n    \u251c\u2500\u2500 evals/\n    \u251c\u2500\u2500 knowledge/\n    \u251c\u2500\u2500 optimizers/\n    \u251c\u2500\u2500 servers/\n    \u2514\u2500\u2500 tools/\n</code></pre></p> <p>Checkpoint: You should have a <code>.super</code> file in your directory. All <code>super</code> commands must run from this directory.</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-2-pull-demo-agent","title":"Step 2: Pull Demo Agent","text":"<p>Pull the DeepAgents research assistant demo:</p> <pre><code>super agent pull research_agent_deepagents\n</code></pre> <p>Expected Output: <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Agent Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502  \ud83e\udd16 Name: Research Agent (DeepAgents)                                        \u2502\n\u2502  \ud83c\udfe2 Industry: Demo | \ud83d\udd2e Tier: Oracles                                        \u2502\n\u2502  \ud83d\udcc1 Location: my_deepagents_project/agents/research_agent_deepagents/        \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> <p>What was created: <pre><code>my_deepagents_project/agents/research_agent_deepagents/\n\u251c\u2500\u2500 playbook/\n\u2502   \u2514\u2500\u2500 research_agent_deepagents_playbook.yaml  # Agent configuration\n\u2514\u2500\u2500 pipelines/                                   # Will be created on compile\n</code></pre></p> <p>Checkpoint: Check that the playbook file exists: <pre><code>cat my_deepagents_project/agents/research_agent_deepagents/playbook/research_agent_deepagents_playbook.yaml | head -20\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#step-3-inspect-agent-configuration","title":"Step 3: Inspect Agent Configuration","text":"<p>Let's look at what we just pulled:</p> <pre><code>cat my_deepagents_project/agents/research_agent_deepagents/playbook/research_agent_deepagents_playbook.yaml\n</code></pre> <p>Key Configuration Sections:</p> <pre><code>metadata:\n  name: Research Agent (DeepAgents)\n  description: Research assistant built with DeepAgents\n\nspec:\n  target_framework: deepagents  # Uses DeepAgents framework\n\n  language_model:\n    provider: google-genai\n    model: google-genai:gemini-2.5-flash  # Use latest model from provider\n    temperature: 0.7\n    max_tokens: 8192\n    # Note: Model names may change as providers release new versions.\n    # Check provider docs for latest models: https://ai.google.dev/models\n\n  backend:\n    type: state  # Default: ephemeral storage (can change to 'store' for persistence)\n\n  persona:\n    role: Expert AI Researcher\n    goal: Conduct thorough research on AI and technology topics\n\n  # BDD test scenarios\n  feature_specifications:\n    scenarios:\n      - name: Simple research query\n        input:\n          query: \"What is LangGraph?\"\n        expected_output:\n          report: \"LangGraph is a framework...\"\n\n  # GEPA optimization configuration\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: response_accuracy\n        auto: medium\n        reflection_lm: google-genai:gemini-2.5-pro  # FREE Gemini Pro for reflection!\n</code></pre> <p>Checkpoint: Note the <code>target_framework: deepagents</code> - this tells SuperOptiX to compile for DeepAgents.</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-4-compile-agent","title":"Step 4: Compile Agent","text":"<p>Transform the YAML playbook into executable Python code:</p> <pre><code>super agent compile research_agent_deepagents --framework deepagents\n</code></pre> <p>Expected Output: <pre><code>================================================================================\n\n\ud83d\udd28 Compiling agent 'research_agent_deepagents'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Research Agent (DeepAgents)                                       \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DeepAgents (LangGraph)                                        \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                      \u2502\n\u2502  \ud83d\udcc1 Output: my_deepagents_project/agents/research_agent_deepagents/          \u2502\n\u2502            pipelines/research_agent_deepagents_deepagents_pipeline.py        \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udc0d Converted field names to snake_case for DSPy compatibility\nTools configuration detected for Genies tier\nSuccessfully compiled with DEEPAGENTS framework\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> <p>What was created: - <code>research_agent_deepagents_deepagents_pipeline.py</code> (~28KB, 766 lines) - Contains <code>ResearchAgentDeepAgentsComponent</code> (BaseComponent wrapper) - Contains <code>ResearchAgentDeepAgentsPipeline</code> (executable pipeline) - Includes <code>_create_backend()</code> method for backend support</p> <p>Checkpoint: Verify the pipeline file exists: <pre><code>ls -lh my_deepagents_project/agents/research_agent_deepagents/pipelines/\n# Should show: research_agent_deepagents_deepagents_pipeline.py (~28KB)\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#step-5-run-agent-first-execution","title":"Step 5: Run Agent (First Execution)","text":"<p>Execute the agent with a simple query:</p> <pre><code>super agent run research_agent_deepagents --goal \"What is 2 + 2? Answer with just the number.\"\n</code></pre> <p>Expected Output: <pre><code>\ud83d\udcca Observability: superoptix\n\ud83d\ude80 Running agent 'research_agent_deepagents'...\n\nRunning with base model (not optimized)...\n\n\ud83d\udcdd Using base pipeline (no optimization available)\n\nLooking for pipeline at: my_deepagents_project/agents/research_agent_deepagents/\npipelines/research_agent_deepagents_deepagents_pipeline.py\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Agent Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83e\udd16 Running Research_Agent_Deepagents Pipeline                                \u2502\n\u2502                                                                              \u2502\n\u2502 Executing Task: What is 2 + 2? Answer with just the number.                  \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nDeepAgents agent initialized with model: google-genai:gemini-2.5-flash\n\n                                Analysis Results                                \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect   \u2503 Value \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Response \u2502 4     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPre-Optimized Pipeline: \u26aa NO\nRuntime Optimization: \u26aa NO\n\nValidation Status: PASSED\n</code></pre></p> <p>\ud83c\udf89 Success! You just ran your first DeepAgents agent with real Gemini API!</p> <p>What happened: 1. Agent loaded with Gemini 2.5 Flash model 2. Made real API call to Gemini 3. Got response: \"4\" 4. All using your FREE API quota!</p> <p>Checkpoint: Try a more complex query: <pre><code>super agent run research_agent_deepagents --goal \"What is LangGraph? Answer in exactly 2 sentences.\"\n</code></pre></p> <p>Expected Response: <pre><code>Response \u2502 LangGraph is a library that helps build stateful, multi-actor \n         \u2502 applications with LLMs, by representing computation as a graph. \n         \u2502 It extends LangChain by enabling cyclic execution flows, allowing \n         \u2502 for more complex and dynamic agent behaviors.\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#step-6-evaluate-agent-baseline-performance","title":"Step 6: Evaluate Agent (Baseline Performance)","text":"<p>Run BDD scenarios to establish baseline performance:</p> <pre><code>super agent evaluate research_agent_deepagents\n</code></pre> <p>Expected Output: <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\ud83e\uddea SuperOptiX BDD Spec Runner - Professional Agent Validation\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udccb Test Configuration\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Attribute       \u2503 Value                              \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Agent           \u2502 research_agent_deepagents          \u2502\n\u2502 Framework       \u2502 DeepAgents                         \u2502\n\u2502 Optimization    \u2502 \u2699\ufe0f  Base Model                     \u2502\n\u2502 Specifications  \u2502 3 BDD scenarios                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udd0d Discovering BDD Specifications...\n\ud83d\udccb Found 3 BDD specifications\n\n\ud83e\uddea Executing BDD Specification Suite\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udd0d Evaluating research_agent_deep_agents...\n\nTesting 3 BDD scenarios:\n\nDeepAgents agent initialized with model: google-genai:gemini-2.5-flash\nSimple research query: PASS\nTechnical comparison: FAIL\nComplex research: FAIL\n\n============================================================\nOverall: 1/3 PASS (33.3%)\n============================================================\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udd34 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502  \ud83d\udcca Total Specs:         3                \ud83c\udfaf Pass Rate:         33.3%        \u2502\n\u2502  Passed:              1                                                   \u2502\n\u2502  Failed:              2                                                   \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        NEEDS WORK                                       \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre></p> <p>Analysis: - Baseline performance: 33.3% (1 out of 3 scenarios pass) - Simple queries work well - Complex research needs improvement - Perfect candidate for GEPA optimization!</p> <p>Checkpoint: Note your baseline score - we'll compare after optimization.</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-7-optimize-with-gepa-the-magic","title":"Step 7: Optimize with GEPA (The Magic!)","text":"<p>Now let's use GEPA to automatically improve the agent:</p> <pre><code>super agent optimize research_agent_deepagents \\\n  --framework deepagents \\\n  --auto medium \\\n  --reflection-lm google-genai:gemini-2.5-pro\n</code></pre> <p>What's happening: - <code>--framework deepagents</code> - Specifies the framework - <code>--auto medium</code> - GEPA budget (light/medium/heavy) - <code>--reflection-lm gemini-2.5-pro</code> - Uses Pro model for better reflection</p> <p>Expected Output: <pre><code>================================================================================\n\n\ud83d\ude80 Optimizing agent 'research_agent_deepagents'...\n\n\ud83c\udf1f Using Universal GEPA Optimizer\n   Framework: deepagents\n\n\ud83d\udd2c Running Universal GEPA Optimization\n   Framework: deepagents\n   Training examples: 3\n   Train: 2, Val: 1\n\n\ud83d\udce6 Creating deepagents component...\n   Component created: research_agent_deep_agents\n   Framework: deepagents\n   Optimizable: True\n\n\ud83d\ude80 Initializing Universal GEPA optimizer...\n   Optimizer created\n   Budget: medium\n   Reflection LM: google-genai:gemini-2.5-pro\n\n\u26a1 Running GEPA optimization...\n   This may take 5-10 minutes...\n\nDeepAgents agent initialized with model: google-genai:gemini-2.5-flash\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nIteration 0: Base program full valset score: 0.33\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nReflection on failures...\nProposing improvements...\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nIteration 1: Testing 3 candidates...\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nCandidate 1: Score 0.50 (+51% improvement!)\nCandidate 2: Score 0.67 (+103% improvement!)\nCandidate 3: Score 0.50 (+51% improvement!)\n\n\ud83c\udfaf Best candidate: #2 with score 0.67\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nIteration 2: Testing 3 candidates...\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nCandidate 1: Score 0.83 (+152% improvement!)\nCandidate 2: Score 0.67 (+103% improvement!)\nCandidate 3: Score 0.67 (+103% improvement!)\n\n\ud83c\udfaf New best! Score: 0.83 (was 0.33)\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nIteration 3: Testing 3 candidates...\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nCandidate 1: Score 0.83 (+152% improvement!)\nCandidate 2: Score 1.00 (+203% improvement! \ud83c\udf89)\nCandidate 3: Score 0.83 (+152% improvement!)\n\n\ud83c\udfaf New best! Score: 1.00 (PERFECT!)\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 OPTIMIZATION COMPLETE!                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udcca Results:\n   Initial Score:  0.33 (33.3%)\n   Final Score:    1.00 (100.0%)\n   Improvement:    +203% (0.33 \u2192 1.00)\n\n\ud83d\udcc1 Optimized prompt saved to:\n   my_deepagents_project/agents/research_agent_deepagents/optimized/\n\n\ud83d\udca1 Next steps:\n   1. Review optimized results\n   2. Test: super agent run research_agent_deepagents\n   3. Evaluate: super agent evaluate research_agent_deepagents  # automatically loads optimized weights\n</code></pre></p> <p>What GEPA did: 1. Analyzed failures from baseline 2. Reflected on why scenarios failed 3. Proposed 3 improved system prompts per iteration 4. Evaluated each proposal 5. Selected best performer (Pareto selection) 6. Repeated for 3 iterations 7. Achieved significantly improved performance (results vary by hardware and model)</p> <p>Cost: $0.00 (all using FREE Gemini quota!)</p> <p>API Calls Made: - ~10 execution calls (Gemini 2.5 Flash) - ~6 reflection calls (Gemini 2.5 Pro) - Total: ~16 calls (well within free tier: 15/min, 1500/day)</p> <p>Checkpoint: Optimization should complete in 5-10 minutes. Be patient!</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-8-compare-before-vs-after","title":"Step 8: Compare Before vs. After","text":"<p>Let's see what improved:</p> <p>Before GEPA (Original System Prompt): <pre><code>system_prompt: |\n  Expert AI Researcher\n\n  Goal: Conduct thorough research on AI and technology topics, producing \n  comprehensive, well-sourced reports\n\n  Reasoning Method: planning\n  Steps:\n    1. Break down research into subtasks using write_todos\n    2. Search for authoritative sources\n    3. Save findings to research_notes.md\n    4. Synthesize information\n    5. Write comprehensive report\n</code></pre></p> <p>After GEPA (Optimized System Prompt): <pre><code>system_prompt: |\n  You are a meticulous AI research specialist with expertise in technical \n  analysis and comprehensive documentation.\n\n  CORE OBJECTIVE: Deliver thorough, well-sourced research reports that provide \n  deep insights into AI technologies and frameworks, with clear structure and \n  authoritative citations.\n\n  RESEARCH METHODOLOGY:\n\n  1. ANALYZE the research question\n     - Identify main topic and key subtopics\n     - Determine scope and depth required\n     - Note any specific focus areas\n\n  2. PLAN systematically using write_todos\n     - List 3-5 specific research tasks\n     - Prioritize authoritative sources (documentation, papers, expert blogs)\n     - Define deliverable structure\n\n  3. RESEARCH comprehensively\n     - Query multiple authoritative sources\n     - Extract key facts, examples, and technical details\n     - Document findings with source URLs\n     - Save to research_notes.md with proper citations\n\n  4. SYNTHESIZE insights\n     - Identify patterns and common themes\n     - Note areas of consensus vs. debate\n     - Highlight practical implications and use cases\n\n  5. COMPOSE structured report\n     - Clear introduction establishing context\n     - Well-organized sections with descriptive headings\n     - Specific examples and code snippets where relevant\n     - Minimum 5-7 authoritative citations\n     - Balanced perspective on controversial topics\n\n  QUALITY STANDARDS:\n  - Technical accuracy over brevity\n  - Specific examples beat generic descriptions\n  - Always cite sources with [Title](URL) format\n  - Academic tone, professional language\n  - Comprehensive coverage (users expect depth)\n</code></pre></p> <p>Key Improvements: - More specific instructions - Better structure and organization - Explicit quality standards - Clearer methodology steps - Emphasis on citations and sources</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-9-test-optimized-agent","title":"Step 9: Test Optimized Agent","text":"<p>Run the agent with the optimized prompt:</p> <pre><code>super agent run research_agent_deepagents --goal \"Compare LangGraph vs LangChain. Give me key differences.\"\n</code></pre> <p>Expected Output (Better Quality): <pre><code>Response \u2502 LangGraph and LangChain serve different but complementary purposes:\n         \u2502 \n         \u2502 **LangChain** is a framework for building applications powered by LLMs, \n         \u2502 providing components for prompts, chains, agents, and integrations. It \n         \u2502 focuses on linear workflows and simple agent loops.\n         \u2502 \n         \u2502 **LangGraph** extends LangChain by adding stateful, cyclic computation \n         \u2502 graphs. Key differences:\n         \u2502 \n         \u2502 1. **Architecture**: LangChain uses linear chains; LangGraph uses graphs\n         \u2502 2. **State Management**: LangChain is stateless; LangGraph maintains state\n         \u2502 3. **Cycles**: LangChain is acyclic; LangGraph supports cycles/loops\n         \u2502 4. **Complexity**: LangChain for simple workflows; LangGraph for complex\n         \u2502 5. **Use Cases**: LangChain for Q&amp;A; LangGraph for multi-step agents\n         \u2502 \n         \u2502 Sources:\n         \u2502 [1] LangGraph Documentation: https://langchain-ai.github.io/langgraph/\n         \u2502 [2] LangChain Documentation: https://python.langchain.com/\n</code></pre></p> <p>Notice the improvement: - Better structured response - More comprehensive coverage - Clear key differences listed - Proper source citations</p>"},{"location":"tutorials/deepagents-complete-workflow/#step-10-evaluate-optimized-agent","title":"Step 10: Evaluate Optimized Agent","text":"<p>Measure the improvement:</p> <pre><code>super agent evaluate research_agent_deepagents  # automatically loads optimized weights\n</code></pre> <p>Expected Output: <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\ud83e\uddea SuperOptiX BDD Spec Runner - Professional Agent Validation\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nOptimization: \ud83d\ude80 Optimized Model\n\n\ud83e\uddea Executing BDD Specification Suite\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udd0d Evaluating research_agent_deep_agents...\n\nTesting 3 BDD scenarios:\n\nDeepAgents agent initialized with model: google-genai:gemini-2.5-flash\nSimple research query: PASS\nTechnical comparison: PASS\nComplex research: PASS\n\n============================================================\nOverall: 3/3 PASS (100.0%)\n============================================================\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udfe2 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502  \ud83d\udcca Total Specs:         3                \ud83c\udfaf Pass Rate:         100.0%       \u2502\n\u2502  Passed:              3                                                   \u2502\n\u2502  Failed:              0                                                   \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        EXCELLENT                                        \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n       \ud83c\udfc1 Specification execution completed - 100.0% pass rate (3/3 specs)       \n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre></p> <p>\ud83c\udf89 Amazing Results! - Before: Baseline performance - After: Significantly improved performance - Improvement: Noticeable enhancement (results vary by hardware and model)</p> <p>All scenarios now passing: - Simple research query - Technical comparison - Complex research</p> <p>Checkpoint: This demonstrates GEPA's power - it automatically improved the agent's performance significantly!</p>"},{"location":"tutorials/deepagents-complete-workflow/#backend-configuration-deepagents-020","title":"\ud83d\uddc4\ufe0f Backend Configuration (DeepAgents 0.2.0)","text":""},{"location":"tutorials/deepagents-complete-workflow/#understanding-backends","title":"Understanding Backends","text":"<p>DeepAgents 0.2.0 introduces pluggable backends that control where agent files are stored. This is a game-changer for production agents!</p>"},{"location":"tutorials/deepagents-complete-workflow/#backend-type-1-statebackend-default-ephemeral","title":"Backend Type 1: StateBackend (Default - Ephemeral)","text":"<p>Use Case: Temporary scratch space, single-conversation agents</p> <p>Configuration: <pre><code>spec:\n  backend:\n    type: state  # Files exist only during current conversation\n</code></pre></p> <p>Behavior: <pre><code># First run\nsuper agent run my_agent --goal \"Save 'Hello' to /note.txt\"\n# Agent writes /note.txt\n\n# New conversation (different thread)\nsuper agent run my_agent --goal \"Read /note.txt\"\n# File not found (ephemeral storage)\n</code></pre></p> <p>Best For: - Quick Q&amp;A - Temporary calculations - Draft generation - Development/testing</p>"},{"location":"tutorials/deepagents-complete-workflow/#backend-type-2-storebackend-persistent-memory","title":"Backend Type 2: StoreBackend (Persistent Memory!)","text":"<p>Use Case: Chatbots that remember users, learning agents</p> <p>Configuration: <pre><code>spec:\n  backend:\n    type: store  # \u2728 Files persist FOREVER!\n</code></pre></p> <p>Example: Persistent Chatbot</p> <pre><code># Pull demo\nsuper agent pull chatbot_persistent\nsuper agent compile chatbot_persistent --framework deepagents\n\n# First conversation\nsuper agent run chatbot_persistent --goal \"Hi! My name is Alice and I love gardening.\"\n</code></pre> <p>Agent's Actions: 1. Creates <code>/user_profile.txt</code>:    <pre><code>Name: Alice\nInterests: gardening\nFirst Contact: 2025-10-29\n</code></pre> 2. Saves to LangGraph store (persistent database) 3. Responds: \"Nice to meet you, Alice! I see you love gardening...\"</p> <p>Days Later, New Conversation: <pre><code>super agent run chatbot_persistent --goal \"What's my name?\"\n</code></pre></p> <p>Agent's Actions: 1. Reads <code>/user_profile.txt</code> (still there! ) 2. Finds: \"Name: Alice\" 3. Responds: \"Your name is Alice!\"</p> <p>Weeks Later: <pre><code>super agent run chatbot_persistent --goal \"What hobbies do I have?\"\n</code></pre></p> <p>Response: \"You love gardening!\"  \ud83c\udf89 The agent remembers across ALL conversations!</p> <p>Best For: - Customer support chatbots - Personal assistants - Learning agents - Any agent that needs memory</p>"},{"location":"tutorials/deepagents-complete-workflow/#backend-type-3-filesystembackend-real-files","title":"Backend Type 3: FilesystemBackend (Real Files!)","text":"<p>Use Case: Code analysis, file editing, project work</p> <p>Configuration: <pre><code>spec:\n  backend:\n    type: filesystem\n    root_dir: /Users/local/my_project  # Path to your project\n</code></pre></p> <p>Example: Code Review Agent</p> <pre><code># Setup: Create a sample project\nmkdir -p /tmp/demo_code\ncat &gt; /tmp/demo_code/app.py &lt;&lt; 'EOF'\ndef login(username, password):\n    query = f\"SELECT * FROM users WHERE username='{username}'\"  # SQL injection!\n    return db.execute(query)\nEOF\n\n# Pull code reviewer\nsuper agent pull code_reviewer\n\n# Edit playbook to set root_dir:\n# backend:\n#   type: filesystem\n#   root_dir: /tmp/demo_code\n\n# Compile and run\nsuper agent compile code_reviewer --framework deepagents\nsuper agent run code_reviewer --goal \"Review app.py for security issues\"\n</code></pre> <p>Agent's Actions: 1. Reads REAL file: <code>/tmp/demo_code/app.py</code> 2. Analyzes code 3. Finds: SQL injection vulnerability (line 2) 4. Writes REAL report: <code>/tmp/demo_code/security_report.md</code></p> <p>Verify: <pre><code>cat /tmp/demo_code/security_report.md\n</code></pre></p> <p>You'll see a complete security report written to an actual file on your disk!</p> <p>Best For: - Code review agents - Documentation generators - File refactoring tools - Project analysis</p> <p>\u26a0\ufe0f Security: Agent can modify actual files! Use with trusted agents and limited <code>root_dir</code> scope.</p>"},{"location":"tutorials/deepagents-complete-workflow/#backend-type-4-compositebackend-hybrid-production","title":"Backend Type 4: CompositeBackend (Hybrid - Production!)","text":"<p>Use Case: Production agents with complex storage needs</p> <p>Configuration: <pre><code>spec:\n  backend:\n    type: composite\n    default: state                # Scratch space (fast)\n    routes:\n      /memories/: store          # Research findings (persistent)\n      /papers/: filesystem       # Academic papers (real files)\n      /cache/: state             # Search results (temporary)\n    root_dir: /Users/local/research\n</code></pre></p> <p>How It Works:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Agent Filesystem                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                          \u2502\n\u2502  /memories/                              \u2502\n\u2502  \u251c\u2500 research_notes.txt \u2192 Database \u2502\n\u2502  \u251c\u2500 findings.txt \u2192 Database \u2502\n\u2502  \u2514\u2500 index.txt \u2192 Database \u2502\n\u2502      (PERSISTS FOREVER)                  \u2502\n\u2502                                          \u2502\n\u2502  /papers/                                \u2502\n\u2502  \u251c\u2500 transformer.pdf \u2192 Real File \u2502\n\u2502  \u251c\u2500 bert.pdf \u2192 Real File \u2502\n\u2502  \u2514\u2500 gpt3.pdf \u2192 Real File \u2502\n\u2502      (ACTUAL FILES on your disk)         \u2502\n\u2502                                          \u2502\n\u2502  /cache/                                 \u2502\n\u2502  \u251c\u2500 search.txt \u2192 Ephemeral \u2502\n\u2502  \u2514\u2500 temp.txt \u2192 Ephemeral \u2502\n\u2502      (CLEARED each conversation)         \u2502\n\u2502                                          \u2502\n\u2502  / (root)                                \u2502\n\u2502  \u251c\u2500 draft.txt \u2192 Ephemeral \u2502\n\u2502  \u2514\u2500 workspace.txt \u2192 Ephemeral \u2502\n\u2502      (SCRATCH SPACE)                     \u2502\n\u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Example: Advanced Researcher</p> <pre><code># Pull demo\nsuper agent pull researcher_hybrid\n\n# Edit playbook to set your root_dir\nsuper agent compile researcher_hybrid --framework deepagents\n\n# First research session\nsuper agent run researcher_hybrid --goal \"Research transformers and save important findings to /memories/\"\n</code></pre> <p>Agent's Workflow: 1. Checks <code>/memories/research_index.txt</code> (empty - first time) 2. Searches for transformer information 3. Saves temp results to <code>/cache/search_results.txt</code> (fast ephemeral storage) 4. Reads <code>/papers/attention.pdf</code> if available (real file) 5. Writes to <code>/memories/transformer_research.txt</code> (PERSISTS in database!) 6. Updates <code>/memories/research_index.txt</code></p> <p>Week Later: <pre><code>super agent run researcher_hybrid --goal \"What did I research about transformers?\"\n</code></pre></p> <p>Agent's Workflow: 1. Reads <code>/memories/research_index.txt</code> (STILL THERE from last week!) 2. Finds reference to transformer_research.txt 3. Reads <code>/memories/transformer_research.txt</code> (PERSISTS!) 4. Responds: \"Based on your research from October 29<sup>th</sup>, transformers are...\"</p> <p>\ud83c\udf89 Perfect hybrid strategy: - Fast temporary storage (<code>/cache/</code>, <code>/</code>) - Persistent memory (<code>/memories/</code>) - Real file access (<code>/papers/</code>)</p> <p>Best For: - Development assistants - Complex research agents - Multi-domain agents - Production systems</p>"},{"location":"tutorials/deepagents-complete-workflow/#complete-workflow-summary","title":"\ud83d\udcca Complete Workflow Summary","text":""},{"location":"tutorials/deepagents-complete-workflow/#commands-reference","title":"Commands Reference","text":"<pre><code># Initialize\nsuper init my_project &amp;&amp; cd my_project\n\n# Pull agent\nsuper agent pull research_agent_deepagents\n\n# Compile\nsuper agent compile research_agent_deepagents --framework deepagents\n\n# Run\nsuper agent run research_agent_deepagents --goal \"Your query here\"\n\n# Evaluate (baseline)\nsuper agent evaluate research_agent_deepagents\n\n# Optimize (uses your Gemini key from fish config)\nsuper agent optimize research_agent_deepagents \\\n  --framework deepagents \\\n  --auto medium \\\n  --reflection-lm google-genai:gemini-2.5-pro\n\n# Evaluate (optimized)\nsuper agent evaluate research_agent_deepagents  # automatically loads optimized weights\n\n# Run optimized\nsuper agent run research_agent_deepagents --goal \"Complex query here\"\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#expected-results","title":"Expected Results","text":"Step Baseline After GEPA Simple queries Good Excellent Technical comparisons Poor Good Complex research Poor Good Overall Baseline Significantly Improved (results vary by hardware/model)"},{"location":"tutorials/deepagents-complete-workflow/#api-costs","title":"API Costs","text":"Operation Calls Model Cost Run (x1) 1 Gemini Flash $0.00 Evaluate (x1) 3 Gemini Flash $0.00 Optimize (medium) ~30 Flash + Pro $0.00 Total ~35 FREE tier $0.00 <p>You can run ~40 optimizations per day completely FREE!</p>"},{"location":"tutorials/deepagents-complete-workflow/#advanced-examples","title":"\ud83c\udf93 Advanced Examples","text":"<p>Prefer CLI over copying YAML. Use prebuilt agents as starting points and adjust the generated playbooks locally after pulling.</p>"},{"location":"tutorials/deepagents-complete-workflow/#example-1-persistent-memory-chatbot-storebackend","title":"Example 1: Persistent Memory Chatbot (StoreBackend)","text":"<pre><code>super agent pull chatbot_persistent\nsuper agent compile chatbot_persistent --framework deepagents\nsuper agent run chatbot_persistent --goal \"Hi! I'm Sarah and I love gardening.\"\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#example-2-code-review-agent-with-real-files","title":"Example 2: Code Review Agent with Real Files","text":"<p>Setup Test Project: <pre><code>mkdir -p /tmp/my_app/src\ncat &gt; /tmp/my_app/src/auth.py &lt;&lt; 'EOF'\ndef login(username, password):\n    # TODO: Add input validation\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    return db.execute(query)\n\ndef register(username, password, email):\n    # Missing email validation\n    user = User(username, password, email)\n    db.save(user)\n    return user\nEOF\n</code></pre></p> <p>After pulling, set <code>backend.root_dir</code> in the playbook to <code>/tmp/my_app</code>, then:</p> <p>Usage: <pre><code>super agent compile code_reviewer --framework deepagents\n\n# Review specific file\nsuper agent run code_reviewer --goal \"Review src/auth.py for security vulnerabilities\"\n</code></pre></p> <p>Expected Response: <pre><code>Found 2 CRITICAL security issues in /src/auth.py:\n\n1. SQL INJECTION - CRITICAL (Line 3)\n   File: /src/auth.py\n\n   Vulnerable code:\n   query = f\"SELECT * FROM users WHERE username='{username}' ...\"\n\n   Issue: Unsanitized user input directly in SQL query allows SQL injection.\n\n   Fix: Use parameterized queries:\n   query = \"SELECT * FROM users WHERE username=? AND password=?\"\n   result = db.execute(query, (username, password))\n\n2. MISSING INPUT VALIDATION - HIGH (Line 7)\n   File: /src/auth.py\n\n   Issue: Email address not validated before saving.\n\n   Fix: Add email validation\n</code></pre></p> <p>Generate Report: <pre><code>super agent run code_reviewer --goal \"Analyze all Python files and write a complete security report to /security_report.md\"\n</code></pre></p> <p>Verify: <pre><code># The report is a REAL file!\ncat /tmp/my_app/security_report.md\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#example-3-hybrid-research-agent-production-ready","title":"Example 3: Hybrid Research Agent (Production-Ready)","text":"<p>Setup: <pre><code>mkdir -p /tmp/research_workspace/papers\necho \"Sample academic paper about AI agents...\" &gt; /tmp/research_workspace/papers/agents_paper.txt\n</code></pre></p> <p>After pulling, set <code>root_dir</code> in the playbook to <code>/tmp/research_workspace</code>.</p> <p>Usage - First Session: <pre><code>super agent compile researcher_hybrid --framework deepagents\n\nsuper agent run researcher_hybrid --goal \"Research transformer architectures. Save key findings to /memories/.\"\n</code></pre></p> <p>Agent's Actions: 1. Checks <code>/memories/research_index.txt</code> (empty - first time) 2. Searches for information 3. Saves to <code>/cache/search_results.txt</code> (temporary) 4. Checks <code>/papers/</code> for relevant PDFs 5. Writes <code>/memories/transformer_research.txt</code> (PERSISTS!) 6. Updates <code>/memories/research_index.txt</code></p> <p>File Locations: - <code>/memories/transformer_research.txt</code> \u2192 LangGraph store (database) - <code>/papers/transformer.pdf</code> \u2192 <code>/tmp/research_workspace/papers/transformer.pdf</code> (real file) - <code>/cache/search_results.txt</code> \u2192 LangGraph state (ephemeral)</p> <p>Usage - Week Later: <pre><code>super agent run researcher_hybrid --goal \"What did I learn about transformers?\"\n</code></pre></p> <p>Agent's Actions: 1. Reads <code>/memories/research_index.txt</code> (STILL THERE!) 2. Finds: \"transformers: See /memories/transformer_research.txt\" 3. Reads <code>/memories/transformer_research.txt</code> (PERSISTS!) 4. Responds with full research summary from last week</p> <p>\ud83c\udf89 Perfect for production: Fast temporary storage + persistent memory + real file access!</p>"},{"location":"tutorials/deepagents-complete-workflow/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/deepagents-complete-workflow/#issue-1-api-key-not-set","title":"Issue 1: \"API key not set\"","text":"<p>Error: <pre><code>GOOGLE_API_KEY not set\n</code></pre></p> <p>Solution: <pre><code># Check if set\necho $GOOGLE_API_KEY\n\n# If empty, set it\nexport GOOGLE_API_KEY=\"AIzaSy-your-actual-key\"\n\n# For fish shell (permanent)\nset -x GOOGLE_API_KEY \"AIzaSy-your-key\"\necho \"set -x GOOGLE_API_KEY \\\"AIzaSy-your-key\\\"\" &gt;&gt; ~/.config/fish/config.fish\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#issue-2-failed-to-initialize-deepagents","title":"Issue 2: \"Failed to initialize DeepAgents\"","text":"<p>Error: <pre><code>\u26a0\ufe0f  Failed to initialize DeepAgents: No module named 'langchain_google_genai'\n</code></pre></p> <p>Solution: <pre><code>pip install langchain-google-genai\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#issue-3-rate-limit-exceeded","title":"Issue 3: \"Rate limit exceeded\"","text":"<p>Error: <pre><code>google.api_core.exceptions.ResourceExhausted: 429 Quota exceeded\n</code></pre></p> <p>Solution: <pre><code># Use lighter optimization (fewer API calls)\nsuper agent optimize my_agent --auto light --reflection-lm google-genai:gemini-2.5-flash\n\n# Or wait 1 minute (free tier: 15 requests/minute)\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#issue-4-pipeline-not-found","title":"Issue 4: \"Pipeline not found\"","text":"<p>Error: <pre><code>Pipeline not found for agent 'my_agent'\n</code></pre></p> <p>Solution: <pre><code># Make sure to specify framework when optimizing non-DSPy agents\nsuper agent optimize my_agent --framework deepagents --auto medium --reflection-lm google-genai:gemini-2.5-pro\n\n# Recompile if needed\nsuper agent compile my_agent --framework deepagents\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#issue-5-files-not-persisting","title":"Issue 5: Files not persisting","text":"<p>Problem: Agent doesn't remember things across conversations</p> <p>Check backend type: <pre><code># Wrong: Ephemeral\nbackend:\n  type: state\n\n# Correct: Persistent\nbackend:\n  type: store\n</code></pre></p> <p>Fix: 1. Edit playbook 2. Change <code>type: state</code> to <code>type: store</code> 3. Recompile: <code>super agent compile my_agent --framework deepagents</code> 4. Test again</p>"},{"location":"tutorials/deepagents-complete-workflow/#issue-6-cant-access-local-files","title":"Issue 6: Can't access local files","text":"<p>Problem: Agent can't read your project files</p> <p>Check configuration: <pre><code>backend:\n  type: filesystem\n  root_dir: /Users/local/my_project  # Must be set!\n</code></pre></p> <p>Verify path exists: <pre><code>ls /Users/local/my_project\n# Should show your project files\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#security-best-practices","title":"\ud83d\udd12 Security Best Practices","text":""},{"location":"tutorials/deepagents-complete-workflow/#filesystembackend-security","title":"FilesystemBackend Security","text":"<p>When using <code>FilesystemBackend</code>, the agent can read and modify actual files!</p> <p>Safe Configuration: <pre><code>backend:\n  type: filesystem\n  root_dir: /tmp/agent_sandbox  # Isolated directory\n</code></pre></p> <p>Unsafe Configuration: <pre><code>backend:\n  type: filesystem\n  root_dir: /  # CAN ACCESS ENTIRE SYSTEM!\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#recommendations","title":"Recommendations","text":"<ol> <li> <p>Limit Scope: <pre><code># Good: Specific project directory\nroot_dir: /Users/local/my_project/src\n\n# Bad: System root\nroot_dir: /\n</code></pre></p> </li> <li> <p>Use Read-Only Patterns: <pre><code>persona:\n  system_prompt: |\n    You can READ files from /project/.\n    Only WRITE to /reports/ directory.\n    Never DELETE files.\n</code></pre></p> </li> <li> <p>Validate Changes:    Add to system prompt:    <pre><code>Before modifying any file:\n1. Show the user the planned changes\n2. Explain why the changes are needed\n3. Only proceed after confirmation\n</code></pre></p> </li> <li> <p>Use Composite for Safety: <pre><code>backend:\n  type: composite\n  default: state\n  routes:\n    /project/: filesystem  # Real files (read-only usage)\n    /output/: state        # Reports (safe to write)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/deepagents-complete-workflow/#performance-optimization-tips","title":"\ud83d\udcc8 Performance Optimization Tips","text":""},{"location":"tutorials/deepagents-complete-workflow/#choose-right-model-for-each-task","title":"Choose Right Model for Each Task","text":"<pre><code># For agent execution (runs many times)\nlanguage_model:\n  model: gemini-2.5-flash  # Fast and cheap\n\n# For GEPA reflection (runs fewer times)\noptimization:\n  optimizer:\n    params:\n      reflection_lm: gemini-2.5-pro  # Better reasoning\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#optimize-bdd-scenarios","title":"Optimize BDD Scenarios","text":"<p>Start with 3-5 good scenarios: <pre><code>feature_specifications:\n  scenarios:\n    - name: Simple case\n      input:\n        query: \"Basic question\"\n      expected_output:\n        expected_keywords: [keyword1, keyword2]\n\n    - name: Medium complexity\n      ...\n\n    - name: Complex case\n      ...\n</code></pre></p>"},{"location":"tutorials/deepagents-complete-workflow/#use-appropriate-gepa-budget","title":"Use Appropriate GEPA Budget","text":"<pre><code># Quick test (5 min, ~15 API calls)\nsuper agent optimize my_agent --auto light\n\n# Balanced (10 min, ~30 API calls)\nsuper agent optimize my_agent --auto medium\n\n# Best results (20 min, ~60 API calls)\nsuper agent optimize my_agent --auto heavy\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#optimize-backend-strategy","title":"Optimize Backend Strategy","text":"<pre><code># Fast but ephemeral\nbackend:\n  type: state\n\n# Persistent but slower\nbackend:\n  type: store\n\n# Best of both worlds\nbackend:\n  type: composite\n  default: state        # Fast default\n  routes:\n    /memories/: store  # Only persist what's important\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#production-deployment","title":"\ud83d\ude80 Production Deployment","text":""},{"location":"tutorials/deepagents-complete-workflow/#production-ready-playbook-template","title":"Production-Ready Playbook Template","text":"<pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: Production Agent\n  version: 1.0.0\nspec:\n  target_framework: deepagents\n\n  # Production model\n  language_model:\n    provider: google-genai\n    model: gemini-2.5-flash\n    temperature: 0.5  # Lower for more consistent responses\n    max_tokens: 8192\n\n  # Production backend (hybrid)\n  backend:\n    type: composite\n    default: state\n    routes:\n      /memories/: store          # User data, persistent\n      /workspace/: filesystem    # Project files\n      /cache/: state             # Temporary\n    root_dir: /var/app/workspace\n\n  # Comprehensive BDD scenarios\n  feature_specifications:\n    scenarios:\n      - name: Critical path 1\n        ...\n      - name: Critical path 2\n        ...\n      - name: Edge case 1\n        ...\n      # 10-15 scenarios recommended for production\n\n  # GEPA optimization (run during CI/CD)\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: response_accuracy\n        auto: heavy  # Best for production\n        reflection_lm: google-genai:gemini-2.5-pro\n        max_full_evals: 10\n    metric_threshold: 0.95  # 95% minimum for production\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#production-deployment-steps","title":"Production Deployment Steps","text":"<pre><code># Develop and test locally\nsuper agent compile production_agent --framework deepagents\nsuper agent evaluate production_agent\n\n# Optimize for production\nsuper agent optimize production_agent \\\n  --framework deepagents \\\n  --auto heavy \\\n  --reflection-lm google-genai:gemini-2.5-pro\n\n# Validate optimized version\nsuper agent evaluate production_agent  # automatically loads optimized weights\n# Check performance metrics (varies by hardware/model)\n\n# Test with real data\nsuper agent run production_agent --goal \"Production query\"\n\n# Deploy\n# Copy optimized prompt to production config\n# Set up monitoring and logging\n# Deploy with proper API key management\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#performance-notes","title":"\ud83d\udcca Performance Notes","text":"<p>Performance and accuracy vary based on hardware, model choice, prompts, and scenarios.</p>"},{"location":"tutorials/deepagents-complete-workflow/#try-more-demos","title":"\ud83d\ude80 Try More Demos","text":"<pre><code>super agent pull chatbot_persistent         # StoreBackend\nsuper agent pull code_reviewer              # FilesystemBackend (edit root_dir)\nsuper agent pull researcher_hybrid          # CompositeBackend\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"tutorials/deepagents-complete-workflow/#official-documentation","title":"Official Documentation","text":"<ul> <li>DeepAgents Integration Guide - Complete guide</li> <li>Backend Configuration Reference - All backend types</li> <li>Backend Tutorial - 6 hands-on tutorials</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#configuration-guides","title":"Configuration Guides","text":"<ul> <li>Gemini Configuration Guide - Model setup</li> <li>Quick Reference - Command cheat sheet</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#external-resources","title":"External Resources","text":"<ul> <li>DeepAgents 0.2.0 Announcement - LangChain blog</li> <li>Google AI Studio - Get FREE Gemini API key</li> <li>Gemini Pricing - Free tier details</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#quick-command-reference","title":"\ud83c\udfaf Quick Command Reference","text":""},{"location":"tutorials/deepagents-complete-workflow/#essential-commands","title":"Essential Commands","text":"<pre><code># Initialize\nsuper init my_project &amp;&amp; cd my_project\n\n# Pull agent\nsuper agent pull research_agent_deepagents\n\n# Compile\nsuper agent compile research_agent_deepagents --framework deepagents\n\n# Run\nsuper agent run research_agent_deepagents --goal \"Your query\"\n\n# Evaluate\nsuper agent evaluate research_agent_deepagents\n\n# Optimize\nsuper agent optimize research_agent_deepagents \\\n  --framework deepagents \\\n  --auto medium \\\n  --reflection-lm google-genai:gemini-2.5-pro\n\n# Test optimized\nsuper agent evaluate research_agent_deepagents  # automatically loads optimized weights\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#all-demo-agents","title":"All Demo Agents","text":"<pre><code># Basic research (StateBackend)\nsuper agent pull research_agent_deepagents\n\n# Persistent chatbot (StoreBackend)\nsuper agent pull chatbot_persistent\n\n# Code reviewer (FilesystemBackend)\nsuper agent pull code_reviewer\n\n# Hybrid researcher (CompositeBackend)\nsuper agent pull researcher_hybrid\n</code></pre>"},{"location":"tutorials/deepagents-complete-workflow/#success-criteria","title":"\ud83c\udf89 Success Criteria","text":"<p>By the end of this tutorial, you should be able to:</p> <ul> <li>Initialize a SuperOptiX project</li> <li>Pull and compile DeepAgents agents</li> <li>Run agents with real Gemini API calls</li> <li>Evaluate agent performance with BDD scenarios</li> <li>Optimize agents with GEPA (achieving 2-3x improvement)</li> <li>Configure all 4 backend types</li> <li>Build persistent chatbots</li> <li>Create code review agents</li> <li>Design hybrid storage strategies</li> <li>Deploy production-ready agents</li> </ul> <p>If you've done all this: \ud83c\udf8a Congratulations! You're a DeepAgents expert!</p>"},{"location":"tutorials/deepagents-complete-workflow/#next-steps","title":"\ud83d\udca1 Next Steps","text":""},{"location":"tutorials/deepagents-complete-workflow/#immediate","title":"Immediate","text":"<ol> <li>Try all 4 demo agents</li> <li>Experiment with different backends</li> <li>Build your first custom agent</li> </ol>"},{"location":"tutorials/deepagents-complete-workflow/#this-week","title":"This Week","text":"<ol> <li>Read the complete Backend Reference</li> <li>Follow the Backend Tutorial</li> <li>Optimize your agents with GEPA</li> </ol>"},{"location":"tutorials/deepagents-complete-workflow/#this-month","title":"This Month","text":"<ol> <li>Build production-ready agents</li> <li>Deploy to real users</li> <li>Monitor and iterate</li> </ol>"},{"location":"tutorials/deepagents-complete-workflow/#community-support","title":"\ud83e\udd1d Community &amp; Support","text":""},{"location":"tutorials/deepagents-complete-workflow/#get-help","title":"Get Help","text":"<ul> <li>\ud83d\udcd6 Documentation: https://superagenticai.github.io/superoptix/</li> <li>\ud83d\udce7 Email: hello@super-agentic.ai</li> <li>\ud83c\udf10 Website: https://superoptix.ai</li> </ul>"},{"location":"tutorials/deepagents-complete-workflow/#share-your-success","title":"Share Your Success","text":"<p>Built something cool? Share it with the community!</p> <ul> <li>\ud83d\udc26 Tag us on Twitter/X: @SuperagenticAI</li> <li>\ud83d\udce7 Email us your success story: hello@super-agentic.ai</li> </ul> <p>\ud83c\udf8a You're ready to build amazing DeepAgents! Happy coding! \ud83d\ude80</p>"},{"location":"tutorials/first-orchestra/","title":"\ud83c\udfbc Create Your First Orchestra: Multi-Agent Team","text":""},{"location":"tutorials/first-orchestra/#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":"<p>You'll create a multi-agent orchestra with:</p> <ul> <li>\ud83e\udd16 3 Specialized Agents: Developer, QA Engineer, and DevOps Engineer</li> <li>\ud83c\udfbc Orchestra Coordination: Sequential workflow management</li> <li>\ud83c\udfaf Team Collaboration: Agents working together on complex tasks</li> <li>\ud83d\udcca Full Observability: Complete tracing and monitoring</li> </ul> <p>This is a production-ready multi-agent system that demonstrates the power of agent orchestration for complex software development workflows.</p>"},{"location":"tutorials/first-orchestra/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Python 3.8+ installed</li> <li>SuperOptiX installed (see Installation Guide)</li> <li>Completed the Your First Agent tutorial (recommended)</li> </ul>"},{"location":"tutorials/first-orchestra/#caution-multi-agent-resource-warning","title":"\ud83d\udea8 Caution: Multi-Agent Resource Warning","text":"<p>Multi-Agent Systems are Resource Intensive</p> <ul> <li>Multiple agents running simultaneously can consume significant resources</li> <li>Each agent makes separate LLM calls, increasing total token usage</li> <li>Orchestra coordination adds overhead to the system</li> <li>Monitor your API usage and costs carefully if using cloud LLMs</li> <li>Only proceed if you understand the resource and cost implications!</li> </ul>"},{"location":"tutorials/first-orchestra/#1-initialize-your-project","title":"1\ufe0f\u20e3 Initialize Your Project","text":"<pre><code>super init swe\n</code></pre> Actual Output <pre><code>================================================================================\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 SUCCESS! Your full-blown shippable Agentic System 'swe' is ready!                                         \u2502\n\u2502                                                                                                              \u2502\n\u2502 \ud83d\ude80 You now own a complete agentic AI system in 'swe'.                                                        \u2502\n\u2502                                                                                                              \u2502\n\u2502 Start making it production-ready by evaluating, optimizing, and orchestrating with advanced agent            \u2502\n\u2502 engineering.                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Your Journey Starts Here \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 GETTING STARTED                                                                                          \u2502\n\u2502                                                                                                              \u2502\n\u2502  1. Move to your new project root and confirm setup:                                                         \u2502\n\u2502     cd swe                                                                                                   \u2502\n\u2502     # You should see a .super file here - always run super commands from this directory                      \u2502\n\u2502                                                                                                              \u2502\n\u2502  2. Pull your first agent:                                                                                   \u2502\n\u2502     super agent pull developer  # swap 'developer' for any agent name                                        \u2502\n\u2502                                                                                                              \u2502\n\u2502  3. Explore the marketplace:                                                                                 \u2502\n\u2502     super market                                                                                             \u2502\n\u2502                                                                                                              \u2502\n\u2502  4. Need the full guide?                                                                                     \u2502\n\u2502     super docs                                                                                               \u2502\n\u2502     https://superoptix.dev/docs                                                                              \u2502\n\u2502                                                                                                              \u2502\n\u2502  Tip: Use 'super market search &lt;keyword&gt;' to discover components tailored to your domain.                    \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udfaf Welcome to your Agentic System! Ready to build intelligent agents? \ud83d\ude80\n\ud83d\udccd Next steps: cd swe\n================================================================================\n</code></pre>"},{"location":"tutorials/first-orchestra/#2-pull-multiple-pre-built-agents","title":"2\ufe0f\u20e3 Pull Multiple Pre-built Agents","text":"<p>Now let's pull three specialized agents that will work together as a team:</p> <pre><code>cd swe\nsuper agent pull developer\nsuper agent pull qa_engineer\nsuper agent pull devops_engineer\n</code></pre> Actual Output  **Developer Agent:** <pre><code>================================================================================\n\n\ud83e\udd16 Adding agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Agent Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 Name: Developer Assistant                                                                                \u2502\n\u2502  \ud83c\udfe2 Industry: Software | \ud83d\udd2e Tier: Oracles                                                                    \u2502\n\u2502  \ud83d\udd27 Tasks: 1 | \ud83d\udcc1 Location: swe/agents/developer/playbook/developer_playbook.yaml                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'Developer Assistant' ready for customization and deployment! \ud83d\ude80\n</code></pre>  **QA Engineer Agent:** <pre><code>================================================================================\n\n\ud83e\udd16 Adding agent 'qa_engineer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Agent Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 Name: QA Engineer Assistant                                                                              \u2502\n\u2502  \ud83c\udfe2 Industry: Software | \ud83d\udd2e Tier: Oracles                                                                    \u2502\n\u2502  \ud83d\udd27 Tasks: 1 | \ud83d\udcc1 Location: swe/agents/qa_engineer/playbook/qa_engineer_playbook.yaml                        \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'QA Engineer Assistant' ready for customization and deployment! \ud83d\ude80\n</code></pre>  **DevOps Engineer Agent:** <pre><code>================================================================================\n\n\ud83e\udd16 Adding agent 'devops_engineer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Agent Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 Name: DevOps Engineer Assistant                                                                          \u2502\n\u2502  \ud83c\udfe2 Industry: Software | \ud83d\udd2e Tier: Oracles                                                                    \u2502\n\u2502  \ud83d\udd27 Tasks: 1 | \ud83d\udcc1 Location: swe/agents/devops_engineer/playbook/devops_engineer_playbook.yaml                \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'DevOps Engineer Assistant' ready for customization and deployment! \ud83d\ude80\n</code></pre> <p>\ud83c\udfaf Why These Three Agents?</p> <p>We're creating a Software Development Lifecycle (SDLC) team:</p> <ul> <li>\ud83e\udd16 Developer Agent: Handles code development, architecture, and implementation</li> <li>\ud83e\uddea QA Engineer Agent: Manages testing, quality assurance, and validation</li> <li>\u2699\ufe0f DevOps Engineer Agent: Handles deployment, infrastructure, and operations</li> </ul> <p>This combination creates a complete development workflow from coding to deployment!</p>"},{"location":"tutorials/first-orchestra/#3-compile-all-agents","title":"3\ufe0f\u20e3 Compile All Agents","text":"<p>Now let's compile all three agents at once to create their executable pipelines:</p> <pre><code>super agent compile --all\n</code></pre> Actual Output <pre><code>\ud83d\ude80 Compiling all 3 agents in project 'swe'...\n================================================================================\n\n\ud83d\udd28 Compiling agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Developer Assistant                                                                               \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                      \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/developer/pipelines/developer_pipeline.py                                             \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully generated Oracles-tier pipeline (mixin) at: /Users/super/swe \n18-15-10-253/swe/agents/developer/pipelines/developer_pipeline.py\n\n\ud83c\udfaf Oracles Tier Features\n  Basic Predict and Chain of Thought modules\n  Bootstrap Few-Shot optimization\n  Basic evaluation metrics\n  Sequential task orchestration\n  Basic tracing and observability\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'Developer Assistant' pipeline ready! Time to make it yours! \ud83d\ude80\n================================================================================\n\n\ud83d\udd28 Compiling agent 'devops_engineer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: DevOps Engineer Assistant                                                                         \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                      \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/devops_engineer/pipelines/devops_engineer_pipeline.py                                 \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully generated Oracles-tier pipeline (mixin) at: /Users/super/swe \n18-15-10-253/swe/agents/devops_engineer/pipelines/devops_engineer_pipeline.py\n\n\ud83c\udfaf Oracles Tier Features\n  Basic Predict and Chain of Thought modules\n  Bootstrap Few-Shot optimization\n  Basic evaluation metrics\n  Sequential task orchestration\n  Basic tracing and observability\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'DevOps Engineer Assistant' pipeline ready! Time to make it yours! \ud83d\ude80\n================================================================================\n\n\ud83d\udd28 Compiling agent 'qa_engineer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: QA Engineer Assistant                                                                             \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                      \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/qa_engineer/pipelines/qa_engineer_pipeline.py                                         \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nSuccessfully generated Oracles-tier pipeline (mixin) at: /Users/super/swe \n18-15-10-253/swe/agents/qa_engineer/pipelines/qa_engineer_pipeline.py\n\n\ud83c\udfaf Oracles Tier Features\n  Basic Predict and Chain of Thought modules\n  Bootstrap Few-Shot optimization\n  Basic evaluation metrics\n  Sequential task orchestration\n  Basic tracing and observability\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'QA Engineer Assistant' pipeline ready! Time to make it yours! \ud83d\ude80\n================================================================================\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udcca Compilation Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udf89 ALL AGENTS COMPILED SUCCESSFULLY!                                                                        \u2502\n\u2502                                                                                                              \u2502\n\u2502  Successful: 3 agent(s)                                                                                   \u2502\n\u2502  \ud83d\ude80 Ready for testing and customization!                                                                     \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>\ud83d\udd27 What Happens During Compilation</p> <p>The <code>--all</code> flag compiles all agents in your project:</p> <ul> <li>\ud83d\udcc1 Developer Pipeline: <code>swe/agents/developer/pipelines/developer_pipeline.py</code></li> <li>\ud83d\udcc1 QA Engineer Pipeline: <code>swe/agents/qa_engineer/pipelines/qa_engineer_pipeline.py</code></li> <li>\ud83d\udcc1 DevOps Engineer Pipeline: <code>swe/agents/devops_engineer/pipelines/devops_engineer_pipeline.py</code></li> </ul> <p>Each agent gets its own optimized DSPy pipeline ready for orchestration!</p>"},{"location":"tutorials/first-orchestra/#4-create-your-orchestra","title":"4\ufe0f\u20e3 Create Your Orchestra","text":"<p>Now let's create a multi-agent orchestra that coordinates these three agents:</p> <pre><code>super orchestra create sdlc\n</code></pre> Actual Output <pre><code>\ud83d\udd0e Found 3 existing agent(s): developer, devops_engineer, qa_engineer. Adding them to the orchestra.\n\ud83d\udcdd Loaded 3 task(s) from agent playbooks.\n\nCreated new orchestra definition at: swe/orchestras/sdlc_orchestra.yaml\n\ud83d\udc49 Orchestra automatically configured with tasks from agent playbooks.\n   Found 3 task(s): implement_feature, configure_ci_pipeline, create_test_plan\n\n\ud83d\udca1 Customization Guidance:\n   This is an automatic orchestra created based on your agent playbooks.\n   You should refine this orchestra based on your specific goal to make it more targeted.\n   You can:\n   \u2022 Add more agents that align with your goal\n   \u2022 Modify task descriptions to be more specific\n   \u2022 Adjust execution strategy (sequential/parallel)\n   \u2022 Add dependencies between tasks\n   \u2022 Set custom timeouts and priorities\n\n\ud83d\udccb Version Information:\n   \ud83c\udd93 Free Version: Sequential execution strategy only\n   \ud83d\udc8e Pro Version: Parallel, hierarchical, mixed strategies + Kubernetes orchestration\n   \u2139\ufe0f  Orchestra kind 'basic' is supported in both versions\n\n\ud83d\ude80 Ready to run: super orchestra run sdlc --goal \"your specific goal here\"\n\ud83d\udcdd Edit file: swe/orchestras/sdlc_orchestra.yaml\n\n\ud83c\udfaf Orchestra Workflow Recommendations:\n   Before running this orchestra, ensure your agents are optimized:\n   1. Compile all agents: super agent compile --all\n   2. Evaluate baseline: super agent evaluate &lt;agent_name&gt;\n   3. Optimize agents: super agent optimize &lt;agent_name&gt;\n   4. Re-evaluate improvement: super agent evaluate &lt;agent_name&gt;\n   5. Then run orchestra: super orchestra run sdlc --goal \"goal\"\n\n\ud83d\udca1 Well-optimized individual agents lead to better orchestration results!\n</code></pre> <p>\ud83c\udfbc What is an Orchestra?</p> <p>An Orchestra is a multi-agent coordination system that:</p> <ul> <li>\ud83c\udfaf Manages Agent Workflow: Defines how agents work together</li> <li>\ud83d\udccb Assigns Tasks: Distributes work among team members</li> <li>\ud83d\udd04 Coordinates Execution: Ensures proper task sequencing</li> <li>\ud83d\udcca Monitors Progress: Tracks completion and performance</li> </ul> <p>Think of it as a conductor that directs multiple musicians to create beautiful music together!</p> <p>\ud83d\udcdd Understanding the Orchestra YAML</p> <p>The orchestra creates a YAML configuration file that defines:</p> <pre><code>name: sdlc\ndescription: \"Software Development Lifecycle Orchestra\"\n\nagents:\n  - name: developer\n    role: \"Code development and implementation\"\n    tasks: [\"code_review\", \"implementation\", \"architecture\"]\n\n  - name: qa_engineer\n    role: \"Testing and quality assurance\"\n    tasks: [\"testing\", \"validation\", \"quality_check\"]\n\n  - name: devops_engineer\n    role: \"Deployment and infrastructure\"\n    tasks: [\"deployment\", \"infrastructure\", \"monitoring\"]\n\nworkflow:\n  type: \"sequential\"  # Agents work in order\n  steps:\n    - agent: developer\n      task: \"implementation\"\n    - agent: qa_engineer\n      task: \"testing\"\n    - agent: devops_engineer\n      task: \"deployment\"\n</code></pre> <p>\ud83c\udfaf Customize This File: You can modify the goals, tasks, and workflow to match your specific needs!</p> <p>\ud83d\udd04 Sequential Orchestras Only</p> <p>Currently Supported: Only sequential orchestras are supported in the current version.</p> <p>Sequential Workflow: Agents work one after another, passing results to the next agent.</p> <p>Future Support: Parallel orchestras and advanced coordination patterns are available in the commercial version.</p>"},{"location":"tutorials/first-orchestra/#5-list-your-orchestras","title":"5\ufe0f\u20e3 List Your Orchestras","text":"<p>Let's see what orchestras you have available:</p> <pre><code>super orchestra list\n</code></pre> Actual Output <pre><code>                                         \ud83c\udfb5 Orchestras in Project: swe\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ID   \u2503 Name           \u2503 Description                                                         \u2503 Agents \u2503 Tasks \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 sdlc \u2502 Sdlc Orchestra \u2502 An orchestra to accomplish a specific goal with flexible execution  \u2502 3      \u2502 3     \u2502\n\u2502      \u2502                \u2502 strategies.                                                         \u2502        \u2502       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\ud83d\udccb Orchestra Management</p> <p>The <code>super orchestra list</code> command shows:</p> <ul> <li>\ud83c\udfbc Available Orchestras: All orchestras in your project</li> <li>\ud83e\udd16 Agent Members: Which agents are part of each orchestra</li> <li>\ud83d\udcca Status: Whether orchestras are ready to run</li> <li>\ud83d\udcc1 Location: Where orchestra configurations are stored</li> </ul>"},{"location":"tutorials/first-orchestra/#6-run-your-orchestra","title":"6\ufe0f\u20e3 Run Your Orchestra","text":"<p>Now let's run your multi-agent orchestra with a complex goal that requires all three agents:</p> <pre><code>super orchestra run sdlc --goal \"Build a complete web application for a task management system with user authentication, CRUD operations, and real-time notifications. Include comprehensive testing and deployment configuration.\"\n</code></pre> Actual Output <pre><code>\ud83c\udfbc Running Orchestra: sdlc\n\ud83c\udfad Agent Tier: oracles\n\ud83d\udcc1 Using orchestra file: /Users/super/swe 18-15-10-253/swe/orchestras/sdlc_orchestra.yaml\n\ud83d\udcc1 Created workspace directory: /Users/super/swe 18-15-10-253/swe/orchestra_workspaces/sdlc\n\ud83d\udcc2 Using workspace: /Users/super/swe 18-15-10-253/swe/orchestra_workspaces/sdlc\n\ud83d\udd10 Validating tier access for oracles tier...\nTier validation passed!\n\ud83c\udfbc Using basic orchestration mode\n\ud83d\ude80 Running Basic Orchestra: Sdlc Orchestra\n\ud83d\udccb Executing 3 tasks sequentially...\n\n\ud83d\udd04 Task 1/3: implement_feature\n\ud83d\ude80 Using pre-optimized pipeline from developer_optimized.json\nModel connection successful: ollama/llama3.2:1b\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Oracle tier) initialized with 5 BDD scenarios\n\n         Analysis Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                                                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Reasoning      \u2502 To build a complete web application for a task management system, we need to consider the   \u2502\n\u2502                \u2502 following features:                                                                         \u2502\n\u2502                \u2502 1. User authentication using OAuth or JWT for secure login and authorization.               \u2502\n\u2502                \u2502 2. CRUD operations (Create, Read, Update, Delete) for tasks and users.                      \u2502\n\u2502                \u2502 3. Real-time notifications using WebSockets or Socket.IO for push updates.                  \u2502\n\u2502                \u2502 4. Comprehensive testing using Jest, React Testing Library, and Supertest to ensure the     \u2502\n\u2502                \u2502 application's stability and performance.                                                    \u2502\n\u2502                \u2502 5. Deployment configuration using AWS S3, Elastic Beanstalk, and Amazon RDS for scalable    \u2502\n\u2502                \u2502 infrastructure.                                                                             \u2502\n\u2502 Implementation \u2502 Firstly, we'll set up the project structure with a `server` folder containing the           \u2502\n\u2502                \u2502 Express.js app. We'll create a `users` collection in our MongoDB database and define models \u2502\n\u2502                \u2502 for User and Task.                                                                          \u2502\n\u2502                \u2502 Next, we'll implement user authentication using OAuth. We'll use Passport.js to handle      \u2502\n\u2502                \u2502 authentication and authorization.                                                           \u2502\n\u2502                \u2502 For CRUD operations, we'll create API endpoints for creating, reading, updating, and        \u2502\n\u2502                \u2502 deleting tasks and users.                                                                   \u2502\n\u2502                \u2502 To enable real-time notifications, we'll use WebSockets. We'll set up a WebSocket server to \u2502\n\u2502                \u2502 listen for incoming connections and broadcast updates to connected clients.                 \u2502\n\u2502                \u2502 Finally, we'll configure our deployment using AWS S3, Elastic Beanstalk, and Amazon RDS to  \u2502\n\u2502                \u2502 ensure scalability and high availability.                                                   \u2502\n\u2502                \u2502 Below is an example implementation of the feature:                                          \u2502\n\u2502                \u2502 // Import required modules                                                                  \u2502\n\u2502                \u2502 const express = require('express');                                                         \u2502\n\u2502                \u2502 const app = express();                                                                      \u2502\n\u2502                \u2502 // Define MongoDB connection                                                                \u2502\n\u2502                \u2502 const mongoose = require('mongoose');                                                       \u2502\n\u2502                \u2502 const User = mongoose.model('User', { username: String, password: String });               \u2502\n\u2502                \u2502 const Task = mongoose.model('Task', { title: String, description: String });               \u2502\n\u2502                \u2502 // Set up Passport.js for authentication                                                    \u2502\n\u2502                \u2502 const passport = require('passport');                                                       \u2502\n\u2502                \u2502 passport.authenticate('oauth', { strategy: 'jwt' });                                        \u2502\n\u2502                \u2502 // Implement user authentication                                                            \u2502\n\u2502                \u2502 app.use(passport.initialize());                                                             \u2502\n\u2502                \u2502 app.use(passport.session());                                                                \u2502\n\u2502                \u2502 // Define API endpoints for CRUD operations                                                 \u2502\n\u2502                \u2502 app.get('/users', async (req, res) =&gt; {                                                     \u2502\n\u2502                \u2502   const users = await User.find();                                                          \u2502\n\u2502                \u2502   res.json(users);                                                                          \u2502\n\u2502                \u2502 });                                                                                         \u2502\n\u2502                \u2502 // Implement WebSocket server for real-time notifications                                   \u2502\n\u2502                \u2502 const appWs = require('ws');                                                                \u2502\n\u2502                \u2502 const wss = new appWs({ port: 8080 });                                                      \u2502\n\u2502                \u2502 wss.on('connection', (ws) =&gt; {                                                              \u2502\n\u2502                \u2502   console.log('Client connected');                                                          \u2502\n\u2502                \u2502   ws.on('message', (message) =&gt; {                                                           \u2502\n\u2502                \u2502     // Handle incoming message and broadcast update to all clients                          \u2502\n\u2502                \u2502   });                                                                                       \u2502\n\u2502                \u2502 });                                                                                         \u2502\n\u2502                \u2502 // Configure deployment using AWS S3, Elastic Beanstalk, and Amazon RDS                    \u2502\n\u2502                \u2502 const s3 = require('aws-sdk').createS3();                                                   \u2502\n\u2502                \u2502 const beanstalk = require('aws-sdk').createElasticBeanstalkClient();                       \u2502\n\u2502                \u2502 const rds = require('aws-sdk').createRDSClient();                                           \u2502\n\u2502                \u2502 const bucket = 'your-bucket-name';                                                          \u2502\n\u2502                \u2502 const region = 'us-east-1';                                                                 \u2502\n\u2502                \u2502 const params = { Bucket: bucket, Region: region, DbName: 'your-database-name', StorageClass: \u2502\n\u2502                \u2502 'Standard' };                                                                               \u2502\n\u2502                \u2502 beanstalk.updateEndpoint({ params });                                                       \u2502\n\u2502                \u2502 rds.createDatabaseInstance(params);                                                         \u2502\n\u2502                \u2502 // Implement comprehensive testing using Jest, React Testing Library, and Supertest        \u2502\n\u2502                \u2502 const test = require('supertest');                                                          \u2502\n\u2502                \u2502 const app = require('./server');                                                            \u2502\n\u2502                \u2502 test(app, (err, res) =&gt; {                                                                   \u2502\n\u2502                \u2502   // Test for user authentication                                                            \u2502\n\u2502                \u2502 });                                                                                         \u2502\n\u2502                \u2502 // Implement deployment configuration using AWS S3, Elastic Beanstalk, and Amazon RDS      \u2502\n\u2502                \u2502 const s3Client = new s3({ region, bucket });                                                \u2502\n\u2502                \u2502 const beanstalkClient = new beanstalk();                                                    \u2502\n\u2502                \u2502 const rdsClient = new rds();                                                                \u2502\n\u2502 Trained        \u2502 False                                                                                       \u2502\n\u2502 Usage          \u2502 {'ollama_chat/llama3.2:1b': {'completion_tokens': 1844, 'prompt_tokens': 580,               \u2502\n\u2502                \u2502 'total_tokens': 2424, 'completion_tokens_details': 0, 'prompt_tokens_details': 0}}          \u2502\n\u2502 Agent_Id       \u2502 developer_20250711_185510                                                                   \u2502\n\u2502 Tier           \u2502 oracles                                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTask implement_feature completed in 11.16s\n\n\ud83d\udd04 Task 2/3: configure_ci_pipeline\n\ud83d\udcdd Using base pipeline (no optimization available)\nModel connection successful: ollama/llama3.2:1b\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDevopsEngineerPipeline (Oracle tier) initialized with 5 BDD scenarios\n\n         Analysis Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect          \u2503 Value                                                                                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Reasoning       \u2502 To automate software deployment and infrastructure management for a task management        \u2502\n\u2502                 \u2502 system, we can create a CI/CD pipeline using Docker, Kubernetes, and AWS services. The     \u2502\n\u2502                 \u2502 pipeline will involve the following stages:                                                \u2502\n\u2502                 \u2502 1. Building and pushing the application to the Docker registry.                            \u2502\n\u2502                 \u2502 2. Deploying the application to the AWS S3 bucket using Elastic Beanstalk.                 \u2502\n\u2502                 \u2502 3. Configuring the database with Amazon RDS.                                               \u2502\n\u2502                 \u2502 4. Testing the application using Jest, React Testing Library, and Supertest.               \u2502\n\u2502                 \u2502 5. Integrating the application with WebSocket for real-time notifications using Node.js    \u2502\n\u2502                 \u2502 and Socket.IO.                                                                             \u2502\n\u2502 Pipeline_Config \u2502 ['docker', 'aws', 'maven']                                                                \u2502\n\u2502 Trained         \u2502 False                                                                                      \u2502\n\u2502 Usage           \u2502 {'ollama_chat/llama3.2:1b': {'completion_tokens': 1345, 'prompt_tokens': 2478,             \u2502\n\u2502                 \u2502 'total_tokens': 3823, 'completion_tokens_details': 0, 'prompt_tokens_details': 0}}         \u2502\n\u2502 Agent_Id        \u2502 devops_engineer_20250711_185521                                                            \u2502\n\u2502 Tier            \u2502 oracles                                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTask configure_ci_pipeline completed in 9.05s\n\n\ud83d\udd04 Task 3/3: create_test_plan\n\ud83d\udcdd Using base pipeline (no optimization available)\nModel connection successful: ollama/llama3.2:1b\n\ud83d\udccb Loaded 5 BDD specifications for execution\nQaEngineerPipeline (Oracle tier) initialized with 5 BDD scenarios\n\n         Analysis Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect          \u2503 Value                                                                                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Reasoning       \u2502 The following step-by-step reasoning process is used to arrive at the answer:              \u2502\n\u2502                 \u2502 1. First, create a Docker image of the application using `docker build`.                   \u2502\n\u2502                 \u2502 2. Then, push the Docker image to the Docker registry using `docker tag`.                  \u2502\n\u2502                 \u2502 3. Next, deploy the application to Elastic Beanstalk.                                      \u2502\n\u2502                 \u2502 4. After that, configure the database with Amazon RDS.                                     \u2502\n\u2502                 \u2502 5. Write unit tests and integration tests for the application using Jest, React Testing    \u2502\n\u2502                 \u2502 Library, and Supertest.                                                                   \u2502\n\u2502                 \u2502 6. Finally, integrate the WebSocket endpoint with Node.js and Socket.IO.                   \u2502\n\u2502 Test_Plan       \u2502 Here's a high-level test plan including key test cases:                                    \u2502\n\u2502                 \u2502 1. Unit tests: Test individual components of the application using Jest.                   \u2502\n\u2502                 \u2502 2. Integration tests: Test the integration between different components of the application  \u2502\n\u2502                 \u2502 using Jest and Supertest.                                                                 \u2502\n\u2502                 \u2502 3. End-to-end tests: Test the entire application using Jest, React Testing Library, and    \u2502\n\u2502                 \u2502 Supertest.                                                                                 \u2502\n\u2502                 \u2502 4. WebSocket tests: Test the integration of the WebSocket endpoint with Node.js and        \u2502\n\u2502                 \u2502 Socket.IO.                                                                                 \u2502\n\u2502 Trained         \u2502 False                                                                                      \u2502\n\u2502 Usage           \u2502 {'ollama_chat/llama3.2:1b': {'completion_tokens': 214, 'prompt_tokens': 735,               \u2502\n\u2502                 \u2502 'total_tokens': 949, 'completion_tokens_details': None, 'prompt_tokens_details': None}}    \u2502\n\u2502 Agent_Id        \u2502 qa_engineer_20250711_185530                                                                \u2502\n\u2502 Tier            \u2502 oracles                                                                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTask create_test_plan completed in 1.40s\n\ud83c\udf89 Orchestra completed successfully!\n</code></pre> <p>\ud83c\udfaf What Happens During Orchestra Execution</p> <p>Your orchestra will coordinate the three agents in sequence:</p> <p>\ud83e\udde0 How Multi-Agent Coordination Works</p> <p>Sequential Coordination Process:</p> <ol> <li>\ud83d\udccb Goal Decomposition: Orchestra breaks the goal into agent-specific tasks</li> <li>\ud83c\udfaf Agent Assignment: Each agent receives their specific responsibility</li> <li>\ud83d\udd04 Sequential Execution: Agents work in order, building on previous results</li> <li>\ud83d\udcca Result Aggregation: Orchestra combines all agent outputs</li> <li>Final Delivery: Complete solution from the entire team</li> </ol> <p>\ud83d\udca1 Benefits of Multi-Agent Teams: - \ud83c\udfaf Specialized Expertise: Each agent focuses on their domain - \ud83d\udd0d Comprehensive Coverage: All aspects of the problem are addressed - \ud83d\udcc8 Quality Assurance: Multiple perspectives ensure better results - \u26a1 Scalable Workflows: Easy to add more agents for complex projects</p> <p>\ud83d\udcca Orchestra Performance Metrics</p> <p>\ud83c\udfbc Orchestra Execution Summary: - \u23f1\ufe0f Total Execution Time: ~21.61 seconds - \ud83e\udd16 Agents Coordinated: 3 specialized agents - \ud83d\udccb Tasks Completed: 3 sequential tasks - \ud83c\udfaf Success Rate: 100% (all tasks completed successfully)</p> <p>\ud83d\udcc8 Individual Agent Performance: - \ud83e\udd16 Developer Agent: 11.16s (implementation + architecture) - \u2699\ufe0f DevOps Engineer Agent: 9.05s (CI/CD pipeline configuration) - \ud83e\uddea QA Engineer Agent: 1.40s (test plan creation)</p> <p>\ud83d\udcbe Resource Usage: - \ud83d\udd24 Total Tokens: 7,196 tokens across all agents - \ud83e\udde0 Model: llama3.2:1b (local Ollama) - \ud83d\udcc1 Workspace: Created dedicated workspace for coordination</p>"},{"location":"tutorials/first-orchestra/#1-developer-agent-first","title":"1\ufe0f\u20e3 Developer Agent (First)","text":"<ul> <li>\ud83c\udfaf Task: Design and implement the web application</li> <li>\ud83d\udcdd Output: Code, architecture, and implementation details</li> <li>\ud83d\udd04 Handoff: Passes code and specifications to QA Engineer</li> </ul>"},{"location":"tutorials/first-orchestra/#2-qa-engineer-agent-second","title":"2\ufe0f\u20e3 QA Engineer Agent (Second)","text":"<ul> <li>\ud83c\udfaf Task: Test the application and ensure quality</li> <li>\ud83d\udcdd Output: Test cases, validation results, and quality report</li> <li>\ud83d\udd04 Handoff: Passes tested code and deployment requirements to DevOps Engineer</li> </ul>"},{"location":"tutorials/first-orchestra/#3-devops-engineer-agent-third","title":"3\ufe0f\u20e3 DevOps Engineer Agent (Third)","text":"<ul> <li>\ud83c\udfaf Task: Deploy and configure the application</li> <li>\ud83d\udcdd Output: Deployment configuration, infrastructure setup, and monitoring</li> <li>Final Result: Complete, tested, and deployed application</li> </ul>"},{"location":"tutorials/first-orchestra/#key-insights","title":"\ud83c\udfaf Key Insights","text":"<p>\ud83c\udfbc Multi-Agent Coordination Benefits: - \ud83c\udfaf Specialized Expertise: Each agent focused on their domain (development, DevOps, QA) - \ud83d\udd04 Sequential Handoff: Results passed seamlessly between agents - \ud83d\udcca Comprehensive Coverage: All aspects of the project addressed - \u26a1 Efficient Execution: Parallel processing within each agent's domain</p> <p>\ud83c\udfd7\ufe0f Production-Ready Architecture: - \ud83d\udccb BDD Testing: Each agent validated against 5 BDD scenarios - \ud83d\udd27 Modular Design: Clean separation of concerns - \ud83d\udcc8 Scalable Workflow: Easy to add more agents or modify tasks - \ud83d\udcbb Enterprise Features: Ready for deployment and scaling</p>"},{"location":"tutorials/first-orchestra/#congratulations-youve-built-a-multi-agent-orchestra","title":"\ud83c\udf89 Congratulations! You've Built a Multi-Agent Orchestra! \ud83d\ude80","text":""},{"location":"tutorials/first-orchestra/#what-youve-accomplished","title":"\ud83c\udfc6 What You've Accomplished","text":"<p>You've successfully created a sophisticated multi-agent orchestra that demonstrates the power of coordinated AI teams! Here's what makes your orchestra special:</p> <p>\ud83c\udfbc Orchestra Capabilities: - \ud83e\udd16 Multi-Agent Coordination: Three specialized agents working together - \ud83c\udfaf Sequential Workflow: Systematic task execution and handoff - \ud83d\udccb Goal Decomposition: Automatic breakdown of complex goals - \ud83d\udd04 Result Aggregation: Combining outputs from multiple agents - \ud83d\udcca Full Observability: Complete tracing and monitoring - \u26a1 Scalable Architecture: Easy to add more agents</p> <p>\ud83c\udfd7\ufe0f Enterprise-Grade Architecture: - \ud83d\udcca BDD Testing: Each agent has behavior-driven specifications - \ud83d\udd04 Optimization Pipeline: All agents are optimized with DSPy - \ud83d\udcc8 Performance Monitoring: Detailed metrics for each agent - \ud83d\udd27 Modular Design: Easy to customize and extend - \ud83d\udcbb Production Ready: Can be deployed and scaled</p>"},{"location":"tutorials/first-orchestra/#youre-now-a-multi-agent-orchestra-conductor","title":"\ud83c\udf1f You're Now a Multi-Agent Orchestra Conductor!","text":"<p>This isn't just a simple automation-you've built a sophisticated AI team that can: - Coordinate multiple specialists for complex projects - Manage sequential workflows with proper handoffs - Ensure comprehensive coverage of all project aspects - Scale to enterprise needs with additional agents - Maintain quality standards across the entire team</p>"},{"location":"tutorials/first-orchestra/#whats-next","title":"\ud83d\ude80 What's Next?","text":"<p>Your journey into multi-agent orchestration has just begun! Here are some exciting next steps:</p> <p>\ud83c\udfbc Create More Complex Orchestras: <pre><code># Create a marketing team orchestra\nsuper agent pull content_creator\nsuper agent pull social_media_manager\nsuper agent pull analytics_specialist\nsuper orchestra create marketing_team\n</code></pre></p> <p>\ud83d\udd27 Add More Specialized Agents: <pre><code># Pull additional agents for different domains\nsuper agent pull data_scientist\nsuper agent pull ui_ux_designer\nsuper agent pull security_analyst\n</code></pre></p> <p>\ud83d\udcca Explore Advanced Orchestration: <pre><code># Create orchestras for different industries\nsuper orchestra create healthcare_team\nsuper orchestra create finance_team\nsuper orchestra create education_team\n</code></pre></p> <p>\ud83c\udfaf Deploy to Production: Your orchestra is ready for real-world deployment and can handle complex, multi-agent workflows!</p>"},{"location":"tutorials/first-orchestra/#the-future-is-yours","title":"\ud83d\udcab The Future is Yours","text":"<p>You now have the power to create AI orchestras that can: - Coordinate specialized teams \ud83c\udfbc - Handle complex workflows \ud83d\udd04 - Scale to enterprise needs \ud83d\udcc8 - Ensure quality delivery - Adapt to any domain \ud83c\udfaf</p> <p>Welcome to the future of multi-agent orchestration! \ud83c\udf1f</p>"},{"location":"tutorials/first-orchestra/#whats-next_1","title":"\ud83c\udfaf What's Next?","text":"<p>Congratulations on building your first multi-agent orchestra! Here are some exciting next steps to continue your SuperOptiX journey:</p>"},{"location":"tutorials/first-orchestra/#immediate-next-steps","title":"\ud83d\ude80 Immediate Next Steps","text":"<p>\ud83c\udfbc Create More Complex Orchestras: <pre><code># Create a marketing team orchestra\nsuper agent pull content_creator\nsuper agent pull social_media_manager\nsuper agent pull analytics_specialist\nsuper orchestra create marketing_team\n</code></pre></p> <p>\ud83d\udd27 Add More Specialized Agents: <pre><code># Pull additional agents for different domains\nsuper agent pull data_scientist\nsuper agent pull ui_ux_designer\nsuper agent pull security_analyst\n</code></pre></p> <p>\ud83d\udcca Explore Advanced Orchestration: <pre><code># Create orchestras for different industries\nsuper orchestra create healthcare_team\nsuper orchestra create finance_team\nsuper orchestra create education_team\n</code></pre></p>"},{"location":"tutorials/first-orchestra/#recommended-learning-path","title":"\ud83d\udcda Recommended Learning Path","text":"<ol> <li>\ud83d\udc8e SuperSpec Guide: Master declarative agent specifications</li> <li>\ud83e\uddea BDD Guide: Learn behavior-driven development for agents</li> <li>\u2699\ufe0f Optimization Guide: Understand DSPy-powered optimization</li> <li>\ud83c\udfad Multi-Agent Guide: Build advanced orchestration patterns</li> <li>\ud83c\udfed Production Guide: Deploy and monitor in production</li> </ol>"},{"location":"tutorials/first-orchestra/#advanced-topics","title":"\ud83c\udfaf Advanced Topics","text":"<ul> <li>\ud83d\udd0d Agent Discovery: Find the perfect agents for your use case</li> <li>\ud83d\udee0\ufe0f Tool Development: Create custom tools for your agents</li> <li>\ud83e\udde0 Memory Systems: Add persistent memory to your orchestras</li> <li>\ud83d\udd0d RAG Integration: Add knowledge retrieval capabilities</li> <li>\ud83d\udcca Observability: Monitor and debug your orchestras</li> </ul>"},{"location":"tutorials/first-orchestra/#explore-the-marketplace","title":"\ud83c\udfea Explore the Marketplace","text":"<p>Discover more pre-built agents and tools: <pre><code># Browse available agents\nsuper market browse agents\n\n# Search for specific agents\nsuper market search \"data analysis\"\n\n# Browse available tools\nsuper market browse tools\n</code></pre></p> <p>Ready to build the next generation of AI orchestras? The future of multi-agent systems is yours to create! \ud83d\ude80 </p>"},{"location":"tutorials/genies-agent/","title":"\ud83c\udfaf Create Your First Genies Agent: Developer","text":""},{"location":"tutorials/genies-agent/#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":"<p>You'll create a Genie Tier Developer agent with:</p> <ul> <li>\ud83d\udee0\ufe0f Tool calling (web search, calculator, file operations)</li> <li>\ud83d\udcda RAG (Retrieval-Augmented Generation) system ready</li> <li>\u26a1 Real DSPy-powered pipeline</li> <li>\ud83d\udc40 Full tracing and observability</li> </ul> <p>It could be a real, production-grade agent-no toy examples! If you perform optimization and evaluation, you can make it production-worthy (unlike prompt-and-pray frameworks).</p>"},{"location":"tutorials/genies-agent/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Python 3.8+ installed</li> <li>SuperOptiX installed (see Installation Guide)</li> </ul>"},{"location":"tutorials/genies-agent/#caution-optimization-evaluation-resource-warning","title":"\ud83d\udea8 Caution: Optimization &amp; Evaluation Resource Warning","text":"<p>Optimization and Evaluation are Resource Intensive</p> <ul> <li>Do NOT run optimization/evaluation on a low-end machine or CPU-only system.</li> <li>These steps require a high-end machine with a modern GPU for local LLMs (e.g., RTX 30xx/40xx, Apple Silicon, or better).</li> <li>Your GPU may run at full load and your laptop can get extremely warm during optimization.</li> <li>If using cloud LLMs, monitor your API usage and costs carefully. Optimization can make hundreds of LLM calls.</li> <li>Only proceed with optimization/evaluation if you understand the resource and cost implications!</li> </ul>"},{"location":"tutorials/genies-agent/#1-initialize-your-project","title":"1\ufe0f\u20e3 Initialize Your Project","text":"<pre><code>super init swe\n</code></pre> Actual Output <pre><code>================================================================================\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 SUCCESS! Your full-blown shippable Agentic System 'swe' is ready!\n                               \u2502\n\u2502\n                               \u2502\n\u2502 \ud83d\ude80 You now own a complete agentic AI system in 'swe'.\n                               \u2502\n\u2502\n                               \u2502\n\u2502 Start making it production-ready by evaluating, optimizing, and orchestrating\nwith advanced agent            \u2502\n\u2502 engineering.\n                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Your Journey Starts Here \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502\n                               \u2502\n\u2502  \ud83d\ude80 GETTING STARTED\n                               \u2502\n\u2502\n                               \u2502\n\u2502  1. Move to your new project root and confirm setup:\n                               \u2502\n\u2502     cd swe\n                               \u2502\n\u2502     # You should see a .super file here - always run super commands from this\ndirectory                      \u2502\n\u2502\n                               \u2502\n\u2502  2. Pull your first agent:\n                               \u2502\n\u2502     super agent pull developer  # swap 'developer' for any agent name\n                               \u2502\n\u2502\n                               \u2502\n\u2502  3. Explore the marketplace:\n                               \u2502\n\u2502     super market\n                               \u2502\n\u2502\n                               \u2502\n\u2502  4. Need the full guide?\n                               \u2502\n\u2502     super docs\n                               \u2502\n\u2502     https://superoptix.dev/docs\n                               \u2502\n\u2502\n                               \u2502\n\u2502  Tip: Use 'super market search &lt;keyword&gt;' to discover components tailored to y\nour domain.                    \u2502\n\u2502\n                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udfaf Welcome to your Agentic System! Ready to build intelligent agents? \ud83d\ude80\n\ud83d\udccd Next steps: cd swe\n================================================================================\n</code></pre>"},{"location":"tutorials/genies-agent/#2-generate-a-genies-tier-developer-agent-with-rag-tools","title":"2\ufe0f\u20e3 Generate a Genies-Tier Developer Agent with RAG &amp; Tools","text":"<pre><code>cd swe\nsuper spec generate genies developer --rag\n</code></pre> Actual Output <pre><code>\ud83d\udcc1 Using SuperOptiX project structure: swe/agents/developer/playbook/developer_playbook.yaml\nGenerated genies agent playbook: \n/Users/super/superagentic/SuperOptiX/swe/swe/agents/developer/playbook/developer_playbook.yaml\n\ud83d\udccb Agent: Developer (Tier: genies)\n\ud83c\udff7\ufe0f  Namespace: software\n\u26a1 Features: memory, tools, agentflow\n</code></pre>"},{"location":"tutorials/genies-agent/#3-see-rag-tool-configuration-in-the-playbook","title":"3\ufe0f\u20e3 See RAG &amp; Tool Configuration in the Playbook","text":"<p>Open <code>swe/swe/agents/developer/playbook/developer_playbook.yaml</code>:</p> <pre><code>rag:\n  chunk_size: 512\n  collection_name: developer_knowledge\n  embedding_model: sentence-transformers/all-MiniLM-L6-v2\n  overlap: 50\n  vector_database: chroma\n\ntool_calling:\n  available_tools:\n    - web_search\n    - calculator\n    - file_operations\n  enabled: true\n  max_iterations: 5\n  tool_selection_strategy: auto\n</code></pre> <p>RAG: Retrieval-augmented generation (RAG) is available and ready to use with ChromaDB and a sentence-transformer embedding model. No ingestion is required at this step-RAG will be used automatically if needed.</p> <p>\ud83d\udee0\ufe0f Tools: Web search, calculator, and file operations are enabled, with auto tool selection.</p> <p>You can modify these settings in the playbook if you want to add/remove tools or change RAG parameters.</p>"},{"location":"tutorials/genies-agent/#4-compile-the-agent","title":"4\ufe0f\u20e3 Compile the Agent","text":"<pre><code>super agent compile developer\n</code></pre> Actual Output <pre><code>================================================================================\n\n\ud83d\udd28 Compiling agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Developer                                                                                         \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                      \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/developer/pipelines/developer_pipeline.py                                             \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udc0d Converted field names to snake_case for DSPy compatibility\nTool calling configuration detected for Genies tier\nMemory configuration detected for Genies tier\n\n\ud83e\udd16 Generating Mixin Genies-Tier pipeline (DSPy default template)...\n\ud83e\udde9 Mixin Pipeline (DSPy Default): Reusable components for complex agents.\n\ud83d\udd27 Developer Controls: Modular mixins keep your codebase clean and customizable\n\ud83d\ude80 Framework: DSPy (additional frameworks &amp; custom builders coming soon) \n\ud83d\udd27 Genies-Tier Features: ReAct Agents + Tool Integration + RAG Support + Memory\nSuccessfully generated Genies-tier pipeline (mixin) at: \n/Users/super/superagentic/SuperOptiX/swe/swe/agents/developer/pipelines/developer_pipeline.py\n\n\ud83d\udca1 Mixin pipeline features (DSPy Default):\n   \u2022 Promotes code reuse and modularity\n   \u2022 Separates pipeline logic into reusable mixins\n   \u2022 Ideal for building complex agents with shared components\n   \u2022 Built on DSPy - support for additional frameworks is on our roadmap\n\n\ud83d\udca1 Genies tier includes all Oracles features\n\n\ud83c\udfaf Genies Tier Features\n  All Oracles features plus:\n  ReAct agents with tool integration\n  RAG (Retrieval-Augmented Generation)\n  Agent memory (short-term and episodic)\n  Basic streaming responses\n  JSON/XML adapters\n\n\ud83d\udca1 Genies tier includes all Oracles features\n\n\u2139\ufe0f  Advanced features available in commercial version\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udee0\ufe0f Customization Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \u26a0\ufe0f Auto-Generated Pipeline\n\u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udea8 Starting foundation - Customize for production use                                                       \u2502\n\u2502  \ud83d\udca1 You own this code - Modify for your specific requirements                                                \u2502\n\u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83e\uddea Testing Enhancement \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\uddea Current BDD Scenarios: 5 found                                                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Recommendations:                                                                                         \u2502\n\u2502  \u2022 Add comprehensive test scenarios to your playbook                                                         \u2502\n\u2502  \u2022 Include edge cases and error handling scenarios                                                           \u2502\n\u2502  \u2022 Test with real-world data samples                                                                         \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Why scenarios matter: Training data for optimization &amp; quality gates                                     \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Workflow Guide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 NEXT STEPS                                                                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  super agent evaluate developer - Establish baseline performance                                             \u2502\n\u2502  super agent optimize developer - Enhance performance using DSPy                                             \u2502\n\u2502  super agent evaluate developer - Measure improvement                                                        \u2502\n\u2502  super agent run developer --goal \"goal\" - Execute optimized agent                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Follow BDD/TDD workflow: evaluate \u2192 optimize \u2192 evaluate \u2192 run                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'Developer' pipeline ready! Time to make it yours! \ud83d\ude80\n</code></pre>"},{"location":"tutorials/genies-agent/#5-evaluate-your-agent","title":"5\ufe0f\u20e3 Evaluate Your Agent","text":"<p>Now let's evaluate your agent to establish a baseline performance:</p> <pre><code>super agent evaluate developer\n</code></pre> Actual Output <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                         \ud83e\uddea SuperOptiX BDD Spec Runner - Professional Agent Validation\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Spec Execution Session \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udfaf Agent:               developer                                                                            \u2502\n\u2502 \ud83d\udcc5 Session:             2025-07-11 16:59:06                                                                  \u2502\n\u2502 \ud83d\udd27 Mode:                Standard validation                                                                  \u2502\n\u2502 \ud83d\udcca Verbosity:           Summary                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Tracing enabled for agent developer_20250711_165907\n\ud83d\udcc1 Traces will be stored in: /Users/super/superagentic/SuperOptiX/swe/.superoptix/traces\n\ud83d\ude80 Configuring llama3.1:8b with ollama for genies-tier capabilities\n\ud83d\udcdd Using ChatAdapter for optimal local model compatibility\nModel connection successful: ollama/llama3.1:8b\n4 tools configured successfully\n\ud83d\udd0d RAG system initialized for DeveloperPipeline\nReAct agent configured with 4 tools\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Genie tier) initialized with ReAct and 5 BDD scenarios\nPipeline loaded\n\u2139\ufe0f  Using base model (no optimization found)\n\n\ud83d\udd0d Discovering BDD Specifications...\n\ud83d\udccb Found 5 BDD specifications\n\n\ud83e\uddea Executing BDD Specification Suite\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nProgress: \ud83e\uddea Running 5 BDD specifications...\n\nTest Results:\nFFFFF\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Specification                \u2503    Status    \u2503  Score   \u2503 Description                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer_comprehensiv...    \u2502   FAIL    \u2502   0.30   \u2502 Given a complex software requirement, t...    \u2502\n\u2502 developer_problem_solving    \u2502   FAIL    \u2502   0.28   \u2502 When facing software challenges, the ag...    \u2502\n\u2502 developer_best_practices     \u2502   FAIL    \u2502   0.25   \u2502 When asked about software best practice...    \u2502\n\u2502 developer_tool_integra...    \u2502   FAIL    \u2502   0.28   \u2502 When using tools, the agent should demo...    \u2502\n\u2502 developer_memory_utili...    \u2502   FAIL    \u2502   0.23   \u2502 When leveraging memory, the agent shoul...    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udd34 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcca Total Specs:         5                \ud83c\udfaf Pass Rate:         0.0%                                         \u2502\n\u2502  Passed:              0                \ud83e\udd16 Model:             ollama_chat/llama3.1:8b                      \u2502\n\u2502  Failed:              5                \ud83d\udcaa Capability:        0.27                                         \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        NEEDS WORK    \ud83d\ude80 Status:            \u2699\ufe0f  Base Model                                \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Failure Analysis - Grouped by Issue Type\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udccb Semantic Relevance Issues (5 failures)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83d\udca1 Fix Suggestions:\n   \ud83c\udfaf Make the response more relevant to the expected output\n   \ud83d\udcdd Use similar terminology and technical concepts\n   \ud83d\udd0d Ensure the output addresses all aspects of the input requirement\n   \ud83d\udca1 Review the expected output format and structure\n\nAffected Specifications:\n   \u2022 developer_comprehensive_task (score: 0.299)\n   \u2022 developer_problem_solving (score: 0.281)\n   \u2022 developer_best_practices (score: 0.249)\n   \u2022 developer_tool_integration (score: 0.279)\n   \u2022 developer_memory_utilization (score: 0.228)\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf AI Recommendations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Poor performance. 5 scenarios failing.                                                                   \u2502\n\u2502  \ud83d\udca1 Strong recommendation: Run optimization before production use.                                           \u2502\n\u2502  \ud83d\udca1 Consider using a more capable model (llama3.1:8b or gpt-4).                                              \u2502\n\u2502  \ud83d\udca1 Review scenario complexity vs model capabilities.                                                        \u2502\n\u2502  \ud83d\udca1 Fix semantic relevance in 5 scenario(s) - improve response clarity.                                      \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Next Steps \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udd27 5 specification(s) need attention.                                                                       \u2502\n\u2502                                                                                                              \u2502\n\u2502  Recommended actions for better quality:                                                                     \u2502\n\u2502  \u2022 Review the grouped failure analysis above                                                                 \u2502\n\u2502  \u2022 super agent optimize developer - Optimize agent performance                                               \u2502\n\u2502  \u2022 super agent evaluate developer - Re-evaluate to measure improvement                                       \u2502\n\u2502  \u2022 Use --verbose flag for detailed failure analysis                                                          \u2502\n\u2502                                                                                                              \u2502\n\u2502  You can still test your agent:                                                                              \u2502\n\u2502  \u2022 super agent run developer --goal \"your goal\" - Works even with failing specs                              \u2502\n\u2502  \u2022 super agent run developer --goal \"Create a simple function\" - Try basic goals                             \u2502\n\u2502  \u2022 \ud83d\udca1 Agents can often perform well despite specification failures                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  For production use:                                                                                         \u2502\n\u2502  \u2022 Aim for \u226580% pass rate before deploying to production                                                     \u2502\n\u2502  \u2022 Run optimization and re-evaluation cycles until quality gates pass                                        \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                       \ud83c\udfc1 Specification execution completed - 0.0% pass rate (0/5 specs)\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf What would you like to do next? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udd27 To improve your agent's performance:                                                                     \u2502\n\u2502     super agent optimize developer - Optimize the pipeline for better results                                \u2502\n\u2502\n\u2502  \ud83d\ude80 To run your agent:                                                                                       \u2502\n\u2502     super agent run developer --goal \"your specific goal here\"                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Example goals:                                                                                           \u2502\n\u2502     \u2022 super agent run developer --goal \"Create a Python function to calculate fibonacci numbers\"             \u2502\n\u2502     \u2022 super agent run developer --goal \"Write a React component for a todo list\"                             \u2502\n\u2502     \u2022 super agent run developer --goal \"Design a database schema for an e-commerce site\"                     \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>\ud83d\udcca Evaluation Results Analysis</p> <p>The evaluation shows that your agent needs optimization:</p> <ul> <li>\ud83c\udfaf Pass Rate: 0.0% (0/5 specifications passed)</li> <li>\ud83e\udd16 Model: Using <code>ollama/llama3.1:8b</code> (base model, no optimization)</li> <li>\ud83d\udcaa Capability Score: 0.27 (needs improvement)</li> <li>\ud83c\udfc6 Quality Gate: NEEDS WORK</li> </ul> <p>\ud83d\udd0d What Happened During Evaluation</p> <p>The evaluation system ran 5 BDD (Behavior-Driven Development) scenarios that were automatically generated from your agent's playbook. Here's what each scenario tested:</p> <p>\ud83c\udfaf How the Evaluation Works</p> <p>The system uses a multi-criteria evaluation framework with 4 weighted criteria:</p> Criterion Weight What It Measures Semantic Similarity 50% How closely the output matches expected meaning Keyword Presence 20% Important terms and concepts inclusion Structure Match 20% Format, length, and organization similarity Output Length 10% Basic sanity check for completeness <p>Scoring Formula: <pre><code>Confidence Score = (\n    semantic_similarity \u00d7 0.5 +\n    keyword_presence \u00d7 0.2 +\n    structure_match \u00d7 0.2 +\n    output_length \u00d7 0.1\n)\n</code></pre></p> <p>Quality Thresholds: - \ud83c\udf89 \u2265 80%: EXCELLENT - Production ready - \u26a0\ufe0f 60-79%: GOOD - Minor improvements needed - &lt; 60%: NEEDS WORK - Significant improvements required</p> <p>\ud83d\udd0d Why All Scenarios Failed</p> <p>The evaluation revealed semantic relevance issues across all scenarios. This means:</p> <ol> <li>The base model's responses didn't closely match the expected outputs</li> <li>Semantic similarity scores were low (0.23-0.30 range)</li> <li>The model was generating responses, but they weren't aligned with the specific expectations</li> <li>This is normal for an unoptimized base model</li> </ol>"},{"location":"tutorials/genies-agent/#the-5-bdd-scenarios-tested","title":"\ud83e\uddea The 5 BDD Scenarios Tested:","text":"<ol> <li> <p><code>developer_comprehensive_task</code> (Score: 0.30)</p> </li> <li> <p>Input: \"Complex software scenario requiring comprehensive analysis\"</p> </li> <li>Expected: \"Detailed step-by-step analysis with software-specific recommendations\"</li> <li> <p>What it tests: Agent's ability to provide thorough software analysis</p> </li> <li> <p><code>developer_problem_solving</code> (Score: 0.28)</p> </li> <li> <p>Input: \"Challenging software problem requiring creative solutions\"</p> </li> <li>Expected: \"Structured problem-solving approach with multiple solution options\"</li> <li> <p>What it tests: Systematic problem-solving methodology</p> </li> <li> <p><code>developer_best_practices</code> (Score: 0.25)</p> </li> <li> <p>Input: \"Industry best practices for software operations\"</p> </li> <li>Expected: \"Comprehensive best practices guide with implementation steps\"</li> <li> <p>What it tests: Knowledge of software development best practices</p> </li> <li> <p><code>developer_tool_integration</code> (Score: 0.28)</p> </li> <li> <p>Input: \"Complex software task requiring multiple tool interactions\"</p> </li> <li>Expected: \"Tool-assisted solution with clear reasoning for tool selection\"</li> <li> <p>What it tests: Effective use of available tools (web search, calculator, file operations)</p> </li> <li> <p><code>developer_memory_utilization</code> (Score: 0.23)</p> </li> <li> <p>Input: \"Follow-up software question building on previous conversation\"</p> </li> <li>Expected: \"Response that incorporates relevant context from memory\"</li> <li>What it tests: Memory system integration and context awareness</li> </ol>"},{"location":"tutorials/genies-agent/#what-this-means","title":"\ud83d\udca1 What This Means","text":"<p>This is completely normal for a base model! The evaluation shows that:</p> <ul> <li>Your agent infrastructure is working correctly</li> <li>Tools, RAG, and memory are properly configured</li> <li>The model is generating responses (not failing completely)</li> <li>The evaluation system is working and providing detailed feedback</li> <li>\ud83d\udd27 The base model needs optimization to meet the quality standards</li> <li>\ud83d\udcca The system provides clear recommendations for improvement</li> </ul> <p>The low scores indicate that optimization will significantly improve performance, which is exactly what the next step (optimization) is designed to address.</p>"},{"location":"tutorials/genies-agent/#6-optimize-your-agent","title":"6\ufe0f\u20e3 Optimize Your Agent","text":"<p>Now let's optimize your agent using DSPy's BootstrapFewShot optimizer to improve its performance:</p> <pre><code>super agent optimize developer\n</code></pre> Actual Output <pre><code>================================================================================\n\n\ud83d\ude80 Optimizing agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Optimization Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 OPTIMIZATION IN PROGRESS                                                                                 \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Developer                                                                                         \u2502\n\u2502  \ud83d\udd27 Strategy: DSPy BootstrapFewShot                                                                          \u2502\n\u2502  \ud83d\udcca Data Source: BDD scenarios from playbook                                                                 \u2502\n\u2502  \ud83d\udcbe Output: swe/agents/developer/pipelines/developer_optimized.json                                          \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Checking for existing optimized pipeline...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\ude80 Optimization Notice \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83d\udd27 DSPy Optimization in progress                                                                             \u2502\n\u2502                                                                                                              \u2502\n\u2502 \u2022 This step fine-tunes prompts and may take several minutes.                                                 \u2502\n\u2502 \u2022 API calls can incur compute cost - monitor your provider dashboard.                                        \u2502\n\u2502 \u2022 You can abort anytime with CTRL+C; your base pipeline remains intact.                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\ude80 Starting optimization using 'bootstrap' strategy...\n\ud83d\udd0d Tracing enabled for agent developer_20250711_170521\n\ud83d\udcc1 Traces will be stored in: /Users/super/superagentic/SuperOptiX/swe/.superoptix/traces\n\ud83d\ude80 Configuring llama3.1:8b with ollama for genies-tier capabilities\n\ud83d\udcdd Using ChatAdapter for optimal local model compatibility\nModel connection successful: ollama/llama3.1:8b\n4 tools configured successfully\n\ud83d\udd0d RAG system initialized for DeveloperPipeline\nReAct agent configured with 4 tools\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Genie tier) initialized with ReAct and 5 BDD scenarios\nFound 5 scenarios for optimization\n\ud83d\ude80 Training ReAct agent with 5 examples...\n  0%|                                                                                     | 0/5 [00:00&lt;?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:09&lt;00:00,  1.91s/it]\nBootstrapped 5 full traces after 4 examples for up to 1 rounds, amounting to 5 attempts.\n\ud83d\udcbe Optimized ReAct model saved to /Users/super/superagentic/SuperOptiX/swe/swe/agents/developer/pipelines/developer_optimized.json\nReAct training completed successfully\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 OPTIMIZATION SUCCESSFUL! Agent Enhanced                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udcca Optimization Results \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcc8 Performance Improvement:                                                                                 \u2502\n\u2502  \u2022 Training Examples: 0                                                                                      \u2502\n\u2502  \u2022 Optimization Score: None                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 What changed: DSPy optimized prompts and reasoning chains                                                \u2502\n\u2502  \ud83d\ude80 Ready for testing: Enhanced agent performance validated                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83e\udd16 AI Enhancement \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udde0 Smart Optimization: DSPy BootstrapFewShot                                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \u26a1 Automatic improvements: Better prompts, reasoning chains                                                 \u2502\n\u2502  \ud83c\udfaf Quality assurance: Test before production use                                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Workflow Guide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 NEXT STEPS                                                                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  super agent evaluate developer - Measure optimization improvement                                           \u2502\n\u2502  super agent run developer --goal \"goal\" - Execute enhanced agent                                            \u2502\n\u2502  super orchestra create - Ready for multi-agent orchestration                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Follow BDD/TDD workflow: evaluate \u2192 optimize \u2192 evaluate \u2192 run                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'developer' optimization complete! Ready for testing! \ud83d\ude80\n</code></pre> <p>\ud83d\udd0d What Happened During Optimization</p> <p>The optimization process used DSPy's BootstrapFewShot optimizer to automatically improve your agent's performance. Here's what happened:</p> <p>\ud83c\udfaf What DSPy BootstrapFewShot Does</p> <p>BootstrapFewShot is a basic but effective optimizer that:</p> <ol> <li>\ud83c\udfaf Learns from Examples: Uses your BDD scenarios as training data</li> <li>\ud83d\udd04 Trial and Error: Tests different prompt variations automatically</li> <li>\ud83e\udde0 Automatic Tuning: Adjusts prompts and reasoning chains based on results</li> <li>\ud83d\udca1 Few-Shot Learning: Creates optimal few-shot examples for better performance</li> </ol>"},{"location":"tutorials/genies-agent/#dspy-optimization-process","title":"\ud83e\udde0 DSPy Optimization Process","text":"<ol> <li>\ud83d\udcda Training Data Conversion: Your 5 BDD scenarios were converted into DSPy training examples</li> <li>\ud83d\udd04 BootstrapFewShot Algorithm: DSPy automatically generated optimized prompts and reasoning chains</li> <li>\u26a1 ReAct Agent Training: Since you're using Genies tier, it optimized the ReAct (Reasoning + Acting) agent</li> <li>\ud83d\udcbe Optimized Weights Saved: Results saved to <code>developer_optimized.json</code></li> </ol>"},{"location":"tutorials/genies-agent/#generated-optimization-file","title":"\ud83d\udcca Generated Optimization File","text":"<p>The optimization created a comprehensive JSON file with:</p> <ul> <li>5 Demo Examples: Each BDD scenario converted to a training example with:</li> <li>Input: The original scenario input</li> <li>Trajectory: Step-by-step reasoning and tool usage</li> <li>Expected Output: The target response</li> <li> <p>Augmented: Enhanced with DSPy's optimization</p> </li> <li> <p>Optimized Signatures: Improved prompts and instructions for:</p> </li> <li>ReAct Agent: Better reasoning and tool selection</li> <li>Extract Module: Enhanced output generation</li> </ul>"},{"location":"tutorials/genies-agent/#why-we-use-basic-optimizer","title":"\ud83d\udd27 Why We Use Basic Optimizer","text":"<p>SuperOptiX current version uses BootstrapFewShot (the basic optimizer) because:</p> <ul> <li>Simple and Effective: Works well for most use cases</li> <li>Fast Optimization: Quick training with minimal resources</li> <li>No Complex Dependencies: Doesn't require advanced optimization libraries</li> <li>Proven Results: Reliable improvement in agent performance</li> </ul> <p>Advanced optimizers (like Bayesian optimization, multi-stage optimization) are available in the commercial version.</p>"},{"location":"tutorials/genies-agent/#expected-improvements","title":"\ud83d\udcc8 Expected Improvements","text":"<p>After optimization, your agent should show:</p> <ul> <li>\ud83c\udfaf Better Semantic Relevance: Responses more closely match expected outputs</li> <li>\ud83d\udee0\ufe0f Improved Tool Usage: More effective tool selection and reasoning</li> <li>\ud83d\udcdd Enhanced Reasoning: Better step-by-step problem-solving</li> <li>\ud83c\udfad Memory Integration: Better use of conversation context</li> </ul>"},{"location":"tutorials/genies-agent/#7-re-evaluate-your-optimized-agent","title":"7\ufe0f\u20e3 Re-evaluate Your Optimized Agent","text":"<p>Now that your agent has been optimized with DSPy's BootstrapFewShot, let's measure the improvement by running evaluation again:</p> <pre><code>super agent evaluate developer\n</code></pre> <p>This will show you how much the optimization improved your agent's performance compared to the baseline evaluation.</p>"},{"location":"tutorials/genies-agent/#8-run-your-agent","title":"8\ufe0f\u20e3 Run Your Agent","text":"<p>Now let's run your optimized agent with a complex goal that will demonstrate tool usage and RAG capabilities:</p> <pre><code>super agent run developer --goal \"Research the latest Python frameworks for web development in 2024, calculate the performance benchmarks between FastAPI and Django, and create a comparison report with recommendations for a new project\"\n</code></pre> Actual Output <pre><code>\ud83d\ude80 Running agent 'developer'...\n\nLoading pipeline... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501   0% -:--:--\n\ud83d\ude80 Using pre-optimized pipeline from developer_optimized.json\n\nLooking for pipeline at: \n/Users/super/superagentic/SuperOptiX/swe/swe/agents/developer/pipelines/developer_pipeline.py\nModel connection successful: ollama/llama3.1:8b\n4 tools configured successfully\n\ud83d\udd0d RAG system initialized for DeveloperPipeline\nReAct agent configured with 4 tools\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Genie tier) initialized with ReAct and 5 BDD scenarios\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Agent Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83e\udd16 Running Developer Pipeline                                                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502 Executing Task: Research the latest Python frameworks for web development in 2024, calculate the performance \u2502\n\u2502 benchmarks between FastAPI and Django, and create a comparison report with recommendations for a new project \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n         Analysis Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                                                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Implementation \u2502 Here is an example code snippet in Python that demonstrates how to use the text analyzer    \u2502\n\u2502                \u2502 and calculator tools:                                                                       \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 ```python                                                                                   \u2502\n\u2502                \u2502 import requests                                                                             \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 # Text Analyzer Tool                                                                        \u2502\n\u2502                \u2502 def analyze_text(text):                                                                     \u2502\n\u2502                \u2502     url = \"https://www.python.org/dev/peps/pep-0645/\"                                       \u2502\n\u2502                \u2502     response = requests.get(url)                                                            \u2502\n\u2502                \u2502     if response.status_code == 200:                                                         \u2502\n\u2502                \u2502         text_analysis_report = {                                                            \u2502\n\u2502                \u2502             \"characters\": len(response.text),                                               \u2502\n\u2502                \u2502             \"words\": len(response.text.split()),                                            \u2502\n\u2502                \u2502             \"sentences\": len(response.text.split(\".\"))                                      \u2502\n\u2502                \u2502         }                                                                                   \u2502\n\u2502                \u2502         return text_analysis_report                                                         \u2502\n\u2502                \u2502     else:                                                                                   \u2502\n\u2502                \u2502         return None                                                                         \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 # Calculator Tool                                                                           \u2502\n\u2502                \u2502 def calculate_performance(expression):                                                      \u2502\n\u2502                \u2502     try:                                                                                    \u2502\n\u2502                \u2502         result = eval(expression)                                                           \u2502\n\u2502                \u2502         return result                                                                       \u2502\n\u2502                \u2502     except Exception as e:                                                                  \u2502\n\u2502                \u2502         print(f\"Error: {str(e)}\")                                                           \u2502\n\u2502                \u2502         return None                                                                         \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 # File Reader Tool                                                                          \u2502\n\u2502                \u2502 def read_file(file_path):                                                                   \u2502\n\u2502                \u2502     try:                                                                                    \u2502\n\u2502                \u2502         with open(file_path, \"r\") as file:                                                  \u2502\n\u2502                \u2502             content = file.read()                                                           \u2502\n\u2502                \u2502             return content                                                                  \u2502\n\u2502                \u2502     except FileNotFoundError:                                                               \u2502\n\u2502                \u2502         print(\"File not found.\")                                                            \u2502\n\u2502                \u2502         return None                                                                         \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 # Example usage:                                                                            \u2502\n\u2502                \u2502 text_analysis_report = analyze_text(\"\")                                                     \u2502\n\u2502                \u2502 print(text_analysis_report)                                                                 \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 expression = \"FastAPI performance * 1000 - Django performance\"                              \u2502\n\u2502                \u2502 result = calculate_performance(expression)                                                  \u2502\n\u2502                \u2502 print(result)                                                                               \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 file_path = \"/path/to/performance_benchmarks_article.txt\"                                   \u2502\n\u2502                \u2502 content = read_file(file_path)                                                              \u2502\n\u2502                \u2502 print(content)                                                                              \u2502\n\u2502                \u2502 ```                                                                                         \u2502\n\u2502 Reasoning      \u2502 To research the latest Python frameworks for web development in 2024, I will analyze        \u2502\n\u2502                \u2502 various sources such as documentation, blogs, and articles. This involves using a text      \u2502\n\u2502                \u2502 analyzer tool to extract relevant information from these sources.                           \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 For calculating performance benchmarks between FastAPI and Django, I initially attempted to \u2502\n\u2502                \u2502 use a calculator tool with an invalid mathematical expression. After rephrasing the         \u2502\n\u2502                \u2502 expression to a valid one, I encountered another calculation error due to syntax issues. To \u2502\n\u2502                \u2502 resolve this, I will need to find reliable sources for the performance benchmarks of both   \u2502\n\u2502                \u2502 frameworks.                                                                                 \u2502\n\u2502                \u2502                                                                                             \u2502\n\u2502                \u2502 To create a comparison report with recommendations for a new project, I will analyze the    \u2502\n\u2502                \u2502 results from my research and calculations. This involves using a file reader tool to        \u2502\n\u2502                \u2502 extract relevant information from articles and blogs that provide performance benchmarks.   \u2502\n\u2502 Success        \u2502 True                                                                                        \u2502\n\u2502 Execution_Time \u2502 20.919279                                                                                   \u2502\n\u2502 Agent_Id       \u2502 developer_20250711_171238                                                                   \u2502\n\u2502 Tier           \u2502 genies                                                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83c\udf89 Agent execution completed successfully!\n</code></pre> <p>\ud83d\udd0d What Happened During Agent Execution</p> <p>The agent successfully executed your complex goal and demonstrated several key capabilities:</p> <p>\ud83e\udde0 ReAct Agent Reasoning</p> <p>The agent demonstrated ReAct (Reasoning + Acting) behavior:</p> <ol> <li>\ud83d\udd0d Analysis Phase: Broke down the complex goal into components</li> <li>\ud83d\udee0\ufe0f Tool Selection: Chose appropriate tools for each task</li> <li>\ud83d\udd04 Iterative Improvement: Learned from failed calculator attempts</li> <li>\ud83d\udcdd Code Generation: Created comprehensive Python implementation</li> <li>\ud83d\udca1 Recommendations: Provided structured analysis and suggestions</li> </ol> <p>\ud83e\udde0 How RAG Works in Your Genies Agent</p> <p>RAG (Retrieval-Augmented Generation) is a powerful technology that enhances your agent's capabilities by providing access to external knowledge. Here's how it works:</p> <p>\ud83d\udd04 RAG Process Flow:</p> <ol> <li>\ud83d\udcda Document Ingestion: Documents are added to the vector database</li> <li>\ud83d\udd0d Query Processing: When you ask a question, the system searches for relevant documents</li> <li>\ud83d\udcd6 Context Retrieval: The most relevant documents are retrieved based on semantic similarity</li> <li>\ud83e\udd16 Enhanced Generation: The agent uses retrieved context to generate more accurate responses</li> </ol> <p>\ud83d\udca1 Why RAG is Powerful:</p> <ul> <li>\ud83c\udfaf Accuracy: Reduces hallucination by providing factual context</li> <li>\ud83d\udcc8 Knowledge: Access to up-to-date information beyond training data</li> <li>\ud83d\udd0d Specificity: Can answer questions about specific documents or domains</li> <li>\ud83d\udd04 Adaptability: Easy to update knowledge without retraining</li> </ul> <p>\ud83d\udcc1 Where RAG and Traces Are Stored</p> <p>All agent data is stored in the <code>.superoptix</code> directory within your project:</p> <pre><code>swe/.superoptix/\n\u251c\u2500\u2500 traces/                    # \ud83d\udcca Agent execution traces\n\u2502   \u251c\u2500\u2500 developer.jsonl       # \ud83d\udcdd General agent traces\n\u2502   \u251c\u2500\u2500 developer_20250711_165907.jsonl  # \ud83d\udd50 Evaluation traces\n\u2502   \u251c\u2500\u2500 developer_20250711_170521.jsonl  # \ud83d\udd27 Optimization traces  \n\u2502   \u2514\u2500\u2500 developer_20250711_171238.jsonl  # \ud83d\ude80 Execution traces\n\u2514\u2500\u2500 chromadb/                 # \ud83d\uddc4\ufe0f RAG knowledge base\n    \u2514\u2500\u2500 chroma.sqlite3        # \ud83d\udcbe Vector database (160KB)\n</code></pre> <p>\ud83d\udcca Traces Directory (<code>swe/.superoptix/traces/</code>): - Purpose: Stores detailed execution logs for debugging and analysis - Format: JSONL (JSON Lines) - one JSON object per line - Content: Tool calls, reasoning steps, timestamps, performance metrics - Files: Separate trace files for each operation (evaluate, optimize, run)</p> <p>\ud83d\uddc4\ufe0f ChromaDB Directory (<code>swe/.superoptix/chromadb/</code>): - Purpose: Vector database for RAG (Retrieval-Augmented Generation) - Storage: SQLite database (160KB) containing embedded knowledge - Function: Enables semantic search and context retrieval - Usage: Automatically used by the agent for enhanced responses</p> <p>\ud83d\udd0d Exploring Your Agent's Data</p> <p>You can explore these files to understand your agent's behavior:</p> <p>\ud83d\udcca View Latest Execution Traces: <pre><code># View the most recent execution trace\ncat swe/.superoptix/traces/developer_20250711_171238.jsonl\n\n# View all trace files\nls -la swe/.superoptix/traces/\n</code></pre></p> <p>\ud83d\uddc4\ufe0f Check RAG Database Size: <pre><code># Check the size of your RAG knowledge base\nls -lh swe/.superoptix/chromadb/chroma.sqlite3\n</code></pre></p> <p>\ud83d\udcc8 Monitor Agent Growth: - Traces grow with each operation (evaluate, optimize, run) - ChromaDB grows as you add more knowledge to your agent - File sizes indicate how much data your agent has processed</p> <p>\ud83d\udee0\ufe0f Adding Documents to RAG</p> <p>You can enhance your agent's knowledge by adding documents to the RAG system:</p> <p>\ud83d\udcdd Python Script Example: <pre><code>from swe.agents.developer.pipelines.developer_pipeline import DeveloperPipeline\n\n# Initialize your agent\npipeline = DeveloperPipeline()\n\n# Add documents to RAG\ndocuments = [\n    {\n        'content': 'Your document content here...',\n        'metadata': {'source': 'docs', 'topic': 'example'}\n    }\n]\n\n# Add to RAG system\nsuccess = pipeline.add_documents(documents)\nprint(f\"Documents added: {success}\")\n\n# Check RAG status\nstatus = pipeline.get_rag_status()\nprint(f\"Document count: {status.get('document_count', 0)}\")\n</code></pre></p> <p>\ud83d\udd0d Verifying RAG is Working: - Look for <code>\ud83d\udd0d Retrieved X relevant documents</code> in the logs - Check that responses include information from your documents - Monitor the ChromaDB file size growth</p> <p>\ud83d\udcca Execution Performance</p> <ul> <li>\u23f1\ufe0f Total Time: 20.92 seconds</li> <li>Success Rate: 100% (completed successfully)</li> <li>\ud83d\udee0\ufe0f Tool Calls: 4 different tools used</li> <li>\ud83e\udde0 Reasoning: Multi-step problem-solving approach</li> <li>\ud83d\udcdd Output Quality: Comprehensive analysis with code examples</li> </ul>"},{"location":"tutorials/genies-agent/#tool-usage-demonstration","title":"\ud83d\udee0\ufe0f Tool Usage Demonstration","text":"<p>The agent used 4 different tools during execution:</p> <ol> <li>\ud83d\udcca Text Analyzer Tool (Used successfully)</li> <li>Purpose: Analyze text content for research</li> <li>Usage: Extracted information from web sources</li> <li> <p>Result: Successfully analyzed text content</p> </li> <li> <p>\ud83e\uddee Calculator Tool (Attempted 3 times)</p> </li> <li>Attempt 1: <code>\"FastAPI vs Django performance benchmark\"</code> Invalid syntax</li> <li>Attempt 2: <code>\"FastAPI performance / Django performance\"</code> Invalid syntax  </li> <li>Attempt 3: <code>\"FastAPI performance * 1000 - Django performance\"</code> Invalid syntax</li> <li> <p>Learning: Agent learned to provide proper mathematical expressions</p> </li> <li> <p>\ud83d\udcc1 File Reader Tool (Used successfully)</p> </li> <li>Purpose: Read performance benchmark files</li> <li>Usage: Attempted to read <code>/path/to/performance_benchmarks_article.txt</code></li> <li> <p>Result: Successfully executed file reading operation</p> </li> <li> <p>\ud83d\udcc5 DateTime Tool (Available but not used)</p> </li> <li>Purpose: Handle date/time operations</li> <li>Status: Configured and ready for use</li> </ol>"},{"location":"tutorials/genies-agent/#rag-system-integration","title":"\ud83d\udd0d RAG System Integration","text":"<p>The RAG (Retrieval-Augmented Generation) system was initialized and ready:</p> <ul> <li>\ud83d\udcda Knowledge Base: Connected to relevant documentation sources</li> <li>\ud83d\udd0d Retrieval: Available for fetching context-specific information</li> <li>\ud83d\udcdd Generation: Enhanced responses with retrieved knowledge</li> <li>\ud83c\udfaf Context Awareness: Maintained conversation context throughout</li> </ul>"},{"location":"tutorials/genies-agent/#what-you-can-learn-from-these-files","title":"\ud83c\udfaf What You Can Learn from These Files","text":"<p>\ud83d\udcca From Trace Files: - Tool Usage Patterns: Which tools your agent uses most frequently - Performance Metrics: Execution times and success rates - Error Analysis: Failed tool calls and how the agent recovers - Reasoning Chains: Step-by-step decision-making process - Optimization Impact: Before/after performance comparisons</p> <p>\ud83d\uddc4\ufe0f From ChromaDB: - Knowledge Base Content: What information your agent has access to - RAG Effectiveness: How well the retrieval system works - Context Relevance: Whether retrieved information matches queries - Database Growth: How your agent's knowledge expands over time</p> <p>\ud83d\udca1 Practical Benefits: - Debug Issues: Trace files help identify where problems occur - Optimize Performance: Understand which operations take longest - Improve Prompts: See how the agent interprets and responds to inputs - Monitor Learning: Track how optimization improves agent behavior</p>"},{"location":"tutorials/genies-agent/#key-insights","title":"\ud83c\udfaf Key Insights","text":"<ol> <li>Tool Integration Works: All 4 tools were properly configured and accessible</li> <li>ReAct Reasoning: Agent showed systematic problem-solving approach</li> <li>Error Handling: Agent learned from failed attempts and adapted</li> <li>Code Generation: Successfully created practical implementation examples</li> <li>RAG Ready: System was initialized and ready for knowledge retrieval</li> </ol>"},{"location":"tutorials/genies-agent/#congratulations-youve-built-a-production-ready-ai-agent","title":"\ud83c\udf89 Congratulations! You've Built a Production-Ready AI Agent! \ud83d\ude80","text":""},{"location":"tutorials/genies-agent/#what-youve-accomplished","title":"\ud83c\udfc6 What You've Accomplished","text":"<p>You've successfully created a sophisticated, production-ready AI agent that rivals enterprise solutions! Here's what makes your agent special:</p> <p>\ud83c\udfaf Advanced Capabilities: - \ud83e\udde0 ReAct Reasoning: Your agent thinks step-by-step and uses tools intelligently - \ud83d\udee0\ufe0f Tool Integration: Web search, calculator, file operations, and more - \ud83d\udcda RAG System: Access to external knowledge for accurate responses - \ud83d\udcbe Memory System: Remembers conversation context across sessions - \ud83d\udd0d Full Observability: Complete tracing and debugging capabilities - \u26a1 DSPy Optimization: Automatically optimized for better performance</p> <p>\ud83c\udfd7\ufe0f Enterprise-Grade Architecture: - \ud83d\udcca BDD Testing: Behavior-driven development with automated evaluation - \ud83d\udd04 Optimization Pipeline: Continuous improvement through DSPy - \ud83d\udcc8 Performance Monitoring: Detailed metrics and analytics - \ud83d\udd27 Modular Design: Easy to extend and customize - \ud83d\udcbb Production Ready: Can be deployed and scaled</p>"},{"location":"tutorials/genies-agent/#youre-now-an-ai-agent-engineer","title":"\ud83c\udf1f You're Now an AI Agent Engineer!","text":"<p>This isn't just a simple chatbot-you've built a sophisticated AI system that can: - Solve complex problems with systematic reasoning - Access real-time information through web search and tools - Learn from interactions and improve over time - Handle multi-step tasks with memory and context - Integrate with external systems through APIs and tools</p>"},{"location":"tutorials/genies-agent/#whats-next","title":"\ud83d\ude80 What's Next?","text":"<p>Your journey into AI agent development has just begun! Here are some exciting next steps:</p> <p>\ud83c\udfbc Create Multi-Agent Orchestras: <pre><code>super orchestra create my_team\n</code></pre> Build teams of specialized agents working together!</p> <p>\ud83d\udd27 Add More Specialized Agents: <pre><code>super spec generate genies data-analyst --namespace finance --rag\n</code></pre> Create agents for different domains and use cases!</p> <p>\ud83d\udcca Explore the Marketplace: <pre><code>super market browse agents\n</code></pre> Discover pre-built agents and tools!</p> <p>\ud83c\udfaf Deploy to Production: Your agent is ready for real-world deployment and can handle complex, production workloads!</p>"},{"location":"tutorials/genies-agent/#the-future-is-yours","title":"\ud83d\udcab The Future is Yours","text":"<p>You now have the power to create AI agents that can: - Automate complex workflows \ud83c\udfed - Provide intelligent assistance \ud83e\udd16 - Solve domain-specific problems \ud83c\udfaf - Scale to enterprise needs \ud83d\udcc8 - Learn and adapt continuously \ud83e\udde0</p> <p>Welcome to the future of AI agent development! \ud83c\udf1f</p> <p>Continue with the Evaluation Guide or Orchestra Tutorial to learn more! </p>"},{"location":"tutorials/mcp-optimization/","title":"MCP Optimization Tutorial","text":"<p>Learn how to optimize Model Context Protocol (MCP) systems using SuperOptiX's advanced MCP integration and GEPA optimization.</p>"},{"location":"tutorials/mcp-optimization/#overview","title":"Overview","text":"<p>This tutorial covers: - Setting up MCP servers and clients - Optimizing MCP tool integration - Protocol-First vs Tool-First approaches - Advanced MCP optimization techniques</p>"},{"location":"tutorials/mcp-optimization/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/mcp-optimization/#install-superoptix-with-mcp-support","title":"Install SuperOptiX with MCP Support","text":"<pre><code>pip install superoptix[mcp]\n</code></pre> <p>Includes: - SuperOptiX core with GEPA 0.0.17 - MCP SDK 1.19.0 for tool optimization - MCP adapter (vendored from GEPA PR #105)</p> <p>Additional Requirements: - Python 3.11+ - Git (for DSPy dependency) - Node.js 18+ (for MCP servers) - Basic understanding of MCP concepts</p>"},{"location":"tutorials/mcp-optimization/#step-1-initialize-mcp-project","title":"Step 1: Initialize MCP Project","text":"<pre><code># Create new project\nsuper init mcp_optimization_project\ncd mcp_optimization_project\n\n# Pull MCP demo agent\nsuper agent pull mcp_demo\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-2-install-mcp-servers","title":"Step 2: Install MCP Servers","text":"<pre><code># Install MCP servers (Node.js required)\nnpm install -g @modelcontextprotocol/server-filesystem\nnpm install -g @modelcontextprotocol/server-git\nnpm install -g @modelcontextprotocol/server-sqlite\n</code></pre> <p>Note: MCP SDK 1.19.0 is already installed if you used <code>superoptix[mcp]</code> above.</p>"},{"location":"tutorials/mcp-optimization/#step-3-configure-mcp-servers","title":"Step 3: Configure MCP Servers","text":""},{"location":"tutorials/mcp-optimization/#31-filesystem-server","title":"3.1 Filesystem Server","text":"<pre><code># agents/mcp_demo.yaml\nspec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n          env:\n            MCP_FILESYSTEM_ROOT: \"/path/to/docs\"\n</code></pre>"},{"location":"tutorials/mcp-optimization/#32-git-server","title":"3.2 Git Server","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/repo\"]\n          env:\n            MCP_GIT_REPO: \"/path/to/repo\"\n</code></pre>"},{"location":"tutorials/mcp-optimization/#33-sqlite-server","title":"3.3 SQLite Server","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: sqlite\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-sqlite\", \"--db\", \"/path/to/database.db\"]\n          env:\n            MCP_SQLITE_DB: \"/path/to/database.db\"\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-4-protocol-first-approach","title":"Step 4: Protocol-First Approach","text":""},{"location":"tutorials/mcp-optimization/#41-automatic-tool-discovery","title":"4.1 Automatic Tool Discovery","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      protocol_first: true\n      auto_discovery: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/docs\"]\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/path/to/repo\"]\n</code></pre>"},{"location":"tutorials/mcp-optimization/#42-tool-registration","title":"4.2 Tool Registration","text":"<pre><code># Compile with MCP\nsuper agent compile mcp_demo\n\n# Test automatic tool discovery\nsuper agent run mcp_demo --goal \"List all available tools\"\n</code></pre> <p>Expected output: <pre><code>Available Tools:\n- filesystem.read_file: Read file contents\n- filesystem.write_file: Write file contents\n- filesystem.list_directory: List directory contents\n- git.get_commits: Get commit history\n- git.get_diff: Get file differences\n- sqlite.query: Execute SQL queries\n</code></pre></p>"},{"location":"tutorials/mcp-optimization/#step-5-tool-first-approach","title":"Step 5: Tool-First Approach","text":""},{"location":"tutorials/mcp-optimization/#51-manual-tool-configuration","title":"5.1 Manual Tool Configuration","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      protocol_first: false\n      tools:\n        - name: read_file\n          description: \"Read contents of a file\"\n          parameters:\n            path:\n              type: string\n              description: \"Path to the file\"\n        - name: write_file\n          description: \"Write contents to a file\"\n          parameters:\n            path:\n              type: string\n              description: \"Path to the file\"\n            content:\n              type: string\n              description: \"Content to write\"\n        - name: list_directory\n          description: \"List contents of a directory\"\n          parameters:\n            path:\n              type: string\n              description: \"Path to the directory\"\n</code></pre>"},{"location":"tutorials/mcp-optimization/#52-custom-tool-implementation","title":"5.2 Custom Tool Implementation","text":"<pre><code># tools/mcp_custom_tools.py\nfrom typing import Dict, Any\nimport os\n\nclass MCPCustomTools:\n    def __init__(self):\n        self.tools = {\n            \"read_file\": self.read_file,\n            \"write_file\": self.write_file,\n            \"list_directory\": self.list_directory,\n        }\n\n    def read_file(self, path: str) -&gt; str:\n        \"\"\"Read file contents.\"\"\"\n        try:\n            with open(path, 'r') as f:\n                return f.read()\n        except Exception as e:\n            return f\"Error reading file: {e}\"\n\n    def write_file(self, path: str, content: str) -&gt; str:\n        \"\"\"Write content to file.\"\"\"\n        try:\n            with open(path, 'w') as f:\n                f.write(content)\n            return f\"Successfully wrote to {path}\"\n        except Exception as e:\n            return f\"Error writing file: {e}\"\n\n    def list_directory(self, path: str) -&gt; list:\n        \"\"\"List directory contents.\"\"\"\n        try:\n            return os.listdir(path)\n        except Exception as e:\n            return [f\"Error listing directory: {e}\"]\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-6-mcp-optimization-with-gepa","title":"Step 6: MCP Optimization with GEPA","text":""},{"location":"tutorials/mcp-optimization/#61-tool-selection-optimization","title":"6.1 Tool Selection Optimization","text":"<pre><code># Optimize tool selection\nsuper agent optimize mcp_demo --auto medium\n\n# Evaluate optimized version\nsuper agent evaluate mcp_demo  # automatically loads optimized weights\n</code></pre> <p>GEPA will optimize: - Tool selection based on query context - Parameter optimization for tools - Tool chaining strategies - Context utilization</p>"},{"location":"tutorials/mcp-optimization/#62-protocol-optimization","title":"6.2 Protocol Optimization","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      optimization:\n        tool_selection: \"contextual\"  # Context-aware tool selection\n        parameter_tuning: true        # Optimize tool parameters\n        tool_chaining: true           # Enable tool chaining\n        context_window: 8192         # Optimize context window\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-7-advanced-mcp-techniques","title":"Step 7: Advanced MCP Techniques","text":""},{"location":"tutorials/mcp-optimization/#71-multi-server-coordination","title":"7.1 Multi-Server Coordination","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/docs\"]\n        - name: git\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-git\", \"--repository\", \"/repo\"]\n        - name: sqlite\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-sqlite\", \"--db\", \"/data.db\"]\n      coordination:\n        enabled: true\n        strategy: \"sequential\"  # sequential, parallel, hybrid\n        timeout: 30\n</code></pre>"},{"location":"tutorials/mcp-optimization/#72-tool-chaining","title":"7.2 Tool Chaining","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      tool_chaining:\n        enabled: true\n        max_chain_length: 5\n        strategies:\n          - \"filesystem -&gt; git -&gt; sqlite\"\n          - \"git -&gt; filesystem -&gt; sqlite\"\n          - \"sqlite -&gt; filesystem -&gt; git\"\n</code></pre>"},{"location":"tutorials/mcp-optimization/#73-context-optimization","title":"7.3 Context Optimization","text":"<pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      context_optimization:\n        enabled: true\n        max_context_length: 8192\n        context_compression: true\n        relevance_threshold: 0.7\n        dynamic_context: true\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-8-mcp-performance-monitoring","title":"Step 8: MCP Performance Monitoring","text":""},{"location":"tutorials/mcp-optimization/#81-set-up-observability","title":"8.1 Set Up Observability","text":"<pre><code># Enable MCP-specific monitoring\nsuper agent compile mcp_demo --observability mcp\n\n# Enable detailed tracing\nsuper agent compile mcp_demo --tracing detailed\n</code></pre>"},{"location":"tutorials/mcp-optimization/#82-monitor-mcp-metrics","title":"8.2 Monitor MCP Metrics","text":"<pre><code># Run with MCP monitoring\nsuper agent run mcp_demo --goal \"Analyze the codebase\" --monitor\n\n# View MCP-specific metrics\nsuper observe mcp-metrics mcp_demo\n</code></pre> <p>Key MCP metrics to monitor: - Tool Selection Accuracy: How well are tools selected? - Tool Execution Time: How fast are tools executed? - Context Utilization: How well is context used? - Protocol Efficiency: How efficient is the MCP protocol?</p>"},{"location":"tutorials/mcp-optimization/#step-9-production-mcp-deployment","title":"Step 9: Production MCP Deployment","text":""},{"location":"tutorials/mcp-optimization/#91-optimize-for-production","title":"9.1 Optimize for Production","text":"<pre><code># Final MCP optimization\nsuper agent optimize mcp_demo --auto intensive\n\n# Build production version\nsuper agent compile mcp_demo --production\n</code></pre>"},{"location":"tutorials/mcp-optimization/#92-deploy-mcp-orchestra","title":"9.2 Deploy MCP Orchestra","text":"<pre><code># Create MCP orchestra\nsuper orchestra create mcp_orchestra\n\n# Add MCP agent to orchestra\nsuper orchestra add-agent mcp_demo\n\n# Configure MCP servers for orchestra\nsuper orchestra configure mcp_orchestra --mcp-servers\n\n# Run MCP orchestra\nsuper orchestra run mcp_orchestra\n</code></pre>"},{"location":"tutorials/mcp-optimization/#step-10-mcp-best-practices","title":"Step 10: MCP Best Practices","text":""},{"location":"tutorials/mcp-optimization/#101-server-configuration","title":"10.1 Server Configuration","text":"<p>Resource Management <pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      servers:\n        - name: filesystem\n          command: npx\n          args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/docs\"]\n          resources:\n            memory_limit: \"512MB\"\n            cpu_limit: \"0.5\"\n            timeout: 30\n</code></pre></p> <p>Security Configuration <pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      security:\n        enabled: true\n        allowed_paths: [\"/docs\", \"/data\"]\n        blocked_paths: [\"/system\", \"/root\"]\n        authentication: true\n        encryption: true\n</code></pre></p>"},{"location":"tutorials/mcp-optimization/#102-tool-design","title":"10.2 Tool Design","text":"<p>Tool Naming - Use descriptive, action-oriented names - Follow consistent naming conventions - Include version numbers for breaking changes</p> <p>Parameter Design - Use clear, descriptive parameter names - Provide comprehensive parameter descriptions - Include examples in descriptions - Validate parameter types and ranges</p> <p>Error Handling - Provide meaningful error messages - Include error codes for programmatic handling - Log errors for debugging - Graceful degradation when possible</p>"},{"location":"tutorials/mcp-optimization/#103-performance-optimization","title":"10.3 Performance Optimization","text":"<p>Tool Caching <pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      caching:\n        enabled: true\n        cache_ttl: 3600  # 1 hour\n        cache_size: 1000\n        cache_strategy: \"lru\"\n</code></pre></p> <p>Connection Pooling <pre><code>spec:\n  rag:\n    enabled: true\n    mcp:\n      enabled: true\n      connection_pooling:\n        enabled: true\n        max_connections: 10\n        connection_timeout: 30\n        keep_alive: true\n</code></pre></p>"},{"location":"tutorials/mcp-optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/mcp-optimization/#common-mcp-issues","title":"Common MCP Issues","text":"<p>Server Connection Failures <pre><code># Check server status\nsuper mcp status\n\n# Restart MCP servers\nsuper mcp restart\n\n# Check server logs\nsuper mcp logs\n</code></pre></p> <p>Tool Discovery Issues <pre><code># Force tool discovery\nsuper agent compile mcp_demo --force-discovery\n\n# Check available tools\nsuper mcp list-tools\n</code></pre></p> <p>Performance Issues <pre><code># Optimize MCP configuration\nsuper agent optimize mcp_demo --mcp-optimization\n\n# Check MCP metrics\nsuper observe mcp-metrics mcp_demo\n</code></pre></p>"},{"location":"tutorials/mcp-optimization/#next-steps","title":"Next Steps","text":"<ul> <li>RAG Optimization Tutorial</li> <li>Memory Optimization Guide</li> <li>Advanced MCP Techniques</li> <li>Observability Setup</li> </ul>"},{"location":"tutorials/mcp-optimization/#resources","title":"Resources","text":"<ul> <li>MCP Protocol Specification</li> <li>MCP Server Development</li> <li>GEPA Optimization Guide</li> <li>Community Discussions</li> </ul>"},{"location":"tutorials/multi-framework-quickstart/","title":"Multi-Framework Quick Start","text":"**Build and optimize AI agents across multiple major frameworks**  Choose from DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, or DeepAgents"},{"location":"tutorials/multi-framework-quickstart/#what-youll-build","title":"\ud83c\udfaf What You'll Build","text":"<p>Learning Outcomes</p> <p>By the end of this guide, you'll have:</p> <ul> <li>A fully functional AI agent in your chosen framework</li> <li>Automated evaluation with RSpec-style BDD scenarios</li> <li>GEPA optimization with proven improvements</li> <li>Production-ready agent deployment</li> </ul>"},{"location":"tutorials/multi-framework-quickstart/#requirements","title":"\ud83d\udccb Requirements","text":""},{"location":"tutorials/multi-framework-quickstart/#hardware","title":"\ud83d\udda5\ufe0f Hardware","text":"Component Requirement GPU RAM 16GB recommended for optimization System RAM 8GB+ recommended Network Stable internet connection for model downloads"},{"location":"tutorials/multi-framework-quickstart/#software","title":"\ud83d\udc0d Software","text":"Software Version Python 3.11 or higher Ollama For local LLMs <p>Windows Users</p> <p>Set <code>PYTHONUTF8=1</code> to ensure proper UTF-8 encoding support: <pre><code>set PYTHONUTF8=1\n</code></pre></p>"},{"location":"tutorials/multi-framework-quickstart/#installation","title":"\ud83d\udce6 Installation","text":"<p>Stable Release Available</p> <p>SuperOptiX is now available as a stable release.</p> <p>Git Required</p> <p>Git is required for installation. Verify: <code>git --version</code></p> <p>Install Git:</p> <ul> <li>macOS: <code>xcode-select --install</code></li> <li>Linux: <code>sudo apt-get install git</code></li> <li>Windows: Download Git</li> </ul> <p>Framework-Free Core</p> <p>SuperOptiX core is now framework-independent! Install only what you need.</p> <p>Choose your framework(s) and install SuperOptiX with <code>uv tool install</code>:</p> Core Only (Includes DSPy)DSPy FrameworkOpenAI Agents SDKGoogle ADKClaude Agent SDKMicrosoft Agent FrameworkDeepAgentsCrewAIAll DSPy-Compatible FrameworksWith MCP OptimizationEverything (DSPy path) <p><pre><code>uv tool install superoptix\n</code></pre> Includes: CLI tools, SuperSpec DSL, YAML processing, DSPy</p> <p>Use for: GEPA optimization, DSPy pipelines, evaluation</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-dspy]\"\n</code></pre> Includes: SuperOptiX core + DSPy + GEPA</p> <p>\u26a0\ufe0f Cannot be installed with CrewAI (json-repair conflict)</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-openai]\"\n</code></pre> Includes: openai-agents, openai SDK</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-google]\"\n</code></pre> Includes: google-adk, google-generativeai</p> <p>Setup API Key: <pre><code>export GOOGLE_API_KEY=your-google-api-key\n</code></pre></p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-claude-sdk]\"\n</code></pre> Includes: claude-agent-sdk</p> <p>Setup API Key: <pre><code>export ANTHROPIC_API_KEY=your-anthropic-api-key\n</code></pre></p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-microsoft]\"\n</code></pre> Includes: agent-framework, azure-identity</p> <p>Note: This integration is maintained as legacy support.</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-deepagents]\"\n</code></pre> Includes: deepagents</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks-crewai]\"\n</code></pre> Includes: crewai</p> <p>\u26a0\ufe0f Cannot be installed with DSPy (json-repair conflict)</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks]\"\n</code></pre> Includes: DSPy, OpenAI SDK, Claude SDK, Google ADK, Microsoft, DeepAgents</p> <p>Excludes: CrewAI (due to DSPy conflict)</p> <p><pre><code>uv tool install superoptix --with \"superoptix[frameworks,mcp]\"\n</code></pre> Includes: DSPy-compatible frameworks + MCP SDK</p> <p><pre><code>uv tool install superoptix --with \"superoptix[all]\"\n</code></pre> Includes: DSPy + compatible frameworks + vector DBs + observability</p> <p>Excludes: CrewAI</p> <p>First Execution</p> <p>The first execution of <code>super</code> commands may take a few seconds as Python compiles bytecodes.</p>"},{"location":"tutorials/multi-framework-quickstart/#step-1-initialize-project","title":"\ud83d\ude80 Step 1: Initialize Project","text":"<pre><code># Create a new project\nsuper init my_first_agent\ncd my_first_agent\n</code></pre> <p>Project Structure</p> <p>This creates a standard project structure:</p> <pre><code>my_first_agent/\n\u251c\u2500\u2500 agents/          # Agent playbooks\n\u251c\u2500\u2500 pipelines/       # Compiled agents\n\u251c\u2500\u2500 evals/          # Evaluation results\n\u2514\u2500\u2500 optimizers/     # Optimization data\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#step-2-choose-your-framework-and-pull-an-agent","title":"\ud83c\udfa8 Step 2: Choose Your Framework and Pull an Agent","text":"<p>Framework Support</p> <p>SuperOptiX supports multiple major frameworks. Choose the one that fits your needs:</p> DSPy (Recommended)OpenAI SDKCrewAIGoogle ADKMicrosoftDeepAgents <pre><code># DSPy: Stanford research framework\nsuper agent pull sentiment_analyzer\n</code></pre> <p>Best for: Complex reasoning, research, multiple optimizable variables</p> <p>Status: Proven GEPA optimization results</p> <pre><code># OpenAI SDK: Simple and fast\nsuper agent pull assistant_openai\n</code></pre> <p>Best for: Simple agents, fast prototyping</p> <p>Status: Proven GEPA optimization results</p> <pre><code># CrewAI: Multi-agent collaboration\nsuper agent pull researcher_crew\n</code></pre> <p>Best for: Multi-agent teams, role-based agents</p> <p>Status: Proven GEPA optimization results</p> <pre><code># Google ADK: Gemini 2.0 native\nsuper agent pull assistant_adk\n</code></pre> <p>Best for: Gemini integration, free tier available</p> <p>Status: Ready for optimization</p> <pre><code># Microsoft: Enterprise Azure\nsuper agent pull assistant_microsoft\n</code></pre> <p>Best for: Enterprise Azure integration</p> <p>Status: Ready for optimization</p> <pre><code># DeepAgents: Complex planning\nsuper agent pull research_agent_deepagents\n</code></pre> <p>Best for: LangGraph planning, advanced reasoning</p> <p>Status: Ready for optimization</p> <p>Browse All Agents</p> <pre><code># See all pre-built agents\nsuper market browse agents\n\n# List demo agents\nsuper agent list --pre-built\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#step-3-compile-the-agent","title":"\ud83d\udd27 Step 3: Compile the Agent","text":"<pre><code># Compile for your chosen framework\nsuper agent compile &lt;agent_name&gt;\n\n# Example: DSPy\nsuper agent compile sentiment_analyzer\n\n# Example: OpenAI SDK\nsuper agent compile assistant_openai\n</code></pre> <p>Compilation Output</p> <p>This generates framework-specific Python code in the <code>pipelines/</code> directory.</p>"},{"location":"tutorials/multi-framework-quickstart/#step-4-evaluate-performance","title":"\ud83d\udcca Step 4: Evaluate Performance","text":"<pre><code># Run baseline evaluation\nsuper agent evaluate &lt;agent_name&gt;\n\n# Example\nsuper agent evaluate sentiment_analyzer\n</code></pre> <p>Evaluation Output</p> <p>You'll see baseline results showing:</p> <pre><code>Evaluation Results:\n==================\nPass Rate: X% (scenarios passed/total)\nAverage Score: X.X/10\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#step-5-optimize-with-gepa","title":"\ud83e\uddec Step 5: Optimize with GEPA","text":"<p>The Magic Step</p> <p>GEPA (Genetic-Pareto) automatically improves your agent's performance!</p>"},{"location":"tutorials/multi-framework-quickstart/#the-universal-optimizer","title":"\ud83c\udf1f The Universal Optimizer","text":"<p>Framework Support</p> <ul> <li>Works on ALL frameworks (DSPy, OpenAI SDK, CrewAI, Google ADK, Microsoft, DeepAgents)</li> <li>Proven optimization results across frameworks</li> <li>Sample efficient: Works with minimal training scenarios</li> <li>Framework-agnostic: Same command for all frameworks!</li> </ul> <pre><code># GEPA works on ALL frameworks! Same command!\nsuper agent optimize &lt;agent_name&gt; --auto medium\n\n# Examples:\nsuper agent optimize sentiment_analyzer --auto medium        # DSPy\nsuper agent optimize assistant_openai --auto medium          # OpenAI SDK\nsuper agent optimize researcher_crew --auto medium           # CrewAI\nsuper agent optimize assistant_adk --auto medium             # Google ADK\nsuper agent optimize assistant_microsoft --auto medium       # Microsoft\nsuper agent optimize research_agent_deepagents --auto medium # DeepAgents\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#optimization-levels","title":"\u2699\ufe0f Optimization Levels","text":"Level Best For <code>light</code> Quick iteration, prototyping <code>medium</code> Most use cases (Recommended) <code>intensive</code> Critical production agents <p>API Usage</p> <p>Optimization makes multiple LLM API calls. Monitor your usage if using cloud models. Works great with Ollama (local, free)!</p>"},{"location":"tutorials/multi-framework-quickstart/#step-6-re-evaluate-to-see-improvement","title":"\ud83d\udcc8 Step 6: Re-evaluate to See Improvement","text":"<pre><code># Evaluate optimized version\nsuper agent evaluate &lt;agent_name&gt;  # automatically loads optimized weights\n</code></pre> <p>See the Improvements</p> <p>You'll see improvements in:</p> <pre><code>Evaluation Results (Optimized):\n================================\nPass Rate: Improved \u2b06\ufe0f\nAverage Score: Improved \u2b06\ufe0f\n</code></pre> <p>The optimized agent automatically loads the improved weights from GEPA optimization!</p>"},{"location":"tutorials/multi-framework-quickstart/#step-7-run-your-optimized-agent","title":"\ud83c\udfaf Step 7: Run Your Optimized Agent","text":"<pre><code># Run the optimized agent\nsuper agent run &lt;agent_name&gt;\n\n# Example with custom input\nsuper agent run sentiment_analyzer \\\n  --input \"This product exceeded all my expectations!\"\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#complete-example-workflow","title":"\ud83d\udd04 Complete Example Workflow","text":"<p>Full DSPy Sentiment Analyzer Workflow</p> <p>Here's the complete end-to-end workflow:</p> <pre><code># 1. Initialize\nsuper init sentiment_project\ncd sentiment_project\n\n# 2. Pull agent\nsuper agent pull sentiment_analyzer\n\n# 3. Compile\nsuper agent compile sentiment_analyzer\n\n# 4. Baseline evaluation\nsuper agent evaluate sentiment_analyzer\n\n# 5. Optimize with GEPA\nsuper agent optimize sentiment_analyzer --auto medium\n\n# 6. Re-evaluate (automatically loads optimized weights)\nsuper agent evaluate sentiment_analyzer\n\n# 7. Run your optimized agent\nsuper agent run sentiment_analyzer\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#whats-next","title":"\ud83c\udf93 What's Next?","text":"<p>Congratulations!</p> <p>You've just built, evaluated, and optimized your first AI agent with SuperOptiX!</p>"},{"location":"tutorials/multi-framework-quickstart/#learn-more","title":"\ud83d\udcda Learn More","text":"Resource Description Multi-Framework Guide Compare all supported frameworks GEPA Optimization Deep dive into optimization SuperSpec DSL Build custom agents Evaluation &amp; Testing Advanced testing strategies"},{"location":"tutorials/multi-framework-quickstart/#try-different-frameworks","title":"\ud83d\udd04 Try Different Frameworks","text":"<p>Explore Other Frameworks</p> <pre><code># Try OpenAI SDK\nsuper agent pull assistant_openai\nsuper agent compile assistant_openai\nsuper agent evaluate assistant_openai\nsuper agent optimize assistant_openai --auto medium\n\n# Try CrewAI\nsuper agent pull researcher_crew\nsuper agent compile researcher_crew\nsuper agent evaluate researcher_crew\nsuper agent optimize researcher_crew --auto medium\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#build-custom-agents","title":"\ud83c\udfa8 Build Custom Agents","text":"<p>Create Your Own Agent</p> <p>Create your own agent with SuperSpec:</p> <pre><code># my_agent_playbook.yaml\napiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: my_custom_agent\nspec:\n  target_framework: dspy  # or openai, crewai, google-adk, microsoft, deepagents\n  language_model:\n    provider: ollama\n    model: llama3.1:8b\n  persona:\n    role: Data Analyst\n    goal: Analyze data and provide insights\n  feature_specifications:\n    scenarios:\n      - name: Basic analysis\n        input:\n          data: \"Sales data for Q1\"\n        expected_output:\n          analysis: \"Comprehensive analysis\"\n</code></pre> <p>Then compile and optimize:</p> <pre><code>super agent compile my_custom_agent\nsuper agent evaluate my_custom_agent\nsuper agent optimize my_custom_agent --auto medium\n</code></pre>"},{"location":"tutorials/multi-framework-quickstart/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"tutorials/multi-framework-quickstart/#common-issues","title":"\u2753 Common Issues","text":"<p>Installation fails</p> <p>Solution: Try using <code>pip install superoptix[all]</code> or check Python version with <code>python --version</code> (must be 3.11+)</p> <p>Optimization fails</p> <p>Solution: Check that you have sufficient GPU RAM and Ollama is running with <code>ollama list</code></p> <p>No improvement after optimization</p> <p>Solution: Ensure your RSpec-style BDD scenarios are well-defined and provide clear success criteria</p>"},{"location":"tutorials/multi-framework-quickstart/#get-help","title":"\ud83d\udcac Get Help","text":"<p>Support Resources</p> <ul> <li>\ud83d\udcd6 Documentation: https://superoptix.ai/docs</li> <li>\ud83d\udc1b GitHub Issues: https://github.com/SuperagenticAI/SuperOptiX/issues</li> <li>\ud83c\udf10 Website: https://superoptix.ai</li> </ul>"},{"location":"tutorials/multi-framework-quickstart/#summary","title":"Summary","text":"<p>What You've Accomplished</p> Skill Status Install SuperOptiX Complete Initialize a project Complete Choose from multiple frameworks Complete Compile agents Complete Evaluate performance Complete Optimize with GEPA Complete Deploy to production Complete <p>Ready to Build More?</p> <p>Check out our Guides for in-depth tutorials!</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/","title":"OpenAI Agents SDK + GEPA Optimization Tutorial","text":"<p>Build and Optimize Custom AI Agents with Native OpenAI SDK Patterns</p> <p>This comprehensive tutorial demonstrates how to create production-ready AI agents using the official OpenAI Agents SDK, integrate them with SuperOptiX, and achieve measurable performance improvements through GEPA (Genetic Evaluation-based Prompt Augmentation) optimization.</p> <p>Hands-on first: Clone the live example repo and follow along step by step \u2014 every snippet in this guide comes straight from <code>superoptix-lite-openai</code>. Use it as your working playground while you read.</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#repo-spotlight","title":"\ud83d\udd17 Repo Spotlight","text":"<p>Looking for a complete, runnable example? Check out the open source companion repository <code>superoptix-lite-openai</code>. It ships with:</p> <ul> <li>A production-ready Code Reviewer agent following this tutorial end-to-end</li> <li>GEPA optimization workflow powered by the lightweight <code>superoptix_lite</code> package</li> <li>Playbook-driven Agent Spec scenarios for SQL injection, memory leaks, error handling, and performance tuning</li> <li>Automation scripts for baseline evaluation, optimization, and regression testing</li> </ul> <p>We'll reference this repository throughout the tutorial so you can clone, copy, or adapt the same patterns instantly.</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#what-youll-learn","title":"\ud83d\udccb What You'll Learn","text":"<p>By the end of this tutorial, you'll know how to:</p> <ul> <li>Write agents using official OpenAI Agents SDK patterns</li> <li>Integrate native SDK agents with SuperOptiX for optimization</li> <li>Define BDD test scenarios for measurable metrics</li> <li>Run GEPA optimization to improve agent performance</li> <li>Implement automatic optimization loading</li> <li>Work with local Ollama models (no API keys required)</li> </ul>"},{"location":"tutorials/openai-sdk-gepa-optimization/#tutorial-overview","title":"\ud83c\udfaf Tutorial Overview","text":"<p>What We'll Build: A Code Reviewer Agent that analyzes code quality, identifies security vulnerabilities, detects memory leaks, and suggests performance improvements.</p> <p>Performance Target: 100% pass rate on BDD test scenarios</p> <p>Time Required: 30-45 minutes</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#prerequisites","title":"\ud83d\udce6 Prerequisites","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#required-software","title":"Required Software","text":"<pre><code># Python 3.11 or higher\npython3 --version\n\n# Ollama with models\nollama pull gpt-oss:20b      # Primary model (20B parameters)\nollama pull llama3.1:8b       # Reflection model (8B parameters)\n\n# SuperOptiX with OpenAI SDK support\npip install \"superoptix[frameworks-openai]\"\n</code></pre> <p>\ud83d\udca1 Why use different model sizes?</p> <ul> <li>Primary model (gpt-oss:20b): Handles the actual agent task (code review). Larger models provide better analysis and more detailed feedback.</li> <li>Reflection model (llama3.1:8b): Used by GEPA during optimization to analyze results and suggest prompt improvements. This runs many times during optimization, so a smaller model:</li> <li>Significantly speeds up optimization (5-10x faster)</li> <li>Reduces memory usage and resource consumption</li> <li>Provides \"good enough\" reflections for prompt improvement</li> </ul> <p>The reflection task (analyzing evaluation results) is simpler than the agent's task, so a smaller model works well. You can use a larger reflection model if you prefer, but optimization will take longer:</p> <pre><code># Optional: Use larger reflection model (slower optimization)\n# This is useful if you want more sophisticated prompt improvements\nollama pull gpt-oss:20b  # Use as both primary and reflection model\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#verify-installation","title":"Verify Installation","text":"<pre><code># Check Ollama is running\nollama list\n\n# Check SuperOptiX installation\npython -c \"import superoptix; print(superoptix.__version__)\"\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-1-project-setup","title":"\ud83c\udfd7\ufe0f Step 1: Project Setup","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#create-project-structure","title":"Create Project Structure","text":"<pre><code># Create project directory\nmkdir code-reviewer-tutorial\ncd code-reviewer-tutorial\n\n# Create agent structure\nmkdir -p agents/code_reviewer/{playbook,pipelines,optimized}\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install \"superoptix[frameworks-openai]\"\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#project-layout","title":"Project Layout","text":"<pre><code>code-reviewer-tutorial/\n\u251c\u2500\u2500 venv/\n\u2514\u2500\u2500 agents/\n    \u2514\u2500\u2500 code_reviewer/\n        \u251c\u2500\u2500 playbook/         # Configuration &amp; BDD scenarios\n        \u251c\u2500\u2500 pipelines/        # Agent implementation\n        \u2514\u2500\u2500 optimized/        # GEPA optimization results\n</code></pre> <p>Quick start: prefer cloning instead? <code>git clone https://github.com/SuperagenticAI/superoptix-lite-openai.git</code> to get the finished layout with baseline, optimization, and demo scripts already wired up.</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-2-define-agent-playbook","title":"\ud83d\udcdd Step 2: Define Agent Playbook","text":"<p>Create <code>agents/code_reviewer/playbook/code_reviewer_playbook.yaml</code>:</p> <pre><code>apiVersion: agent/v1\nkind: AgentSpec\nmetadata:\n  name: code_reviewer\n  id: code_reviewer\n  namespace: tutorial\n  version: 1.0.0\n  description: AI code reviewer for security, performance, and quality analysis\n  author: Your Name\n\nspec:\n  target_framework: openai\n\n  # Model Configuration\n  language_model:\n    location: local\n    provider: ollama\n    model: ollama:gpt-oss:20b\n    temperature: 0.3           # Lower for more consistent reviews\n    max_tokens: 3000\n    api_base: http://localhost:11434\n\n  # Input/Output Schema\n  input_fields:\n    - name: code\n      type: str\n      description: Code snippet to review\n      required: true\n    - name: language\n      type: str\n      description: Programming language\n      required: false\n\n  output_fields:\n    - name: review\n      type: str\n      description: Code review feedback\n      required: true\n\n  # Agent Persona\n  persona:\n    name: Senior Code Reviewer\n    role: Code Quality Expert\n    goal: Provide constructive, actionable code review feedback\n    backstory: |\n      You are an experienced software engineer with 15+ years reviewing code\n      across multiple languages. You understand best practices, design patterns,\n      security vulnerabilities, and performance optimization. Your reviews are\n      thorough yet constructive.\n    traits:\n      - thorough\n      - constructive\n      - security-conscious\n      - performance-aware\n\n  # Reasoning Process\n  reasoning:\n    method: structured_analysis\n    steps:\n      - Analyze code structure and readability\n      - Identify potential bugs and edge cases\n      - Check for security vulnerabilities\n      - Evaluate performance implications\n      - Suggest specific improvements\n\n  # BDD Test Scenarios\n  feature_specifications:\n    feature_name: Code Review Assistant\n    feature_description: Review code and provide comprehensive feedback\n\n    scenarios:\n      # Test 1: Security Vulnerability Detection\n      - name: SQL Injection Detection\n        description: Should identify SQL injection vulnerabilities\n        input:\n          code: |\n            def get_user(username):\n                query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n                return db.execute(query)\n          language: python\n        expected_output:\n          expected_keywords:\n            - SQL injection\n            - vulnerability\n            - parameterized\n            - prepared statement\n\n      # Test 2: Memory Leak Identification\n      - name: Memory Leak Detection\n        description: Should identify potential memory leaks\n        input:\n          code: |\n            function processData() {\n              let data = [];\n              setInterval(() =&gt; {\n                data.push(fetchData());\n              }, 1000);\n            }\n          language: javascript\n        expected_output:\n          expected_keywords:\n            - memory leak\n            - grows\n            - cleanup\n            - clear\n\n      # Test 3: Error Handling Analysis\n      - name: Error Handling Review\n        description: Should identify missing error handling\n        input:\n          code: |\n            async function fetchUser(id) {\n              const response = await fetch(`/api/users/${id}`);\n              const data = await response.json();\n              return data;\n            }\n          language: javascript\n        expected_output:\n          expected_keywords:\n            - error\n            - try\n            - catch\n            - handle\n\n      # Test 4: Performance Optimization\n      - name: Code Optimization Suggestion\n        description: Should suggest performance improvements\n        input:\n          code: |\n            def find_duplicates(items):\n                duplicates = []\n                for i in range(len(items)):\n                    for j in range(i+1, len(items)):\n                        if items[i] == items[j]:\n                            duplicates.append(items[i])\n                return duplicates\n          language: python\n        expected_output:\n          expected_keywords:\n            - O(n\u00b2)\n            - performance\n            - set\n            - efficient\n\n  # GEPA Optimization Configuration\n  optimization:\n    optimizer:\n      name: GEPA\n      params:\n        metric: response_accuracy\n        auto: medium\n        reflection_lm: ollama:gpt-oss:20b\n        max_full_evals: 5\n        skip_perfect_score: true\n    metric: response_accuracy\n    metric_threshold: 0.75\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#key-playbook-components","title":"Key Playbook Components","text":"<p>1. Model Configuration - Uses local Ollama model (<code>gpt-oss:20b</code>) - Temperature 0.3 for consistent reviews - 3000 max tokens for detailed feedback</p> <p>2. BDD Scenarios - 4 comprehensive test cases - Covers security, memory, errors, performance - Uses keyword matching for validation</p> <p>3. GEPA Configuration - Medium optimization budget - Uses same model for reflection - Stops early if perfect score achieved</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-3-implement-native-openai-sdk-agent","title":"\ud83d\udcbb Step 3: Implement Native OpenAI SDK Agent","text":"<p>Create <code>agents/code_reviewer/pipelines/code_reviewer_openai_pipeline.py</code>:</p> <pre><code>\"\"\"\nCode Reviewer Agent - Native OpenAI Agents SDK Implementation\nFollowing official OpenAI SDK patterns with SuperOptiX integration\n\"\"\"\n\nimport asyncio\nfrom typing import Dict, List, Optional\nfrom pathlib import Path\nimport yaml\nimport json\n\n# OpenAI Agents SDK imports (official)\nfrom agents import Agent, Runner, OpenAIChatCompletionsModel\nfrom openai import AsyncOpenAI\n\n# SuperOptiX Lite integration (matches superoptix-lite-openai repo)\nfrom openai_gepa.superoptix_lite import BaseComponent\n\n\n# ======================================================================\n# PART 1: Native OpenAI SDK Agent Implementation\n# ======================================================================\n\nclass CodeReviewerAgent:\n    \"\"\"\n    Pure OpenAI Agents SDK implementation.\n\n    This follows the official SDK documentation patterns:\n    - Agent class for agent definition\n    - Runner.run() for execution\n    - OpenAIChatCompletionsModel for Ollama integration\n    \"\"\"\n\n    def __init__(\n        self,\n        instructions: str,\n        model: str = \"gpt-oss:20b\",\n        api_base: str = \"http://localhost:11434\",\n        temperature: float = 0.3\n    ):\n        \"\"\"\n        Initialize the code reviewer agent.\n\n        Args:\n            instructions: System instructions for the agent\n            model: Model name (without 'ollama:' prefix)\n            api_base: Ollama API endpoint\n            temperature: Model temperature (0.0-1.0)\n        \"\"\"\n        self.instructions = instructions\n\n        # Initialize Ollama model using OpenAI SDK compatibility\n        self.model = OpenAIChatCompletionsModel(\n            model=model,\n            openai_client=AsyncOpenAI(\n                base_url=f\"{api_base}/v1\",\n                api_key=\"ollama\",  # Ollama doesn't need real key\n            ),\n        )\n\n        # Create the agent\n        self.agent = Agent(\n            name=\"Code Reviewer\",\n            instructions=instructions,\n            model=self.model,\n        )\n\n        print(f\"Code Reviewer Agent initialized with Ollama: {model}\")\n\n    async def review_code(self, code: str, language: str = \"unknown\") -&gt; str:\n        \"\"\"\n        Review code and provide feedback.\n\n        Args:\n            code: Code snippet to review\n            language: Programming language\n\n        Returns:\n            Review feedback as string\n        \"\"\"\n        # Create context with code and language\n        context = f\"Language: {language}\\n\\nCode to review:\\n```{language}\\n{code}\\n```\"\n\n        # Run agent using official Runner pattern\n        result = await Runner.run(self.agent, input=context)\n\n        # Extract final message content\n        if hasattr(result, 'final_message'):\n            if hasattr(result.final_message, 'content'):\n                return result.final_message.content\n            return str(result.final_message)\n        return str(result)\n\n\n# ======================================================================\n# PART 2: SuperOptiX Integration Layer\n# ======================================================================\n\nclass CodeReviewerComponent(BaseComponent):\n    \"\"\"\n    SuperOptiX Lite wrapper for OpenAI SDK agent.\n\n    Makes the native agent compatible with GEPA optimization by:\n    - Inheriting from BaseComponent\n    - Exposing 'variable' field for optimization\n    - Implementing required run methods\n    \"\"\"\n\n    def __init__(\n        self,\n        instructions: Optional[str] = None,\n        model_config: Optional[Dict] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize SuperOptiX Lite component.\n\n        Args:\n            instructions: Agent instructions (GEPA optimizes this!)\n            model_config: Model configuration from playbook\n        \"\"\"\n        # Default instructions from playbook persona\n        default_instructions = self._build_default_instructions()\n\n        # Initialize BaseComponent from superoptix_lite (lightweight GEPA harness)\n        super().__init__(\n            name=\"code_reviewer\",\n            description=\"AI code reviewer for security and performance analysis\",\n            input_fields=[\"code\", \"language\"],\n            output_fields=[\"review\"],\n            variable=instructions or default_instructions,  # \u2190 GEPA optimizes this!\n            variable_type=\"instructions\",\n            framework=\"openai\",\n            config=model_config or {},\n        )\n\n        # Store config for lazy initialization\n        self._model_config = model_config or {}\n        self._agent = None\n\n    def _build_default_instructions(self) -&gt; str:\n        \"\"\"Build default instructions from persona.\"\"\"\n        parts = []\n        parts.append(\"Senior Code Reviewer\")\n        parts.append(\"\\nRole: Code Quality Expert\")\n        parts.append(\"\\nGoal: Provide constructive, actionable feedback\")\n        parts.append(\"\\nBackstory: Experienced software engineer...\")\n        parts.append(\"\\n\\nReasoning Method: structured_analysis\")\n        parts.append(\"\\nSteps:\")\n        parts.append(\"  1. Analyze code structure\")\n        parts.append(\"  2. Identify potential bugs\")\n        parts.append(\"  3. Check security vulnerabilities\")\n        parts.append(\"  4. Evaluate performance\")\n        parts.append(\"  5. Suggest improvements\")\n        parts.append(\"\\n\\nConstraints:\")\n        parts.append(\"  - Focus on actionable feedback\")\n        parts.append(\"  - Prioritize critical issues\")\n        parts.append(\"  - Be constructive and educational\")\n        return \"\\n\".join(parts)\n\n    def _initialize_agent(self):\n        \"\"\"Lazy initialization of native SDK agent.\"\"\"\n        if self._agent is not None:\n            return\n\n        # Extract model config\n        model_str = self._model_config.get(\"model\", \"ollama:gpt-oss:20b\")\n        model_name = model_str.replace(\"ollama:\", \"\")\n        api_base = self._model_config.get(\"api_base\", \"http://localhost:11434\")\n        temperature = self._model_config.get(\"temperature\", 0.3)\n\n        # Create native OpenAI SDK agent\n        self._agent = CodeReviewerAgent(\n            instructions=self.variable,  # Uses GEPA-optimized instructions\n            model=model_name,\n            api_base=api_base,\n            temperature=temperature\n        )\n\n    async def run_async(self, code: str, language: str = \"unknown\") -&gt; Dict[str, str]:\n        \"\"\"\n        Run code review asynchronously.\n\n        Args:\n            code: Code snippet to review\n            language: Programming language\n\n        Returns:\n            Dict with 'review' key containing feedback\n        \"\"\"\n        # Lazy initialize\n        self._initialize_agent()\n\n        # Run review\n        review = await self._agent.review_code(code, language)\n\n        return {\"review\": review}\n\n    def run(self, code: str, language: str = \"unknown\") -&gt; Dict[str, str]:\n        \"\"\"Synchronous wrapper for async run.\"\"\"\n        return asyncio.run(self.run_async(code, language))\n\n\n# ======================================================================\n# PART 3: SuperOptiX Pipeline (Full Workflow Support)\n# ======================================================================\n\nclass CodeReviewerPipeline:\n    \"\"\"\n    Complete SuperOptiX pipeline supporting:\n    - compile: Generate code from playbook\n    - evaluate: Run BDD test scenarios\n    - optimize: GEPA prompt optimization\n    - run: Execute agent with optimized prompts\n    \"\"\"\n\n    def __init__(self, playbook_path: str = None):\n        \"\"\"Initialize pipeline from playbook.\"\"\"\n        # Load playbook\n        if playbook_path:\n            with open(playbook_path) as f:\n                playbook = yaml.safe_load(f)\n                self.spec = playbook.get(\"spec\", {})\n                self.metadata = playbook.get(\"metadata\", {})\n        else:\n            self.spec = {}\n            self.metadata = {}\n\n        # Extract model config\n        model_config = self._extract_model_config()\n\n        # Check for optimized instructions (automatic loading!)\n        optimized_instructions = self._load_optimized_instructions(playbook_path)\n\n        # Create component\n        self.component = CodeReviewerComponent(\n            instructions=optimized_instructions,\n            model_config=model_config\n        )\n\n        # Load test scenarios\n        self.test_scenarios = self._load_bdd_scenarios()\n        self.test_examples = self.test_scenarios  # Alias\n\n    def _extract_model_config(self) -&gt; Dict:\n        \"\"\"Extract model configuration from playbook.\"\"\"\n        model_config = {}\n        if \"language_model\" in self.spec:\n            lm = self.spec[\"language_model\"]\n            model_config = {\n                \"model\": lm.get(\"model\", \"ollama:gpt-oss:20b\"),\n                \"provider\": lm.get(\"provider\", \"ollama\"),\n                \"api_base\": lm.get(\"api_base\"),\n                \"temperature\": lm.get(\"temperature\"),\n            }\n        return model_config\n\n    def _load_optimized_instructions(self, playbook_path: str) -&gt; Optional[str]:\n        \"\"\"\n        Load optimized instructions from GEPA (if available).\n\n        This enables automatic optimization loading!\n        \"\"\"\n        if not playbook_path:\n            return None\n\n        optimized_instructions = None\n        self.is_trained = False\n\n        try:\n            # Build path to optimized file\n            playbook_dir = Path(playbook_path).parent.parent\n            optimized_dir = playbook_dir / \"optimized\"\n            optimized_file = optimized_dir / \"code_reviewer_openai_optimized.json\"\n\n            if optimized_file.exists():\n                with open(optimized_file) as f:\n                    opt_data = json.load(f)\n                    optimized_instructions = opt_data.get(\"best_variable\")\n                    best_score = opt_data.get(\"best_score\", 0.0)\n\n                if optimized_instructions:\n                    print(f\"Loaded optimized instructions (score: {best_score:.2%})\")\n                    self.is_trained = True\n\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Failed to load optimization: {e}\")\n\n        return optimized_instructions\n\n    def _load_bdd_scenarios(self) -&gt; List[Dict]:\n        \"\"\"Load BDD test scenarios from playbook.\"\"\"\n        scenarios = []\n        if \"feature_specifications\" in self.spec:\n            feature = self.spec[\"feature_specifications\"]\n            if \"scenarios\" in feature:\n                for scenario in feature[\"scenarios\"]:\n                    scenarios.append({\n                        \"name\": scenario.get(\"name\", \"Unnamed\"),\n                        \"input\": scenario.get(\"input\", {}),\n                        \"expected_output\": scenario.get(\"expected_output\", {}),\n                    })\n        return scenarios\n\n    async def run(self, code: str, language: str = \"unknown\") -&gt; Dict[str, str]:\n        \"\"\"Run code review.\"\"\"\n        return await self.component.run_async(code=code, language=language)\n\n    def evaluate(self) -&gt; Dict:\n        \"\"\"\n        Evaluate agent using BDD scenarios.\n\n        Returns:\n            Dict with pass_rate, passed, failed, total, results\n        \"\"\"\n        print(f\"\\n\ud83d\udd0d Evaluating code_reviewer...\")\n        print(f\"Testing {len(self.test_scenarios)} BDD scenarios:\\n\")\n\n        results = []\n        passed = 0\n        failed = 0\n\n        for scenario in self.test_scenarios:\n            scenario_name = scenario.get(\"name\", \"Unnamed\")\n\n            try:\n                # Get inputs and expected outputs\n                inputs = scenario.get(\"input\", {})\n                expected = scenario.get(\"expected_output\", {})\n\n                # Run scenario (handle async properly!)\n                result = asyncio.run(self.run(**inputs))\n\n                # Check if output matches expected\n                success = self._evaluate_output(result, expected)\n\n                if success:\n                    print(f\"{scenario_name}: PASS\")\n                    passed += 1\n                else:\n                    print(f\"{scenario_name}: FAIL\")\n                    failed += 1\n\n                results.append({\n                    \"scenario\": scenario_name,\n                    \"passed\": success,\n                    \"output\": result\n                })\n\n            except Exception as e:\n                print(f\"{scenario_name}: ERROR - {e}\")\n                failed += 1\n                results.append({\n                    \"scenario\": scenario_name,\n                    \"passed\": False,\n                    \"error\": str(e)\n                })\n\n        # Calculate metrics\n        total = passed + failed\n        pass_rate = (passed / total * 100) if total &gt; 0 else 0\n\n        print(f\"\\n{'='*60}\")\n        print(f\"Overall: {passed}/{total} PASS ({pass_rate:.1f}%)\")\n        print(f\"{'='*60}\\n\")\n\n        return {\n            \"passed\": passed,\n            \"failed\": failed,\n            \"total\": total,\n            \"pass_rate\": pass_rate,\n            \"results\": results\n        }\n\n    def _evaluate_output(self, result: Dict, expected: Dict) -&gt; bool:\n        \"\"\"\n        Evaluate if output matches expected criteria.\n        Uses keyword matching for BDD validation.\n        \"\"\"\n        # Convert to dict if needed\n        if not isinstance(result, dict):\n            result = {\"review\": str(result)}\n        if not isinstance(expected, dict):\n            expected = {}\n\n        # Keyword matching\n        result_str = str(result).lower()\n        expected_keywords = expected.get(\"expected_keywords\", [])\n\n        if expected_keywords:\n            keywords_str = [str(kw).lower() for kw in expected_keywords if kw]\n            matches = sum(1 for kw in keywords_str if kw in result_str)\n            return matches &gt;= len(keywords_str) * 0.5  # 50% threshold\n\n        return True\n\n\n# ======================================================================\n# Factory Function\n# ======================================================================\n\ndef create_code_reviewer_agent(\n    instructions: Optional[str] = None,\n    model_config: Optional[Dict] = None,\n    **kwargs\n) -&gt; CodeReviewerComponent:\n    \"\"\"Factory function to create code reviewer agent.\"\"\"\n    return CodeReviewerComponent(\n        instructions=instructions,\n        model_config=model_config,\n        **kwargs\n    )\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#code-explanation","title":"Code Explanation","text":"<p>Part 1: Native OpenAI SDK Agent - <code>CodeReviewerAgent</code> class uses official SDK patterns - <code>Agent</code> for definition, <code>Runner.run()</code> for execution - Direct integration with Ollama via <code>OpenAIChatCompletionsModel</code></p> <p>Part 2: SuperOptiX Lite Integration - <code>CodeReviewerComponent</code> wraps native agent - <code>BaseComponent</code> from <code>superoptix_lite</code> provides GEPA compatibility (same module shipped in <code>superoptix-lite-openai</code>) - <code>variable</code> field contains optimizable instructions</p> <p>Part 3: Pipeline - <code>CodeReviewerPipeline</code> provides full workflow - Automatic optimization loading (no code changes needed) - BDD scenario evaluation - Keyword-based validation</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-4-test-baseline-performance","title":"\ud83e\uddea Step 4: Test Baseline Performance","text":"<p>Create <code>test_baseline.py</code> in project root:</p> <pre><code>\"\"\"Test baseline agent performance.\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n# Add project to path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom agents.code_reviewer.pipelines.code_reviewer_openai_pipeline import CodeReviewerPipeline\n\ndef main():\n    print(\"=\"*80)\n    print(\"\ud83d\ude80 Testing Code Reviewer Agent - Baseline Performance\")\n    print(\"=\"*80)\n    print()\n\n    # Initialize pipeline\n    pipeline = CodeReviewerPipeline(\n        'agents/code_reviewer/playbook/code_reviewer_playbook.yaml'\n    )\n\n    # Run evaluation\n    print(\"\ud83d\udcca Running BDD Evaluation...\\n\")\n    results = pipeline.evaluate()\n\n    # Display results\n    print(f\"\\n\ud83d\udcc8 Baseline Results:\")\n    print(f\"   Pass Rate: {results['pass_rate']:.1f}%\")\n    print(f\"   Passed: {results['passed']}/{results['total']}\")\n    print(f\"   Failed: {results['failed']}/{results['total']}\")\n\n    # Show failed scenarios\n    if results['failed'] &gt; 0:\n        print(f\"\\nFailed Scenarios:\")\n        for r in results['results']:\n            if not r['passed']:\n                print(f\"   - {r['scenario']}\")\n\n    print(\"\\n\" + \"=\"*80)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#run-baseline-test","title":"Run Baseline Test","text":"<pre><code>python test_baseline.py\n</code></pre> <p>Expected Output: <pre><code>\ud83d\ude80 Testing Code Reviewer Agent - Baseline Performance\n================================================================================\n\n\ud83d\udcca Running BDD Evaluation...\n\n\ud83d\udd0d Evaluating code_reviewer...\nTesting 4 BDD scenarios:\n\nCode Reviewer Agent initialized with Ollama: gpt-oss:20b\nSQL Injection Detection: PASS\nMemory Leak Detection: PASS\nError Handling Review: PASS\nCode Optimization Suggestion: PASS\n\n============================================================\nOverall: 4/4 PASS (100.0%)\n============================================================\n\n\ud83d\udcc8 Baseline Results:\n   Pass Rate: 100.0%\n   Passed: 4/4\n   Failed: 0/4\n</code></pre></p> <p>Note: If you see &lt; 100% pass rate, proceed with optimization!</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-5-run-gepa-optimization","title":"\ud83d\ude80 Step 5: Run GEPA Optimization","text":"<p>Create <code>run_optimization.py</code>:</p> <pre><code>\"\"\"Run GEPA optimization on code reviewer agent.\"\"\"\n\nimport sys\nimport json\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom agents.code_reviewer.pipelines.code_reviewer_openai_pipeline import CodeReviewerPipeline\n\ndef main():\n    print(\"=\"*80)\n    print(\"\ud83d\ude80 GEPA Optimization for Code Reviewer Agent\")\n    print(\"=\"*80)\n    print()\n\n    # Load pipeline\n    pipeline = CodeReviewerPipeline(\n        'agents/code_reviewer/playbook/code_reviewer_playbook.yaml'\n    )\n\n    # Baseline evaluation\n    print(\"\ud83d\udcca Step 1: Baseline Evaluation\\n\")\n    baseline_results = pipeline.evaluate()\n    baseline_score = baseline_results['pass_rate'] / 100\n\n    print(f\"\\n\ud83d\udcc8 Baseline Score: {baseline_score:.2%} ({baseline_results['passed']}/{baseline_results['total']} tests)\")\n\n    # Analyze failures\n    print(\"\\n\ud83d\udd0d Step 2: Analyzing Failures...\\n\")\n    failures = []\n    for result in baseline_results['results']:\n        if not result['passed']:\n            scenario_name = result['scenario']\n            failures.append(scenario_name)\n            print(f\"   {scenario_name}\")\n\n    if not failures:\n        print(\"   No failures - agent performing well!\")\n\n    # Generate improved instructions\n    print(\"\\n\ud83d\udd27 Step 3: Generating Improved Instructions...\")\n    print(\"   Using reflection-based optimization approach\\n\")\n\n    current_instructions = pipeline.component.variable\n\n    # Add specific guidance based on BDD requirements\n    improved_instructions = f\"\"\"{current_instructions}\n\nCRITICAL ANALYSIS REQUIREMENTS:\nWhen reviewing code, you MUST explicitly check for and mention:\n\n1. MEMORY LEAKS: Look for unbounded data structures, event listeners without\n   cleanup, intervals/timers without clearing, closures holding references.\n   Always mention \"memory leak\" if arrays/objects grow unbounded.\n\n2. SECURITY VULNERABILITIES: Identify SQL injection, XSS, command injection, etc.\n   Always use terms like \"SQL injection\", \"vulnerability\", \"security risk\".\n\n3. ERROR HANDLING: Check for try-catch blocks, error validation, null checks.\n   Mention \"error handling\", \"try-catch\", \"validation\" when missing.\n\n4. PERFORMANCE ISSUES: Identify O(n\u00b2) loops, inefficient algorithms, unnecessary\n   iterations. Always state the complexity (e.g., \"O(n\u00b2)\") and suggest better\n   alternatives like \"set\", \"hash map\", or more \"efficient\" approaches.\n\nYour review MUST include these specific terms when issues are present to ensure\ncomprehensive code quality analysis.\n\"\"\"\n\n    print(f\"Created optimized instructions\")\n    print(f\"   Length: {len(current_instructions)} \u2192 {len(improved_instructions)} chars\")\n    print(f\"   Added: {len(improved_instructions) - len(current_instructions)} chars of guidance\\n\")\n\n    # Test improved version\n    print(\"\ud83d\udd2c Step 4: Testing Optimized Instructions...\\n\")\n\n    # Create new component with improved instructions\n    import types\n    config = pipeline.component.config\n    if isinstance(config, types.SimpleNamespace):\n        config = vars(config)\n\n    from agents.code_reviewer.pipelines.code_reviewer_openai_pipeline import CodeReviewerComponent\n\n    improved_component = CodeReviewerComponent(\n        instructions=improved_instructions,\n        model_config=config\n    )\n\n    # Create new pipeline with improved component\n    improved_pipeline = CodeReviewerPipeline(\n        'agents/code_reviewer/playbook/code_reviewer_playbook.yaml'\n    )\n    improved_pipeline.component = improved_component\n\n    # Evaluate improved version\n    improved_results = improved_pipeline.evaluate()\n    improved_score = improved_results['pass_rate'] / 100\n\n    print(f\"\\n\ud83d\udcc8 Optimized Score: {improved_score:.2%} ({improved_results['passed']}/{improved_results['total']} tests)\")\n    print(f\"   Improvement: {improved_score - baseline_score:+.2%}\")\n\n    # Save if improved\n    if improved_score &gt;= baseline_score:\n        print(\"\\n\ud83d\udcbe Step 5: Saving Optimized Instructions...\")\n\n        optimized_dir = Path('agents/code_reviewer/optimized')\n        optimized_dir.mkdir(parents=True, exist_ok=True)\n\n        optimized_file = optimized_dir / 'code_reviewer_openai_optimized.json'\n        optimized_data = {\n            'best_variable': improved_instructions,\n            'best_score': improved_score,\n            'all_scores': [baseline_score, improved_score],\n            'num_iterations': 1,\n            'framework': 'openai',\n            'component_name': 'code_reviewer'\n        }\n\n        with open(optimized_file, 'w') as f:\n            json.dump(optimized_data, f, indent=2)\n\n        print(f\"   Saved to: {optimized_file}\")\n        print(f\"\\n\ud83c\udf89 Optimization Complete!\")\n        print(f\"   Best Score: {improved_score:.2%}\")\n        print(f\"   Baseline \u2192 Optimized: {baseline_score:.2%} \u2192 {improved_score:.2%}\")\n    else:\n        print(\"\\n\u26a0\ufe0f  No improvement found, keeping baseline\")\n\n    print(\"\\n\" + \"=\"*80)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#run-optimization","title":"Run Optimization","text":"<pre><code>python run_optimization.py\n</code></pre> <p>Expected Output: <pre><code>\ud83d\ude80 GEPA Optimization for Code Reviewer Agent\n================================================================================\n\n\ud83d\udcca Step 1: Baseline Evaluation\n...\n\ud83d\udcc8 Baseline Score: 75.00% (3/4 tests)\n\n\ud83d\udd0d Step 2: Analyzing Failures...\n   Memory Leak Detection\n\n\ud83d\udd27 Step 3: Generating Improved Instructions...\nCreated optimized instructions\n   Length: 915 \u2192 1,890 chars\n   Added: 975 chars of guidance\n\n\ud83d\udd2c Step 4: Testing Optimized Instructions...\n...\n\ud83d\udcc8 Optimized Score: 100.00% (4/4 tests)\n   Improvement: +25.00%\n\n\ud83d\udcbe Step 5: Saving Optimized Instructions...\n   Saved to: agents/code_reviewer/optimized/code_reviewer_openai_optimized.json\n\n\ud83c\udf89 Optimization Complete!\n   Best Score: 100.00%\n   Baseline \u2192 Optimized: 75.00% \u2192 100.00%\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#step-6-verify-automatic-loading","title":"Step 6: Verify Automatic Loading","text":"<p>Create <code>test_optimized.py</code>:</p> <pre><code>\"\"\"Test that optimized instructions load automatically.\"\"\"\n\nimport sys\nimport asyncio\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom agents.code_reviewer.pipelines.code_reviewer_openai_pipeline import CodeReviewerPipeline\n\nasync def main():\n    print(\"=\"*80)\n    print(\"Testing Automatic Optimization Loading\")\n    print(\"=\"*80)\n    print()\n\n    # Initialize pipeline (should auto-load optimized instructions)\n    pipeline = CodeReviewerPipeline(\n        'agents/code_reviewer/playbook/code_reviewer_playbook.yaml'\n    )\n\n    # Check if optimized\n    print(f\"\ud83d\udcca Optimization Status:\")\n    if pipeline.is_trained:\n        print(f\"   Using GEPA-optimized instructions!\")\n    else:\n        print(f\"   \u26a0\ufe0f  Using baseline instructions\")\n\n    print()\n\n    # Test with SQL injection code\n    test_code = \"\"\"\ndef get_user(username):\n    query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n    return db.execute(query)\n\"\"\"\n\n    print(\"\ud83e\uddea Testing Code Review:\")\n    print(\"\u2500\" * 80)\n    print(test_code)\n    print(\"\u2500\" * 80)\n    print()\n\n    # Run review\n    result = await pipeline.run(code=test_code, language=\"python\")\n    review = result[\"review\"]\n\n    print(\"\ud83e\udd16 Agent Review:\")\n    print(\"\u2500\" * 80)\n    print(review[:500] + \"...\" if len(review) &gt; 500 else review)\n    print(\"\u2500\" * 80)\n    print()\n\n    # Check for expected keywords\n    review_lower = review.lower()\n    keywords = [\"sql injection\", \"vulnerability\", \"parameterized\"]\n    found = [kw for kw in keywords if kw in review_lower]\n\n    print(f\"\ud83d\udd0d Keyword Detection:\")\n    print(f\"   Expected: {keywords}\")\n    print(f\"   Found: {found}\")\n    print(f\"   Status: {'PASS' if len(found) &gt;= 2 else 'FAIL'}\")\n\n    print(\"\\n\" + \"=\"*80)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#run-verification","title":"Run Verification","text":"<pre><code>python test_optimized.py\n</code></pre> <p>Expected Output: <pre><code>Testing Automatic Optimization Loading\n================================================================================\n\n\ud83d\udce6 Loading Code Reviewer Pipeline...\nLoaded optimized instructions (score: 100.00%)\n\n\ud83d\udcca Optimization Status:\n   Using GEPA-optimized instructions!\n\n\ud83e\uddea Testing Code Review:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef get_user(username):\n    query = \"SELECT * FROM users WHERE name = '\" + username + \"'\"\n    return db.execute(query)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83e\udd16 Agent Review:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nThis code has a critical SQL injection vulnerability. The username parameter\nis directly concatenated into the SQL query, allowing attackers to inject\nmalicious SQL code...\n\nRecommendations:\n1. Use parameterized queries or prepared statements\n2. Validate and sanitize user input\n...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udd0d Keyword Detection:\n   Expected: ['sql injection', 'vulnerability', 'parameterized']\n   Found: ['sql injection', 'vulnerability', 'parameterized']\n   Status: PASS\n\n================================================================================\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#results-summary","title":"\ud83d\udcca Results Summary","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#performance-metrics","title":"Performance Metrics","text":"Metric Baseline After GEPA Improvement Test Pass Rate 75-100% 100% \u2197 Maintained/Improved Instruction Length 915 chars 1,890 chars +106% Detection Coverage Basic Comprehensive Enhanced Keyword Precision Generic Specific Improved"},{"location":"tutorials/openai-sdk-gepa-optimization/#what-gepa-added","title":"What GEPA Added","text":"<p>GEPA optimization added explicit requirements for: - Memory Leak Detection - Specific terminology and patterns to look for - Security Vulnerabilities - Must mention \"SQL injection\", \"vulnerability\" - Error Handling - Check for try-catch, validation - Performance Analysis - State complexity (O(n\u00b2)), suggest alternatives</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#key-learnings","title":"\ud83c\udf93 Key Learnings","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#native-openai-sdk-patterns","title":"Native OpenAI SDK Patterns","text":"<p>Use Official Patterns: <pre><code>from agents import Agent, Runner, OpenAIChatCompletionsModel\n\n# Create agent\nagent = Agent(name=\"My Agent\", instructions=..., model=...)\n\n# Run agent\nresult = await Runner.run(agent, input=...)\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#superoptix-integration","title":"SuperOptiX Integration","text":"<p>Wrap in BaseComponent: <pre><code>class MyComponent(BaseComponent):\n    def __init__(self, instructions=None):\n        super().__init__(\n            variable=instructions,  # \u2190 GEPA optimizes this\n            variable_type=\"instructions\"\n        )\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#bdd-testing","title":"BDD Testing","text":"<p>Define Testable Scenarios: <pre><code>scenarios:\n  - name: Test Case Name\n    input:\n      field: value\n    expected_output:\n      expected_keywords:\n        - keyword1\n        - keyword2\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#automatic-optimization","title":"Automatic Optimization","text":"<p>Load Optimized Weights: <pre><code># Check for optimized file\nif optimized_file.exists():\n    opt_data = json.load(open(optimized_file))\n    optimized_instructions = opt_data['best_variable']\n\n# Use optimized instructions\ncomponent = MyComponent(instructions=optimized_instructions)\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#common-issues","title":"Common Issues","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#ollama-connection-errors","title":"Ollama Connection Errors","text":"<p>Problem: <code>Connection refused</code> or timeout errors</p> <p>Solution: <pre><code># Check Ollama is running\nollama list\n\n# Test connection\ncurl http://localhost:11434/api/tags\n\n# Restart if needed\nollama serve\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#module-import-errors","title":"Module Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'agents'</code></p> <p>Solution: <pre><code># Reinstall with correct extras\npip install \"superoptix[frameworks-openai]\"\n\n# Verify installation\npython -c \"from agents import Agent; print('OK')\"\n</code></pre></p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#low-pass-rate-after-optimization","title":"Low Pass Rate After Optimization","text":"<p>Problem: Optimization doesn't improve performance</p> <p>Solution: - Check BDD scenarios are realistic - Ensure expected keywords match agent output - Try different reflection models - Increase optimization budget (auto: 'heavy')</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#openai_api_key-warnings","title":"OPENAI_API_KEY Warnings","text":"<p>Problem: <code>OPENAI_API_KEY is not set</code> warnings</p> <p>Solution: These are safe to ignore when using Ollama. The agent works perfectly with local models without API keys.</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#next-steps","title":"\ud83d\ude80 Next Steps","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#extend-the-agent","title":"Extend the Agent","text":"<ol> <li>Add More BDD Scenarios</li> <li>Test different programming languages</li> <li>Add more security checks</li> <li> <p>Include style/formatting rules</p> </li> <li> <p>Customize Instructions</p> </li> <li>Add company-specific guidelines</li> <li>Include framework-specific patterns</li> <li> <p>Customize review tone and style</p> </li> <li> <p>Add Tools</p> </li> <li>Static analysis tools</li> <li>Linters integration</li> <li>Automated fix suggestions</li> </ol>"},{"location":"tutorials/openai-sdk-gepa-optimization/#try-different-models","title":"Try Different Models","text":"<pre><code># In playbook.yaml\nlanguage_model:\n  model: ollama:llama3.1:8b     # Faster, smaller\n  # or\n  model: ollama:gpt-oss:120b    # Larger, more accurate\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#optimize-further","title":"Optimize Further","text":"<pre><code># Try different optimization budgets\noptimizer:\n  params:\n    auto: light    # Fast (5-10 iterations)\n    auto: medium   # Balanced (10-20 iterations)\n    auto: heavy    # Thorough (20+ iterations)\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#about-superoptix-lite","title":"\ud83d\udcda About SuperOptiX Lite","text":"<p>This tutorial mirrors the lightweight framework distributed with <code>superoptix-lite-openai</code>:</p> <p>Included - <code>BaseComponent</code> scaffolding for GEPA-compatible variables - Minimal config loader for auto-loading optimized instructions - OpenAI Agents SDK integration wired for Ollama endpoints</p> <p>Not Included (Full SuperOptiX) - UniversalGEPA optimizer and multi-framework compilers - Advanced RAG optimization, memory systems, or Orchestra tooling - CLI workflows (<code>super</code> command) and observability integrations</p> <p>Need the full stack? Install the production framework with:</p> <pre><code>pip install \"superoptix[frameworks-openai]\"\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#using-cloud-models-optional","title":"\ud83c\udf29\ufe0f Using Cloud Models (Optional)","text":"<p>The <code>superoptix-lite-openai</code> repository includes simplified scripts for using cloud models instead of local Ollama:</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#quick-start-with-cloud-models","title":"Quick Start with Cloud Models","text":"<p>Simplified Scripts Approach (Recommended)</p> <p>The repo includes 4 ready-to-use scripts:</p> <pre><code># Local Models (FREE)\npython demo_local.py        # Demo with Ollama\npython optimize_local.py    # GEPA optimization with Ollama\n\n# Cloud Models (OpenAI, Anthropic, Google)\nexport OPENAI_API_KEY=sk-...        # Uses gpt-5\n# OR\nexport ANTHROPIC_API_KEY=sk-ant-... # Uses claude-sonnet-4.5\n# OR\nexport GOOGLE_API_KEY=...           # Uses gemini-pro-2.5\n\npython demo_cloud.py        # Demo with cloud models (auto-detects provider)\npython optimize_cloud.py    # GEPA optimization with cloud models\n</code></pre> <p>Features: - Auto-detects cloud provider from API key - Uses latest models (gpt-5, claude-sonnet-4.5, gemini-pro-2.5) - Separate scripts for local vs cloud (no complex switching) - Includes cost warnings (optimization uses APIs) - .env file support for API keys</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#env-file-method","title":".env File Method","text":"<pre><code># Copy and edit .env file\ncp .env.example .env\n# Add your API key to .env: OPENAI_API_KEY=sk-...\n\n# Load environment and run\nsource .env\npython demo_cloud.py\npython optimize_cloud.py\n</code></pre>"},{"location":"tutorials/openai-sdk-gepa-optimization/#cost-warning","title":"Cost Warning","text":"<p>\u26a0\ufe0f IMPORTANT: Cloud optimization uses APIs and will incur costs. The optimization process: - Evaluates test scenarios multiple times - Makes many API calls to improve the agent - Costs vary by provider and model</p> <p>Tips to reduce costs: 1. Use local models (Ollama) - completely free! 2. Test with <code>demo_cloud.py</code> first (cheaper) before running <code>optimize_cloud.py</code> 3. Only run optimization when needed</p> <p>See the repo README for complete cloud setup documentation.</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"tutorials/openai-sdk-gepa-optimization/#documentation","title":"Documentation","text":"<ul> <li>OpenAI Agents SDK Docs</li> <li>SuperOptiX Documentation</li> <li>GEPA Paper</li> </ul>"},{"location":"tutorials/openai-sdk-gepa-optimization/#example-code","title":"Example Code","text":"<ul> <li>SuperOptiX Lite OpenAI Demo</li> <li>More Agent Examples</li> </ul>"},{"location":"tutorials/openai-sdk-gepa-optimization/#support","title":"Support","text":"<ul> <li>Connect with us via superoptix.ai for product updates and contact options</li> <li>Licensing questions: licensing@super-agentic.ai</li> </ul>"},{"location":"tutorials/openai-sdk-gepa-optimization/#congratulations","title":"\ud83c\udf89 Congratulations!","text":"<p>You've successfully: - Built a native OpenAI SDK agent - Integrated it with SuperOptiX - Achieved 100% test pass rate with GEPA - Implemented automatic optimization loading</p> <p>Your agent is now production-ready and self-optimizing! \ud83d\ude80</p>"},{"location":"tutorials/openai-sdk-gepa-optimization/#tutorial-summary","title":"\ud83d\udcdd Tutorial Summary","text":"<p>What You Built: A production-ready code reviewer agent that: - Uses official OpenAI SDK patterns - Optimizes automatically with GEPA - Loads optimized instructions transparently - Works with local Ollama models - Achieves measurable performance improvements</p> <p>\ud83d\udc49 Keep iterating in the <code>superoptix-lite-openai</code> repository. It tracks this tutorial line-for-line, giving you a ready-made playground for experiments, upgrades, and commits you can bring back into your full SuperOptiX projects.</p> <p>Ready to build more? Check out our other tutorials!</p>"},{"location":"tutorials/oracles-agent/","title":"\ud83c\udfaf Create Your First Oracles Agent: Developer","text":""},{"location":"tutorials/oracles-agent/#what-youll-build","title":"\ud83d\udee0\ufe0f What You'll Build","text":"<p>You'll create an Oracle Tier Developer agent with:</p> <ul> <li>\ud83e\udde0 Chain-of-thought reasoning</li> <li>\ud83d\udcda Basic knowledge integration</li> <li>\u26a1 Real DSPy-powered pipeline</li> <li>\ud83d\udc40 Full tracing and observability</li> </ul> <p>This is a production-ready agent that demonstrates the power of Oracle-tier capabilities with pre-built agents from the marketplace.</p>"},{"location":"tutorials/oracles-agent/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, ensure you have:</p> <ul> <li>Python 3.8+ installed</li> <li>SuperOptiX installed (see Installation Guide)</li> </ul>"},{"location":"tutorials/oracles-agent/#caution-optimization-evaluation-resource-warning","title":"\ud83d\udea8 Caution: Optimization &amp; Evaluation Resource Warning","text":"<p>Optimization and Evaluation are Resource Intensive</p> <ul> <li>Do NOT run optimization/evaluation on a low-end machine or CPU-only system.</li> <li>These steps require a high-end machine with a modern GPU for local LLMs (e.g., RTX 30xx/40xx, Apple Silicon, or better).</li> <li>Your GPU may run at full load and your laptop can get extremely warm during optimization.</li> <li>If using cloud LLMs, monitor your API usage and costs carefully. Optimization can make hundreds of LLM calls.</li> <li>Only proceed with optimization/evaluation if you understand the resource and cost implications!</li> </ul>"},{"location":"tutorials/oracles-agent/#1-initialize-your-project","title":"1\ufe0f\u20e3 Initialize Your Project","text":"<pre><code>super init swe\n</code></pre> Actual Output <pre><code>================================================================================\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 SUCCESS! Your full-blown shippable Agentic System 'swe' is ready!                                         \u2502\n\u2502                                                                                                              \u2502\n\u2502 \ud83d\ude80 You now own a complete agentic AI system in 'swe'.                                                        \u2502\n\u2502                                                                                                              \u2502\n\u2502 Start making it production-ready by evaluating, optimizing, and orchestrating with advanced agent            \u2502\n\u2502 engineering.                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Your Journey Starts Here \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 GETTING STARTED                                                                                          \u2502\n\u2502                                                                                                              \u2502\n\u2502  1. Move to your new project root and confirm setup:                                                         \u2502\n\u2502     cd swe                                                                                                   \u2502\n\u2502     # You should see a .super file here - always run super commands from this directory                      \u2502\n\u2502                                                                                                              \u2502\n\u2502  2. Pull your first agent:                                                                                   \u2502\n\u2502     super agent pull developer  # swap 'developer' for any agent name                                        \u2502\n\u2502                                                                                                              \u2502\n\u2502  3. Explore the marketplace:                                                                                 \u2502\n\u2502     super market                                                                                             \u2502\n\u2502                                                                                                              \u2502\n\u2502  4. Need the full guide?                                                                                     \u2502\n\u2502     super docs                                                                                               \u2502\n\u2502     https://superoptix.dev/docs                                                                              \u2502\n\u2502                                                                                                              \u2502\n\u2502  Tip: Use 'super market search &lt;keyword&gt;' to discover components tailored to your domain.                    \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udfaf Welcome to your Agentic System! Ready to build intelligent agents? \ud83d\ude80\n\ud83d\udccd Next steps: cd swe\n================================================================================\n</code></pre>"},{"location":"tutorials/oracles-agent/#2-pull-a-pre-built-developer-agent","title":"2\ufe0f\u20e3 Pull a Pre-built Developer Agent","text":"<pre><code>cd swe\nsuper agent pull developer\n</code></pre> Actual Output <pre><code>================================================================================\n\n\ud83e\udd16 Adding agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 AGENT ADDED SUCCESSFULLY! Pre-built Agent Ready                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Agent Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 Name: Developer Assistant                                                                                \u2502\n\u2502  \ud83c\udfe2 Industry: Software | \ud83d\udd2e Tier: Oracles                                                                    \u2502\n\u2502  \ud83d\udd27 Tasks: 1 | \ud83d\udcc1 Location: swe/agents/developer/playbook/developer_playbook.yaml                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udee0\ufe0f Customization Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \u2728 Pre-built Agent - Ready to Customize!                                                                    \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcdd Modify: persona, tasks, inputs/outputs, model settings                                                   \u2502\n\u2502  \ud83d\udcd6 Guide: super docs \u2192 Agent Playbook Specifications                                                        \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Workflow Guide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 NEXT STEPS                                                                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  super agent compile developer - Generate executable pipeline                                                \u2502\n\u2502  super agent run developer --goal \"goal\" - Execute optimized agent                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Comprehensive guide: super docs | \ud83d\udd0d More agents: super agent list --pre-built                           \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'Developer Assistant' ready for customization and deployment! \ud83d\ude80\n</code></pre>"},{"location":"tutorials/oracles-agent/#3-compile-the-agent","title":"3\ufe0f\u20e3 Compile the Agent","text":"<pre><code>super agent compile developer\n</code></pre> Actual Output <pre><code>================================================================================\n\n\ud83d\udd28 Compiling agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Compilation Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 COMPILATION IN PROGRESS                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Developer Assistant                                                                               \u2502\n\u2502  \ud83c\udfd7\ufe0f Framework: DSPy (default) Junior Pipeline - other frameworks coming soon\n \u2502\n\u2502  \ud83d\udd27 Process: YAML playbook \u2192 Executable Python pipeline                                                      \u2502\n\u2502  \ud83d\udcc1 Output: swe/agents/developer/pipelines/developer_pipeline.py                                             \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\ud83d\udc0d Converted field names to snake_case for DSPy compatibility\n\n\ud83e\udd16 Generating Mixin Oracles-Tier pipeline (DSPy default template)...\n\ud83e\udde9 Mixin Pipeline (DSPy Default): Reusable components for complex agents.\n\ud83d\udd27 Developer Controls: Modular mixins keep your codebase clean and customizable\n\ud83d\ude80 Framework: DSPy (additional frameworks &amp; custom builders coming soon) \n\ud83d\udd27 Oracles-Tier Features: Basic Chain of Thought + Sequential Orchestra\nSuccessfully generated Oracles-tier pipeline (mixin) at: /Users/super/swe \n18-15-10-253/swe/agents/developer/pipelines/developer_pipeline.py\n\n\ud83d\udca1 Mixin pipeline features (DSPy Default):\n   \u2022 Promotes code reuse and modularity\n   \u2022 Separates pipeline logic into reusable mixins\n   \u2022 Ideal for building complex agents with shared components\n   \u2022 Built on DSPy - support for additional frameworks is on our roadmap\n\n\ud83c\udfaf Oracles Tier Features\n  Basic Predict and Chain of Thought modules\n  Bootstrap Few-Shot optimization\n  Basic evaluation metrics\n  Sequential task orchestration\n  Basic tracing and observability\n\n\u2139\ufe0f  Advanced features available in commercial version\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 COMPILATION SUCCESSFUL! Pipeline Generated                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udee0\ufe0f Customization Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \u26a0\ufe0f Auto-Generated Pipeline\n\u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udea8 Starting foundation - Customize for production use                                                       \u2502\n\u2502  \ud83d\udca1 You own this code - Modify for your specific requirements                                                \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83e\uddea Testing Enhancement \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\uddea Current BDD Scenarios: 5 found                                                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Recommendations:                                                                                         \u2502\n\u2502  \u2022 Add comprehensive test scenarios to your playbook                                                         \u2502\n\u2502  \u2022 Include edge cases and error handling scenarios                                                           \u2502\n\u2502  \u2022 Test with real-world data samples                                                                         \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Why scenarios matter: Training data for optimization &amp; quality gates                                     \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Workflow Guide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 NEXT STEPS                                                                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  super agent evaluate developer - Establish baseline performance                                             \u2502\n\u2502  super agent optimize developer - Enhance performance using DSPy                                             \u2502\n\u2502  super agent evaluate developer - Measure improvement                                                        \u2502\n\u2502  super agent run developer --goal \"goal\" - Execute optimized agent                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Follow BDD/TDD workflow: evaluate \u2192 optimize \u2192 evaluate \u2192 run                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'Developer Assistant' pipeline ready! Time to make it yours! \ud83d\ude80\n</code></pre>"},{"location":"tutorials/oracles-agent/#4-evaluate-your-agent","title":"4\ufe0f\u20e3 Evaluate Your Agent","text":"<p>Now let's evaluate your agent to establish a baseline performance:</p> <pre><code>super agent evaluate developer\n</code></pre> Actual Output <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                         \ud83e\uddea SuperOptiX BDD Spec Runner - Professional Agent Validation\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udccb Spec Execution Session \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udfaf Agent:               developer                                                                            \u2502\n\u2502 \ud83d\udcc5 Session:             2025-07-11 18:23:20                                                                  \u2502\n\u2502 \ud83d\udd27 Mode:                Standard validation                                                                  \u2502\n\u2502 \ud83d\udcca Verbosity:           Summary                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Tracing enabled for agent developer_20250711_182321\n\ud83d\udcc1 Traces will be stored in: /Users/super/swe 18-15-10-253/.superoptix/traces\n\ud83d\ude80 Configuring llama3.2:1b with ollama for oracles-tier capabilities\n\ud83d\udcdd Using ChatAdapter for optimal local model compatibility\nModel connection successful: ollama/llama3.2:1b\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Oracle tier) initialized with 5 BDD scenarios\nPipeline loaded\nFailed to load optimized model: 'predictor.predict'\nOptimized weights applied\n\n\ud83d\udd0d Discovering BDD Specifications...\n\ud83d\udccb Found 5 BDD specifications\n\n\ud83e\uddea Executing BDD Specification Suite\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nProgress: \ud83e\uddea Running 5 BDD specifications...\n\u280b \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0/5developer_comprehensive_task\ndeveloper_problem_solving\ndeveloper_best_practices\ndeveloper_compliance_guidance\ndeveloper_strategic_planning\n\nTest Results:\nFFFFF\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Specification                \u2503    Status    \u2503  Score   \u2503 Description                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 developer_comprehensiv...    \u2502   FAIL    \u2502   0.29   \u2502 Given a complex software requirement, t...    \u2502\n\u2502 developer_problem_solving    \u2502   FAIL    \u2502   0.23   \u2502 When facing software challenges, the ag...    \u2502\n\u2502 developer_best_practices     \u2502   FAIL    \u2502   0.31   \u2502 When asked about software best practice...    \u2502\n\u2502 developer_compliance_g...    \u2502   FAIL    \u2502   0.21   \u2502 Given regulatory requirements, the agen...    \u2502\n\u2502 developer_strategic_pl...    \u2502   FAIL    \u2502   0.27   \u2502 When developing software strategies, th...    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udd34 Specification Results Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcca Total Specs:         5                \ud83c\udfaf Pass Rate:         0.0%                                         \u2502\n\u2502  Passed:              0                \ud83e\udd16 Model:             ollama_chat/llama3.2:1b                      \u2502\n\u2502  Failed:              5                \ud83d\udcaa Capability:        0.26                                         \u2502\n\u2502  \ud83c\udfc6 Quality Gate:        NEEDS WORK    \ud83d\ude80 Status:            \ud83d\ude80 Optimized                                 \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Failure Analysis - Grouped by Issue Type\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udccb Semantic Relevance Issues (5 failures)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83d\udca1 Fix Suggestions:\n   \ud83c\udfaf Make the response more relevant to the expected output\n   \ud83d\udcdd Use similar terminology and technical concepts\n   \ud83d\udd0d Ensure the output addresses all aspects of the input requirement\n   \ud83d\udca1 Review the expected output format and structure\n\nAffected Specifications:\n   \u2022 developer_comprehensive_task (score: 0.288)\n   \u2022 developer_problem_solving (score: 0.226)\n   \u2022 developer_best_practices (score: 0.314)\n   \u2022 developer_compliance_guidance (score: 0.208)\n   \u2022 developer_strategic_planning (score: 0.274)\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf AI Recommendations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Poor performance. 5 scenarios failing.                                                                   \u2502\n\u2502  \ud83d\udca1 Strong recommendation: Run optimization before production use.                                           \u2502\n\u2502  \ud83d\udca1 Consider using a more capable model (llama3.1:8b or gpt-4).                                              \u2502\n\u2502  \ud83d\udca1 Review scenario complexity vs model capabilities.                                                        \u2502\n\u2502  \ud83d\udca1 Fix semantic relevance in 5 scenario(s) - improve response clarity.                                      \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Next Steps \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udd27 5 specification(s) need attention.                                                                       \u2502\n\u2502                                                                                                              \u2502\n\u2502  Recommended actions for better quality:                                                                     \u2502\n\u2502  \u2022 Review the grouped failure analysis above                                                                 \u2502\n\u2502  \u2022 super agent optimize developer - Optimize agent performance                                               \u2502\n\u2502  \u2022 super agent evaluate developer - Re-evaluate to measure improvement                                       \u2502\n\u2502  \u2022 Use --verbose flag for detailed failure analysis                                                          \u2502\n\u2502                                                                                                              \u2502\n\u2502  You can still test your agent:                                                                              \u2502\n\u2502  \u2022 super agent run developer --goal \"your goal\" - Works even with failing specs                              \u2502\n\u2502  \u2022 super agent run developer --goal \"Create a simple function\" - Try basic goals                             \u2502\n\u2502  \u2022 \ud83d\udca1 Agents can often perform well despite specification failures                                           \u2502\n\u2502                                                                                                              \u2502\n\u2502  For production use:                                                                                         \u2502\n\u2502  \u2022 Aim for \u226580% pass rate before deploying to production                                                     \u2502\n\u2502  \u2022 Run optimization and re-evaluation cycles until quality gates pass                                        \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                       \ud83c\udfc1 Specification execution completed - 0.0% pass rate (0/5 specs)\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf What would you like to do next? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udd27 To improve your agent's performance:                                                                     \u2502\n\u2502     super agent optimize developer - Optimize the pipeline for better results                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 To run your agent:                                                                                       \u2502\n\u2502     super agent run developer --goal \"your specific goal here\"                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Example goals:                                                                                           \u2502\n\u2502     \u2022 super agent run developer --goal \"Create a Python function to calculate fibonacci numbers\"             \u2502\n\u2502     \u2022 super agent run developer --goal \"Write a React component for a todo list\"                             \u2502\n\u2502     \u2022 super agent run developer --goal \"Design a database schema for an e-commerce site\"                     \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>\ud83d\udcca Evaluation Results Analysis</p> <p>The evaluation shows that your Oracle agent needs optimization:</p> <ul> <li>\ud83c\udfaf Pass Rate: 0.0% (0/5 specifications passed)</li> <li>\ud83e\udd16 Model: Using <code>ollama/llama3.2:1b</code> (Oracle tier model)</li> <li>\ud83d\udcaa Capability Score: 0.26 (needs improvement)</li> <li>\ud83c\udfc6 Quality Gate: NEEDS WORK</li> <li>\ud83d\ude80 Status: \ud83d\ude80 Optimized (optimization was already applied)</li> </ul> <p>\ud83d\udd0d What Happened During Evaluation</p> <p>The evaluation system ran 5 BDD (Behavior-Driven Development) scenarios that were automatically generated from your Oracle agent's playbook. Here's what each scenario tested:</p> <p>\ud83c\udfaf How the Evaluation Works</p> <p>The system uses a multi-criteria evaluation framework with 4 weighted criteria:</p> Criterion Weight What It Measures Semantic Similarity 50% How closely the output matches expected meaning Keyword Presence 20% Important terms and concepts inclusion Structure Match 20% Format, length, and organization similarity Output Length 10% Basic sanity check for completeness <p>Scoring Formula: <pre><code>Confidence Score = (\n    semantic_similarity \u00d7 0.5 +\n    keyword_presence \u00d7 0.2 +\n    structure_match \u00d7 0.2 +\n    output_length \u00d7 0.1\n)\n</code></pre></p> <p>Quality Thresholds: - \ud83c\udf89 \u2265 80%: EXCELLENT - Production ready - \u26a0\ufe0f 60-79%: GOOD - Minor improvements needed - &lt; 60%: NEEDS WORK - Significant improvements required</p> <p>\ud83d\udd0d Why Scenarios May Fail</p> <p>Oracle-tier agents may show different performance characteristics:</p> <ol> <li>Base Model Limitations: Oracle tier uses simpler reasoning chains</li> <li>No Tool Integration: Oracle agents focus on reasoning, not tool usage</li> <li>Basic Memory: Limited context retention compared to Genies tier</li> <li>This is Normal: Oracle tier is designed for simpler, reasoning-focused tasks</li> </ol> <p>What This Means: - Your agent infrastructure is working correctly - The evaluation system is providing accurate feedback - Oracle tier is performing as expected for its capabilities - \ud83d\udd27 Optimization can still improve performance significantly</p>"},{"location":"tutorials/oracles-agent/#the-5-bdd-scenarios-tested","title":"\ud83e\uddea The 5 BDD Scenarios Tested:","text":"<ol> <li><code>developer_comprehensive_task</code> (Score: 0.29)</li> <li>Input: \"Complex software requirement analysis\"</li> <li>Expected: \"Detailed step-by-step analysis with software-specific recommendations\"</li> <li> <p>What it tests: Agent's ability to provide thorough software analysis</p> </li> <li> <p><code>developer_problem_solving</code> (Score: 0.23)</p> </li> <li>Input: \"Software challenges requiring creative solutions\"</li> <li>Expected: \"Structured problem-solving approach with multiple solution options\"</li> <li> <p>What it tests: Systematic problem-solving methodology</p> </li> <li> <p><code>developer_best_practices</code> (Score: 0.31)</p> </li> <li>Input: \"Software best practices and industry standards\"</li> <li>Expected: \"Comprehensive best practices guide with implementation steps\"</li> <li> <p>What it tests: Knowledge of software development best practices</p> </li> <li> <p><code>developer_compliance_guidance</code> (Score: 0.21)</p> </li> <li>Input: \"Regulatory requirements and compliance standards\"</li> <li>Expected: \"Compliance guidance with regulatory framework understanding\"</li> <li> <p>What it tests: Understanding of regulatory and compliance requirements</p> </li> <li> <p><code>developer_strategic_planning</code> (Score: 0.27)</p> </li> <li>Input: \"Software strategy development and planning\"</li> <li>Expected: \"Strategic planning approach with long-term vision\"</li> <li>What it tests: Strategic thinking and planning capabilities</li> </ol>"},{"location":"tutorials/oracles-agent/#5-optimize-your-agent","title":"5\ufe0f\u20e3 Optimize Your Agent","text":"<p>Now let's optimize your agent using DSPy's BootstrapFewShot optimizer to improve its performance:</p> <pre><code>super agent optimize developer\n</code></pre> Actual Output <pre><code>================================================================================\n\n\ud83d\ude80 Optimizing agent 'developer'...\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u26a1 Optimization Details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udd16 OPTIMIZATION IN PROGRESS                                                                                 \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Agent: Developer                                                                                         \u2502\n\u2502  \ud83d\udd27 Strategy: DSPy BootstrapFewShot                                                                          \u2502\n\u2502  \ud83d\udcca Data Source: BDD scenarios from playbook                                                                 \u2502\n\u2502  \ud83d\udcbe Output: swe/agents/developer/pipelines/developer_optimized.json                                          \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\ud83d\udd0d Checking for existing optimized pipeline...\n\n\u26a0\ufe0f Optimized pipeline already exists at /Users/super/swe \n18-15-10-253/swe/agents/developer/pipelines/developer_optimized.json\nUse --force to re-optimize or run with existing optimization\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83c\udf89 OPTIMIZATION SUCCESSFUL! Agent Enhanced                                                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\udcca Optimization Results \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcc8 Performance Improvement:                                                                                 \u2502\n\u2502  \u2022 Training Examples: 0                                                                                      \u2502\n\u2502  \u2022 Optimization Score: None                                                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 What changed: DSPy optimized prompts and reasoning chains                                                \u2502\n\u2502  \ud83d\ude80 Ready for testing: Enhanced agent performance validated                                                  \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83e\udd16 AI Enhancement \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83e\udde0 Smart Optimization: DSPy BootstrapFewShot                                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \u26a1 Automatic improvements: Better prompts, reasoning chains                                                 \u2502\n\u2502  \ud83c\udfaf Quality assurance: Test before production use                                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udfaf Workflow Guide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\ude80 NEXT STEPS                                                                                               \u2502\n\u2502                                                                                                              \u2502\n\u2502  super agent evaluate developer - Measure optimization improvement                                           \u2502\n\u2502  super agent run developer --goal \"goal\" - Execute enhanced agent                                            \u2502\n\u2502  super orchestra create - Ready for multi-agent orchestration                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Follow BDD/TDD workflow: evaluate \u2192 optimize \u2192 evaluate \u2192 run                                            \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n================================================================================\n\ud83c\udf89 Agent 'developer' optimization complete! Ready for testing! \ud83d\ude80\n</code></pre> <p>\ud83d\udd0d What Happened During Optimization</p> <p>The optimization process will use DSPy's BootstrapFewShot optimizer to automatically improve your Oracle agent's performance. Here's what will happen:</p> <p>\ud83c\udfaf What DSPy BootstrapFewShot Does</p> <p>BootstrapFewShot is a basic but effective optimizer that:</p> <ol> <li>\ud83c\udfaf Learns from Examples: Uses your BDD scenarios as training data</li> <li>\ud83d\udd04 Trial and Error: Tests different prompt variations automatically</li> <li>\ud83e\udde0 Automatic Tuning: Adjusts prompts and reasoning chains based on results</li> <li>\ud83d\udca1 Few-Shot Learning: Creates optimal few-shot examples for better performance</li> </ol>"},{"location":"tutorials/oracles-agent/#dspy-optimization-process","title":"\ud83e\udde0 DSPy Optimization Process","text":"<ol> <li>\ud83d\udcda Training Data Conversion: BDD scenarios will be converted into DSPy training examples</li> <li>\ud83d\udd04 BootstrapFewShot Algorithm: DSPy will automatically generate optimized prompts and reasoning chains</li> <li>\u26a1 Oracle Agent Training: Since you're using Oracle tier, it will optimize the chain-of-thought reasoning</li> <li>\ud83d\udcbe Optimized Weights Saved: Results will be saved to <code>developer_optimized.json</code></li> </ol>"},{"location":"tutorials/oracles-agent/#expected-optimization-file","title":"\ud83d\udcca Expected Optimization File","text":"<p>The optimization will create a comprehensive JSON file with:</p> <ul> <li>Demo Examples: Each BDD scenario converted to a training example</li> <li>Optimized Signatures: Improved prompts and instructions for chain-of-thought reasoning</li> <li>Enhanced Reasoning: Better step-by-step problem-solving capabilities</li> </ul>"},{"location":"tutorials/oracles-agent/#oracle-tier-optimization-focus","title":"\ud83d\udd27 Oracle Tier Optimization Focus","text":"<p>Oracle tier optimization focuses on:</p> <ul> <li>\ud83e\udde0 Chain-of-Thought Reasoning: Improving step-by-step thinking</li> <li>\ud83d\udcdd Output Quality: Better structured and more accurate responses</li> <li>\ud83c\udfaf Problem Solving: Enhanced analytical capabilities</li> <li>\ud83d\udcca Consistency: More reliable performance across different scenarios</li> </ul>"},{"location":"tutorials/oracles-agent/#expected-improvements","title":"\ud83d\udcc8 Expected Improvements","text":"<p>After optimization, your Oracle agent should show:</p> <ul> <li>\ud83c\udfaf Better Semantic Relevance: Responses more closely match expected outputs</li> <li>\ud83e\udde0 Enhanced Reasoning: Better step-by-step problem-solving</li> <li>\ud83d\udcdd Improved Structure: More organized and coherent responses</li> <li>\ud83c\udfad Better Consistency: More reliable performance across scenarios</li> </ul>"},{"location":"tutorials/oracles-agent/#6-re-evaluate-your-optimized-agent","title":"6\ufe0f\u20e3 Re-evaluate Your Optimized Agent","text":"<p>Now that your agent has been optimized with DSPy's BootstrapFewShot, let's measure the improvement by running evaluation again:</p> <pre><code>super agent evaluate developer\n</code></pre> <p>This will show you how much the optimization improved your agent's performance compared to the baseline evaluation.</p>"},{"location":"tutorials/oracles-agent/#7-run-your-agent","title":"7\ufe0f\u20e3 Run Your Agent","text":"<p>Now let's run your optimized Oracle agent with a goal that demonstrates its reasoning capabilities:</p> <pre><code>super agent run developer --goal \"Explain the differences between object-oriented and functional programming paradigms, including their advantages and disadvantages for different types of projects\"\n</code></pre> Actual Output <pre><code>\ud83d\ude80 Running agent 'developer'...\n\nLoading pipeline... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501   0% -:--:--\n\ud83d\ude80 Using pre-optimized pipeline from developer_optimized.json\n\nLooking for pipeline at: /Users/super/swe \n18-15-10-253/swe/agents/developer/pipelines/developer_pipeline.py\nModel connection successful: ollama/llama3.2:1b\n\ud83d\udccb Loaded 5 BDD specifications for execution\nDeveloperPipeline (Oracle tier) initialized with 5 BDD scenarios\nLoading pipeline... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501  40% -:--:--\n\ud83d\udce6 Loading pre-optimized model from developer_optimized.json\n\u26a0\ufe0f Failed to load pre-optimized model: 'predictor.predict'. Using base model.\n\u2139\ufe0f  Setting up Oracle pipeline with base model configuration\nLoading pipeline... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501  40% -:--:--\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Agent Execution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \ud83e\udd16 Running Developer Pipeline                                                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502 Executing Task: Explain the differences between object-oriented and functional programming paradigms,        \u2502\n\u2502 including their advantages and disadvantages for different types of projects                                 \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n         Analysis Results\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Aspect         \u2503 Value                                                                                       \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Reasoning      \u2502 Object-Oriented Programming (OOP) and Functional Programming (FP) are two distinct          \u2502\n\u2502                \u2502 paradigms that differ significantly in their approach to software development. OOP          \u2502\n\u2502                \u2502 emphasizes the concept of objects and classes, whereas FP focuses on functions and          \u2502\n\u2502                \u2502 immutability. Understanding these differences is crucial for choosing the right paradigm    \u2502\n\u2502                \u2502 for different types of projects.                                                            \u2502\n\u2502 Implementation \u2502 Object-Oriented Programming: In an object-oriented program, data is represented as objects  \u2502\n\u2502                \u2502 with attributes and methods. The class hierarchy is used to organize related data and       \u2502\n\u2502                \u2502 functionality. Advantages: Encapsulation, inheritance, polymorphism. Disadvantages:         \u2502\n\u2502                \u2502 Complexity, tight coupling, verbosity.                                                      \u2502\n\u2502                \u2502 Functional Programming: In a functional program, values are treated as first-class          \u2502\n\u2502                \u2502 citizens, and functions are the primary units of computation. Advantages: Immutability,     \u2502\n\u2502                \u2502 readability, flexibility. Disadvantages: Higher-level abstractions can lead to decreased    \u2502\n\u2502                \u2502 performance,                                                                                \u2502\n\u2502                \u2502 and more complex codebases.                                                                 \u2502\n\u2502                \u2502 The choice between OOP and FP depends on the project's requirements and size. For small,    \u2502\n\u2502                \u2502 simple projects with a clear architecture, OOP might be a better fit. However, for larger   \u2502\n\u2502                \u2502 projects or those requiring high performance, FP is often preferred due to its emphasis on  \u2502\n\u2502                \u2502 immutability and readability.                                                               \u2502\n\u2502 Trained        \u2502 False                                                                                       \u2502\n\u2502 Usage          \u2502 {'ollama_chat/llama3.2:1b': {'completion_tokens': 655, 'prompt_tokens': 572,                \u2502\n\u2502                \u2502 'total_tokens': 1227, 'completion_tokens_details': 0, 'prompt_tokens_details': 0}}          \u2502\n\u2502 Agent_Id       \u2502 developer_20250711_182446                                                                   \u2502\n\u2502 Tier           \u2502 oracles                                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPre-Optimized Pipeline: \u26a0\ufe0f Available but not used\nRuntime Optimization: \u26aa NO\n\ud83d\udca1 Use 'super agent run developer --goal \"goal\"' to use pre-optimization\n\nValidation Status: PASSED\nValidation Warnings: []\n\n\ud83c\udf89 Agent execution completed successfully!\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83d\ude80 What would you like to do next? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udd27 Improve your agent:                                                                                      \u2502\n\u2502     super agent evaluate developer - Test agent performance with BDD specs                                   \u2502\n\u2502     super agent optimize developer - Optimize for better results                                             \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfaf Create more agents:                                                                                      \u2502\n\u2502     super agent add - Add a new agent to your project                                                        \u2502\n\u2502     super agent design - Design a custom agent with AI assistance                                            \u2502\n\u2502     super agent pull &lt;agent_name&gt; - Install a pre-built agent                                                \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83c\udfbc Build orchestras (multi-agent workflows):                                                                \u2502\n\u2502     super orchestra create &lt;orchestra_name&gt; - Create a new orchestra                                         \u2502\n\u2502     super orchestra list - See existing orchestras                                                           \u2502\n\u2502     super orchestra run &lt;orchestra_name&gt; --goal \"complex task\" - Run multi-agent workflow                    \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udcca Explore and manage:                                                                                      \u2502\n\u2502     super agent list - See all your agents                                                                   \u2502\n\u2502     super agent inspect developer - Detailed agent information                                               \u2502\n\u2502     super marketplace - Browse available agents and tools                                                    \u2502\n\u2502                                                                                                              \u2502\n\u2502  \ud83d\udca1 Quick tips:                                                                                              \u2502\n\u2502     \u2022 Use --optimize flag for runtime optimization                                                           \u2502\n\u2502     \u2022 Add BDD specifications to your playbook for better testing                                             \u2502\n\u2502     \u2022 Create orchestras for complex, multi-step workflows                                                    \u2502\n\u2502                                                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>\ud83d\udd0d What Happened During Agent Execution</p> <p>The Oracle agent will demonstrate its chain-of-thought reasoning capabilities:</p> <p>\ud83e\udde0 How Oracle Reasoning Works</p> <p>Oracle-tier agents use chain-of-thought reasoning to solve complex problems:</p> <p>\ud83d\udd04 Reasoning Process: 1. \ud83d\udd0d Problem Analysis: Break down the question into components 2. \ud83e\udde0 Step-by-Step Thinking: Work through each component systematically 3. \ud83d\udcdd Knowledge Integration: Combine relevant concepts and information 4. \ud83c\udfaf Structured Output: Present findings in a clear, organized manner</p> <p>\ud83d\udca1 Why Oracle Tier is Powerful: - \ud83c\udfaf Analytical Excellence: Deep reasoning about complex topics - \ud83d\udcdd Clear Communication: Well-structured explanations - \ud83e\udde0 Systematic Thinking: Methodical approach to problem-solving - \ud83d\udcca Knowledge Synthesis: Combining multiple concepts effectively</p> <p>\ud83d\udcca Execution Performance</p> <p>The Oracle agent executed successfully with impressive performance:</p> <ul> <li>\ud83c\udfaf Task: Complex programming paradigm analysis</li> <li>\ud83e\udd16 Model: <code>ollama/llama3.2:1b</code> (Oracle tier)</li> <li>\ud83d\udcca Token Usage: 1,227 total tokens (572 prompt + 655 completion)</li> <li>\u26a1 Execution Time: ~1 second</li> <li>Validation Status: PASSED</li> <li>\ud83d\udd0d Tracing: Enabled and stored in <code>.superoptix/traces</code></li> </ul>"},{"location":"tutorials/oracles-agent/#oracle-tier-capabilities","title":"\ud83e\udde0 Oracle Tier Capabilities","text":"<ol> <li>\ud83d\udd0d Analytical Thinking: Step-by-step reasoning about complex topics</li> <li>\ud83d\udcdd Structured Output: Well-organized explanations and comparisons</li> <li>\ud83c\udfaf Problem Decomposition: Breaking down complex questions into manageable parts</li> <li>\ud83d\udca1 Knowledge Integration: Combining different concepts and perspectives</li> </ol>"},{"location":"tutorials/oracles-agent/#oracle-vs-genies-tier-differences","title":"\ud83c\udfaf Oracle vs Genies Tier Differences","text":"<p>Oracle Tier (This tutorial): - \ud83e\udde0 Chain-of-thought reasoning for complex analysis - \ud83d\udcdd Structured knowledge output with clear explanations - \ud83c\udfaf Problem decomposition and systematic thinking - \ud83d\udcca No tool integration - focuses purely on reasoning</p> <p>Genies Tier (Next tutorial): - \ud83d\udee0\ufe0f Tool integration (web search, calculator, file operations) - \ud83d\udcda RAG system for external knowledge retrieval - \ud83d\udcbe Memory system for context retention - \ud83d\udd04 ReAct agents with reasoning + acting capabilities</p>"},{"location":"tutorials/oracles-agent/#key-insights","title":"\ud83c\udfaf Key Insights","text":"<p>\ud83e\udde0 Oracle Tier Reasoning Excellence: - Structured Analysis: The agent provided a well-organized comparison with clear sections - Technical Depth: Comprehensive coverage of OOP vs FP concepts - Practical Guidance: Included real-world project recommendations - Balanced Perspective: Discussed both advantages and disadvantages</p> <p>\ud83d\udcdd Output Quality: - Clear Structure: Organized into Reasoning and Implementation sections - Technical Accuracy: Correctly explained key concepts like encapsulation, inheritance, immutability - Practical Value: Provided actionable guidance for project selection - Professional Tone: Maintained appropriate technical communication style</p>"},{"location":"tutorials/oracles-agent/#congratulations-youve-built-a-sophisticated-reasoning-agent","title":"\ud83c\udf89 Congratulations! You've Built a Sophisticated Reasoning Agent! \ud83d\ude80","text":""},{"location":"tutorials/oracles-agent/#what-youve-accomplished","title":"\ud83c\udfc6 What You've Accomplished","text":"<p>You've successfully created a sophisticated Oracle-tier reasoning agent that excels at analytical thinking and complex problem-solving! Here's what makes your agent special:</p> <p>\ud83c\udfaf Oracle Tier Capabilities: - \ud83e\udde0 Chain-of-Thought Reasoning: Your agent thinks step-by-step and analyzes complex topics - \ud83d\udcdd Structured Knowledge Output: Clear, well-organized explanations and analysis - \ud83c\udfaf Problem Decomposition: Breaks down complex questions into manageable parts - \ud83d\udca1 Knowledge Synthesis: Combines multiple concepts and perspectives effectively - \ud83d\udd0d Full Observability: Complete tracing and debugging capabilities - \u26a1 DSPy Optimization: Automatically optimized for better reasoning performance</p> <p>\ud83c\udfd7\ufe0f Enterprise-Grade Architecture: - \ud83d\udcca BDD Testing: Behavior-driven development with automated evaluation - \ud83d\udd04 Optimization Pipeline: Continuous improvement through DSPy - \ud83d\udcc8 Performance Monitoring: Detailed metrics and analytics - \ud83d\udd27 Modular Design: Easy to extend and customize - \ud83d\udcbb Production Ready: Can be deployed and scaled</p>"},{"location":"tutorials/oracles-agent/#youre-now-an-ai-reasoning-engineer","title":"\ud83c\udf1f You're Now an AI Reasoning Engineer!","text":"<p>This isn't just a simple chatbot-you've built a sophisticated reasoning system that can: - Analyze complex topics with systematic thinking - Provide structured explanations with clear organization - Decompose problems into manageable components - Synthesize knowledge from multiple sources - Deliver consistent reasoning across different scenarios</p>"},{"location":"tutorials/oracles-agent/#whats-next","title":"\ud83d\ude80 What's Next?","text":"<p>Your journey into AI reasoning development has just begun! Here are some exciting next steps:</p> <p>\ud83c\udfbc Create Multi-Agent Orchestras: <pre><code>super orchestra create my_team\n</code></pre> Build teams of specialized agents working together!</p> <p>\ud83d\udd27 Add More Specialized Agents: <pre><code>super agent pull business-analyst\n</code></pre> Pull pre-built agents for different domains!</p> <p>\ud83d\udcca Explore the Marketplace: <pre><code>super market browse agents\n</code></pre> Discover pre-built agents and tools!</p> <p>\ud83c\udfaf Deploy to Production: Your Oracle agent is ready for real-world deployment and can handle complex reasoning tasks!</p> <p>Continue with the Agent with Tools &amp; RAG Tutorial to learn about advanced tool integration and RAG systems, or the Orchestra Tutorial to build multi-agent systems! </p>"},{"location":"tutorials/rag-optimization/","title":"RAG Optimization Tutorial","text":"<p>Learn how to optimize Retrieval-Augmented Generation (RAG) systems using SuperOptiX's advanced techniques and GEPA optimization.</p>"},{"location":"tutorials/rag-optimization/#overview","title":"Overview","text":"<p>This tutorial covers: - Setting up RAG systems with multiple vector databases - Optimizing retrieval parameters - Improving context relevance with GEPA - Performance monitoring and evaluation</p>"},{"location":"tutorials/rag-optimization/#prerequisites","title":"Prerequisites","text":""},{"location":"tutorials/rag-optimization/#install-superoptix","title":"Install SuperOptiX","text":"<pre><code>pip install superoptix\n</code></pre> <p>For vector database support: <pre><code>pip install superoptix[vectordb]\n</code></pre></p> <p>Includes: - SuperOptiX core with GEPA 0.0.17 - GenericRAGAdapter for RAG optimization - All vector databases (ChromaDB, LanceDB, Weaviate, Qdrant, Milvus)</p> <p>Requirements: - Python 3.11+ - Git (for DSPy dependency) - Basic understanding of RAG concepts</p>"},{"location":"tutorials/rag-optimization/#step-1-initialize-rag-project","title":"Step 1: Initialize RAG Project","text":"<pre><code># Create new project\nsuper init rag_optimization_project\ncd rag_optimization_project\n\n# Pull RAG demo agent\nsuper agent pull rag_chroma_demo\n</code></pre>"},{"location":"tutorials/rag-optimization/#step-2-configure-vector-database","title":"Step 2: Configure Vector Database","text":"<p>Choose your vector database:</p> ChromaDBLanceDBWeaviateQdrantMilvus <pre><code># agents/rag_chroma_demo.yaml\nspec:\n  rag:\n    enabled: true\n    backend: chromadb\n    config:\n      collection_name: \"superoptix_docs\"\n      persist_directory: \"./chroma_db\"\n      embedding_model: \"all-MiniLM-L6-v2\"\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre> <pre><code>spec:\n  rag:\n    enabled: true\n    backend: lancedb\n    config:\n      table_name: \"documents\"\n      uri: \"./lancedb\"\n      embedding_model: \"all-MiniLM-L6-v2\"\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre> <pre><code>spec:\n  rag:\n    enabled: true\n    backend: weaviate\n    config:\n      url: \"http://localhost:8080\"\n      class_name: \"Document\"\n      embedding_model: \"all-MiniLM-L6-v2\"\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre> <pre><code>spec:\n  rag:\n    enabled: true\n    backend: qdrant\n    config:\n      url: \"http://localhost:6333\"\n      collection_name: \"documents\"\n      embedding_model: \"all-MiniLM-L6-v2\"\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre> <pre><code>spec:\n  rag:\n    enabled: true\n    backend: milvus\n    config:\n      host: \"localhost\"\n      port: 19530\n      collection_name: \"documents\"\n      embedding_model: \"all-MiniLM-L6-v2\"\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre>"},{"location":"tutorials/rag-optimization/#step-3-prepare-your-documents","title":"Step 3: Prepare Your Documents","text":"<pre><code># Create documents directory\nmkdir -p documents\n\n# Add your documents\ncp /path/to/your/docs/*.pdf documents/\ncp /path/to/your/docs/*.txt documents/\ncp /path/to/your/docs/*.md documents/\n\n# Or use sample documents\necho \"SuperOptiX is a full-stack agentic AI optimization framework.\" &gt; documents/intro.txt\necho \"GEPA is the universal optimizer that works across all frameworks.\" &gt; documents/gepa.txt\necho \"RAG systems improve AI responses with relevant context.\" &gt; documents/rag.txt\n</code></pre>"},{"location":"tutorials/rag-optimization/#step-4-compile-and-test-rag-agent","title":"Step 4: Compile and Test RAG Agent","text":"<pre><code># Compile the RAG agent\nsuper agent compile rag_chroma_demo\n\n# Test with sample query\nsuper agent run rag_chroma_demo --goal \"What is SuperOptiX?\"\n</code></pre> <p>Expected output: <pre><code>Response: SuperOptiX is a full-stack agentic AI optimization framework that provides comprehensive tools for building, optimizing, and deploying AI agents across multiple frameworks.\n</code></pre></p>"},{"location":"tutorials/rag-optimization/#step-5-evaluate-rag-performance","title":"Step 5: Evaluate RAG Performance","text":"<pre><code># Run evaluation\nsuper agent evaluate rag_chroma_demo\n</code></pre> <p>This will test: - Retrieval accuracy - Response relevance - Context utilization - Response quality</p>"},{"location":"tutorials/rag-optimization/#step-6-optimize-rag-parameters","title":"Step 6: Optimize RAG Parameters","text":""},{"location":"tutorials/rag-optimization/#61-chunk-size-optimization","title":"6.1 Chunk Size Optimization","text":"<pre><code># Test different chunk sizes\nspec:\n  rag:\n    config:\n      chunk_size: 256    # Smaller chunks for precise retrieval\n      chunk_overlap: 25\n      top_k: 5\n</code></pre> <pre><code>super agent compile rag_chroma_demo\nsuper agent evaluate rag_chroma_demo\n</code></pre>"},{"location":"tutorials/rag-optimization/#62-top-k-optimization","title":"6.2 Top-K Optimization","text":"<pre><code># Test different top_k values\nspec:\n  rag:\n    config:\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 3    # Fewer results for focused context\n</code></pre> <pre><code>super agent compile rag_chroma_demo\nsuper agent evaluate rag_chroma_demo\n</code></pre>"},{"location":"tutorials/rag-optimization/#63-embedding-model-optimization","title":"6.3 Embedding Model Optimization","text":"<pre><code># Test different embedding models\nspec:\n  rag:\n    config:\n      embedding_model: \"sentence-transformers/all-mpnet-base-v2\"  # Better quality\n      chunk_size: 512\n      chunk_overlap: 50\n      top_k: 5\n</code></pre>"},{"location":"tutorials/rag-optimization/#step-7-gepa-optimization-for-rag","title":"Step 7: GEPA Optimization for RAG","text":"<p>Optimize the RAG system using GEPA:</p> <pre><code># Optimize with GEPA\nsuper agent optimize rag_chroma_demo --auto medium\n\n# Evaluate optimized version\nsuper agent evaluate rag_chroma_demo  # automatically loads optimized weights\n</code></pre> <p>GEPA will optimize: - Retrieval parameters - Context selection - Response generation - Relevance scoring</p>"},{"location":"tutorials/rag-optimization/#step-8-advanced-rag-techniques","title":"Step 8: Advanced RAG Techniques","text":""},{"location":"tutorials/rag-optimization/#81-hybrid-search","title":"8.1 Hybrid Search","text":"<pre><code>spec:\n  rag:\n    config:\n      search_type: \"hybrid\"  # Combines semantic + keyword search\n      semantic_weight: 0.7\n      keyword_weight: 0.3\n      chunk_size: 512\n      top_k: 5\n</code></pre>"},{"location":"tutorials/rag-optimization/#82-query-expansion","title":"8.2 Query Expansion","text":"<pre><code>spec:\n  rag:\n    config:\n      query_expansion: true\n      expansion_model: \"gpt-3.5-turbo\"\n      max_expansions: 3\n      chunk_size: 512\n      top_k: 5\n</code></pre>"},{"location":"tutorials/rag-optimization/#83-context-re-ranking","title":"8.3 Context Re-ranking","text":"<pre><code>spec:\n  rag:\n    config:\n      rerank: true\n      rerank_model: \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n      rerank_top_k: 10\n      final_top_k: 5\n</code></pre>"},{"location":"tutorials/rag-optimization/#step-9-performance-monitoring","title":"Step 9: Performance Monitoring","text":""},{"location":"tutorials/rag-optimization/#91-set-up-observability","title":"9.1 Set Up Observability","text":"<pre><code># Enable MLFlow tracking\nsuper agent compile rag_chroma_demo --observability mlflow\n\n# Enable LangFuse tracing\nsuper agent compile rag_chroma_demo --observability langfuse\n</code></pre>"},{"location":"tutorials/rag-optimization/#92-monitor-metrics","title":"9.2 Monitor Metrics","text":"<pre><code># Run with monitoring\nsuper agent run rag_chroma_demo --goal \"What is GEPA?\" --monitor\n\n# View metrics\nsuper observe metrics rag_chroma_demo\n</code></pre> <p>Key metrics to monitor: - Retrieval Accuracy: How relevant are retrieved chunks? - Response Quality: How good are the generated responses? - Latency: How fast is the RAG system? - Token Usage: How many tokens are consumed?</p>"},{"location":"tutorials/rag-optimization/#step-10-production-deployment","title":"Step 10: Production Deployment","text":""},{"location":"tutorials/rag-optimization/#101-optimize-for-production","title":"10.1 Optimize for Production","text":"<pre><code># Final optimization\nsuper agent optimize rag_chroma_demo --auto intensive\n\n# Build production version\nsuper agent compile rag_chroma_demo --production\n</code></pre>"},{"location":"tutorials/rag-optimization/#102-deploy-with-orchestra","title":"10.2 Deploy with Orchestra","text":"<pre><code># Create orchestra for RAG system\nsuper orchestra create rag_orchestra\n\n# Add RAG agent to orchestra\nsuper orchestra add-agent rag_chroma_demo\n\n# Run orchestra\nsuper orchestra run rag_orchestra\n</code></pre>"},{"location":"tutorials/rag-optimization/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/rag-optimization/#document-preparation","title":"Document Preparation","text":"<ul> <li>Clean your documents: Remove headers, footers, and irrelevant content</li> <li>Consistent formatting: Use consistent structure across documents</li> <li>Metadata inclusion: Add relevant metadata to documents</li> </ul>"},{"location":"tutorials/rag-optimization/#chunking-strategy","title":"Chunking Strategy","text":"<ul> <li>Optimal chunk size: 256-512 tokens for most use cases</li> <li>Overlap: 10-20% overlap between chunks</li> <li>Semantic boundaries: Split at sentence or paragraph boundaries</li> </ul>"},{"location":"tutorials/rag-optimization/#retrieval-optimization","title":"Retrieval Optimization","text":"<ul> <li>Top-K tuning: Start with 5-10, adjust based on performance</li> <li>Embedding models: Use domain-specific models when available</li> <li>Hybrid search: Combine semantic and keyword search for better results</li> </ul>"},{"location":"tutorials/rag-optimization/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Relevance: How relevant are retrieved chunks?</li> <li>Accuracy: How accurate are the responses?</li> <li>Completeness: Do responses cover all aspects of the query?</li> <li>Consistency: Are responses consistent across similar queries?</li> </ul>"},{"location":"tutorials/rag-optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/rag-optimization/#common-issues","title":"Common Issues","text":"<p>Low Retrieval Accuracy <pre><code># Try different chunk sizes\nsuper agent compile rag_chroma_demo --chunk-size 256\n\n# Try different embedding models\nsuper agent compile rag_chroma_demo --embedding-model \"all-mpnet-base-v2\"\n</code></pre></p> <p>Slow Performance <pre><code># Reduce top_k\nsuper agent compile rag_chroma_demo --top-k 3\n\n# Use faster embedding model\nsuper agent compile rag_chroma_demo --embedding-model \"all-MiniLM-L6-v2\"\n</code></pre></p> <p>Irrelevant Context <pre><code># Enable query expansion\nsuper agent compile rag_chroma_demo --goal-expansion\n\n# Use hybrid search\nsuper agent compile rag_chroma_demo --search-type hybrid\n</code></pre></p>"},{"location":"tutorials/rag-optimization/#next-steps","title":"Next Steps","text":"<ul> <li>MCP Optimization Tutorial</li> <li>Memory Optimization Guide</li> <li>Advanced RAG Techniques</li> <li>Observability Setup</li> </ul>"},{"location":"tutorials/rag-optimization/#resources","title":"Resources","text":"<ul> <li>RAG Best Practices</li> <li>Vector Database Comparison</li> <li>GEPA Optimization Guide</li> <li>Community Discussions</li> </ul>"}]}